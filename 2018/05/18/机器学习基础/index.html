<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">


 <script>     
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,深度学习," />





  <link rel="alternate" href="/atom.xml" title="Frank's Blog" type="application/atom+xml" />






<meta name="keywords" content="机器学习,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础（一）">
<meta property="og:url" content="http://frankblog.site/2018/05/18/机器学习基础/index.html">
<meta property="og:site_name" content="Frank&#39;s Blog">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E8%87%AA%E5%8A%A9%E6%B3%95.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cvec%7Bx%27%7D+%3D+%5Cfrac%7B%5Cvec%7Bx%7D%7D%7Bl%28%5Cvec%7Bx%7D%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx%7D%7Bmax%28%7CX%7C%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx+-+min%28X%29%7D%7Bmax%28X%29+-+min%28X%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx+-+%5Cmu%7D%7B%5Csigma%7D">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/L1,L2%E6%AD%A3%E5%88%99%E5%8C%96.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9A%E4%B9%89_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E8%81%94%E5%90%88%E7%86%B5_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%A1%E4%BB%B6%E7%86%B5.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B01.jpg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B02.jpg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B03.jpg">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/40658-24f62052d3f57559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E4%BC%98%E5%8A%BF.png">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg">
<meta property="og:image" content="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/sklearn%20%E4%B8%AD%E6%96%87.png">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/overfitting.jpg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E4%B8%8E%E7%A1%AE.jpeg">
<meta property="og:image" content="http://p4rlzrioq.bkt.clouddn.com/machine%20learning%20funny.jpg">
<meta property="og:updated_time" content="2018-05-29T11:56:56.817Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习基础（一）">
<meta name="twitter:image" content="http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://frankblog.site/2018/05/18/机器学习基础/"/>





  <title>机器学习基础（一） | Frank's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  
  
    
  
  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://github.com/fgyeason" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Frank's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Enjoy everything fun and challenging</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            作者
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>
<script src="https://cdn.bootcss.com/aplayer/1.6.0/APlayer.min.js"></script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://frankblog.site/2018/05/18/机器学习基础/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="FGY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/风景.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Frank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习基础（一）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
              
                <span class="post-meta-item-text">发表于</span>
              

              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T12:57:16+08:00">
                2018-05-18
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-05-29T19:56:56+08:00">
                2018-05-29
              </time>
            
          </span>


          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          





          
            
          

        
          


          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7,133 字  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  25 分钟  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg" alt=""></p>
<a id="more"></a>
<blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote>

<hr>
<h1 id="【机器学习应用】"><a href="#【机器学习应用】" class="headerlink" title="【机器学习应用】"></a>【机器学习应用】</h1><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><p>典型的应用包括：<strong>人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索</strong>等等。</p>
<h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><p>典型的应用包括：<strong>搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译</strong>等等。</p>
<h2 id="社会网络分析"><a href="#社会网络分析" class="headerlink" title="社会网络分析"></a>社会网络分析</h2><p>典型的应用包括：<strong>用户画像、网络关联分析、欺诈作弊发现、热点发现</strong>等等。</p>
<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>典型的应用包括：<strong>虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”</strong>等等。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt=""></p>
<h1 id="【数据处理】"><a href="#【数据处理】" class="headerlink" title="【数据处理】"></a>【数据处理】</h1><h2 id="数据分类"><a href="#数据分类" class="headerlink" title="数据分类"></a>数据分类</h2><p>正例(positive example)<br>反例(negative example)<br>训练集(training set)<br>验证集(validation set)<br>用作超参数验证<br>测试集(test set)<br>用作模型测试<br>类别不平衡数据集（class-imbalanced data set）<br>两个类别的标签的分布频率有很大的差异。</p>
<h2 id="采样方式"><a href="#采样方式" class="headerlink" title="采样方式"></a>采样方式</h2><h3 id="分层采样-stratified-sampling"><a href="#分层采样-stratified-sampling" class="headerlink" title="分层采样(stratified sampling)"></a>分层采样(stratified sampling)</h3><p>保留类别比例的采样方式通常称为分层采样</p>
<h3 id="留出法（hold-out）"><a href="#留出法（hold-out）" class="headerlink" title="留出法（hold-out）"></a>留出法（hold-out）</h3><p>直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p>
<p><table><tr><td bgcolor="black"></td></tr></table></p>
<p></p>
<h3 id="k折交叉验证（k-fold-cross-validation）"><a href="#k折交叉验证（k-fold-cross-validation）" class="headerlink" title="k折交叉验证（k-fold cross validation）"></a>k折交叉验证（k-fold cross validation）</h3><p>交叉验证先将数据集D划分为k个大小相似的互斥子集，每个子集从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。</p>
<h3 id="自助法-bootstrapping"><a href="#自助法-bootstrapping" class="headerlink" title="自助法(bootstrapping)"></a>自助法(bootstrapping)</h3><p>对数据集D有放回的随机采样m次后，一个样本不在样本集D1出现的概率：</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%87%AA%E5%8A%A9%E6%B3%95.png" alt=""></p>
<p>当n足够大时，大约有36.8%的样本不会被采到，用没采到的部分做测试集，也是包外估计（out-of-bag-estimate）。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。</p>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><ul>
<li>使用一个全局常量填补</li>
<li>使用属性的中心度量（平均数、中位数、众数等）</li>
<li>先聚类，再使用同类的中心度量</li>
<li>使用回归计算填补<br>LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。</li>
</ul>
<h3 id="处理噪声"><a href="#处理噪声" class="headerlink" title="处理噪声"></a>处理噪声</h3><ul>
<li>在机器学习中，下列都是异常值：<ul>
<li>高绝对值的权重。</li>
<li>与实际值差距过大的预测值。</li>
<li>比平均值多大约 3 个标准差的输入数据的值。</li>
</ul>
</li>
<li>噪声数据处理：</li>
</ul>
<ol>
<li>分箱（考察近邻数据值，有箱均值、箱中位数、箱边界光滑方法）</li>
<li>回归</li>
<li>离群点分析</li>
</ol>
<h1 id="【特征工程】"><a href="#【特征工程】" class="headerlink" title="【特征工程】"></a>【特征工程】</h1><h2 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h2><ul>
<li>独热编码（one-hot encoding）<br>  独热编码的优点：能够处理非数值属性；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。</li>
<li>特征二值化（Binarization）<br>  特征二元化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。</li>
</ul>
<h2 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h2><p><img src="https://www.zhihu.com/equation?tex=%5Cvec%7Bx%27%7D+%3D+%5Cfrac%7B%5Cvec%7Bx%7D%7D%7Bl%28%5Cvec%7Bx%7D%29%7D" alt="link"></p>
<p>在处理自然图像时，我们获得的像素值在 [0,255] 区间中，常用的处理是将这些像素值除以 255，使它们缩放到 [0,1] 中。</p>
<h2 id="区间缩放（scaling）"><a href="#区间缩放（scaling）" class="headerlink" title="区间缩放（scaling）"></a>区间缩放（scaling）</h2><p><img src="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx%7D%7Bmax%28%7CX%7C%29%7D" alt="link"></p>
<p><img src="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx+-+min%28X%29%7D%7Bmax%28X%29+-+min%28X%29%7D" alt="link"></p>
<h2 id="标准化（Standardization）"><a href="#标准化（Standardization）" class="headerlink" title="标准化（Standardization）"></a>标准化（Standardization）</h2><p><img src="https://www.zhihu.com/equation?tex=x%27+%3D+%5Cfrac%7Bx+-+%5Cmu%7D%7B%5Csigma%7D" alt="link"></p>
<h2 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h2><ul>
<li>特征交叉（feature cross）<br>  将特征进行交叉（乘积或者笛卡尔乘积）运算后得到的合成特征。特征交叉有助于表示非线性关系。</li>
<li>特征降维（DimensionalityReduction）<br>  使用数据编码或变换，以便得到原数据的归约或“压缩”表示。归约分为无损的和有损的。有效的有损维归约方法为：小波变换和主成分分析</li>
</ul>
<h1 id="【线性回归模型】"><a href="#【线性回归模型】" class="headerlink" title="【线性回归模型】"></a>【线性回归模型】</h1><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><ul>
<li>线性回归的L1正则化通常称为Lasso回归，α来调节损失函数的均方差项和正则化项的权重。</li>
<li>Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0，故具有特征选择的功能，增强了模型的泛化能力。</li>
</ul>
<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><ul>
<li>线性回归的L2正则化通常称为Ridge回归。</li>
<li>Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。</li>
<li>Ridge回归的求解比较简单，一般用最小二乘法。</li>
</ul>
<p><strong>L1正则化产生稀疏的权值, 具有特征选择的作用；L2正则化产生平滑的权值</strong>。</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/L1,L2%E6%AD%A3%E5%88%99%E5%8C%96.png" alt=""></p>
<h2 id="最小二乘法的局限性"><a href="#最小二乘法的局限性" class="headerlink" title="最小二乘法的局限性"></a>最小二乘法的局限性</h2><ol>
<li>最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了</li>
<li>当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。</li>
<li>如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用</li>
<li>当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。</li>
</ol>
<h1 id="【逻辑回归模型】"><a href="#【逻辑回归模型】" class="headerlink" title="【逻辑回归模型】"></a>【逻辑回归模型】</h1><p>逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。用sigmoid函数对线性回归模型进行变换。</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9A%E4%B9%89_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<p>θ为分类模型的要求出的模型参数。对于模型输出hθ(x)，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果hθ(x)&gt;0.5 ，即xθ&gt;0, 则y为1。如果hθ(x)&lt;0.5，即xθ&lt;0。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li>概率分布函数<br>根据二元逻辑回归的定义，假设我们的样本输出是0或者1两类。</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>似然函数</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>损失函数<br>这里我们用对数似然函数最大化，对数似然函数取反即为我们的损失函数J(θ)</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>简写成矩阵形式</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>损失函数优化方法<br>用梯度下降法对损失函数进行优化，得到迭代公式:</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<h1 id="【决策树】"><a href="#【决策树】" class="headerlink" title="【决策树】"></a>【决策树】</h1><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>熵用来度量事物的不确定性</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>联合熵<br>两个变量X和Y的联合熵</li>
</ul>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%94%E5%90%88%E7%86%B5_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p>
<ul>
<li>条件熵</li>
</ul>
<ol>
<li>条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？从上面的描述大家可以看出，它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。在决策树ID3算法中叫做信息增益。</li>
<li>用下面这个图很容易明白他们的关系。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。</li>
</ol>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%A1%E4%BB%B6%E7%86%B5.png" alt=""></p>
<h2 id="决策树构建中的分裂准则"><a href="#决策树构建中的分裂准则" class="headerlink" title="决策树构建中的分裂准则"></a>决策树构建中的分裂准则</h2><p>决策树可以通过一系列规则递归地分割特征空间</p>
<h3 id="信息增益（information-gain）"><a href="#信息增益（information-gain）" class="headerlink" title="信息增益（information gain）"></a><strong>信息增益（information gain）</strong></h3><p>属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合</p>
<h3 id="增益率（gain-ratio）"><a href="#增益率（gain-ratio）" class="headerlink" title="增益率（gain ratio）"></a><strong>增益率（gain ratio）</strong></h3><p>信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。</p>
<h3 id="基尼指数-Gini-index"><a href="#基尼指数-Gini-index" class="headerlink" title="基尼指数(Gini index)"></a><strong>基尼指数(Gini index)</strong></h3><p>基尼指数是另外一种数据的不纯度的度量方法，其定义如下：</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B01.jpg" alt="">　　</p>
<p>其中的m仍然表示数据集D中类别C的个数，Pi表示D中任意一个记录属于Ci的概率，计算时Pi=(D中属于Ci类的集合的记录个数/|D|)。如果所有的记录都属于同一个类中，则P1=1，Gini(D)=0，此时不纯度最低。<br>在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树，对每个属性都会枚举其属性的非空真子集，以属性R分裂后的基尼系数为：</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B02.jpg" alt=""></p>
<p>D1为D的一个非空真子集，D2为D1在D的补集，即D1+D2=D，对于属性R来说，有多个真子集，即GiniR(D)有多个值，但我们选取最小的那么值作为R的基尼指数。最后：</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B03.jpg" alt=""></p>
<p>对于二类分类，基尼系数和熵之半的曲线如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/40658-24f62052d3f57559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p>
<p>从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代</p>
<h2 id="常用决策树模型"><a href="#常用决策树模型" class="headerlink" title="常用决策树模型"></a>常用决策树模型</h2><h3 id="决策树模型总结"><a href="#决策树模型总结" class="headerlink" title="决策树模型总结"></a>决策树模型总结</h3><table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h3 id="CART决策树属性分裂方法"><a href="#CART决策树属性分裂方法" class="headerlink" title="CART决策树属性分裂方法"></a>CART决策树属性分裂方法</h3><ol>
<li>m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。</li>
<li>对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。</li>
</ol>
<h2 id="决策树优化方法"><a href="#决策树优化方法" class="headerlink" title="决策树优化方法"></a>决策树优化方法</h2><h3 id="后剪枝（postpruning）"><a href="#后剪枝（postpruning）" class="headerlink" title="后剪枝（postpruning）"></a>后剪枝（postpruning）</h3><p>先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。</p>
<h3 id="预剪枝（prepruning）"><a href="#预剪枝（prepruning）" class="headerlink" title="预剪枝（prepruning）"></a>预剪枝（prepruning）</h3><p>在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。</p>
<h3 id="抑制单颗决策树的复杂度的方法"><a href="#抑制单颗决策树的复杂度的方法" class="headerlink" title="抑制单颗决策树的复杂度的方法"></a>抑制单颗决策树的复杂度的方法</h3><ol>
<li>限制树的最大深度</li>
<li>限制叶子节点的最少样本数量</li>
<li>限制节点分裂时的最少样本数量</li>
<li>吸收 bagging 的思想对训练样本采样，在学习单颗决策树时只使用一部分训练样本</li>
<li>借鉴随机森林的思路在学习单颗决策树时只采样一部分特征，在目标函数中添加正则项惩罚复杂的树结。</li>
</ol>
<h2 id="决策树算法的优点"><a href="#决策树算法的优点" class="headerlink" title="决策树算法的优点"></a>决策树算法的优点</h2><ol>
<li>基本不需要预处理，不需要提前归一化，处理缺失值。</li>
<li>使用决策树预测的代价是O(log2m)O(log2m)。 m为样本数。</li>
<li>既可以处理离散值也可以处理连续值。很多算法只是4.专注于离散值或者连续值。</li>
<li>可以处理多维度输出的分类问题。</li>
<li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li>
<li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li>
<li>对于异常点的容错能力好，健壮性高。</li>
</ol>
<h2 id="决策树算法的缺陷"><a href="#决策树算法的缺陷" class="headerlink" title="决策树算法的缺陷"></a>决策树算法的缺陷</h2><ol>
<li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li>
<li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li>
<li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li>
<li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li>
<li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li>
</ol>
<h1 id="【梯度】"><a href="#【梯度】" class="headerlink" title="【梯度】"></a>【梯度】</h1><p>在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。</p>
<h2 id="梯度下降法的超参数"><a href="#梯度下降法的超参数" class="headerlink" title="梯度下降法的超参数"></a>梯度下降法的超参数</h2><ul>
<li>步长（step size）<br>学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" alt=""></li>
</ul>
<h2 id="梯度下降的算法调优"><a href="#梯度下降的算法调优" class="headerlink" title="梯度下降的算法调优"></a>梯度下降的算法调优</h2><ol>
<li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
</li>
<li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
</li>
<li><p>标准化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据标准化，这样特征的新期望为0，新方差为1，迭代次数可以大大加快。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E4%BC%98%E5%8A%BF.png" alt=""></p>
</li>
</ol>
<h2 id="梯度下降方法总结"><a href="#梯度下降方法总结" class="headerlink" title="梯度下降方法总结"></a>梯度下降方法总结</h2><h3 id="批梯度下降-batch-gradient-descent-BGD"><a href="#批梯度下降-batch-gradient-descent-BGD" class="headerlink" title="批梯度下降(batch gradient descent/BGD)"></a>批梯度下降(batch gradient descent/BGD)</h3><p>求梯度的时候就用了所有m个样本的梯度数据。</p>
<h3 id="随机梯度下降（stochastic-gradient-descent-SGD）"><a href="#随机梯度下降（stochastic-gradient-descent-SGD）" class="headerlink" title="随机梯度下降（stochastic gradient descent/SGD）"></a>随机梯度下降（stochastic gradient descent/SGD）</h3><p>随机梯度下降法由于每次仅仅采用一个样本来迭代。优点是速度快以及可以跳出局部最优解，缺点是导致迭代方向变化很大，不能很快的收敛到局部最优解。</p>
<h3 id="小批量随机梯度下降（mini-batch-stochastic-gradient-descent）"><a href="#小批量随机梯度下降（mini-batch-stochastic-gradient-descent）" class="headerlink" title="小批量随机梯度下降（mini-batch stochastic gradient descent）"></a>小批量随机梯度下降（mini-batch stochastic gradient descent）</h3><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。</p>
<h2 id="梯度下降法与最小二乘法"><a href="#梯度下降法与最小二乘法" class="headerlink" title="梯度下降法与最小二乘法"></a>梯度下降法与最小二乘法</h2><ul>
<li>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。</li>
<li>梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</li>
</ul>
<h1 id="【分类模型指标】"><a href="#【分类模型指标】" class="headerlink" title="【分类模型指标】"></a>【分类模型指标】</h1><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p><img src="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png" alt="link"></p>
<h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p><strong>准确率</strong>是预测和标签一致的样本在所有样本中所占的比例</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg" alt="link"></p>
<h2 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h2><p><strong>精确率</strong>是你预测为正类的数据中，有多少确实是正类</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg" alt="link"></p>
<h2 id="查全率（Recall）"><a href="#查全率（Recall）" class="headerlink" title="查全率（Recall）"></a>查全率（Recall）</h2><p><strong>查全率</strong>是所有正类的数据中，你预测为正类的数据占比</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg" alt="link"></p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg" alt="link"></p>
<p>不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。</p>
<h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg" alt="link"></p>
<h2 id="F-beta-Score"><a href="#F-beta-Score" class="headerlink" title="F-beta Score"></a>F-beta Score</h2><p>有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg" alt="link"></p>
<h2 id="ROC-（receiver-operating-characteristic-curve）"><a href="#ROC-（receiver-operating-characteristic-curve）" class="headerlink" title="ROC （receiver operating characteristic curve）"></a>ROC （receiver operating characteristic curve）</h2><p>绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。<br>ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</p>
<p><img src="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif" alt=""></p>
<h2 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h2><p>ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p>
<p>理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p>
<h1 id="【集成学习】"><a href="#【集成学习】" class="headerlink" title="【集成学习】"></a>【集成学习】</h1><ol>
<li>一般来说，集成学习指的同质基学习器的集成，而同质基学习器使用最多的模型是CART决策树和神经网络。</li>
<li>同质基学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是基学习器之间存在强依赖关系，基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。</li>
</ol>
<h2 id="boosting算法"><a href="#boosting算法" class="headerlink" title="boosting算法"></a>boosting算法</h2><ol>
<li>用初始权重训练出一个弱学习器1，根据其误差率表现来更新训练样本的权重，使误差率高的训练样本权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。</li>
<li>重复进行第一步骤，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。</li>
</ol>
<h2 id="bagging算法"><a href="#bagging算法" class="headerlink" title="bagging算法"></a>bagging算法</h2><ol>
<li>基学习器的训练集是通过随机采样得到的。通过T次的基于自助法的随机采样，我们就可以得到T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。</li>
<li>大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。</li>
</ol>
<ul>
<li>随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择。<h2 id="bagging-amp-boosting"><a href="#bagging-amp-boosting" class="headerlink" title="bagging&amp;boosting"></a>bagging&amp;boosting</h2>对于 Bagging 算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差（variance），因为采用了相互独立的基分类器多了以后，h 的值自然就会靠近。所以对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树。</li>
</ul>
<p>对于 Boosting 来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias），所以对于每个基分类器来说，问题就在于如何选择 variance 更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><ul>
<li>一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</li>
</ul>
<h3 id="xgboost相比GBDT的优势"><a href="#xgboost相比GBDT的优势" class="headerlink" title="xgboost相比GBDT的优势"></a>xgboost相比GBDT的优势</h3><p>1、传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。<br>2、传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。<br>3、xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。<br>4、Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）<br>5、列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。<br>6、xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
<h2 id="随机森林的优劣"><a href="#随机森林的优劣" class="headerlink" title="随机森林的优劣"></a>随机森林的优劣</h2><ul>
<li>优势：<br>1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。<br>2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。<br>3） 在训练后，可以给出各个特征对于输出的重要性<br>4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。<br>5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。<br>6） 对部分特征缺失不敏感。</li>
<li>缺点：<br>1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。<br>2)  取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。</li>
</ul>
<h2 id="集合策略"><a href="#集合策略" class="headerlink" title="集合策略"></a>集合策略</h2><ul>
<li>对于数值类的回归预测问题，通常使用的结合策略是平均法。</li>
<li>对于分类问题的预测，我们通常使用的是投票法。</li>
<li>对于学习误差较大的情况，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果，这种方法称为学习法。简单来说，对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</li>
</ul>
<h1 id="【聚类】"><a href="#【聚类】" class="headerlink" title="【聚类】"></a>【聚类】</h1><h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2><ul>
<li><p>算法思想：按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。</p>
</li>
<li><p>优点：<br>1）原理比较简单，实现也是很容易，收敛速度快。<br>2）聚类效果较优。<br>3）算法的可解释度比较强。<br>4）主要需要调参的参数仅仅是簇数k。</p>
</li>
<li>缺点：<br>1）K值的选取不好把握<br>2）对于不是凸的数据集比较难收敛<br>3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。<br>4） 采用迭代方法，得到的结果只是局部最优。<br>5） 对噪音和异常点比较的敏感。</li>
</ul>
<h1 id="【关联规则】"><a href="#【关联规则】" class="headerlink" title="【关联规则】"></a>【关联规则】</h1><ol>
<li>项集（itemset）</li>
<li>事务（transaction）：为一个非空项集</li>
<li>频度（frequency）</li>
<li>关联规则（association rules），X=&gt;Y，X，Y是两个不相交的非空项集。</li>
<li>强关联规则：支持度和置信度都高于阈值</li>
<li>支持度（support）：包含X∪YX∪Y的事务的出现概率</li>
<li>置信度（confidence）：包含X的事务同时也包含Y的概率，P(Y|X)</li>
</ol>
<h1 id="【文本挖掘】"><a href="#【文本挖掘】" class="headerlink" title="【文本挖掘】"></a>【文本挖掘】</h1><ul>
<li>Word2Vec(词向量学习模型)</li>
<li>TF(Term Frequency词频)</li>
<li>TF-IDF(Term Frequency-Inverse DocumentFrequency 词频-逆向文档频率)</li>
<li>IG(InformationGain 信息增益)</li>
<li>IGR(Information Gain Ratio 信息增益率)</li>
</ul>
<h1 id="【模型选择与评估】"><a href="#【模型选择与评估】" class="headerlink" title="【模型选择与评估】"></a>【模型选择与评估】</h1><h2 id="算法选择"><a href="#算法选择" class="headerlink" title="算法选择"></a>算法选择</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/sklearn%20%E4%B8%AD%E6%96%87.png" alt=""></p>
<h2 id="泛化能力、欠拟合和过拟合"><a href="#泛化能力、欠拟合和过拟合" class="headerlink" title="泛化能力、欠拟合和过拟合"></a>泛化能力、欠拟合和过拟合</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/overfitting.jpg" alt=""></p>
<h2 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h2><p>偏差方差分解解释了机器的泛化误差。偏差度量了算法的期望预测与真实结果之间的误差。方差度量了训练集的变动所导致的学习性能的变化。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E4%B8%8E%E7%A1%AE.jpeg" alt="link"></p>
<ul>
<li>有一些算法天生是高方差的算法。如KNN、决策树。非参数学习通常是高方差算法，对数据较为敏感，因为不对数据进行任何假设。</li>
<li>有一些算法天生就是高偏差算法。如线性回归。参数学习通常是高偏差算法，因为对数据具有极强的假设。</li>
<li>机器学习的主要挑战来自于方差，解决高方差的通常手段有：<ul>
<li>1.降低模型复杂度</li>
<li>2.减少数据维度；降噪</li>
<li>3.增加样本数</li>
<li>4.使用验证集</li>
<li>5.模型正则化</li>
</ul>
</li>
</ul>
<p>此图献给奋战在一线的调参侠们！</p>
<p><img src="http://p4rlzrioq.bkt.clouddn.com/machine%20learning%20funny.jpg" alt="link"></p>

      
    </div>
    
    
    

    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate comment here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="FGY 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="FGY 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    FGY
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://frankblog.site/2018/05/18/机器学习基础/" title="机器学习基础（一）">http://frankblog.site/2018/05/18/机器学习基础/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/08/git学习/" rel="next" title="Git学习">
                <i class="fa fa-chevron-left"></i> Git学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/30/机器学习基础（二）/" rel="prev" title="机器学习基础（二）">
                机器学习基础（二） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>
  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/风景.jpeg"
                alt="FGY" />
            
              <p class="site-author-name" itemprop="name">FGY</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">
            

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>

            

          </nav>


          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/fgyeason" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.kaggle.com/fgy0303" target="_blank" title="Kaggle">
                      
                        <i class="fa fa-fw fa-spinner"></i>Kaggle</a>
                  </span>
                
            </div>
          

          
          

          

          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud" title="Py_whl" target="_blank">Py_whl</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://public.tableau.com/en-us/s/gallery" title="Tableau" target="_blank">Tableau</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.coursera.org/" title="Coursera" target="_blank">Coursera</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://lightgbm.apachecn.org/cn/latest/" title="LightGBM" target="_blank">LightGBM</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://segmentfault.com/" title="Segmentfault" target="_blank">Segmentfault</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://glyphsearch.com/" title="Font" target="_blank">Font</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://liyangbit.com/math/jupyter-latex/" title="LaTex" target="_blank">LaTex</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://juejin.im" title="掘金" target="_blank">掘金</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://uqer.io/data/browse/0/?page=1" title="量化" target="_blank">量化</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.jiqizhixin.com/" title="机器之心" target="_blank">机器之心</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://tool.chinaz.com/" title="站长工具" target="_blank">站长工具</a>
                  </li>
                
              </ul>
            </div>
          
          

          
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#【机器学习应用】"><span class="nav-number">1.</span> <span class="nav-text">【机器学习应用】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#计算机视觉"><span class="nav-number">1.1.</span> <span class="nav-text">计算机视觉</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自然语言处理"><span class="nav-number">1.2.</span> <span class="nav-text">自然语言处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#社会网络分析"><span class="nav-number">1.3.</span> <span class="nav-text">社会网络分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推荐系统"><span class="nav-number">1.4.</span> <span class="nav-text">推荐系统</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【数据处理】"><span class="nav-number">2.</span> <span class="nav-text">【数据处理】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据分类"><span class="nav-number">2.1.</span> <span class="nav-text">数据分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采样方式"><span class="nav-number">2.2.</span> <span class="nav-text">采样方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分层采样-stratified-sampling"><span class="nav-number">2.2.1.</span> <span class="nav-text">分层采样(stratified sampling)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#留出法（hold-out）"><span class="nav-number">2.2.2.</span> <span class="nav-text">留出法（hold-out）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k折交叉验证（k-fold-cross-validation）"><span class="nav-number">2.2.3.</span> <span class="nav-text">k折交叉验证（k-fold cross validation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自助法-bootstrapping"><span class="nav-number">2.2.4.</span> <span class="nav-text">自助法(bootstrapping)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据清洗"><span class="nav-number">2.3.</span> <span class="nav-text">数据清洗</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#处理缺失值"><span class="nav-number">2.3.1.</span> <span class="nav-text">处理缺失值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理噪声"><span class="nav-number">2.3.2.</span> <span class="nav-text">处理噪声</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【特征工程】"><span class="nav-number">3.</span> <span class="nav-text">【特征工程】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征编码"><span class="nav-number">3.1.</span> <span class="nav-text">特征编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化（Normalization）"><span class="nav-number">3.2.</span> <span class="nav-text">归一化（Normalization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#区间缩放（scaling）"><span class="nav-number">3.3.</span> <span class="nav-text">区间缩放（scaling）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#标准化（Standardization）"><span class="nav-number">3.4.</span> <span class="nav-text">标准化（Standardization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征变换"><span class="nav-number">3.5.</span> <span class="nav-text">特征变换</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【线性回归模型】"><span class="nav-number">4.</span> <span class="nav-text">【线性回归模型】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归的正则化"><span class="nav-number">4.1.</span> <span class="nav-text">线性回归的正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lasso回归"><span class="nav-number">4.1.1.</span> <span class="nav-text">Lasso回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#岭回归"><span class="nav-number">4.1.2.</span> <span class="nav-text">岭回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘法的局限性"><span class="nav-number">4.2.</span> <span class="nav-text">最小二乘法的局限性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【逻辑回归模型】"><span class="nav-number">5.</span> <span class="nav-text">【逻辑回归模型】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">5.1.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【决策树】"><span class="nav-number">6.</span> <span class="nav-text">【决策树】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#熵"><span class="nav-number">6.1.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树构建中的分裂准则"><span class="nav-number">6.2.</span> <span class="nav-text">决策树构建中的分裂准则</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益（information-gain）"><span class="nav-number">6.2.1.</span> <span class="nav-text">信息增益（information gain）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增益率（gain-ratio）"><span class="nav-number">6.2.2.</span> <span class="nav-text">增益率（gain ratio）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基尼指数-Gini-index"><span class="nav-number">6.2.3.</span> <span class="nav-text">基尼指数(Gini index)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用决策树模型"><span class="nav-number">6.3.</span> <span class="nav-text">常用决策树模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树模型总结"><span class="nav-number">6.3.1.</span> <span class="nav-text">决策树模型总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART决策树属性分裂方法"><span class="nav-number">6.3.2.</span> <span class="nav-text">CART决策树属性分裂方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树优化方法"><span class="nav-number">6.4.</span> <span class="nav-text">决策树优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#后剪枝（postpruning）"><span class="nav-number">6.4.1.</span> <span class="nav-text">后剪枝（postpruning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预剪枝（prepruning）"><span class="nav-number">6.4.2.</span> <span class="nav-text">预剪枝（prepruning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#抑制单颗决策树的复杂度的方法"><span class="nav-number">6.4.3.</span> <span class="nav-text">抑制单颗决策树的复杂度的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树算法的优点"><span class="nav-number">6.5.</span> <span class="nav-text">决策树算法的优点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树算法的缺陷"><span class="nav-number">6.6.</span> <span class="nav-text">决策树算法的缺陷</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【梯度】"><span class="nav-number">7.</span> <span class="nav-text">【梯度】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降法的超参数"><span class="nav-number">7.1.</span> <span class="nav-text">梯度下降法的超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降的算法调优"><span class="nav-number">7.2.</span> <span class="nav-text">梯度下降的算法调优</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降方法总结"><span class="nav-number">7.3.</span> <span class="nav-text">梯度下降方法总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#批梯度下降-batch-gradient-descent-BGD"><span class="nav-number">7.3.1.</span> <span class="nav-text">批梯度下降(batch gradient descent/BGD)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机梯度下降（stochastic-gradient-descent-SGD）"><span class="nav-number">7.3.2.</span> <span class="nav-text">随机梯度下降（stochastic gradient descent/SGD）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小批量随机梯度下降（mini-batch-stochastic-gradient-descent）"><span class="nav-number">7.3.3.</span> <span class="nav-text">小批量随机梯度下降（mini-batch stochastic gradient descent）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降法与最小二乘法"><span class="nav-number">7.4.</span> <span class="nav-text">梯度下降法与最小二乘法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【分类模型指标】"><span class="nav-number">8.</span> <span class="nav-text">【分类模型指标】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#混淆矩阵（confusion-matrix）"><span class="nav-number">8.1.</span> <span class="nav-text">混淆矩阵（confusion matrix）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#准确率（Accuracy）"><span class="nav-number">8.2.</span> <span class="nav-text">准确率（Accuracy）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#精确率（Precision）"><span class="nav-number">8.3.</span> <span class="nav-text">精确率（Precision）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查全率（Recall）"><span class="nav-number">8.4.</span> <span class="nav-text">查全率（Recall）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F1-Score"><span class="nav-number">8.5.</span> <span class="nav-text">F1 Score</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-beta-Score"><span class="nav-number">8.6.</span> <span class="nav-text">F-beta Score</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ROC-（receiver-operating-characteristic-curve）"><span class="nav-number">8.7.</span> <span class="nav-text">ROC （receiver operating characteristic curve）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AUC（Area-Under-the-Curve）"><span class="nav-number">8.8.</span> <span class="nav-text">AUC（Area Under the Curve）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【集成学习】"><span class="nav-number">9.</span> <span class="nav-text">【集成学习】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#boosting算法"><span class="nav-number">9.1.</span> <span class="nav-text">boosting算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging算法"><span class="nav-number">9.2.</span> <span class="nav-text">bagging算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging-amp-boosting"><span class="nav-number">9.3.</span> <span class="nav-text">bagging&amp;boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">9.3.1.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost相比GBDT的优势"><span class="nav-number">9.3.2.</span> <span class="nav-text">xgboost相比GBDT的优势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林的优劣"><span class="nav-number">9.4.</span> <span class="nav-text">随机森林的优劣</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集合策略"><span class="nav-number">9.5.</span> <span class="nav-text">集合策略</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【聚类】"><span class="nav-number">10.</span> <span class="nav-text">【聚类】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#K-means算法"><span class="nav-number">10.1.</span> <span class="nav-text">K-means算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【关联规则】"><span class="nav-number">11.</span> <span class="nav-text">【关联规则】</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【文本挖掘】"><span class="nav-number">12.</span> <span class="nav-text">【文本挖掘】</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【模型选择与评估】"><span class="nav-number">13.</span> <span class="nav-text">【模型选择与评估】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#算法选择"><span class="nav-number">13.1.</span> <span class="nav-text">算法选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#泛化能力、欠拟合和过拟合"><span class="nav-number">13.2.</span> <span class="nav-text">泛化能力、欠拟合和过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差和方差"><span class="nav-number">13.3.</span> <span class="nav-text">偏差和方差</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=450 src="//music.163.com/outchain/player?type=0&id=2130685222&auto=1&height=430"></iframe>

      

    </div>
  </aside>




        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">FGY</span>

  
</div>



  <div class="powered-by">
  <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
    本站访客数: <span id="busuanzi_value_site_uv"></span>人
  </span>
  </div>
<span>|</span>
  <div class="powered-by">
  <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
    访问量: <span id="busuanzi_value_site_pv"></span>次
  </span>
  </div>


<!--  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>



-->
<!--
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共25.3k字</span>
</div>
-->




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  

  

<!-- -->
  
</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
