<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frank&#39;s Blog</title>
  
  <subtitle>Enjoy everything fun and challenging</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://frankblog.site/"/>
  <updated>2018-06-19T17:37:30.817Z</updated>
  <id>http://frankblog.site/</id>
  
  <author>
    <name>FGY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习之风控评分卡模型</title>
    <link href="http://frankblog.site/2018/06/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%A3%8E%E6%8E%A7%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B/"/>
    <id>http://frankblog.site/2018/06/20/机器学习之风控评分卡模型/</id>
    <published>2018-06-19T17:20:35.600Z</published>
    <updated>2018-06-19T17:37:30.817Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7%E5%B0%81%E9%9D%A2.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p>一般来说风控领域在意的是前两个模型种类，排序类以及决策类。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7.jpg" alt="link"><br>其中：巴塞尔协议定义了金融风险类型：市场风险、作业风险、信用风险。信用风险ABC模型有进件申请评分、行为评分、催收评分。</p><div class="table-container"><table><thead><tr><th style="text-align:left">模型</th><th style="text-align:left">解释</th><th style="text-align:left">应用场景</th></tr></thead><tbody><tr><td style="text-align:left">Logistics回归</td><td style="text-align:left">影响程度大小与显著性，解释力度强，但只是线性，没有顾及到非线性，预测精度较低</td><td style="text-align:left">申请评分、流失预测</td></tr><tr><td style="text-align:left">决策树</td><td style="text-align:left">1、描述性，重建用户场景，可做变量提取与用户画像 2、树的结构不稳定，可以得出变量重要性，可以作为变量筛选</td><td style="text-align:left">流失模式识别</td></tr><tr><td style="text-align:left">随机森林</td><td style="text-align:left">随机森林比决策树在变量筛选中，变量排序比较优秀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">神经网络</td><td style="text-align:left">1、不可解释，内部使用，预测精度较高。可以作为初始模型的金模型（用以评估在给定数据条件下，逻辑回归可达到的最精确程度）2、线性（逻辑回归）+非线性关系，可用于行为评分的预测模型（行为评分对模型可解释性不强），可用于申请评分的金模型3、使用场景：先做一个神经网络，让预测精度（AUC）达到最大时，再用逻辑回归</td><td style="text-align:left">申请评分的金模型；行为评分的预测模型</td></tr></tbody></table></div><h1 id="监控模型指标"><a href="#监控模型指标" class="headerlink" title="监控模型指标"></a>监控模型指标</h1><p>决策类：准确率/误分率、利润/成本<br>排序类：ROC指标（一致性）、Gini指数、KS统计量、提升度</p><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p><img src="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png" alt="link"></p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p><strong>准确率</strong>是预测和标签一致的样本在所有样本中所占的比例</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg" alt="link"></p><h2 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h2><p><strong>精确率</strong>是你预测为正类的数据中，有多少确实是正类</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg" alt="link"></p><h2 id="查全率（Recall）"><a href="#查全率（Recall）" class="headerlink" title="查全率（Recall）"></a>查全率（Recall）</h2><p><strong>查全率</strong>是所有正类的数据中，你预测为正类的数据占比</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg" alt="link"></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg" alt="link"></p><p>不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg" alt="link"></p><h2 id="F-beta-Score"><a href="#F-beta-Score" class="headerlink" title="F-beta Score"></a>F-beta Score</h2><p>有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg" alt="link"></p><h2 id="ROC-（receiver-operating-characteristic-curve）"><a href="#ROC-（receiver-operating-characteristic-curve）" class="headerlink" title="ROC （receiver operating characteristic curve）"></a>ROC （receiver operating characteristic curve）</h2><p>绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。<br>ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</p><p><img src="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif" alt=""></p><h2 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h2><p>ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。</p><p><img src="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p><h2 id="Lift提升图"><a href="#Lift提升图" class="headerlink" title="Lift提升图"></a>Lift提升图</h2><p><strong>Lift</strong> =[TP/(TP+FP)] / [(TP+FN)/(TP+FP+FN+TN)] = PV_plus / pi1，它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。</p><p>不利用模型，我们只能利用“正例的比例是(TP+FN)/(TP+FP+FN+TN)”这个样本信息来估计正例的比例（baseline model），而利用模型之后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集TP+FP中挑选正例，这时预测的准确率PV_plus(Precision)为TP/(TP+FP)。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/lift%E8%AF%84%E4%BC%B0.png" alt=""></p><p>上图的纵坐标是lift，横坐标是正例集百分比。随着阈值的减小，更多的客户就会被归为正例，也就是预测成正例的比例变大。当阈值设得够大，只有一小部分观测值会归为正例，但这一小部分一定是最具有正例特征的观测值集合（用前面银行向客户推荐信用卡的例子来看，这一部分人群对推荐的反应最为活跃），所以在这个设置下，对应的lift值最大。同样，当阈值设定得足够的小，那么几乎所有的观测值都会被归为正例（占比几乎为100%）——这时分类的效果就跟baseline model差不多了，相对应的lift值就接近于1。</p><blockquote><p>ROC曲线和lift曲线都能够评价逻辑回归模型的效果：类似信用评分的场景，希望能够尽可能完全地识别出有违约风险的客户，选择ROC曲线及相应的AUC作为指标；</p><p>类似数据库精确营销的场景，希望能够通过对全体消费者的分类而得到具有较高响应率的客户群从而提高投入产出比，选择lift曲线作为指标；</p></blockquote><h2 id="Gain增益图"><a href="#Gain增益图" class="headerlink" title="Gain增益图"></a>Gain增益图</h2><p>Gains(增益) 与 Lift （提升）类似：Lift 曲线是不同阈值下Lift和Depth的轨迹，Gain曲线则是不同阈值下PV_plus和Depth的轨迹，而PV_plus=Lift*pi1= TP/TP+FP，所以它们显而易见的区别就在于纵轴刻度的不同。</p><p>增益图是描述整体精准率的指标。按照模型预测出的概率从高到低排列，将每一个百分位数内的精准率指标标注在图形区域内，就形成了非累积的增益图。如果对每一个百分位及其之前的精准率求和，并将值标注在图形区域内，则形成累积的增益图。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B0%E8%AF%84%E4%BB%B7.png" alt="Gain图"></p><h2 id="K-S图"><a href="#K-S图" class="headerlink" title="K-S图"></a>K-S图</h2><p>正样本洛伦兹曲线记为f(x)，负样本洛伦兹曲线记为g(x)，K-S曲线实际上是f(x)与g(x)的差值曲线。K-S曲线的最高点（最大值）定义为KS值，KS值越大，模型分值的区分度越好，KS值为0代表是最没有区分度的随机模型。准确的来说，K-S是用来度量阳性与阴性分类区分程度的。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/ks%E5%9B%BE%E8%AF%84%E4%BC%B0.png" alt=""></p><h2 id="PSI-群体稳定性指标-population-stability-index"><a href="#PSI-群体稳定性指标-population-stability-index" class="headerlink" title="PSI 群体稳定性指标(population stability index)"></a>PSI 群体稳定性指标(population stability index)</h2><p>psi = sum(（实际占比-预期占比）* ln(实际占比/预期占比))</p><p>一般认为psi小于0.1时候模型稳定性很高，0.1-0.25一般，大于0.25模型稳定性差，建议重做。</p><h1 id="常用特征"><a href="#常用特征" class="headerlink" title="常用特征"></a>常用特征</h1><ul><li><p>个人信息：学历 性别 收入</p></li><li><p>负债能力：在申请的金融机构或者其他金融机构的负债情况（例如月还债金额超过月收入的60%，说明负债较高），例如多头借贷信息等</p></li><li>消费能力：商品购买记录，出境游，奢侈品消费</li><li>历史信用记录：历史逾期行为</li><li>其他数据：个人交际、网络足迹、个人财务等</li></ul><p>备注：客户还款能力*还款意愿 = 还款等级</p><h1 id="非平衡样本的处理方法"><a href="#非平衡样本的处理方法" class="headerlink" title="非平衡样本的处理方法"></a>非平衡样本的处理方法</h1><ul><li>过采样：优点方法简单，缺点容易造成模型过拟合。</li><li>欠采样：优点和过采样类似，缺点是容易造成模型的欠拟合；</li><li>SMOTE：优点是不易过拟合，能够保留大量的信息，缺点是不能对缺失值和类别变量做处理。</li></ul><h2 id="SMOTE算法"><a href="#SMOTE算法" class="headerlink" title="SMOTE算法"></a>SMOTE算法</h2><ol><li>采样最近邻算法，计算出每个少数类样本的K个同类近邻；</li><li>从K个同类近邻中随机挑选N个样本进行随机线性插值；</li><li>构造新的少数类样本：<br>New=Xi+rand(0,1)∗(yj−xi),j=1,2,3,4…..N<br>其中Xi为少类中的一个观测点，Yj为K个近邻中随机抽取的样本</li><li><p>将新样本与原数据合成，产生新的训练集</p><p>例子：选取了一个X1为年龄为22岁，月收入为8000元，则X1=（22，8000），选取了一个近邻点为X2，X2=(28,5000)，随机系数为0.5，计算逻辑为22+(28−22)∗0.5=25,8000+(5000−8000)∗0.5=6500，这样得到的一个新的X3点为(25,6500)。</p></li></ol><h1 id="构建申请评分卡"><a href="#构建申请评分卡" class="headerlink" title="构建申请评分卡"></a>构建申请评分卡</h1><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h3 id="对于类别型变量"><a href="#对于类别型变量" class="headerlink" title="对于类别型变量"></a>对于类别型变量</h3><ol><li>删除缺失率超过50%的变量</li><li>缺失值作为一种新的状态</li></ol><h3 id="对于连续型变量"><a href="#对于连续型变量" class="headerlink" title="对于连续型变量"></a>对于连续型变量</h3><ol><li>删除缺失率超过80%的变量</li><li>均值，众数，预测填充等</li></ol><h2 id="特征的分箱"><a href="#特征的分箱" class="headerlink" title="特征的分箱"></a>特征的分箱</h2><h3 id="1-分箱的重要性"><a href="#1-分箱的重要性" class="headerlink" title="1. 分箱的重要性"></a>1. 分箱的重要性</h3><ol><li>稳定性：避免特征中无意义的波动对评分带来的波动</li><li>健壮性：避免了极端值的影响</li></ol><h3 id="2-分箱的优势"><a href="#2-分箱的优势" class="headerlink" title="2. 分箱的优势"></a>2. 分箱的优势</h3><ol><li>可以将缺失作为独立的一个箱带入模型中</li><li>将所有变量变换到相似的尺度上</li></ol><h3 id="3-分箱的限制"><a href="#3-分箱的限制" class="headerlink" title="3. 分箱的限制"></a>3. 分箱的限制</h3><ol><li>分箱后需要编码</li><li>计算量大</li></ol><h3 id="4-分箱的方法"><a href="#4-分箱的方法" class="headerlink" title="4. 分箱的方法"></a>4. 分箱的方法</h3><ul><li>有监督的分箱</li></ul><ol><li>Best—KS</li><li>Chimerge</li></ol><ul><li>无监督的分箱</li></ul><ol><li>等频分箱</li><li>等距分箱</li><li>聚类分箱</li></ol><h3 id="5-有监督的分箱—最小熵法分箱"><a href="#5-有监督的分箱—最小熵法分箱" class="headerlink" title="5. 有监督的分箱—最小熵法分箱"></a>5. 有监督的分箱—最小熵法分箱</h3><p>(1) 假设因变量为分类变量，可取值1，… ，J。令pij表示第i个分箱内因变量取值为j的观测的比例，i=1，…，k，j=1，…，J；那么第i个分箱的熵值为∑Jj=0−pij×logpij。如果第i个分箱内因变量各类别的比例相等，即p11=p12=p1J=1/J，那么第i个分箱的熵值达到最大值；如果第i个分箱内因变量只有一种取值，即某个pij等于1而其他类别的比例等于0，那么第i个分箱的熵值达到最小值。</p><p>(2) 令ri表示第i个分箱的观测数占所有观测数的比例；那么总熵值为∑ki=0∑Jj=0(−pij×logpij)。需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。</p><h3 id="6-卡方分箱法-ChiMerge"><a href="#6-卡方分箱法-ChiMerge" class="headerlink" title="6. 卡方分箱法(ChiMerge)"></a>6. 卡方分箱法(ChiMerge)</h3><p>自底向上的(即基于合并的)数据离散化方法。</p><p>它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。</p><p>基本思想:<br>对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%A1%E6%96%B9%E5%88%86%E7%AE%B1.png" alt=""><br>根据显著性水平和自由度得到卡方值,自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度（10%显著性水平)下，卡方的值为4.6。</p><p>类别和属性独立时，有90%的可能性，计算得到的卡方值会小于4.6，这样，大于阈值的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大，区间合并就会进行很多次，离散后的区间数量少、区间大。<br>【注】：</p><ol><li>ChiMerge算法推荐使用0.90、0.95、0.99置信度，最大区间数取10到15之间.</li><li>也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限，最多几个区间，最少几个区间。</li><li>对于类别型变量，需要分箱时需要按照某种方式进行排序</li></ol><h3 id="无监督分箱法"><a href="#无监督分箱法" class="headerlink" title="无监督分箱法"></a>无监督分箱法</h3><ul><li>等距分箱<br>从最小值到最大值之间，均分为 N 等份， 这样， 如果 A,B 为最小最大值， 则每个区间的</li></ul><p>长度为 W=(B−A)/N , 则区间边界值为 A+W,A+2W,….A+(N−1)W .</p><ul><li><p>等频分箱<br>区间的边界值要经过选择，使得每个区间包含大致相等的实例数量。比如说 N=10 ，每个区间应该包含大约10%的实例。</p></li><li><p>两种算法的弊端<br>比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。</p></li></ul><h3 id="分箱的注意点"><a href="#分箱的注意点" class="headerlink" title="分箱的注意点"></a>分箱的注意点</h3><ul><li>对于连续型变量，</li></ul><ol><li>使用ChiMerge进行分箱(默认分成5个箱)</li><li>检查分箱后的bad  rate单调性；倘若不满足，需要进行相邻两箱的合并，直到bad rate为止</li><li>上述过程是收敛的，因为当箱数为2时，bad rate自然单调</li><li>分箱必须覆盖所有训练样本外可能存在的值！</li></ol><ul><li>对于类别型变量</li></ul><ol><li>当类别数较少时，原则上不需要分箱</li><li>当某个或者几个类别的bad rate为0时，需要和最小的非0bad rate的箱进行合并</li><li>当该变量可以完全区分目标变量时，需要认真检查该变量的合理性</li><li>例如：“该申请者在本机构历史信用行为”把客群的好坏样本完全区分时，需要检查该变量的合理性(有可能是事后变量)</li></ol><h2 id="woe编码-amp-IV值"><a href="#woe编码-amp-IV值" class="headerlink" title="woe编码&amp;IV值"></a>woe编码&amp;IV值</h2><h3 id="1-woe编码的定义"><a href="#1-woe编码的定义" class="headerlink" title="1.woe编码的定义"></a>1.woe编码的定义</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/woe%E7%BC%96%E7%A0%81.png" alt=""></p><p>其中，pyi是这个组中响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）占所有样本中所有响应客户的比例，pni是这个组中未响应客户占样本中所有未响应客户的比例，#yi是这个组中响应客户的数量，#ni是这个组中未响应客户的数量，#yT是样本中所有响应客户的数量，#nT是样本中所有未响应客户的数量。</p><p>从这个公式中我们可以体会到，WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。</p><h3 id="2-woe-编码的优劣"><a href="#2-woe-编码的优劣" class="headerlink" title="2.woe 编码的优劣"></a>2.woe 编码的优劣</h3><p>一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值<br>优势：将特征的值规范到相近的尺度上(经验上讲，WOE的绝对值波动范围在0.1～3之间)<br>缺点：需要每箱中同时包含好、坏两个类别</p><h3 id="3-WOE编码的意义"><a href="#3-WOE编码的意义" class="headerlink" title="3.WOE编码的意义"></a>3.WOE编码的意义</h3><ul><li>符号与好样本比例相关</li><li>要求回归模型的系数为负</li></ul><h3 id="4-IV-Information-Value"><a href="#4-IV-Information-Value" class="headerlink" title="4. IV(Information Value)"></a>4. IV(Information Value)</h3><p>用IV去衡量变量预测能力<br><img src="http://p4rlzrioq.bkt.clouddn.com/iv%E5%80%BC.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/iv%E5%80%BC%E9%A2%84%E6%B5%8B.png" alt=""></p><h3 id="5-特征信息度的计算和意义"><a href="#5-特征信息度的计算和意义" class="headerlink" title="5. 特征信息度的计算和意义"></a>5. 特征信息度的计算和意义</h3><h4 id="挑选变量"><a href="#挑选变量" class="headerlink" title="挑选变量"></a>挑选变量</h4><ol><li>非负指标</li><li>高IV表示该特征和目标变量的关联度高</li><li>目标变量只能是二分类</li><li>过高的IV，可能有潜在的风险</li><li>特征分箱越细，IV越高</li></ol><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="1-特征衍生"><a href="#1-特征衍生" class="headerlink" title="1.特征衍生"></a>1.特征衍生</h3><p>特征衍生是指利用现有的特征进行某种组合生成新的特征。</p><p>1.时间切片<br>2.比例数据（负责比例，收入支出比例等）<br>3.平均数据（月均收入）</p><h3 id="2-特征抽象"><a href="#2-特征抽象" class="headerlink" title="2.特征抽象"></a>2.特征抽象</h3><p>特征抽象是指将数据转换成算法可以理解的数据。</p><ol><li><p>分类型变量转换数值型变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loans[&apos;delinq_2yrs&apos;] = loans[&apos;delinq_2yrs&apos;].apply(lambda x: float(x))</span><br><span class="line">loans[&apos;total_acc&apos;] = loans[&apos;total_acc&apos;].apply(lambda x: float(x))</span><br><span class="line">loans[&apos;revol_bal&apos;] = loans [&apos;revol_bal&apos;].apply(lambda x: float(x))</span><br></pre></td></tr></table></figure></li><li><p>有序特征的映射<br>A &lt;B &lt;C &lt; D &lt; E &lt; F &lt; G ; 信用风险从低到高排序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;grade&quot;:&#123;</span><br><span class="line">&quot;A&quot;: 1,</span><br><span class="line">&quot;B&quot;: 2,</span><br><span class="line">&quot;C&quot;: 3,</span><br><span class="line">&quot;D&quot;: 4,</span><br><span class="line">&quot;E&quot;: 5,</span><br><span class="line">&quot;F&quot;: 6,</span><br><span class="line">&quot;G&quot;: 7</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>独热编码（one-hot encoding）<br>pandas的get_dummies( )方法创建虚拟特征，虚拟特征的每一列各代表变量属性的一个分类</p></li><li><p>特征缩放<br>特征缩放本质是一个去量纲的过程，同时可以加快算法收敛的速度。目前，将不同变量缩放到相同的区间有两个常用的方法：归一化（normalization）和标准化（standardization）</p></li></ol><h2 id="建模后续"><a href="#建模后续" class="headerlink" title="建模后续"></a>建模后续</h2><p>我们将客户违约的概率表示为p，则正常的概率为1-p<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B1.png" alt=""></p><p>评分卡设定的分值刻度可以通过将分值表示为比率对数的线性表达式来定义<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B2.png" alt=""><br>A和B是常数。式中的负号可以使得违约概率越低，得分越高。通常情况下，这是分值的理想变动方向，即高分值代表低风险，低分值代表高风险。<br>式中的常数A、B的值可以通过将两个已知或假设的分值带入计算得到。通常情况下，需要设定两个假设：</p><p>（1）给某个特定的比率设定特定的预期分值；<br>（2）确定比率翻番的分数（PDO）</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B3.png" alt=""><br>最后，进行分数分级<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E6%95%B0%E5%88%86%E7%BA%A7.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7%E5%B0%81%E9%9D%A2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="风控" scheme="http://frankblog.site/tags/%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>Spark生态体系</title>
    <link href="http://frankblog.site/2018/06/13/Spark%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB/"/>
    <id>http://frankblog.site/2018/06/13/Spark生态体系/</id>
    <published>2018-06-13T12:45:29.856Z</published>
    <updated>2018-06-13T12:49:59.646Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/spark%E6%A1%86%E6%9E%B6.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="Spark的基本架构"><a href="#Spark的基本架构" class="headerlink" title="Spark的基本架构"></a>Spark的基本架构</h1><p>当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。<br>Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark.jpg" alt=""></a></p><h2 id="1-Spark-Application"><a href="#1-Spark-Application" class="headerlink" title="1. Spark Application"></a>1. Spark Application</h2><p>Spark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。<br>Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark1.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark1.jpg" alt=""></a></p><p>群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。<br>在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。</p><ul><li>Spark有一些集群管理器，负责调度可用资源。</li><li>驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。</li></ul><h2 id="2-Spark’s-Languge-APIs"><a href="#2-Spark’s-Languge-APIs" class="headerlink" title="2. Spark’s Languge APIs"></a>2. Spark’s Languge APIs</h2><p>尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark2.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark2.jpg" alt=""></a></p><p><code>Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。</code></p><h2 id="3-SparkSession"><a href="#3-SparkSession" class="headerlink" title="3. SparkSession"></a>3. SparkSession</h2><p>我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark3.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark3.jpg" alt=""></a></p><h2 id="4-Dataframe"><a href="#4-Dataframe" class="headerlink" title="4. Dataframe"></a>4. Dataframe</h2><p>DataFrame是最常见的<code>Structured API</code>（结构化API），只是表示有类型的<code>包含行和列的数据表</code>。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark4.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark4.jpg" alt=""></a></p><p>DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的<code>Spark’s Language APIs</code>，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。</p><p><strong>注意</strong><br>Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。<strong>以下概念适用于所有的核心抽象。</strong></p><h2 id="5-Partitions"><a href="#5-Partitions" class="headerlink" title="5. Partitions"></a>5. Partitions</h2><p>为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。<br>值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark5.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark5.jpg" alt=""></a></p><h2 id="6-Transformations"><a href="#6-Transformations" class="headerlink" title="6. Transformations"></a>6. Transformations</h2><p>在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为<code>转换</code>。<br>转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark6.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark6.jpg" alt=""></a><br>宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。</p><h2 id="7-Lazy-Evaluation"><a href="#7-Lazy-Evaluation" class="headerlink" title="7. Lazy Evaluation"></a>7. Lazy Evaluation</h2><p>Lazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。</p><h2 id="8-Actions"><a href="#8-Actions" class="headerlink" title="8. Actions"></a>8. Actions</h2><p>转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。<br>在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI，UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。</p><h2 id="9-Dataframe-amp-SQL"><a href="#9-Dataframe-amp-SQL" class="headerlink" title="9. Dataframe &amp; SQL"></a>9. Dataframe &amp; SQL</h2><p>Spark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。<br>Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。</p><p>现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。<br>通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。</p><p><strong>With SQl</strong><br><a href="http://p4rlzrioq.bkt.clouddn.com/spark7.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark7.jpg" alt=""></a><br><strong>With DataFrame</strong><br><a href="http://p4rlzrioq.bkt.clouddn.com/spark8.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark8.jpg" alt=""></a></p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark9.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark9.jpg" alt=""></a></p><p>现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。</p><ol><li>第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation）</li><li>第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前<strong>指定聚合</strong>。</li><li>因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。</li><li>第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！</li><li>第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。</li><li>第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。</li><li>最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark10.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark10.jpg" alt=""></a><br>虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/spark%E6%A1%86%E6%9E%B6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://frankblog.site/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>GBDT&amp;XGBOOST（一）</title>
    <link href="http://frankblog.site/2018/06/12/GBDT&amp;XGBOOST%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://frankblog.site/2018/06/12/GBDT&amp;XGBOOST（一）/</id>
    <published>2018-06-12T14:03:50.675Z</published>
    <updated>2018-06-13T13:29:03.096Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/boosting2.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p><h1 id="GBDT（梯度提升决策树）"><a href="#GBDT（梯度提升决策树）" class="headerlink" title="GBDT（梯度提升决策树）"></a>GBDT（梯度提升决策树）</h1><p>GBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终结果.</p><p>要理解 GBDT，就要先理解 GB ，然后才是 DT，而且这里的 DT 是回归树，而不是分类树.</p><h2 id="DT（决策回归树）"><a href="#DT（决策回归树）" class="headerlink" title="DT（决策回归树）"></a><strong>DT（决策回归树）</strong></h2><p>这里先补充一下什么是回归树，因为之前所讲的决策树都是属于分类树.</p><p>先回顾下分类树:</p><blockquote><p>我们知道 分类树在每次分枝时，是穷举每一个 feature 的每一个阈值，找到使得按照 <code>feature &lt;= 阈值</code>和<code>feature &gt; 阈值</code>分成两枝的熵最大的 feature 和阈值，按照该标准分枝得到的两个新节点，按同样的方法递归分裂下去，直到所有样本都被分入唯一的叶子节点，或达到预设的终止条件（如果叶子节点的样本不唯一，则以多数类作为叶子节点的分类结果）.</p></blockquote><p>回归树大致流程类似:</p><blockquote><p>不过在每个节点（不一定是叶子节点）都会得到一个预测值，以人的年龄为例，该预测值等于属于这个节点的所有人的年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差—即（每个人的年龄-预测年龄）平方的总和除以 N（总人数），或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。直到每个叶子节点上人的年龄都唯一，或者达到预设终止条件，若最终叶子节点上的人的年龄不唯一，就以该节点上所有人的平均年龄作为该叶子节点的预测值.</p><p>就是分类树是按多数投票，以多数类的类别标号作为叶子节点的分类类别；回归树是按叶子节点的样本平均值作为该节点的预测值.</p></blockquote><h2 id="GBDT-实例"><a href="#GBDT-实例" class="headerlink" title="GBDT 实例"></a><strong>GBDT 实例</strong></h2><p>GBDT的核心就在于，<strong>_每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量_</strong>。</p><p>比如以年龄预测的例子来说明，A 的真实年龄是 18 岁，但第一棵树的预测年龄是 12 岁，差了 6 岁，即残差为 6 岁。那么在第二棵树里我们把 A 的年龄设为 6 岁去学习，如果第二棵树真的能把 A 分到 6 岁的叶子节点，那累加两棵树的结论就是 A 的真实年龄；如果第二棵树的结论是 5 岁，则 A 仍然存在 1 岁的残差，第三棵树里 A 的年龄就变成 1 岁，继续学。</p><p>我们来详细的说一下这个例子，为简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图1所示结果：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gbdt1_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt="gbdt1"></p><p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gbdt2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt="gbdt2"></p><p>在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。</p><p>此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是 16-15=1（注意，A 的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是 15，如果还有树则需要都累加起来作为 A 的预测值）。进而得到 A,B,C,D 的残差分别为 (-1,1,-1,1)。</p><p>然后我们拿残差替代 A,B,C,D 的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值 1 和 -1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。</p><p>换句话说，现在 A,B,C,D 的预测值都和真实年龄一致了。Perfect!：</p><p>A: 14 岁高一学生，购物较少，经常问学长问题；预测年龄 A = 15 + (– 1) = 14</p><p>B: 16 岁高三学生，购物较少，经常被学弟问问题；预测年龄 B= 15 + (1) = 16</p><p>C: 24 岁应届毕业生，购物较多，经常问师兄问题；预测年龄 C = 25 + (-1) = 24</p><p>D: 26 岁工作两年员工，购物较多，经常被师弟问问题；预测年龄 D = 25 + (– 1) = 26</p><blockquote><p>那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量 (-1, 1, -1, 1) 都是它的全局最优方向，这就是Gradient。</p><p>其实 GBDT 大致的过程就是这例子中所讲的，下面我们再做一些深入的理解</p></blockquote><h2 id="GBDT-深入理解"><a href="#GBDT-深入理解" class="headerlink" title="GBDT 深入理解"></a><strong>GBDT 深入理解</strong></h2><p>前面说了，GBDT 是先有 GB（梯度提升），再有 DT（决策树），所以我们先从 GB 讲起.</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="_Boosting_"></a><strong>_Boosting_</strong></h3><p>boosting 在前面讲 AdaBoost提升算法的时候讲了，就是通过训练多个弱分类器来组合成一个强分类器，形式如下:</p><script type="math/tex; mode=display">F_m(x) = f_0 + \alpha_1 f_1(x) + \alpha_2 f_2(x) + \cdots + \alpha_m f_m(x)</script><p>其中，\(f_i(x),i = 1,2,\cdots,m\),是弱分类器，比如在 AdaBoost提升中是 C4.5决策树；\(F_m(x)\)是最终得到的强分类器。</p><h3 id="Gradient-Boosting-Modeling"><a href="#Gradient-Boosting-Modeling" class="headerlink" title="_Gradient Boosting Modeling_"></a><strong>_Gradient Boosting Modeling_</strong></h3><p>给定一个问题，我们如何构造这些弱分类器呢？</p><p>Gradient Boosting Modeling （梯度提升模型） 就是构造这些弱分类器的一种方法。它指的不是某个具体的算法，而是一种思想.</p><p>我们先从普通的梯度优化问题入手来理解:</p><script type="math/tex; mode=display">find \ \hat{x} = arg \min_{x} f(x)</script><p>针对这种问题，有个经典的算法叫 <strong>_Steepest Gradient Descent_</strong>，也就是最深梯度下降法。算法的大致过程是:</p><ol><li>给定一个起始点 \(x_0\)</li><li>对 \(i = 1,2,\cdots,K\)分别做如下迭代:<br>\(\qquad x_i = x_{i-1} + \gamma_{i-1} \times g_{i-1}\)<br>其中 \(g_{i-1} = -\frac{\partial f}{\partial x} |_{x = x_{i-1}}\)表示  f在 \(x_{i-1}\)点的梯度</li><li>直到 \(|g_{i-1}\)足够小，或者是 \(|x_i - x_{i-1}|\)足够小</li></ol><p>以上迭代过程可以理解为: <strong>_整个寻优的过程就是小步快跑的过程，每跑一小步，都往函数当前下降最快的那个方向走一点，直到达到可接受的点_</strong>.</p><p>我们将这个迭代过程展开得到寻优的结果:</p><script type="math/tex; mode=display">x_k = x_0 + \gamma_1 g_1 + \gamma_2 g_2 + \cdots + \gamma_k g_k</script><p>这个形式是不是与最开始我们要求的\(F_m(x)\)类似；构造\(F_m(x)\) 本身也是一个寻优的过程，只不过我们寻找的不是一个最优点，而是一个最优的函数。</p><p>寻找最优函数这个目标，也是定义一个损失函数来做:</p><script type="math/tex; mode=display">find \ F_m = arg \ \min_{F} L(F) = arg \  \min_{F} \sum_{i=0}^N Loss(F(x_i),y_i)</script><p>其中，\(Loss(F(x_i),y_i)\)表示损失函数\(Loss()\)  在第i个样本上的损失值， \(x_i,y_i\)分别表示第i个样本的特征和目标值。<br>类似最速梯度下降法，我们可以通过梯度下降法来构造弱分类器 \(f_1,f_2,\cdots,f_m\)，只不过每次迭代时，令:</p><script type="math/tex; mode=display">g_i = -\frac{\partial L}{\partial F}|_{F=F_{i-1}}</script><p>即损失函数L()对F求取梯度。</p><p>但是函数对函数求导不好理解，而且通常都无法通过上述公式直接求解。于是就采取一个近似的方法，把函数  理解成在所有样本上的离散的函数值，即:\(F_{i-1}\)理解成在所有样本上的离散的函数值，即:</p><script type="math/tex; mode=display">\left[ F_{i-1}(x_1),F_{i-1}(x_2),\cdots,F_{i-1}(x_N) \right]</script><p>这是一个N 维向量，然后计算:</p><script type="math/tex; mode=display">\hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N</script><p>这是一个函数对向量的求导，得到的也是一个梯度向量。注意，这里求导时的变量还是函数F，不是样本Xk ，只不过对F(Xk)求导时，其他的 Xi 都可以看成常数。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/GBM%E8%BF%87%E7%A8%8B.png" alt=""></p><h3 id="Gradient-Boosting-Decision-Tree"><a href="#Gradient-Boosting-Decision-Tree" class="headerlink" title="_Gradient Boosting Decision Tree_"></a><strong>_Gradient Boosting Decision Tree_</strong></h3><p>在上述算法过程中，如何通过 \(\hat{g}_{i-1}(x_j),j = 1,2,\cdots,N\)构造拟合函数\(g_{i-1}\)呢，这里我们用的就是 Decision Tree（决策树）了.</p><p>所以理解 GBDT，重点是先理解 Gradient Boosting，其次才是 Decision Tree；也就是说 GBDT 是 Gradient Boosting 的一种具体实现，这个拟合函数也可以改用其他的方法，只不过决策树好用一点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="_损失函数_"></a><strong>_损失函数_</strong></h3><p>谈到 GBDT 常听到的一种描述是 <code>先构造一个(决策)树，然后不断在已有模型和实际样本输出的残差上再构造一棵树，依次迭代</code>；其实这个说法不全面，尽管在本篇开头的那个 GBDT 的例子中是这样描述的，但拟合残差只是 GDBT 的一种特殊情况，下面对损失函数进行解释就清楚了.</p><p>从对 GBM 的描述里可以看到 Gradient Boosting 过程和具体用什么样的弱分类器是完全独立的，可以任意组合，因此这里不再刻意强调用决策树来构造弱分类器，转而我们来仔细看看弱分类器拟合的目标值，即梯度| \(\hat{g}_{i-1}(x_j)\)，之前我们已经提到过</p><script type="math/tex; mode=display">\hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N</script><p>因此  \(\frac{\partial L}{\partial F(x_k)}\)很重要，以平方差损失函数为例，得:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = 2(F_{i-1}(x_k) - y_k)</script><p>忽略 2 倍，后面括号中正是当前已经构造好的函数\(F_{i-1}\)在样本上和目标值Yk之间的差值.<br>如果我们换一个损失函数，比如绝对差:</p><script type="math/tex; mode=display">Loss(F(x_i),y_i) = |F(x_i) - y_i|</script><p>这个损失函数的梯度是个符号函数:</p><script type="math/tex; mode=display">{\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = sign(F_{i-1}(x_k) - y_k)}</script><p>由此可以看到，只有当损失函数为平方差函数时，才能说 GBDT 是通过拟合残差来构造弱分类器的，比如上面说的对残差不断的用决策回归树来拟合。</p><h1 id="知乎上关于xgboost-gbdt讨论的经典问答"><a href="#知乎上关于xgboost-gbdt讨论的经典问答" class="headerlink" title="知乎上关于xgboost/gbdt讨论的经典问答"></a>知乎上关于xgboost/gbdt讨论的经典问答</h1><p>【问】xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？<br>  用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？<br>【答】<br>  这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。<br>  一句话的解释，来自周志华老师的机器学习教科书（ 机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。<br>  随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。<br>  Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。<br>  其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X^2)-[E(X)]^2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。<br>  如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。</p><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p11.png" alt=""></p><p>  当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。</p><p>  也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。<br>  对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。<br>  对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p><p>【问】机器学习算法中GBDT和XGBOOST的区别有哪些？<br>【答】<br>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</p><p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p><p>列抽样（column subsampling）即特征抽样。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p><p>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p><p>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<br>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p><p>多种语言封装支持。</p><p>【问】为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？<br>【答】<br>作者：马超<br>链接：<a href="https://www.zhihu.com/question/51818176/answer/127637712" target="_blank" rel="noopener">https://www.zhihu.com/question/51818176/answer/127637712</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。我主要从三个方面来回答楼主这个问题。</p><ol><li>理论模型 （站在 vc-dimension 的角度）</li><li>实际数据</li><li>系统的实现 （主要基于 xgboost）<br>通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。</li></ol><p>（1）站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件：</p><ol><li>模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。</li><li>模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit.当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off.</li></ol><p>好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis.</p><p>在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？</p><p>区别就在于 “模型的可控性”。</p><p>先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。</p><p>我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.</p><p>（2）站在数据的角度</p><p>除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。</p><p>除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。</p><p>（3）站在系统实现的角度</p><p>除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征：</p><ol><li>正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。</li><li>系统具有灵活、深度的定制功能。</li><li>系统简单易用。</li><li>系统具有可扩展性, 可以从容处理更大的数据。</li></ol><p>到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。</p><p>在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。</p><p>在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。</p><p>最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。</p><p>有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。</p><p>综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/boosting2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://frankblog.site/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="boosting" scheme="http://frankblog.site/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>pyecharts链接mysql进行数据可视化</title>
    <link href="http://frankblog.site/2018/06/12/Pyecharts%E8%BF%9E%E6%8E%A5Mysql%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://frankblog.site/2018/06/12/Pyecharts连接Mysql进行可视化/</id>
    <published>2018-06-12T12:34:52.054Z</published>
    <updated>2018-06-12T12:42:33.911Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/echarts.png" alt=""></p><a id="more"></a><hr><h2 id="Pandas常用读取方式"><a href="#Pandas常用读取方式" class="headerlink" title="Pandas常用读取方式"></a>Pandas常用读取方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># 读取Mysql整个表为DataFrame：</span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">query = &quot;select * from table&quot;</span><br><span class="line">df = pd.read_sql(query,conn) # conn对象创建参考下文</span><br><span class="line"></span><br><span class="line">conn.close()</span><br><span class="line"># 读取json文件</span><br><span class="line">json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号</span><br><span class="line">js = pd.read_json(json_str)</span><br><span class="line"></span><br><span class="line"># 读取html</span><br><span class="line">url = &quot;http://quote.stockstar.com&quot;</span><br><span class="line">dfs = pd.read_html(url)</span><br><span class="line">dfs[0]</span><br><span class="line"></span><br><span class="line">dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定table</span><br><span class="line">dfs1[0]</span><br><span class="line"></span><br><span class="line"># 读取粘贴板文件称DataFrame</span><br><span class="line"></span><br><span class="line">df = pd.read_clipboard(sep=&apos;,&apos;, header=None)</span><br><span class="line"></span><br><span class="line"># 读取Mysql整个表为DataFrame：</span><br><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">query = &quot;select * from table&quot;</span><br><span class="line">df = pd.read_sql(query,conn) # conn对象创建参考下文</span><br><span class="line"></span><br><span class="line">conn.close()</span><br><span class="line"># 读取json文件</span><br><span class="line">json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号</span><br><span class="line">js = pd.read_json(json_str)</span><br><span class="line"></span><br><span class="line"># 读取html</span><br><span class="line">url = &quot;http://quote.stockstar.com&quot;</span><br><span class="line">dfs = pd.read_html(url)</span><br><span class="line">dfs[0]</span><br><span class="line"></span><br><span class="line">dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定table</span><br><span class="line">dfs1[0]</span><br><span class="line"></span><br><span class="line"># 读取粘贴板文件称DataFrame</span><br><span class="line"></span><br><span class="line">df = pd.read_clipboard(sep=&apos;,&apos;, header=None)</span><br></pre></td></tr></table></figure><h2 id="每日交易额汇总"><a href="#每日交易额汇总" class="headerlink" title="每日交易额汇总"></a>每日交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select day_date from day_sum order by day_date&quot;)</span><br><span class="line">tran_data = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select day_sum from day_sum order by day_date&quot;)</span><br><span class="line">transum = cursor.fetchall()</span><br><span class="line">tran_sum = [*map(lambda x :x[0]/100,list(transum))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br><span class="line">conn.colse()</span><br><span class="line"></span><br><span class="line">tran_add = [0]</span><br><span class="line">for i in range (len(tran_sum)):</span><br><span class="line"> if i &gt; 0:</span><br><span class="line"> tran_add.append((tran_sum[i] - tran_sum[i-1]) / tran_sum[i-1] * 100)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Bar, Line, Overlap</span><br><span class="line"></span><br><span class="line">attr1 = tran_data</span><br><span class="line">v1 = tran_sum</span><br><span class="line">v2 = tran_add</span><br><span class="line">bar1 = Bar(&quot;每日交易信息汇总&quot;)</span><br><span class="line">bar1.add(&quot;日期 /金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)</span><br><span class="line">line1 = Line()</span><br><span class="line">line1.add(&apos;环比增长率(%)&apos;,attr1, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True )</span><br><span class="line"></span><br><span class="line">line2 = Line()</span><br><span class="line">line2.add(&quot;日期/ 金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)</span><br><span class="line"></span><br><span class="line">overlap = Overlap()</span><br><span class="line">overlap.add(bar1)</span><br><span class="line"># overlap.add(bar2)</span><br><span class="line">overlap.add(line1)</span><br><span class="line">overlap.render()</span><br><span class="line">overlap</span><br><span class="line"></span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">#bar.render()</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts.png" alt="png"></a></p><h2 id="各出口交易金额"><a href="#各出口交易金额" class="headerlink" title="各出口交易金额"></a>各出口交易金额</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">#2016-04-29每个出口id交易额信息</span><br><span class="line">cursor.execute(&quot;select in_id from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)</span><br><span class="line">in_plazaid = cursor.fetchall()</span><br><span class="line">#2016-04-29每个出口交易额</span><br><span class="line">cursor.execute(&quot;select in_sum from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)</span><br><span class="line">in_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr2 = in_plazaid</span><br><span class="line">v2 = in_transum</span><br><span class="line">bar2 = Bar(&quot;2016-4-29日各出口交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar2.add(&quot;出口ID/金额（元）&quot;, attr2, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar2.render()</span><br><span class="line">bar2</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts1.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts1.png" alt="png"></a></p><h2 id="入出口交易金额"><a href="#入出口交易金额" class="headerlink" title="入出口交易金额"></a>入出口交易金额</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select ent_plazaid from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)</span><br><span class="line">ent_plazaid = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)</span><br><span class="line">ent_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr3 = ent_plazaid</span><br><span class="line">v3 = ent_transum</span><br><span class="line">bar3 = Bar(&quot;2016-4-29日各入口交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar3.add(&quot;入口ID/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar3.render()</span><br><span class="line">bar3</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts2.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts2.png" alt="png"></a></p><h2 id="某入口交易额汇总"><a href="#某入口交易额汇总" class="headerlink" title="某入口交易额汇总"></a>某入口交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select trans_date from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)</span><br><span class="line">ent_date = cursor.fetchall()</span><br><span class="line">cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)</span><br><span class="line">transum_ = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr3 = ent_date</span><br><span class="line">v3 = transum_</span><br><span class="line">bar3 = Bar(&quot;某出口每日交易额&quot;,&quot;&quot;)</span><br><span class="line">bar3.add(&quot;出口/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar3.render()</span><br><span class="line">bar3</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts3.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts3.png" alt="png"></a></p><h2 id="金额突增原因分析"><a href="#金额突增原因分析" class="headerlink" title="金额突增原因分析"></a>金额突增原因分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Funnel</span><br><span class="line"></span><br><span class="line">#根据2016-4-29出入口及各车型的交易额信息，每个组选取前4个贡献率最高的样本。</span><br><span class="line">#利用除均操作对这三种因素的影响率进行标准化。</span><br><span class="line">#下图可以看出2016-4-29日，这三种影响因素的前四个样本分别占改组贡献率的比例</span><br><span class="line">#100108红门主站出京入口、100861榆垡南出京出口、车型-1对该日的收费的贡献率最大</span><br><span class="line"></span><br><span class="line">attr = [&apos;100861榆垡南出京出&apos;,&apos;100158璃河南出京出&apos;,&apos;100109红门主站进京出&apos;,&apos;100135六里桥站进京出&apos;,&apos;100108红门主站出京入&apos;,&apos;100134六里桥站出京入&apos;,&apos;100862榆垡南进京入&apos;,&apos;100175西红门南桥出京入&apos;,&apos;车型-1&apos;,&apos;车型-4&apos;,&apos;车型-3&apos;,&apos;车型-2&apos;]</span><br><span class="line">value = [6.24, 6.01, 4.72, 4.71, 16.75, 15.52, 10.85, 8.08, 4.46, 0.29, 0.13, 0.1]</span><br><span class="line">funnel = Funnel(&quot;&quot;)</span><br><span class="line">funnel.add(&quot;因素&quot;, attr, value, is_label_show=True,</span><br><span class="line"> label_pos=&quot;inside&quot;, label_text_color=&quot;#fff&quot;)</span><br><span class="line">funnel.render()</span><br><span class="line">funnel</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts4.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts4.png" alt="png"></a></p><h2 id="各车型交易汇总"><a href="#各车型交易汇总" class="headerlink" title="各车型交易汇总"></a>各车型交易汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select distinct vehtype from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)</span><br><span class="line">type_id = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select distinct trans_sum from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)</span><br><span class="line">type_sum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line">#========================</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr4 = type_id</span><br><span class="line">v4 = type_sum</span><br><span class="line">bar4 = Bar(&quot;2016-4-29日各车型交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar4.add(&quot;车型/金额（元）&quot;, attr4, v4, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar4.render()</span><br><span class="line">bar4</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts5.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts5.png" alt="png"></a></p><h2 id="全日期各车型交易额汇总"><a href="#全日期各车型交易额汇总" class="headerlink" title="全日期各车型交易额汇总"></a>全日期各车型交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select * from tran_vehtype_sum&quot;)</span><br><span class="line">typedata = cursor.fetchall()</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">typedata = pd.DataFrame(list(typedata))</span><br><span class="line">typedata.columns = [&apos;type&apos;,&apos;date&apos;,&apos;value&apos;]</span><br><span class="line"></span><br><span class="line">data=pd.pivot_table(typedata,index=&quot;date&quot;,columns=&quot;type&quot;,values=&quot;value&quot;,aggfunc=np.sum,fill_value=0)</span><br><span class="line"># data.to_csv(&quot;data.csv&quot;)</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">y, x1, x2, x3, x4, x5 = [], [], [], [], [], []</span><br><span class="line">for i in range(len(data)):</span><br><span class="line"> y.append(data.index[i])</span><br><span class="line"> x1.append(data[1][i])</span><br><span class="line"> x2.append(data[2][i])</span><br><span class="line"> x3.append(data[3][i])</span><br><span class="line"> x4.append(data[4][i])</span><br><span class="line"> x5.append(data[5][i])</span><br><span class="line"></span><br><span class="line">bar = Bar(&quot;各车型每日交易额&quot;)</span><br><span class="line">bar.add(&quot;车型-1&quot;, y, x1, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-2&quot;, y, x2, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-3&quot;, y, x3, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-4&quot;, y, x4, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-5&quot;, y, x5, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.render()</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts6.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts6.png" alt="png"></a></p><h2 id="PyMysql使用"><a href="#PyMysql使用" class="headerlink" title="PyMysql使用"></a>PyMysql使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">#创建连接</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回收影响行数</span><br><span class="line">effect_row = cursor.execute(&quot;select * from tran_day_sum&quot;)</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回受影响行数</span><br><span class="line">#effect_row = cursor.execute(&quot;update tb7 set pass = &apos;123&apos; where nid = %s&quot;, (11,))</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回受影响行数,执行多次</span><br><span class="line">#effect_row = cursor.executemany(&quot;insert into tb7(user,pass,licnese)values(%s,%s,%s)&quot;, [(&quot;u1&quot;,&quot;u1pass&quot;,&quot;11111&quot;),(&quot;u2&quot;,&quot;u2pass&quot;,&quot;22222&quot;)])</span><br><span class="line"></span><br><span class="line"># 提交，不然无法保存新建或者修改的数据</span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"># 关闭游标</span><br><span class="line">cursor.close()</span><br><span class="line"># 关闭连接</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#分析每个入口的交易笔数和交易总额</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select ent_plazaid from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_id = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select count(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_count = [*map(lambda x :x[0],list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select sum(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_sum = [*map(lambda x :int(x[0]),list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/echarts.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://frankblog.site/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="pyecharts" scheme="http://frankblog.site/tags/pyecharts/"/>
    
  </entry>
  
  <entry>
    <title>linux常用命令</title>
    <link href="http://frankblog.site/2018/06/11/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://frankblog.site/2018/06/11/linux常用命令/</id>
    <published>2018-06-11T06:01:21.977Z</published>
    <updated>2018-06-11T07:02:37.438Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/linux.jpg" alt=""></p><a id="more"></a><hr><p>日常Linux操作记录，用于查询不考虑编写顺序</p><p><code>[root@localhost ~]# root</code>：当前登录用户<br><code>localhost</code>：主机名<br><code>~</code>：当前所在目录（家目录）<br><code>#</code>：超级用户提示符 ==&gt; 家目录(/root)<br><code>$</code>：普通用户提示符 ==&gt; 家目录(/home/user01)<br>文件类型（- 文件；d 目录；l 快捷方式）</p><p>命令格式：<br>命令 [选项] [参数]<br>例：查询目录内容<br><code>ls</code> [选项] [文件或目录]<br><code>ls -a</code> 显示所有文件，包括隐藏文件(.开头)<br><code>ls -l</code> 显示详细信息(也可用ll)</p><p>下载器：<br>更新系统（不建议使用apt-get dist-upgrade，会导致文件不匹配卸载的情况）<br><code>apt-get update</code> &amp;&amp; <code>apt-get upgrade</code><br>下载安装文件<br><code>apt-get install</code> [文件名]<br>安装过程中存在依赖关系<br><code>apt-get install -f</code></p><p>知道网址下载方法： wget [粘贴网址]</p><p>不同类型文件解压：<br><code>tar -xvf file.tar</code>//解压 tar包<br><code>tar -xzvf file.tar.gz</code>//解压tar.gz<br><code>tar -xjvf file.tar.bz2</code> //解压 tar.bz2<br><code>tar -xZvf file.tar.Z</code>//解压tar.Z<br><code>unrar e file.rar</code> //解压rar<br><code>unzip file.zip</code>//解压zip</p><p>未完待续。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/linux.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="操作系统" scheme="http://frankblog.site/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://frankblog.site/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>浅谈推荐系统</title>
    <link href="http://frankblog.site/2018/06/10/%E6%B5%85%E8%B0%88%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://frankblog.site/2018/06/10/浅谈推荐系统/</id>
    <published>2018-06-10T13:03:49.318Z</published>
    <updated>2018-06-11T07:00:22.418Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p><h1 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h1><h2 id="1-为什么需要推荐系统"><a href="#1-为什么需要推荐系统" class="headerlink" title="1. 为什么需要推荐系统"></a>1. 为什么需要推荐系统</h2><p><strong>结论是，为了解决互联网时代下的信息超载问题。</strong></p><p>正如《大数据时代》中作者所言，这仅仅是一个开始，人们与世界的交流方式，从原来对因果关系的渴求，转变为现在对相关关系的发现和使用上。</p><h2 id="2-搜索引擎与推荐系统"><a href="#2-搜索引擎与推荐系统" class="headerlink" title="2. 搜索引擎与推荐系统"></a>2. 搜索引擎与推荐系统</h2><p>众所周知，解决信息过载问题，最有代表性的解决方案是【分类目录】和【搜索引擎】，这两种解决方案分别催生了互联网领域的两家著名公司—雅虎和谷歌。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%92%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.jpeg" alt=""></p><p>推荐系统，和搜索引擎一样，是一种帮助用户快速发展有用信息的工具。通过分析用户的历史行为，给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。</p><p>并且，推荐系统能够很好的发掘物品的长尾，挑战传统的2/8原则（80%的销售额来自20%的热门品牌）。</p><p>从技术角度来看，搜索引擎和推荐系统的区别在于：<br>1）搜索引擎，注重搜索结果之间的关系和排序；<br>2）推荐系统，需要研究用户的兴趣模型，利用社交网络的信息进行个性化的计算；<br>3）搜索引擎，由用户主导，需要输入关键词，自行选择结果。如果结果不满意，需要修改关键词，再次搜索；<br>4）推荐系统，由系统主导，根据用户的浏览顺序，引导用户发现自己感兴趣的信息；</p><h1 id="推荐算法详述"><a href="#推荐算法详述" class="headerlink" title="推荐算法详述"></a>推荐算法详述</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p><h2 id="1-基于内容的推荐"><a href="#1-基于内容的推荐" class="headerlink" title="1 基于内容的推荐"></a>1 基于内容的推荐</h2><p>原理是基于用户感兴趣的物品A，找到和A内容信息相近的物品B。<br>利用用户和物品本身的内容特征，如用户的地理位置、性别、年龄，电影物品的导演、演员、发布时间等。<br>所以提取推荐对象的特征，是内容推荐算法的关键。但是对于多媒体内容，如视频、音乐，很难找到它们之间的特性关联性。</p><p><strong>基于内容的推荐的优点如下：</strong><br>(1) 简单、有效，推荐结果直观，容易理解，不需要领域知识。<br>(2) 不需要用户的历史数据，如对对象的评价等。<br>(3) 没有关于新推荐对象出现的冷启动问题。<br>(4) 没有稀疏问题。<br>(5) 算法成熟，如数据挖掘、聚类分析等。</p><p><strong>基于内容的推荐的缺点如下：</strong><br>(1) 受到了推荐对象特征提取能力的限制。<br>比如图像、视频，没有有效的特征提取方法。即便是文本资源，特征提取也只能反应一部分内容，难以提取内容质量，会影响用户满意度。<br>(2) 很难出现新的推荐结果。<br>根据用户兴趣的喜好进行推荐，很难出现惊喜。对于时间敏感的内容，如新闻，推荐内容基本相同，体验度较差。<br>(3)存在新用户出现时的冷启动问题。<br>当新用户出现时， 系统较难获得该用户的兴趣偏好，无法进行有效推荐。<br>(4) 推荐对象内容分类方法需要的数据量较大。</p><h2 id="2-协同过滤算法"><a href="#2-协同过滤算法" class="headerlink" title="2 协同过滤算法"></a>2 协同过滤算法</h2><p>仅仅基于用户行为数据设计的推荐算法，称为协同过滤算法。此方法主要根据用户对物品的历史行为，寻找用户或物品的近邻集合，以此计算用户对物品的偏好。</p><h3 id="基于用户的协同过滤算法（UserCF）"><a href="#基于用户的协同过滤算法（UserCF）" class="headerlink" title="基于用户的协同过滤算法（UserCF）"></a>基于用户的协同过滤算法（UserCF）</h3><p>算法的关键是计算两个用户的兴趣相似度。协同过滤计算用户兴趣相似度是利用用户行为的相似度。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A41.jpeg" alt=""></p><p>关键在于计算用户与用户之间的兴趣相似度。</p><ul><li>计算用户相似度的方法有3种：<ul><li>余弦相似性</li><li>相关相似性（皮尔森系数相关）</li><li>修正的余弦相似性<br>这里主要使用余弦相似度来计算：</li></ul></li></ul><script type="math/tex; mode=display">w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|| N(v)|}}</script><p>\(w_{uv}\)代表用户 u 与 v 之间的兴趣相似度，N(u)表示用户 u 曾经喜欢过的物品集合, N(v) 表示用户 v 曾经喜欢过的物品集合。</p><p>根据上述核心思想，可以有如下算法步骤：</p><ol><li>建立物品-用户的倒排表</li><li>用户与用户之间的共现矩阵 C[u][v]，表示用户u与v喜欢相同物品的个数</li><li>用户与用户之间的相似度矩阵 W[u][v]，根据上述相似度计算公式计算。</li><li>用上面的相似度矩阵来给用户推荐和他兴趣相似的用户喜欢的物品。用户 u 对物品 i 的兴趣程度可以估计为</li></ol><p><a href="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A42.gif" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A42.gif" alt=""></a></p><p>S(u,K) 为和用户 u 兴趣最接近的 K 个用户， N(i) 为对物品 i 有正反馈的用户集合， W[u][v] 为用户 u 和用户 v 的兴趣相似度，rvi 为用户 v 对物品 i 的兴趣。</p><h3 id="基于物品的协同过滤算法（ItemCF）"><a href="#基于物品的协同过滤算法（ItemCF）" class="headerlink" title="基于物品的协同过滤算法（ItemCF）"></a>基于物品的协同过滤算法（ItemCF）</h3><p>这种算法给用户推荐和他之前喜欢的物品相似的物品。<br>该算法是目前业界应用最多的算法，如亚马逊、Netflix、YouTube，都是以该算法为基础。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A43.gif" alt=""></p><p>物品之间的相似度可以使用如下公式计算：</p><script type="math/tex; mode=display">w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)|| N(j)|}}</script><p>从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。</p><ul><li>用户活跃度对物品相似度的影响<br>IUF（Inverse User Frequence），用户活跃度对数的倒数的参数。<br>论文提出的观点是，活跃用户对物品相似度的贡献应该小于不活跃的用户。用IUF修正物品相似度的计算。</li><li>物品相似度的归一化<br>研究表明，将ItemCF的相似度矩阵按最大值归一，可以提高推荐的准确率。</li></ul><p>根据上述核心思想，可以有如下算法步骤：</p><ol><li>建立用户-物品的倒排表</li><li>物品与物品之间的共现矩阵 C[i][j]，表示物品 i 与 j 共同被多少用户所喜欢。</li><li>用户与用户之间的相似度矩阵 W[i][j] ， 根据上述相似度计算公式计算。</li><li>用上面的相似度矩阵来给用户推荐与他所喜欢的物品相似的其他物品。用户 u 对物品 j 的兴趣程度可以估计为</li></ol><p><a href="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4.gif" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A44.gif" alt=""></a></p><p>S(j,K) 为和物品 j 最相似的前 K 个物品， N(u) 为对用户 u 所喜欢的物品集合， W[j][i] 为物品 j 和物品 i 之间的相似度， rui 为用户 u 对物品 i 的兴趣。</p><p>ItemCF 与 基于内容的推荐算法的区别</p><ul><li>基于内容的推荐算法，计算的是物品内容属性之间的相似度。如，电影的导演是不是同一个人；</li><li>ItemCF是通过用户的行为计算物品之间的相似度。如，物品A、B具有很大相似度，是因为喜欢物品A的用户也大都喜欢物品B。</li></ul><ul><li>UserCF 与 ItemCF 的优缺点</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A45.jpeg" alt=""></p><h2 id="3-基于关联规则的推荐"><a href="#3-基于关联规则的推荐" class="headerlink" title="3 基于关联规则的推荐"></a>3 基于关联规则的推荐</h2><p>关联规则分析中的关键概念包括：<strong>支持度(Support)、置信度(Confidence)与提升度(Lift)</strong>。首先，我们简单温故下这3个关键指标~</p><p><strong>1、支持度 (Support)：</strong>支持度是两件商品（A∩B）在总销售笔数(N)中出现的概率，<strong>即A与B同时被购买的概率。</strong>类似于中学学的交集，需要原始同时满足条件。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99.jpg" alt=""></p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%991.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>比如某超市2016年有100w笔销售，顾客购买可乐又购买薯片有20w笔，顾客购买可乐又购买面包有10w笔，那可乐和薯片的关联规则的支持度是20%，可乐和面包的支持度是10%。</p><p><strong>2、置信度 (Confidence)：置信度是购买A后再购买B的条件概率。</strong>简单来说就是交集部分C在A中比例，如果比例大说明购买A的客户很大期望会购买B商品。</p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%992.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>某超市2016年可乐购买次数40w笔，购买可乐又购买了薯片是30w笔，顾客购买可乐又购买面包有10w笔，则购买可乐又会购买薯片的置信度是75%，购买可乐又购买面包的置信度是25%，这说明买可乐也会买薯片的关联性比面包强，营销上可以做一些组合策略销售。</p><p><strong>3、提升度 (Lift)：提升度表示先购买A对购买B的概率的提升作用</strong>，用来判断规则是否有实际价值，即使用规则后商品在购物车中出现的次数是否高于商品单独出现在购物车中的频率。如果大于1说明规则有效，小于1则无效。</p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%993.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>可乐和薯片的关联规则的支持度是20%，购买可乐的支持度是3%，购买薯片的支持度是5%，则提升度是1.33&gt;1, A-B规则对于商品B有提升效果。</p><h2 id="4-隐语义模型"><a href="#4-隐语义模型" class="headerlink" title="4 隐语义模型"></a>4 隐语义模型</h2><p>待学习</p><h1 id="推荐系统评测"><a href="#推荐系统评测" class="headerlink" title="推荐系统评测"></a>推荐系统评测</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E8%AF%84%E6%B5%8B.png" alt="link"></p><h2 id="1-实验方法"><a href="#1-实验方法" class="headerlink" title="1. 实验方法"></a>1. 实验方法</h2><p>获得评测指标的实验方法，通常分3种：</p><ul><li>离线实验（offline experiment）</li><li>用户调查（user study）</li><li>在线实验（online experiment）</li></ul><p>我们分别介绍3种实验方法的优缺点。</p><h3 id="1）离线实验"><a href="#1）离线实验" class="headerlink" title="1）离线实验"></a>1）离线实验</h3><p>离线实验的方法的步骤如下：<br>a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；<br>b）将数据集按照一定的规则分成训练集和测试集；<br>c）在训练集上训练用户兴趣模型，在测试集上进行预测；<br>d）通过事先定义的离线指标，评测算法在测试集上的预测结果。</p><p>从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。</p><p><strong>离线实验的优点是：</strong></p><ul><li>不需要有对实际系统的控制权；</li><li>不需要用户参与实践；</li><li>速度快，可以测试大量算法；</li></ul><p><strong>缺点是：</strong></p><ul><li>数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果；</li><li>评价结果的客观性，无法得到用户主观性的评价；</li><li>难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系；</li></ul><h3 id="2）用户调查"><a href="#2）用户调查" class="headerlink" title="2）用户调查"></a>2）用户调查</h3><p>用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。</p><p>最后，我们通过分析他们的行为和答案，了解测试系统的性能。</p><p><strong>用户调查的优点是：</strong></p><ul><li>可以获得用户主观感受的指标，出错后容易弥补；</li></ul><p><strong>缺点是：</strong></p><ul><li>招募测试用户代价较大；</li><li>无法组织大规模的测试用户，统计意义不足；</li></ul><h3 id="3）在线实验"><a href="#3）在线实验" class="headerlink" title="3）在线实验"></a>3）在线实验</h3><p>在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。</p><p>在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。</p><p>它的核心思想是:<br>a) 多个方案并行测试;<br>b) 每个方案只有一个变量不同;<br>c) 以某种规则优胜劣汰。</p><p>其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。<br>对于推荐系统的评价中，唯一变量就是—推荐算法。</p><blockquote><p>有个很棒的网站，<a href="https://link.jianshu.com/?t=http%3A%2F%2Fwww.abtests.com" target="_blank" rel="noopener">http://www.abtests.com</a>，里面有很多通过实际AB测试提高网站用户满意度的例子。</p></blockquote><p><strong>AB测试的优点是：</strong></p><ul><li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标；</li></ul><p><strong>缺点是：</strong></p><ul><li>周期较长，必须进行长期的实验才能得到可靠的结果；</li></ul><p>大型网站做AB测试，可能会因为不同团队同时进行各种测试对结果造成干扰，所以切分流量是AB测试中的关键。</p><h3 id="4）总结"><a href="#4）总结" class="headerlink" title="4）总结"></a>4）总结</h3><p>一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。</p><ul><li>首先，通过离线实验证明它在很多离线指标上优于现有的算法；</li><li>其次，通过用户调查确定用户满意度不低于现有的算法；</li><li>最后，通过在线AB测试确定它在我们关心的指标上优于现有的算法；</li></ul><h2 id="2-评测指标"><a href="#2-评测指标" class="headerlink" title="2. 评测指标"></a>2. 评测指标</h2><p>评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。</p><h3 id="1）用户满意度"><a href="#1）用户满意度" class="headerlink" title="1）用户满意度"></a>1）用户满意度</h3><p>用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。</p><p>调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。</p><p>在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。</p><p>一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。</p><h3 id="2）预测准确度"><a href="#2）预测准确度" class="headerlink" title="2）预测准确度"></a>2）预测准确度</h3><p>预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。</p><p>大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。</p><p>由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。</p><h2 id="a）预测评分准确度"><a href="#a）预测评分准确度" class="headerlink" title="a）预测评分准确度"></a>a）预测评分准确度</h2><p>预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。<br>这针对于一些需要用户给物品评分的网站。</p><p>预测评分的准确度指标，一般通过以下指标计算：</p><ul><li>平均绝对误差（MAE）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E6%B5%8B%E5%87%86%E7%A1%AE%E5%BA%A6png.png" alt=""></p><p>MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。</p><p>所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。</p><ul><li>均方根误差（RMSE）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E6%B5%8B%E5%87%86%E7%A1%AE%E5%BA%A62.png" alt=""></p><p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。<br>研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。</p><h2 id="b）TopN推荐"><a href="#b）TopN推荐" class="headerlink" title="b）TopN推荐"></a>b）TopN推荐</h2><p>网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。</p><p>TopN推荐的预测准确率，一般通过2个指标度量：</p><ul><li>准确率（precision）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/topN.jpeg" alt=""></p><ul><li>召回率（recall）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/topN1.jpeg" alt=""></p><p>R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。</p><p>TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。</p><h3 id="3）覆盖率"><a href="#3）覆盖率" class="headerlink" title="3）覆盖率"></a>3）覆盖率</h3><p>覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。<br>最简单的定义是，推荐系统推荐出来的物品占总物品的比例。</p><p>假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为：</p><p><img src="https://upload-images.jianshu.io/upload_images/610388-dc450d844bcc84be.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/229" alt=""></p><p>覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。</p><p>除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。</p><p>如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。</p><p>信息论和经济学中有两个著名指标，可以定义覆盖率：</p><ul><li>信息熵</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B5.jpeg" alt=""></p><p>p(i)是物品i的流行度除以所有物品流行度之和。</p><ul><li>基尼系数（Gini Index）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%88Gini%20Index%EF%BC%89.jpeg" alt=""></p><p>p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%88Gini%20Index%EF%BC%892.jpeg" alt=""></p><ul><li>评测马太效应</li></ul><p>马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。</p><p>但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。</p><p>如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。</p><h3 id="4）多样性"><a href="#4）多样性" class="headerlink" title="4）多样性"></a>4）多样性</h3><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。</p><p>多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E6%A0%B7%E6%80%A71.jpeg" alt=""></p><p>推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E6%A0%B7%E6%80%A72.jpeg" alt=""></p><h3 id="5）新颖性"><a href="#5）新颖性" class="headerlink" title="5）新颖性"></a>5）新颖性</h3><p>新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。</p><p>评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。</p><p>此计算比较粗糙，需要配合用户调查准确统计新颖度。</p><h3 id="6）惊喜度"><a href="#6）惊喜度" class="headerlink" title="6）惊喜度"></a>6）惊喜度</h3><p>推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。</p><p>目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。</p><h3 id="7）信任度"><a href="#7）信任度" class="headerlink" title="7）信任度"></a>7）信任度</h3><p>如果用户信任推荐系统，就会增加用户和推荐系统的交互。</p><p><strong>提高信任度的方式有两种：</strong></p><ul><li><p>增加系统透明度<br>提供推荐解释，让用户了解推荐系统的运行机制。</p></li><li><p>利用社交网络，通过好友信息给用户做推荐<br>通过好友进行推荐解释</p></li></ul><p>度量信任度的方式，只能通过问卷调查。</p><h3 id="8）实时性"><a href="#8）实时性" class="headerlink" title="8）实时性"></a>8）实时性</h3><p>推荐系统的实时性，包括两方面：</p><ul><li>实时更新推荐列表满足用户新的行为变化；</li><li>将新加入系统的物品推荐给用户；</li></ul><h3 id="9）健壮性"><a href="#9）健壮性" class="headerlink" title="9）健壮性"></a>9）健壮性</h3><p>任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。</p><p>健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。</p><blockquote><p>2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。<br>就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。</p></blockquote><p><strong>评测算法的健壮性，主要利用模拟攻击：</strong></p><p>a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；<br>b）用常用的攻击方法向数据集中注入噪声数据；<br>c）利用算法在有噪声的数据集上再次生成推荐列表；<br>d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。</p><p><strong>提高系统健壮性的方法：</strong></p><ul><li>选择健壮性高的算法；</li><li>选择代价较高的用户行为，如购买行为比浏览行为代价高；</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h2 id="3-评测维度"><a href="#3-评测维度" class="headerlink" title="3. 评测维度"></a>3. 评测维度</h2><p>增加评测维度的目的，就是知道一个算法在什么情况下性能最好。</p><p>一般评测维度分3种：</p><ul><li>用户维度<br>主要包括用户的人口统计学信息、活跃度以及是不是新用户等；</li><li>物品维度<br>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等；</li><li>时间维度<br>包括季节，是工作日还是周末，白天还是晚上等；</li></ul><p>如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。</p><h1 id="冷启动问题"><a href="#冷启动问题" class="headerlink" title="冷启动问题"></a>冷启动问题</h1><p><strong>1 解决方案：</strong></p><ul><li>提供非个性化推荐，如热门排行。等有了数据之后再推荐。</li><li>利用用户注册信息，做粗粒度的个性化。</li><li>利用用户的社交网络账号，导入用户的好友，推荐好友喜欢的物品。</li><li>用户初次登录时，对一些物品进行反馈，根据这些信息做个性化。</li><li>对于新上线的物品，利用内容信息，推荐给喜欢类似物品的用户。</li><li>系统冷启动，可以引入外部资源，如专家知识，建立起物品的相关度。</li></ul><p><strong>2 冷启动，启动用户兴趣的物品需要具有以下特点：</strong></p><ul><li>比较热门</li><li>具有代表性和区分性</li><li>启动物品集合需要有多样性</li></ul><p><strong>3 选择启动物品集合的系统</strong></p><p>如何设计一个选择启动物品集合的系统？Nadav Golbandi在论文中提出用一个决策树解决。</p><p>首先，给定一群用户，用这群用户对物品评分的方差度量这群用户兴趣的一致程度。如果方差很小，说明这一群用户的兴趣不太一致，也就是物品具有比较大的区分度，反之则说明这群用户的兴趣比较一致。</p><p>再根据用户的评分方差计算物品的区分度。</p><p>也就是说，对于物品i，将用户分为3类—喜欢物品i的用户，不喜欢物品i的用户和不知道物品i的用户。如果这3类用户集合内的用户对其他的物品兴趣很不一致，说明物品i具有较高的区分度。</p><p>算法首先从所有用户中找到具有最高区分度的物品i，然后将用户分成3类。然后在每类用户中再找到最具区分度的物品，然后将每一类用户又各自分为3类，也就是将总用户分为9类，然后继续这样下去，最终可以通过对一系列物品的看法将用户进行分类。</p><p>在冷启动时，从根节点开始询问用户对该节点物品的看法，然后根据用户的选择将用户放到不同的分枝，直到进入最后的叶子节点，此时对用户的兴趣有了比较清楚的了解，从而可以开始对用户进行比较准确地个性化推荐。</p><p><strong>4 利用物品的内容信息</strong></p><p>就是基于内容的推荐，很适合解决物品冷启动问题。</p><p>物品冷启动对诸如新闻网站等时效性很强的网站的推荐非常重要，因为那些网站中时时刻刻都有新加入的物品，而且每个物品必须能够在第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。</p><p>一般来说，物品的内容可以通过向量空间模型表示，该模型会将物品表示成一个关键词向量。</p><p>如果物品的内容是诸如导演、演员等实体，可以直接将实体作为关键词。<br>如果内容是文本，需要引入自然语言的技术抽取关键词。如何建立文章、话题和关键词的关系是话题模型研究的重点，代表性的话题模型有LDA。</p><p>最后推荐几篇博文</p><ul><li>基于内容和用户画像的推荐：此种算法，可见之前的一篇文章：<a href="http://www.rowkey.me/blog/2016/04/07/up-recommend/" target="_blank" rel="noopener">http://www.rowkey.me/blog/2016/04/07/up-recommend/</a>。</li><li>基于矩阵分解的推荐: 基于SVD/ALS算法对用户进行内容推荐。相比起SVD，ALS更加适合解决稀疏矩阵的问题。Spark mlib中已经集成了对als算法的实现，需要做的就是在etl-1中把数据转换为als需要的数据格式以及调整als算法的各种参数。这里有一篇文章比较具体地描述了如何使用spark来做基于ALS的推荐：<a href="http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/" target="_blank" rel="noopener">http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/</a>。</li><li>用户&amp;物品协同过滤推荐：包括UserBased CF和ItemBased CF。对于这两者，需要根据业务的不同来选择不同的算法。当用户非常多的时候，考虑到维护用户矩阵的成本，一般是不推荐选择用户协同过滤的，而对于候选item很多的时候，则不推荐使用物品协同过滤</li></ul><p>参考文献：<br>项亮《推荐系统实践》<br>参考论文：<br><a href="http://t.cn/RjXktmC" target="_blank" rel="noopener">http://t.cn/RjXktmC</a><br><a href="http://t.cn/RjXkiFP" target="_blank" rel="noopener">http://t.cn/RjXkiFP</a><br><a href="http://blog.csdn.net/qingqingpiaoguo/article/details/60882309" target="_blank" rel="noopener">http://blog.csdn.net/qingqingpiaoguo/article/details/60882309</a> <a href="https://www.zhihu.com/question/27141495/answer/161027882" target="_blank" rel="noopener">https://www.zhihu.com/question/27141495/answer/161027882</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://frankblog.site/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>浅谈分布式</title>
    <link href="http://frankblog.site/2018/06/07/%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://frankblog.site/2018/06/07/浅谈分布式/</id>
    <published>2018-06-07T12:15:05.369Z</published>
    <updated>2018-06-08T02:01:56.248Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="分布式基础"><a href="#分布式基础" class="headerlink" title="分布式基础"></a>分布式基础</h2><p>分布式，按字面意思就是并行计算，对于一些串行的计算可以运用。一些本来就并行的就不可以。</p><p>之前聊起这个，就会说mapreduce，就是把任务分拆开来，最后计算的时候再合并。但后来发现这样的回答太水了，看了几篇介绍分布式计算的PPT，特此写下来分布式到底是怎么一回事。</p><p>我们用一个矩阵运算作为例子：</p><p>对于两个矩阵，A和B：</p><p>A = \(\begin{pmatrix}1 &amp; 2 &amp; 3\\\\  4 &amp; 5 &amp;0 \\\\  7 &amp; 8&amp;9\\\\  10 &amp; 11&amp;12\end{pmatrix}\)</p><p>B =  \(\begin{pmatrix}10 &amp; 15\\\\  0 &amp;2 \\\\ 11 &amp;9\end{pmatrix}\)</p><p>有</p><p>C = AB = \(\begin{pmatrix}43 &amp; 46\\\\  40 &amp;70 \\\\ 169 &amp;202\\\\ 232 &amp;280\end{pmatrix}\)</p><p>我们要改变矩阵存储方式，只存储那些非零的数值。</p><p>所以矩阵A可以表示成</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/A%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>其中，每一行第一个字段为行标签i，第二个字段为列标签j，第三个字段是A[i,j],即矩阵对应的值。</p><p>同理，对于矩阵B，可以变换为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/B%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>下图中，对于矩阵A，因为矩阵B一共两列，所以p=2，所以key会有（1，1）和（1，2）。而对于每个key，每一行中每一个位置的数据都得变成value，即value=”a:j,aij”</p><p>同理，对于矩阵B，k就在key的前面，因为是每一列拿出来做相乘。Value中也变成按行遍历。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce%201_%E7%9C%8B%E5%9B%BE%E7%8E%8B_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><p>在shuffle阶段，就把相同的key放在一起<br>在reduce阶段，在同一个key中，value第一维相同的匹配一起，其第二维相乘，最后把所有相加（如key（1，1）中，\(1<em>10+3</em>11\)，因为a:2没有匹配的，所以去掉a:2）</p><p>最后把每一个key相加就可以得到结果了。</p><center>  <img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce3.png" width="120"> </center><p>整体的流程实现如下图：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/MapReduceWordCountOverview1.png" alt=""></p><h2 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a><strong>Python代码实现</strong></h2><p>对于wordcount，如果是串行的话，等于是对于每个word进行一次循环。而并行的话可以多个word一起遍历。</p><p>map代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import sys </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    word_list = line.strip().split(&apos; &apos;)    </span><br><span class="line">    for word in word_list:</span><br><span class="line">        print &apos;\t&apos;.join([word.strip(), str(1)])</span><br></pre></td></tr></table></figure><p>reduce代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">cur_word = None</span><br><span class="line">sum = 0    </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    ss = line.strip().split(&apos;\t&apos;)        </span><br><span class="line">    if len(ss) &lt; 2:</span><br><span class="line">        continue    </span><br><span class="line">    word = ss[0].strip()</span><br><span class="line">    count = ss[1].strip()    </span><br><span class="line">    if cur_word == None:</span><br><span class="line">        cur_word = word    </span><br><span class="line">    if cur_word != word:</span><br><span class="line">        print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">        cur_word = word</span><br><span class="line">        sum = 0        </span><br><span class="line">    sum += int(count)    </span><br><span class="line">print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">sum = 0</span><br></pre></td></tr></table></figure><p>然后在linux上运行以下代码：</p><p>src.txt为自己随便找的文本文件txt格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat src.txt | python map.py | sort -k 1 | python reduce.py</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/python%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.jpg" alt=""></p><p>还算比较简单的，就不过多解释了。本意就是map中算出每一行中的word，然后传到reduce中处理。</p><h2 id="实战中怎么运用并行化处理"><a href="#实战中怎么运用并行化处理" class="headerlink" title="实战中怎么运用并行化处理"></a><strong>实战中怎么运用并行化处理</strong></h2><p>Python中有几个package可以做并行，如joblib，pp，multiprocessing等，</p><p>此处仅介绍joblib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from joblib import Parallel, delayed</span><br><span class="line">from math import sqrt</span><br><span class="line">Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))</span><br></pre></td></tr></table></figure><p>得到：</p><p>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</p><p>本质就是你要定义好一个子函数，这个子函数是并行任务中每个任务的输出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="分布式" scheme="http://frankblog.site/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="mapreduce" scheme="http://frankblog.site/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之聚类</title>
    <link href="http://frankblog.site/2018/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/"/>
    <id>http://frankblog.site/2018/06/07/机器学习之聚类/</id>
    <published>2018-06-07T06:48:23.561Z</published>
    <updated>2018-06-08T03:50:40.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>无监督学习方法在现实场景中还是有很多应用的，例如在金融和计算广告等反欺诈场景中，我们是不可能能够获取大量的标签数据的，因为欺诈用户不会告诉你他是欺诈用户的。这时候，如果想要利用机器学习方法检测出他们来，只能使用无监督方法。</p><h1 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>下面具体看到该算法的步骤：</p><p>（1）根据设定的聚类数 K，随机地选择 K 个聚类中心（Cluster Centroid）</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>（2）评估各个样本到聚类中心的距离，如果样本距离第 i 个聚类中心更近，则认为其属于第 ii 簇</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%92%E5%BD%92%E5%88%B0%E7%B0%87.png" alt=""></p><p>（3）计算每个簇中样本的<strong>平均（Mean）</strong>位置，将聚类中心移动至该位置</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%A7%BB%E5%8A%A8%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>重复以上步骤直至各个聚类中心的位置不再发生改变。</p><h2 id="变量标准化"><a href="#变量标准化" class="headerlink" title=".变量标准化"></a>.变量标准化</h2><p>在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。</p><p>常用的方法有：</p><p><strong>正态标准化</strong>：\(x_i=\frac {x_i-mean(X)}{std(X}\)<br><strong>归一化</strong>：\(x_i=\frac {x_i-min(X)}{max(X)-min(X)}\)</p><h2 id="如何确定聚类数"><a href="#如何确定聚类数" class="headerlink" title="如何确定聚类数"></a>如何确定聚类数</h2><p>实际上，一开始是很难确定聚类数的，下图的两种聚类数似乎都是可行的：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B01.png" alt=""><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B02.png" alt=""></p><p>但是，也存在一种称之为<strong>肘部法则（Elbow Method）</strong>的方法来选定适当的K值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png" alt=""></p><p>上图曲线类似于人的手肘，“肘关节”部分对应的 K 值就是最恰当的 K值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B2%A1%E6%9C%89%E8%82%98.png" alt=""></p><p>一般来说，K-Means 得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择 K 值。在下面这个例子中，假定我们的衣服想要是分为 S,M,L 三个尺码，就设定 K=3，如果我们想要 XS、S、M、L、XL 5 个衣服的尺码，就设定 K=5：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%8D%E9%A5%B0%E5%A4%A7%E5%B0%8F.png" alt=""></p><h2 id="质心的目标函数"><a href="#质心的目标函数" class="headerlink" title="质心的目标函数"></a>质心的目标函数</h2><p>聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性</p><p>常见的邻近度、质心和目标函数组合</p><div class="table-container"><table><thead><tr><th>邻近度函数</th><th>质心</th><th>目标函数</th></tr></thead><tbody><tr><td>曼哈顿距离</td><td>中位数</td><td>最小化对象与质心的绝对误差和SAE</td></tr><tr><td>平方欧几里得距离</td><td>均值</td><td>最小化对象与质心的误差平方和SSE</td></tr><tr><td>余弦</td><td>均值</td><td>最大化对象与质心的余弦相似度和</td></tr><tr><td>Bregman散度</td><td>均值</td><td>最小化对象到质心的Bregman散度和</td></tr></tbody></table></div><p><code>Bregman散度</code>实际上是一类紧邻性度量，包括平方欧几里得距离。<code>Bregman散度函数</code>的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。</p><h2 id="K-means优缺点"><a href="#K-means优缺点" class="headerlink" title="K-means优缺点"></a>K-means优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>简单并且可以用于各种数据类型；</li><li>具备适合的空间复杂度和计算负责度，适用于大样本数据；</li><li>K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1. 属于“硬聚类” ， 每个样本只能有一个类别。 其他的一些聚类方法(GMM或者模糊K-means允许“软聚类”)。<br>2. K-means对异常点的“免疫力” 很差， 我们可以通过一些调整，比如中心不直接取均值， 而是找均值最近的样本点代替——k-medoids算法。<br>3. 对于团状的数据点集区分度好， 对于带状(环绕)等“非凸”形状不太好。 (用谱聚类或者做特征映射)</p><h1 id="密度聚类方法"><a href="#密度聚类方法" class="headerlink" title="密度聚类方法"></a>密度聚类方法</h1><h2 id="基本名词"><a href="#基本名词" class="headerlink" title="基本名词"></a>基本名词</h2><ul><li><strong>核心点</strong>： 如果该点满足给定的邻域内（半径为EpsEps的范围内）的点的个数超过给定的阈值MinptsMinpts，则该点为满足该条件下的核心点。</li><li><strong>边界点</strong>： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。</li><li><p><strong>噪声点</strong>： 噪声点既非核心点，也不是边界点</p></li><li><p><strong>ϵ 邻域</strong></p></li></ul><p>对于样本集 D 中样本点 xi，它的 ϵ 邻域定义为与 xi 距离不大于 ϵ 的样本的集合，即 Nϵ(xi)={x∈D|dist(x,xi)≤ϵ}；</p><ul><li><strong>核心对象</strong></li></ul><p>如果样本 x 的 ϵ 邻域内至少包含 mps 个样本，即 |Nϵ(xi)|≥mps，则称 x 为核心对象；</p><ul><li><strong>密度直达</strong></li></ul><p>如果 xi 是一个核心对象，并且 xj 位于它的 ϵ邻域内，那么我们称 xj 由 xi 密度直达；</p><ul><li><strong>密度可达</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在这样的样本序列 p1,p2,⋯,pn，其中 p1=xi,pn=xj且\(p_{i+1}\)由pi密度直达，则称 xi 与 xj 密度可达；</p><ul><li><strong>密度相连</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在第三个样本点 xk 使得 xi与 xj均由 xk密度可达，则称 xi与 xj密度相连。</p><h2 id="DBSCAN算法步骤"><a href="#DBSCAN算法步骤" class="headerlink" title="DBSCAN算法步骤"></a>DBSCAN算法步骤</h2><blockquote><p><1> 将所有点标记为核心点、边界点或噪音点</1></p><p><2> 删除噪音点</2></p><p><3> 为距离在 EpsEps 之内的所有核心点之间赋予一条边</3></p><p><4> 每组联通的核心点形成一个簇</4></p><p><5> 将每个边界点指派到一个与之关联的核心点的簇中</5></p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB.jpg" alt="link"></p><h2 id="选择-DBSCAN-参数"><a href="#选择-DBSCAN-参数" class="headerlink" title="选择 DBSCAN 参数"></a>选择 DBSCAN 参数</h2><ul><li><strong>k−距离</strong>：如何选择 Eps 和 MinPts 参数，基本的方法是观察点到它的第 k 个最近邻的距离。<br>考虑下，如何 k不大于簇个数的话， k−距离相对较小，反之对于不在簇中的点（噪音点或异常值）则k距离较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 k (一般为4)，计算所有点的k−距离，并递增排序，画出曲线图，则我们会看到k−距离的变化，并依照此图选择出合适的 MinPts参数，即对应拐点的位置。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB2.jpg" alt=""></p><h2 id="优点和缺点"><a href="#优点和缺点" class="headerlink" title="优点和缺点"></a>优点和缺点</h2><ul><li>对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。</li><li>当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。</li></ul><h1 id="层次聚类方法"><a href="#层次聚类方法" class="headerlink" title="层次聚类方法"></a>层次聚类方法</h1><p>有两种产生层次聚类的基本方法：</p><ul><li>凝聚型： 从点作为个体簇开始，每一步合并两个最接近的簇</li><li>分裂型： 从包含所有的点某个簇开始，每一步分裂一个簇，直到成为单点簇<br>到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。</li></ul><h2 id="簇之间的临近性"><a href="#簇之间的临近性" class="headerlink" title="簇之间的临近性"></a>簇之间的临近性</h2><ul><li><strong>MIN</strong>：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做<strong>单链（sigle link）</strong></li><li><strong>MAX</strong>：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做<strong>全链（complete link）</strong></li><li><strong>组平均</strong>：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。</li></ul><p><img src="https://img-blog.csdn.net/20160624104427613" alt="link"></p><h2 id="层次聚类的主要问题"><a href="#层次聚类的主要问题" class="headerlink" title="层次聚类的主要问题"></a>层次聚类的主要问题</h2><h3 id="处理不同大小簇的能力"><a href="#处理不同大小簇的能力" class="headerlink" title="处理不同大小簇的能力"></a>处理不同大小簇的能力</h3><p>对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法：</p><ul><li><strong>非加权</strong>：平等的对待所有簇，赋予不同大小的簇中点不同的权值</li><li><strong>加权</strong>： 赋予不同大小簇中点相同的权值</li></ul><h3 id="合并不可逆"><a href="#合并不可逆" class="headerlink" title="合并不可逆"></a>合并不可逆</h3><p>对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。</p><p>有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。</p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>众所周知，对于有监督学习方法的分类预测结果，我们有很多种不同的评价指标来度量分类效果的好坏。例如，召回率、精准率、准确率、F1-Score 以及 AUC 值等等。但是，由于无监督学习方法与有监督学习不同，绝大多数情况下，我们根本不知道它的真实类别标签，所以我们不可能完全依据有监督学习方法的评价指标来度量聚类算法。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.jpg" alt=""></p><p>无监督聚类算法的评价指标大致可以分为两大类：一类是聚类的结果具有某个参考模型作为人为基准进行比较，称之为<strong>外部指标</strong>；第二种是直接考察聚类结果而不参考任何模型，称之为内部指标。</p><h2 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h2><p>对数据集 \(D=\{x_1, x_2,\cdots,x_m\}\)，假定通过聚类算法将样本聚为 \(C=\{C_1,C_2,\cdots,C_k\}\)，参考模型给出的簇划分为 \(C^<em>=\{C_1^</em>,C_2^<em>,\cdots,C_s^</em>\}\)。相应的，令 λ 与 \(λ^∗\) 分别表示与 C 与 \(C^∗\)对应的簇标记向量。我们将样本两两配对考虑，定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%8711.png" alt=""></p><p>其中集合 S1包含了在 C 中属于相同的簇并且在 C∗中也属于相同的簇的样本； S2 包含了在 C 中属于相同的簇并且在 C∗中不属于相同的簇的样本……以此类推。对每个样本对 (xi,xj) (i &lt; j) 仅能出现在一个集合中，因此有 a+b+c+d=m(m−1)/2。</p><p>基于以上定义，对无监督聚类算法的聚类结果，我们有如下的性能度量指标：</p><ul><li>Jaccard 系数（简记为 JCI）</li></ul><script type="math/tex; mode=display">\begin{align} JCI = \frac{a}{a+b+c} \end{align}</script><ul><li>FM 指数（简记为FMI）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/fm%E6%8C%87%E6%95%B0.png" alt=""></p><ul><li>Rand 指数（简记为 RI）</li></ul><script type="math/tex; mode=display">\begin{align} RI=\frac{2(a+d)}{m(m-1)} \end{align}</script><p>很显然，上述指数值都在 <code>[0,1]</code> 之间，并且值越大越好。</p><h2 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h2><p>对于聚类结果\(C=\{C_1,C_2,\cdots,C_k\}\)，作如下定义：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%86%85%E9%83%A8%E6%8C%87%E6%A0%87.png" alt="内部指标"></p><p>其中，\(dist(x_i,x_j)\)用于计算两个样本间的距离 \(μ_i\)代表类 \(C_j\) 的样本中心。基于以上定义如下内部指标：</p><ul><li><strong>DB 指数（简称 DBI）</strong><script type="math/tex; mode=display">\begin{align} DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\bigl(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\bigl) \end{align}</script></li><li><strong>Dunn 指数（简称 DI）</strong></li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/dunn%E6%8C%87%E6%95%B02.png" alt=""><br>显然，DBI 的值越小越好，而 DI 值越大越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类算法" scheme="http://frankblog.site/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之KNN</title>
    <link href="http://frankblog.site/2018/06/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN/"/>
    <id>http://frankblog.site/2018/06/06/机器学习之KNN/</id>
    <published>2018-06-06T03:48:14.506Z</published>
    <updated>2018-06-06T04:05:38.617Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h1><p>KNN采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h2 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/kd%E6%A0%91.jpg" alt=""></p><p>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h2 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h2><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h1 id="KNN算法原理"><a href="#KNN算法原理" class="headerlink" title="KNN算法原理"></a>KNN算法原理</h1><p>KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别：</p><p>假设X_test为未标记的数据样本，X_train为已标记类别的样本，算法原理伪代码如下：</p><ul><li>遍历X_train中所有样本，计算每个样本与X_test的距离，并保存在Distance数组中</li><li>对Distance数组进行排序，取距离最近的k个点，记为X_knn</li><li>在X_knn中统计每个类别的个数</li><li>代表记得样本的类别，就是在X_knn中样本最多的类别</li></ul><h2 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h2><p><strong>优点：</strong>准确性高，对异常值和噪声有较高的容忍度</p><p><strong>缺点：</strong>计算量大，对内存的需求也较大</p><h2 id="算法参数（k）"><a href="#算法参数（k）" class="headerlink" title="算法参数（k）"></a>算法参数（k）</h2><p><strong>k越大：模型偏差越大，对噪声越不敏感。过大是造成欠拟合</strong></p><p><strong>k越小：模型的方差就会越大。太小是会造成过拟合</strong></p><h2 id="算法的变种"><a href="#算法的变种" class="headerlink" title="算法的变种"></a>算法的变种</h2><p><strong>增加邻居的权重：</strong>默认情况下X_knn的权重相等，我们可以指定算法的<code>weights</code>参数调整成距离越近权重越大</p><p><strong>使用一定半径内的点取代距离最近的kk个点</strong>，<code>RadiusNeighborsClassifier</code>类实现了这个算法</p><h1 id="使用KNN作分类"><a href="#使用KNN作分类" class="headerlink" title="使用KNN作分类"></a>使用KNN作分类</h1><ul><li><code>sklearn.neighbors.KNeighborsClassifier</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">k = 5</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># X_test = np.array([0, 2]).reshape(1,-1)</span><br><span class="line">X_test = [[0,2]]</span><br><span class="line">y_test = clf.predict(X_test)</span><br><span class="line">neighbors = clf.kneighbors(X_test, return_distance=False)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)  # 样本</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;) # 中心点</span><br><span class="line">plt.scatter(X_test[0][0], X_test[0][1], marker=&apos;x&apos;, s=50, c=&apos;blue&apos;) # 中心点</span><br><span class="line"></span><br><span class="line">for i in neighbors[0]:</span><br><span class="line">    plt.plot([X[i][0], X_test[0][0]], [X[i][1], X_test[0][1]],&apos;k--&apos;, linewidth=0.5)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB.png" alt=""></p><h1 id="KNN回归拟合"><a href="#KNN回归拟合" class="headerlink" title="KNN回归拟合"></a>KNN回归拟合</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">import numpy as np</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">n = 50</span><br><span class="line">X = 5 * np.random.rand(n ,1)</span><br><span class="line">y = np.cos(X).ravel()</span><br><span class="line"># 添加一些噪声</span><br><span class="line">y += 0.2 * np.random.rand(n) - 0.1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = 5</span><br><span class="line">knn = KNeighborsRegressor(k)</span><br><span class="line">knn.fit(X, y)</span><br></pre></td></tr></table></figure><blockquote><p>KNeighborsRegressor(algorithm=’auto’, leaf_size=30,metric=’minkowski’,metric_params=None, n_jobs=1,n_neighbors=5, p=2,weights=’uniform’)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T = np.linspace(0,5, 500)[:, np.newaxis]</span><br><span class="line">y_pred = knn.predict(T)</span><br><span class="line">knn.score(X,y)</span><br></pre></td></tr></table></figure><blockquote><p>0.9909058023770559</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">plt.scatter(X, y, label=&apos;data&apos;, s=10)</span><br><span class="line">plt.scatter(T, y_pred, label=&apos;prediction&apos;, lw=4, s=0.1)</span><br><span class="line">plt.axis(&apos;tight&apos;)</span><br><span class="line">plt.title(&quot;KNeighborsRegressor (k=%i)&quot; % k)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/kneighborsRegressor.png" alt=""></p><h1 id="糖尿病预测"><a href="#糖尿病预测" class="headerlink" title="糖尿病预测"></a>糖尿病预测</h1><p>总共有768个数据、8个特征，其中Outcome为标记值（1表示有糖尿病）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br></pre></td></tr></table></figure></p><h2 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h2><ul><li>分别使用普通KNN，加权重KNN，和指定权重的KNN分别对数据拟合计算评分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=10)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(</span><br><span class="line">n_neighbors=10, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=10, radius=500.0)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    results.append((name, model.score(X_test, y_test)))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; score:&#123;&#125;&quot;.format(results[i][0], results[i][1]))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; score:0.7207792207792207<br>name:KNN + weights; score:0.6818181818181818<br>name:Radius Neighbors; score:0.6558441558441559</p></blockquote><ul><li>此时单从得分上看，普通的KNN性能是最好的，但是我们的训练样本和测试样本是随机分配的，不同的训练集、测试集会造成不同得分。</li><li>为了消除随机样本集对得分结果可能的影响，scikit-learn提供了<code>KFold和cross_val_score()</code>函数来处理这个问题</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name , model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X, y, cv=kfold) # 这里要给模型全部的样本集</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; cross_val_score:&#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; cross_val_score:0.74865003417635<br>name:KNN + weights; cross_val_score:0.7330485304169514<br>name:Radius Neighbors; cross_val_score:0.6497265892002735</p></blockquote><h2 id="用查准率和召回率以及F1对该模型进行评估："><a href="#用查准率和召回率以及F1对该模型进行评估：" class="headerlink" title="用查准率和召回率以及F1对该模型进行评估："></a>用查准率和召回率以及F1对该模型进行评估：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(10)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;该模型查准率为：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型召回率为：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型F1_score为：&quot;, f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><blockquote><p>该模型查准率为： 0.6086956521739131<br>该模型召回率为： 0.5283018867924528<br>该模型F1_score为： 0.5656565656565657</p></blockquote><h2 id="模型的训练及分析-–-学习曲线"><a href="#模型的训练及分析-–-学习曲线" class="headerlink" title="模型的训练及分析 – 学习曲线"></a>模型的训练及分析 – 学习曲线</h2><p>下面就选择用普通KNN算法模型对数据集进行训练，并查看训练样本的拟合情况及对策测试样本的预测准确性：</p><p>输入参数：</p><blockquote><p>estimator : 你用的分类器。<br>title : 表格的标题。<br>X : 输入的feature，numpy类型<br>y : 输入的target vector<br>ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点<br>cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)<br>n_jobs : 并行的的任务数(默认1))</p></blockquote><p>输出参数：</p><blockquote><p>train_sizes_abs :训练样本数<br>train_scores:训练集上准确率<br>test_scores:交叉验证集上的准确率) </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=2)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">train_score = knn.score(X_train, y_train)</span><br><span class="line">test_score = knn.score(X_test, y_test)</span><br><span class="line">print(&apos;训练集得分：&apos;,train_score)</span><br><span class="line">print(&apos;测试集得分：&apos;,test_score)</span><br></pre></td></tr></table></figure><blockquote><p>训练集得分： 0.8517915309446255<br>测试集得分： 0.6948051948051948</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line"># from common.utils import plot_learning_curve</span><br><span class="line"></span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 10)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">         plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plot_learn_curve(KNeighborsClassifier(2),&quot;KNN score&quot;,X, y, ylim=(0.5, 1), cv=cv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%8A%E5%88%86%E6%9E%90%20%E2%80%93%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>当训练集和测试集的误差收敛但却很高时，为高偏差。<br>左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。<br>我们可以增加模型参数，比如，构建更多的特征，减小正则项。<br>此时通过增加数据量是不起作用的。</p><p>当训练集和测试集的误差之间有大的差距时，为高方差。<br>当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。<br>右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。<br>我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。</p><p>理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。</p><h2 id="特征选择及数据可视化"><a href="#特征选择及数据可视化" class="headerlink" title="特征选择及数据可视化"></a>特征选择及数据可视化</h2><p><strong>使用sklearn.feature_selection.SelectKBest选择相关性最大的两个特征</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.neighborse import KN</span><br><span class="line"></span><br><span class="line">selector = SelectKBest(k=2)</span><br><span class="line">X_new = selector.fit_transform(X,y)</span><br><span class="line">X_new[0:5] #把相关性最大的两个特征放到X_new里并查看前5个数据样本</span><br></pre></td></tr></table></figure><blockquote><p>array([[148. ,  33.6],[ 85. ,  26.6],[183. ,  23.3],[ 89. ,  28.1],[137. ,  43.1]])</p></blockquote><ul><li>使用相关性最大的两个特征，对3种不同的KNN算法进行检验</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=5)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(n_neighbors=5, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=5, radius=500.0)))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X_new, y, cv=kfold)</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name: &#123;&#125;; cross_val_score: &#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name: KNN; cross_val_score: 0.7369104579630894<br>name: KNN + weights; cross_val_score: 0.7199419002050581<br>name: Radius Neighbors; cross_val_score: 0.6510252904989747</p></blockquote><p>从输出结果来看，还是普通KNN的准确性更高，与所有特征放到一起训练的准确性差不多，这也侧面证明了SelectKNBest特征选取的准确性。</p><p>回到目标上来，我们是想看看为什么KNN不能很好的拟合训练样本。现在我们至于2个特征可以很方便的在二维坐标上画出所有的训练样本，观察这些数据分布情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plt.ylabel(&quot;BMI&quot;)</span><br><span class="line">plt.xlabel(&quot;Glucose&quot;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_new[y==0][:,0], X_new[y==0][:,1], marker=&apos;o&apos;, s=10)</span><br><span class="line">plt.scatter(X_new[y==1][:,0], X_new[y==1][:,1], marker=&apos;^&apos;, s=10)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt=""></p><p>横坐标是血糖值，纵坐标是BMI值反应身体肥胖情况。在数据密集的区域，代表糖尿病的阴性和阳性的样本几乎重叠到了一起。这样就很直观的看到，KNN在糖尿病预测的这个问题上无法达到很高的预测准确性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="KNN" scheme="http://frankblog.site/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>决策树之泰坦之灾</title>
    <link href="http://frankblog.site/2018/06/05/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E6%B3%B0%E5%9D%A6%E4%B9%8B%E7%81%BE/"/>
    <id>http://frankblog.site/2018/06/05/决策树之泰坦之灾/</id>
    <published>2018-06-05T10:17:19.653Z</published>
    <updated>2018-06-06T02:29:11.443Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/titanic.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><ul><li>筛选特征值，丢掉不需要的特征数据</li><li>对性别进行二值化处理（转换为0和1）</li><li>港口转换成数值型数据</li><li>处理缺失值（如年龄，有很多缺失值）</li></ul><h2 id="1、首先读取数据"><a href="#1、首先读取数据" class="headerlink" title="1、首先读取数据"></a>1、首先读取数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def read_dataset(fname):</span><br><span class="line">#     指定第一列作为行索引</span><br><span class="line">    data = pd.read_csv(fname, index_col=0)</span><br><span class="line">#     丢弃无用数据</span><br><span class="line">    data.drop([&apos;Name&apos;, &apos;Ticket&apos;, &apos;Cabin&apos;], axis=1, inplace=True)</span><br><span class="line">#     处理性别数据</span><br><span class="line">    lables = data[&apos;Sex&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Sex&apos;] = [*map(lambda x: lables.index(x) , data[&apos;Sex&apos;])]</span><br><span class="line">#     处理登船港口数据</span><br><span class="line">    lables = data[&apos;Embarked&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Embarked&apos;] = data[&apos;Embarked&apos;].apply(lambda n: lables.index(n))</span><br><span class="line">#     处理缺失数据填充0</span><br><span class="line">    data = data.fillna(0)</span><br><span class="line">    return data</span><br><span class="line">train = read_dataset(&apos;code/datasets/titanic/train.csv&apos;)</span><br></pre></td></tr></table></figure><h2 id="2、拆分数据集"><a href="#2、拆分数据集" class="headerlink" title="2、拆分数据集"></a>2、拆分数据集</h2><p>把<code>Survived</code>列提取出来作为标签，然后在元数据集中将其丢弃。同时拆分数据集和交叉验证数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">y = train[&apos;Survived&apos;].values</span><br><span class="line">X = train.drop([&apos;Survived&apos;], axis=1).values</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">print(&quot;X_train_shape:&quot;, X_train.shape, &quot; y_train_shape:&quot;, y_train.shape)</span><br><span class="line">print(&quot;X_test_shape:&quot;, X_test.shape,&quot;  y_test_shape:&quot;, y_test.shape)</span><br></pre></td></tr></table></figure></p><blockquote><p>X_train_shape: (712, 7)  y_train_shape: (712,)<br>X_test_shape: (179, 7)   y_test_shape: (179,)</p></blockquote><h2 id="3、拟合数据集"><a href="#3、拟合数据集" class="headerlink" title="3、拟合数据集"></a>3、拟合数据集</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(&quot;train score:&quot;, clf.score(X_train, y_train))</span><br><span class="line">print(&quot;test score:&quot;, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.9845505617977528<br>test score: 0.7597765363128491</p></blockquote><h1 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h1><h2 id="1、通过max-depth参数来优化模型"><a href="#1、通过max-depth参数来优化模型" class="headerlink" title="1、通过max_depth参数来优化模型"></a>1、通过<code>max_depth</code>参数来优化模型</h2><p>从以上输出数据可以看出，针对训练样本评分很高，但针对测试数据集评分较低。很明显这是过拟合的特征。解决决策树过拟合的方法是剪枝，包括前剪枝和后剪枝。但是<code>sklearn</code>不支持后剪枝，这里通过<code>max_depth</code>参数限定决策树深度，在一定程度上避免过拟合。</p><p>这里先创建一个函数使用不同的模型深度训练模型，并计算评分数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def cv_score(d):</span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=d)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return(clf.score(X_train, y_train), clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">scores = [cv_score(d) for d in depths]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line"># 找出交叉验证数据集评分最高的索引</span><br><span class="line">tr_best_index = np.argmax(tr_scores)</span><br><span class="line">te_best_index = np.argmax(te_scores)</span><br><span class="line"></span><br><span class="line">print(&quot;bestdepth:&quot;, te_best_index+1, &quot; bestdepth_score:&quot;, te_scores[te_best_index], &apos;\n&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>bestdepth: 5  bestdepth_score: 0.8603351955307262 </p></blockquote><p><strong>这里由于以上<code>train_test_split</code>方法对数据切分是随机打散的，造成每次用不同的数据集训练模型总得到不同的最佳深度。</strong>这里写个循环反复测试，最终验证这里看到最佳的分支深度为5出现的频率最高，初步确定5为深度模型最佳。</p><p>把模型参数和对应的评分画出来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&apos;max depth of decison tree&apos;)</span><br><span class="line">plt.ylabel(&apos;Scores&apos;)</span><br><span class="line">plt.plot(depths, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(depths, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0.jpg" alt=""></p><h2 id="2、通过min-impurity-decrease来优化模型"><a href="#2、通过min-impurity-decrease来优化模型" class="headerlink" title="2、通过min_impurity_decrease来优化模型"></a>2、通过<code>min_impurity_decrease</code>来优化模型</h2><p>这个参数用来指定信息墒或者基尼不纯度的阈值，当决策树分裂后，其信息增益低于这个阈值时则不再分裂。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line"></span><br><span class="line">def minsplit_score(val):</span><br><span class="line">    clf = DecisionTreeClassifier(criterion=&apos;gini&apos;, min_impurity_decrease=val)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return (clf.score(X_train, y_train), clf.score(X_test, y_test), )</span><br><span class="line"></span><br><span class="line"># 指定参数范围，分别训练模型并计算得分</span><br><span class="line"></span><br><span class="line">vals = np.linspace(0, 0.2, 100)</span><br><span class="line">scores = [minsplit_score(v) for v in vals]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line">bestmin_index = np.argmax(te_scores)</span><br><span class="line">bestscore = te_scores[bestmin_index]</span><br><span class="line">print(&quot;bestmin:&quot;, vals[bestmin_index])</span><br><span class="line">print(&quot;bestscore:&quot;, bestscore)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&quot;min_impurity_decrease&quot;)</span><br><span class="line">plt.ylabel(&quot;Scores&quot;)</span><br><span class="line">plt.plot(vals, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(vals, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><blockquote><p>bestmin: 0.00202020202020202<br>bestscore: 0.7988826815642458</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%A5%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B.jpg" alt=""></p><p><strong>问题：每次使用不同随机切割的数据集得出最佳参数为0.002很接近0，该怎么解读？</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">至此为我们找到了两个参数,最佳深度depth=5 和最佳min_impurity_decrease=0.002，下面我来用两个参数简历模型进行测试：</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn import metrics </span><br><span class="line"></span><br><span class="line">model = DecisionTreeClassifier(max_depth=5, min_impurity_decrease=0.002)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;tees_score:&quot;, model.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;查准率:&quot;,metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率:&quot;,metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1_score:&quot;,metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure></p><blockquote><p>tees_score: 0.7821229050279329<br>查准率: 0.8461538461538461<br>召回率: 0.5866666666666667<br>F1_score: 0.6929133858267718</p></blockquote><h1 id="模型参数选择工具包"><a href="#模型参数选择工具包" class="headerlink" title="模型参数选择工具包"></a>模型参数选择工具包</h1><p>至此发现以上两种模型优化方法有两问题：</p><ul><li><p>1、数据不稳定：–&gt; 每次重新分配训练集测试集，原参数就不是最优了。 解决办法是多次计算求平均值。</p></li><li><p>2、不能一次选择多个参数：–&gt; 想考察max_depth和min_impurity_decrease两者结合起来的最优参数就没法实现。</p></li></ul><p>所幸<code>scikit-learn</code>在<code>sklearn.model_selection</code>包提供了大量的模型选择和评估的工具供我们使用。针对该问题可以使用<code>GridSearchCV</code>类来解决。</p><h2 id="利用GridSearchCV求最优参数"><a href="#利用GridSearchCV求最优参数" class="headerlink" title="利用GridSearchCV求最优参数"></a>利用<code>GridSearchCV</code>求最优参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;:thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(&quot;best_parms:&#123;0&#125;\nbest_score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best_parms:{‘min_impurity_decrease’: 0.00816326530612245}<br>best_score:0.8114478114478114</p></blockquote><p>模型解读：<br>1、关键字参数<code>param_grid</code>是一个字典，字典的关键字对应的值是一个列表。<code>GridSearchCV</code>会枚举列表里所有值来构建模型多次计算训练模型，并计算模型评分，最终得出指定参数值的平均评分及标准差。</p><p>2、关键参数<code>sv</code>，用来指定交叉验证数据集的生成规则。这里sv=5表示每次计算都把数据集分成5份，拿其中一份作为交叉验证数据集，其他作为训练集。最终得出最优参数及最优评分保存在<code>clf.best_params_</code>和<code>clf.best_score_</code>里。</p><p>3、此外<code>clf.cv_results_</code>里保存了计算过程的所有中间结果。</p><h2 id="画出学习曲线："><a href="#画出学习曲线：" class="headerlink" title="画出学习曲线："></a>画出学习曲线：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def plot_curve(train_sizes, cv_results, xlabel):</span><br><span class="line">    train_scores_mean = cv_results[&apos;mean_train_score&apos;]</span><br><span class="line">    train_scores_std = cv_results[&apos;std_train_score&apos;]</span><br><span class="line">    test_scores_mean = cv_results[&apos;mean_test_score&apos;]</span><br><span class="line">    test_scores_std = cv_results[&apos;std_test_score&apos;]</span><br><span class="line">    plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line">    plt.title(&apos;parameters turning&apos;)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(&apos;score&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &apos;.--&apos;, color=&quot;r&quot;,label=&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, &apos;.-&apos;, color=&quot;g&quot;,</span><br><span class="line">    label=&quot;Cross-validation score&quot;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&quot;best&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line"># Set the parameters by cross-validation</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;: thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param: &#123;0&#125;\nbest score: &#123;1&#125;&quot;.format(clf.best_params_, </span><br><span class="line"> clf.best_score_))</span><br><span class="line"></span><br><span class="line"># plot_curve(thresholds, clf.cv_results_, xlabel=&apos;gini thresholds&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>best param: {‘min_impurity_decrease’: 0.00816326530612245}<br>best score: 0.8114478114478114</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%EF%BC%9A.jpg" alt=""></p><h2 id="多组参数之间选择最优参数："><a href="#多组参数之间选择最优参数：" class="headerlink" title="多组参数之间选择最优参数："></a>多组参数之间选择最优参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">entropy_thresholds = np.linspace(0, 1, 100)</span><br><span class="line">gini_thresholds = np.linspace(0, 0.2, 100)</span><br><span class="line">#设置参数矩阵：</span><br><span class="line">param_grid = [&#123;&apos;criterion&apos;: [&apos;entropy&apos;], &apos;min_impurity_decrease&apos;:entropy_thresholds&#125;,&#123;&apos;criterion&apos;: [&apos;gini&apos;], &apos;min_impurity_decrease&apos;: gini_thresholds&#125;,&#123;&apos;max_depth&apos;: np.arange(2,10)&#125;,&#123;&apos;min_samples_split&apos;: np.arange(2,30,2)&#125;]</span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param:&#123;0&#125;\nbest score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best param:{‘min_impurity_decrease’: 0.00816326530612245}<br>best score:0.8114478114478114</p></blockquote><p>结果1、{‘criterion’: ‘gini’, ‘min_impurity_decrease’: 0.00816326530612245} -&gt;6</p><p>结果2、{‘min_samples_split’: 22} -&gt;10</p><p>结果3、{‘min_samples_split’: 20} -&gt;4</p><p><strong>结果波动很大，这里做了20次测试，对应结果1出现6次，结果2出现10次，结果3出现4次。</strong></p><p><strong>代码解读</strong>：<br>关键部分还是<code>param_grid</code>参数，他是一个列表。很对列表的第一个字典，选择信息墒<code>（entropy）</code>作为判断标准，取值0～1范围50等分；</p><p>第二个字典选择基尼系数，<code>min_impurity_decrease</code>取值0～0.2范围50等分。</p><p><code>GridSearchCV</code>会针对列表中的每个字典进行迭代，最终比较列表中每个字典所对应的参数组合，选择出最优的参数。</p><h2 id="生成决策树图形"><a href="#生成决策树图形" class="headerlink" title="生成决策树图形"></a>生成决策树图形</h2><p>下面代码可以生成.dot文件，需要电脑上安装<code>graphviz</code>才能把文件转换成图片格式。</p><p><code>Mac</code>上可以使用<code>brew install graphviz</code>命令来安装，它会同时安装8个依赖包。这里一定注意<code>Mac</code>环境下的权限问题：由于<code>Homebrew</code>默认是安装在<code>/usr/local</code>下，而<code>Mac</code>有强制保护不支持<code>sudo chown -R uname local</code>对<code>local</code>文件夹进行权限修改。</p><p>这里的解决方式是把<code>local</code>下<code>bin</code>,<code>lib</code>,<code>Cellar</code>等所需单个文件夹下进行赋权，即可成功安装。</p><ol><li>在电脑上安装 graphviz</li><li>运行 <code>dot -Tpng tree.dot -o filename.png</code></li><li>在当前目录查看生成的决策树 filename.png</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier </span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier(min_samples_split=22)</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line">train_score = clf.score(X_train, y_train)</span><br><span class="line">test_score = clf.score(X_test, y_test)</span><br><span class="line">print(&apos;train score: &#123;0&#125;; test score: &#123;1&#125;&apos;.format(train_score, test_score))</span><br><span class="line"></span><br><span class="line"># 导出 titanic.dot 文件</span><br><span class="line">with open(&quot;tree.dot&quot;, &apos;w&apos;) as f:</span><br><span class="line">    f = tree.export_graphviz(clf, out_file=f)</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.8834269662921348; test score: 0.8268156424581006</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E7%BB%84%E5%8F%82%E6%95%B0%E4%B9%8B%E9%97%B4%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%8F%82%E6%95%B0%EF%BC%9A.jpg" alt=""></p><h1 id="模型调参注意事项："><a href="#模型调参注意事项：" class="headerlink" title="模型调参注意事项："></a>模型调参注意事项：</h1><ul><li>当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型</li><li>如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。</li><li>推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</li><li>在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。</li><li>决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。</li><li>如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/titanic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="http://frankblog.site/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>sklearn之数据预处理和创建模型</title>
    <link href="http://frankblog.site/2018/06/05/sklearn%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B/"/>
    <id>http://frankblog.site/2018/06/05/sklearn之数据预处理和创建模型/</id>
    <published>2018-06-05T07:43:32.158Z</published>
    <updated>2018-06-06T04:14:14.934Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"></span><br><span class="line">data = np.array([[ 3, -1.5, 2, -5.4],</span><br><span class="line">[ 0, 4, -0.3, 2.1],</span><br><span class="line">[ 1, 3.3, -1.9, -4.3]])</span><br></pre></td></tr></table></figure><h2 id="均值移除-mean-removal"><a href="#均值移除-mean-removal" class="headerlink" title="均值移除 mean removal"></a>均值移除 mean removal</h2><ul><li>“通常我们会把每个特征的平均值移除，以保证特征均值为0（即标准化处理）。这样做可以消除特征彼此间的偏差（bias）”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_standardized = preprocessing.scale(data)</span><br><span class="line">print (&quot;\nMean特征均值 =&quot;, data_standardized.mean(axis=0))</span><br><span class="line">print (&quot;Std deviation标准偏差 =&quot;, data_standardized.std(axis=0))</span><br></pre></td></tr></table></figure><blockquote><p>Mean特征均值 = [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]<br>Std deviation标准偏差 = [1. 1. 1. 1.]</p></blockquote><h2 id="范围缩放-min-max-scaling"><a href="#范围缩放-min-max-scaling" class="headerlink" title="范围缩放 min max scaling"></a>范围缩放 min max scaling</h2><ul><li>“数据点中每个特征的数值范围可能变化很大，因此，有时将特征的数值范围缩放到合理的大小是非常重要的。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data_scaled = data_scaler.fit_transform(data)</span><br><span class="line">print (&quot;\nMin max scaled data范围缩放数据:\n&quot;, data_scaled)</span><br></pre></td></tr></table></figure><blockquote><p>Min max scaled data范围缩放数据:<br> [[1.         0.         1.         0.        ]<br> [0.         1.         0.41025641 1.        ]<br> [0.33333333 0.87272727 0.         0.14666667]]</p></blockquote><h2 id="归一化-normalization"><a href="#归一化-normalization" class="headerlink" title="归一化 normalization"></a>归一化 normalization</h2><ul><li>“数据归一化用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放到相同的数值范围。机器学习中最常用的归一化形式就是将特征向量调整为L1范数，使特征向量的数值之和为1。”</li><li>“这个方法经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级，提高不同特征数据的可比性。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_normalized = preprocessing.normalize(data, norm=&apos;l1&apos;)</span><br><span class="line">print (&quot;\nL1 normalized data归一化后数据:\n&quot;, data_normalized)</span><br></pre></td></tr></table></figure><blockquote><p>L1 normalized data归一化后数据:<br> [[ 0.25210084 -0.12605042  0.16806723 -0.45378151]<br> [ 0.          0.625      -0.046875    0.328125  ]<br> [ 0.0952381   0.31428571 -0.18095238 -0.40952381]]</p></blockquote><h2 id="二值化-binarization"><a href="#二值化-binarization" class="headerlink" title="二值化 binarization"></a>二值化 binarization</h2><ul><li>“二值化用于将数值特征向量转换为布尔类型向量。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)</span><br><span class="line">print (&quot;\n二值化 data:\n&quot;, data_binarized)</span><br></pre></td></tr></table></figure><blockquote><p>二值化 data:<br> [[1. 0. 1. 0.]<br> [0. 1. 0. 1.]<br> [0. 1. 0. 0.]]</p></blockquote><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><ul><li><p>one hot encoding独热编码<br>“通常，需要处理的数值都是稀疏地、散乱地分布在空间中，然而，我们并不需要存储这些大数值，这时就需要使用独热编码（One-Hot Encoding）。可以把独热编码看作是一种收紧 （tighten）特征向量的工具。它把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k 的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，这样可以更加有效地表示空间。例如，我们需要处理4维向量空间，当给一个特性向量的第n 个特征进行编码时，编码器会遍历每个特征向量的第n 个特征，然后进行非重复计数。如果非重复计数的值是K ，那么就把这个特征转换为只有一个值是1其他值都是0的K 维向量。”</p></li><li><p>“在下面的示例中，观察一下每个特征向量的第三个特征，分别是1 、5 、2 、4 这4个不重复的值，也就是说独热编码向量的长度是4。如果你需要对5 进行编码，那么向量就是[0, 1, 0, 0] 。向量中只有一个值是1。第二个元素是1，对应的值是5 。”</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = preprocessing.OneHotEncoder()</span><br><span class="line">encoder.fit([[0, 2, 1, 12], [1, 3, 5, 3], [2, 3, 2, 12], [1, 2, 4, 3]])</span><br><span class="line">encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()</span><br><span class="line">print (&quot;\n编码矢量:\n&quot;, encoded_vector)</span><br></pre></td></tr></table></figure><blockquote><p>编码矢量:<br> [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]</p></blockquote><h2 id="标记编码方法"><a href="#标记编码方法" class="headerlink" title="标记编码方法"></a>标记编码方法</h2><p>在监督学习中，经常需要处理各种各样的标记。这些标记可能是数字，也可能是单词。如果标记是数字，那么算法可以直接使用它们，但是，许多情况下，标记都需要以人们可理解的形式存在，因此，人们通常会用单词标记训练数据集。标记编码就是要把单词标记转换成数值形式，让算法懂得如何操作标记。接下来看看如何标记编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line"># 定义一个标记编码器</span><br><span class="line">label_encoder = preprocessing.LabelEncoder()</span><br><span class="line"></span><br><span class="line"># label_encoder对象知道如何理解单词标记，接下来创建标记</span><br><span class="line">input_classes = [&apos;audi&apos;, &apos;ford&apos;, &apos;audi&apos;, &apos;toyota&apos;, &apos;ford&apos;, &apos;bmw&apos;]</span><br><span class="line"># 开始标记</span><br><span class="line">label_encoder.fit(input_classes)</span><br><span class="line">print(&quot;Classes mapping: 结果显示单词背转换成从0开始的索引值&quot;)</span><br><span class="line">for i, item in enumerate(lable_encoder.classes_):</span><br><span class="line">print(item, &apos;--&gt;&apos;, i)</span><br></pre></td></tr></table></figure><blockquote><p>Classes mapping: 结果显示单词背转换成从0开始的索引值<br>audi —&gt; 0<br>bmw —&gt; 1<br>ford —&gt; 2<br>toyota —&gt; 3</p></blockquote><p>这时，如果遇到一组数据就可以轻松的转换它们了。（如药品数据的药品名）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">labels = [&apos;toyota&apos;, &apos;f</span><br><span class="line">ord&apos;, &apos;audi&apos;]</span><br><span class="line">encoded_labels = label_encoder.transform(labels)</span><br><span class="line">print (&quot;\nLabels =&quot;, labels)</span><br><span class="line">print (&quot;Encoded labels =&quot;, list(encoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>Labels = [‘toyota’, ‘ford’, ‘audi’]<br>Encoded labels = [3, 2, 0]</p></blockquote><p>还可以数字反转回单词（或字符串）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoded_labels = [2,1,0,3,1]</span><br><span class="line">decoded_labels = label_encoder.inverse_transform(encoded_labels)</span><br><span class="line">print(encoded_labels)</span><br><span class="line">print(list(decoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>[2, 1, 0, 3, 1]<br>[‘ford’, ‘bmw’, ‘audi’, ‘toyota’, ‘bmw’]</p></blockquote><h1 id="创建线性回归"><a href="#创建线性回归" class="headerlink" title="创建线性回归"></a>创建线性回归</h1><p>回归是估计输入数据与连续值输出数据之间关系的过程。数据通常是实数形式的，我们的目标是<strong>估计满足输入到输出映射关系的基本函数。</strong></p><p>线性回归的目标是提取输入变量与输出变量的关联线性模型，这就要求实际输出与线性方程预测的输出的残差平方和（sum of squares of differences）最小化。这种方法被称为普通最小二乘法 （Ordinary Least Squares，OLS）。</p><p>你可能觉得用一条曲线对这些点进行拟合效果会更好，但是线性回归不允许这样做。线性回归的主要优点就是方程简单。如果你想用非线性回归，可能会得到更准确的模型，但是拟合速度会慢很多。线性回归模型就像前面那张图里显示的，<strong>用一条直线近似数据点的趋势</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line"># 加载数据</span><br><span class="line">filename = sys.argv[1]</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_singlevar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import linear_model</span><br><span class="line">linear_regr = linear_model.LinearRegression()</span><br><span class="line">linear_regr.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>我们利用训练数据集训练了线性回归器。向fit 方法提供输入数据即可训练模型。用下面的代码看看它如何拟合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(&apos;训练集拟合效果&apos;)</span><br><span class="line">y_train_pred = linear_regr.predict(X_train)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X_train, y_train)</span><br><span class="line">plt.plot(X_train, y_train_pred, color=&apos;green&apos;, linewidth=2)</span><br><span class="line">plt.title(&apos;Training Data&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%861.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = linear_regr.predict(X_test)</span><br><span class="line">print(&quot;测试集拟合效果&quot;)</span><br><span class="line">plt.scatter(X_test, y_test)</span><br><span class="line">plt.plot(X_test, y_test_pred, color=&apos;green&apos;)</span><br><span class="line">plt.title(&quot;Test Data&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%862.png" alt=""></p><h1 id="计算回归准确性"><a href="#计算回归准确性" class="headerlink" title="计算回归准确性"></a>计算回归准确性</h1><p>现在已经建立了回归器，接下来最重要的就是如何评价回归器的拟合效果。在模型评价的相关内容中，用误差 （error）表示实际值与模型预测值之间的差值。</p><p>下面快速了解几个衡量回归器拟合效果的重要指标（metric）。回归器可以用许多不同的指标进行衡量，部分指标如下所示。</p><ul><li><p><strong>平均绝对误差（mean absolute error）</strong> ：这是给定数据集的所有数据点的绝对误差平均值。</p></li><li><p><strong>均方误差（mean squared error）</strong> ：这是给定数据集的所有数据点的误差的平方的平均值。这是最流行的指标之一。</p></li><li><p><strong>中位数绝对误差（median absolute error）</strong> ：这是给定数据集的所有数据点的误差的中位数。这个指标的主要优点是可以消除异常值（outlier）的干扰。测试数据集中的单个坏点不会影响整个误差指标，均值误差指标会受到异常点的影响。</p></li><li><p><strong>解释方差分（explained variance score）</strong> ：这个分数用于衡量我们的模型对数据集波动的解释能力。如果得分1.0分，那么表明我们的模型是完美的。</p></li><li><p><strong>R方得分（R2 score）</strong> ：这个指标读作“R方”，是指确定性相关系数，用于衡量模型对未知样本预测的效果。最好的得分是1.0，值也可以是负数。</p></li></ul><p>“每个指标都描述得面面俱到是非常乏味的，因此只选择一两个指标来评估我们的模型。通常的做法是尽量保证均方误差最低，而且解释方差分最高”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 0.54<br>均方误差（mean squared error） ： 0.38<br>中位数绝对误差（median absolute error） ： 0.54<br>解释方差分（explained variance score） ： 0.68<br>R方得分（R2 score） ： 1.0</p></blockquote><h1 id="保存模型数据"><a href="#保存模型数据" class="headerlink" title="保存模型数据"></a>保存模型数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">regr = pickle.dumps(linear_regr) # 保存</span><br><span class="line">regr1 = pickle.loads(regr) # 加载</span><br><span class="line">regr1.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p></blockquote><p>在scikit的具体情况下，使用 joblib 替换 pickle（ joblib.dump &amp; joblib.load ）可能会更有趣，这对大数据更有效，但只能序列化 (pickle) 到磁盘而不是字符串变量:</p><p>之后，您可以加载已保存的模型（可能在另一个 Python 进程中）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.externals import joblib</span><br><span class="line">joblib.dump(linear_regr, &apos;regr.pkl&apos;) </span><br><span class="line">regr2 = joblib.load(&apos;regr.pkl&apos;) </span><br><span class="line">regr2.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p><h1 id="创建岭回归"><a href="#创建岭回归" class="headerlink" title="创建岭回归"></a>创建岭回归</h1></blockquote><p>线性回归的主要问题是对异常值敏感。在真实世界的数据收集过程中，经常会遇到错误的度量结果。而线性回归使用的普通最小二乘法，其目标是使平方误差最小化。这时，由于异常值误差的绝对值很大，因此会引起问题，从而破坏整个模型。</p><p>普通最小二乘法在建模时会考虑每个数据点的影响，因此，最终模型就会瘦异常值影响较大。显然，我们发现这个模型不是最优的。为了避免这个问题，我们引入正则化项 的系数作为阈值来消除异常值的影响。这个方法被称为岭回归 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_multivar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,3)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 3)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p>alpha 参数控制回归器的复杂程度。当alpha 趋于0 时，岭回归器就是用普通最小二乘法的线性回归器。因此，如果你希望模型对异常值不那么敏感，就需要设置一个较大的alpha 值。这里把alpha 值设置为0.01 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">rid = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)</span><br><span class="line"></span><br><span class="line">rid.fit(X_train, y_train)</span><br><span class="line">y_test_pred = rid.predict(X_test)</span><br><span class="line"></span><br><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 3.95<br>均方误差（mean squared error） ： 23.15<br>中位数绝对误差（median absolute error） ： 3.69<br>解释方差分（explained variance score） ： 0.84<br>R方得分（R2 score） ： 1.0</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Line</span><br><span class="line">line = Line(&quot;期望值测试对比&quot;)</span><br><span class="line">line.add(&apos;测试目标值&apos;, np.linspace(-20,40,len(y_test)), y_test, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line.add(&apos;实际测试值&apos;, np.linspace(-20,40,len(y_test)),  y_test_pred, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line</span><br><span class="line"></span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%9B%E5%BB%BA%E5%B2%AD%E5%9B%9E%E5%BD%92.png" alt=""></p><h1 id="创建多项式回归器（重点）"><a href="#创建多项式回归器（重点）" class="headerlink" title="创建多项式回归器（重点）"></a>创建多项式回归器（重点）</h1><p>数据点本身的模式中带有自然的曲线，而线性模型是不能捕捉到这一点的。多项式回归模型的曲率是由多项式的次数决定的。随着模型曲率的增加，模型变得更准确。但是，增加曲率的同时也增加了模型的复杂性，因此拟合速度会变慢。当我们对模型的准确性的理想追求与计算能力限制的残酷现实发生冲突时，就需要综合考虑了。</p><p>下面使用岭回归的数据，注意和简单线性回归的区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">#将曲线的多项式次数初始值设置为3</span><br><span class="line">poly = PolynomialFeatures(degree = 20)</span><br><span class="line"># “其中，X_train_transformed 表示多项式形式的输入，与线性回归模型是一样的。”</span><br><span class="line">X_train_transformed = poly.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line">#测试一下</span><br><span class="line">dp = X_train[0].reshape(1,-1)</span><br><span class="line">poly_dp = poly.fit_transform(dp)</span><br><span class="line"></span><br><span class="line">poly_liner = linear_model.LinearRegression()</span><br><span class="line">poly_liner.fit(X_train_transformed, y_train) #这里注意输入转换后的X_train</span><br><span class="line"></span><br><span class="line">print (&quot;\nLinear regression:&quot;, rid.predict(dp)[0])</span><br><span class="line">print (&quot;\nPolynomial regression:&quot;, poly_liner.predict(poly_dp)[0]) ##这输入转换后的X_test</span><br></pre></td></tr></table></figure><blockquote><p>Linear regression: -11.058646635286552<br>Polynomial regression: -8.070076359128953</p></blockquote><ul><li>多项式次数为1时 返回预测结果为：-11.058729498335897，欠拟合</li><li>多项式次数为10时 返回预测结果为：-8.206005341193759，这里与真实值-8.07已经非常接近了</li><li>多项式次数为20时 返回预测结果为：-8.070076359128953，针对这个值的预测最完美</li><li>多项式次数为100时 返回预测结果为：10.01397529328105，说明出现过拟合</li></ul><h1 id="AdaBoost算法估算房屋价格"><a href="#AdaBoost算法估算房屋价格" class="headerlink" title="AdaBoost算法估算房屋价格"></a>AdaBoost算法估算房屋价格</h1><p><strong>利用AdaBoost算法的决策树回归器<code>（decision tree regreessor）</code>来估算房屋价格</strong></p><p>决策树是一个树状模型，每个节点都做出一个决策，从而影响最终结果。叶子节点表示输出数值，分支表示根据输入特征做出的中间决策。<code>AdaBoost</code>算法是指自适应增强（<code>adaptive boosting</code>）算法，这是一种利用其他系统增强模型准确性的技术。这种技术是将不同版本的算法结果进行组合，用加权汇总的方式获得最终结果，被称为弱学习器 （<code>weak learners</code>）。<code>AdaBoost</code>算法在每个阶段获取的信息都会反馈到模型中，这样学习器就可以在后一阶段重点训练难以分类的样本。这种学习方式可以增强系统的准确性。</p><p>首先使用<code>AdaBoost</code>算法对数据集进行回归拟合，再计算误差，然后根据误差评估结果，用同样的数据集重新拟合。可以把这些看作是回归器的调优过程，直到达到预期的准确性。假设你拥有一个包含影响房价的各种参数的数据集，我们的目标就是估计这些参数与房价的关系，这样就可以根据未知参数估计房价了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">from sklearn.ensemble import AdaBoostRegressor</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.metrics import mean_squared_error, explained_variance_score</span><br><span class="line">from sklearn.utils import shuffle</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">hous_data = datasets.load_boston()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 利用shuffle函数把数据的顺序打乱（参数random_state用来控制如何打乱数据）</span><br><span class="line">X, y = shuffle(hous_data.data, hous_data.target, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.8 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line"># 选择最大深度为5的决策树回归模型</span><br><span class="line">dtre = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 再用带AdaBoost算法的决策树回归模型进行拟合与上面进行比较</span><br><span class="line">abre = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=400, random_state=7)</span><br><span class="line">abre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">#  看看AdaBoost算法对决策树回归器的训练效果有多大改善</span><br><span class="line">y_pred_dt = dtre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_dt)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_dt)</span><br><span class="line">print(&quot;决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;决策树-解释方差: &quot;, evs)</span><br><span class="line"></span><br><span class="line">y_pred_ab = abre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_ab)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_ab)</span><br><span class="line">print(&quot;\nAbaBoost决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;AbaBoost决策树-解释方差: &quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>决策树-均方误差:  12.74782456548819<br>决策树-解释方差:  0.8454595720920495<br>AbaBoost决策树-均方误差:  7.015648111222207<br>AbaBoost决策树-解释方差:  0.9147414844474588</p></blockquote><h1 id="计算特征的相对重要性-（如交通案例计算各出口贡献率）"><a href="#计算特征的相对重要性-（如交通案例计算各出口贡献率）" class="headerlink" title="计算特征的相对重要性 （如交通案例计算各出口贡献率）"></a>计算特征的相对重要性 （如交通案例计算各出口贡献率）</h1><p><strong>(_modle.feature__importances_)</strong></p><p>在这个案例中，我们用了13个特征，它们对模型都有贡献。但是，有一个重要的问题出现了：如何判断哪个特征更加重要？显然，所有的特征对结果的贡献是不一样的。如果需要忽略一些特征，就需要知道哪些特征不太重要。scikit-learn里面有这样的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def plot_feature_importances(feature_importances, title, feature_names):</span><br><span class="line"> # 将重要性值标准化</span><br><span class="line">    feature_importances = 100.0 * (feature_importances / max(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 将得分从高到低排序</span><br><span class="line">    index_sorted = np.flipud(np.argsort(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 让X坐标轴上的标签居中显示</span><br><span class="line">    pos = np.arange(index_sorted.shape[0]) + 0.5</span><br><span class="line"></span><br><span class="line"> # 画条形图</span><br><span class="line"> plt.figure()</span><br><span class="line"> plt.bar(pos, feature_importances[index_sorted], align=&apos;center&apos;)</span><br><span class="line"> plt.xticks(pos, feature_names[index_sorted])</span><br><span class="line"> plt.ylabel(&apos;Relative Importance&apos;)</span><br><span class="line"> plt.title(title)</span><br><span class="line"> plt.show()</span><br><span class="line"></span><br><span class="line"># 画出特征的相对重要性</span><br><span class="line">plot_feature_importances(dtre.feature_importances_, &apos;Decision Tree regressor&apos;, hous_data.feature_names)</span><br><span class="line">plot_feature_importances(abre.feature_importances_, &apos;AdaBoost regressor&apos;, hous_data.feature_names)</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A71.png" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A72.png" alt=""></p><p>上图可以看出不带AbaBoost的决策树回归器显示最重要的特征是RM，而带AbaBoost算法的决策回归器现实的最主要特征是LASTAT。现实生活中如果对这个数据集建立不同的回归器会发现最重要的特征就是LSTAT，这足以体现AbaBoost算法对决策树训练效果的改善。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;影响房价的因素分析&quot;)</span><br><span class="line">pie.add(&quot;决策树回归器&quot;, hous_data.feature_names, dtre.feature_importances_, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[35, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie.add(&quot;AbaBoost决策树&quot;, hous_data.feature_names, abre.feature_importances_,is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[75, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A73.png" alt=""></p><h1 id="随机森林评估共享单车的需求分布"><a href="#随机森林评估共享单车的需求分布" class="headerlink" title="随机森林评估共享单车的需求分布"></a>随机森林评估共享单车的需求分布</h1><p><strong>采用随机森林回归器<code>(random forest regressor)</code>估计输出结果。</strong></p><p>随机森林死一个决策树合集，它基本上就是用一组由数据集的若干子集构建的决策树构成，再用决策树平均值改善整体学习效果</p><p>我们将使用bike_day.csv文件中的数据集，它可以在 <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a> 获取。这份数据集一共16列，前两列是序列号与日期，分析的时候可以不用；最后三列数据是不同类型的输出结果；最后一列是第十四列与第十五列的和，因此建立模型时可以不考虑第十四列与第十五列。</p><p>参数<code>n_estimators</code>是指评估器<code>（estimator）</code>的数量，表示随机森林需要使用的决策树数量；<br>参数<code>max_depth</code> 是指每个决策树的最大深度；参数<code>min_samples_split</code>是指决策树分裂一个节点需要用到的最小数据样本量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from housing import plot_feature_importances   #这个方法源码参考上例</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&apos;bike_day.csv&apos;,sep=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">X = data[data.columns[2:13]]</span><br><span class="line">y = data[data.columns[-1]]</span><br><span class="line">f_name = X.columns</span><br><span class="line"></span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line">rf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=12)</span><br><span class="line">rf_regr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = rf_regr.predict(X_test)</span><br><span class="line"></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;随机森林回归器效果：&quot;)</span><br><span class="line">print(&quot;均方误差：&quot;, round(mse, 2))</span><br><span class="line">print(&quot;解释方差分：&quot;, round(evs, 2))</span><br></pre></td></tr></table></figure></p><blockquote><p>随机森林回归器效果：<br>均方误差： 368026.24<br>解释方差分： 0.89</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A6.png" alt=""></p><p><strong>利用按小时的数据计算相关性</strong></p><p>这里要用到3～14列<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(X_train)</span><br></pre></td></tr></table></figure></p><blockquote><p>15641</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hour_data = pd.read_csv(&apos;bike_hour.csv&apos;, sep=&apos;,&apos;)</span><br><span class="line">X = hour_data[hour_data.columns[2:14]]</span><br><span class="line">y = hour_data[hour_data.columns[-1]]</span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9*len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line">f_names = X.columns</span><br><span class="line"></span><br><span class="line">hrf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=10)</span><br><span class="line">hrf_regr.fit(X_train, y_train)</span><br><span class="line">y_pred = hrf_regr.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差：&quot;, mse)</span><br><span class="line">print(&quot;解释方差分：&quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>均方误差： 1884.1767363623571<br>解释方差分： 0.9414038595964176</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">attr = f_names</span><br><span class="line">v1 = hrf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, </span><br><span class="line">legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A62.png" alt=""></p><p>由图可见，其中最重要的特征是一天中的不同时间点（hr），其次重要的是温度，这完全符合人们的直觉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="sklearn" scheme="http://frankblog.site/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之逻辑回归</title>
    <link href="http://frankblog.site/2018/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://frankblog.site/2018/06/04/机器学习之逻辑回归/</id>
    <published>2018-06-04T06:31:16.158Z</published>
    <updated>2018-06-08T04:10:19.100Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="逻辑回归基础"><a href="#逻辑回归基础" class="headerlink" title="逻辑回归基础"></a>逻辑回归基础</h1><h2 id="Sigmoid预测函数"><a href="#Sigmoid预测函数" class="headerlink" title="Sigmoid预测函数"></a>Sigmoid预测函数</h2><p>在逻辑回归中，定义预测函数为：</p><script type="math/tex; mode=display">h_\theta (x) = g(z)</script><p>其中，\(z=\theta^Tx\)是<strong>分类边界</strong>，且\(g(z)=\frac{1}{1+e^{-z}}\)<br>g(z)称之为 Sigmoid Function，亦称 Logic Function，其函数图像如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/sigmoid%E5%87%BD%E6%95%B0.jpg" alt=""></p><p>可以看到，预测函数 hθ(x)被很好地限制在了 0、1 之间，并且，sigmoid 是一个非常好的阈值函数：阈值为 0.5，大于 0.5为 1 类，反之为 0 类。函数曲线过渡光滑自然，关于 0.5中心对称也极具美感。</p><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><ul><li><strong>线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><ul><li><strong>非线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><h2 id="预测代价函数"><a href="#预测代价函数" class="headerlink" title="预测代价函数"></a>预测代价函数</h2><p>下面两幅图中，左图这样犬牙差互的代价曲线（非凸函数）显然会使我们在做梯度下降的时候陷入迷茫，任何一个极小值都有可能被错认为最小值，但无法获得最优预测精度。但在右图的代价曲线中，就像滑梯一样，我们就很容易达到最小值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E4%B8%8E%E9%9D%9E%E5%87%B8%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>逻辑回归定义的代价函数为：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})</script><p>为保证代价函数呈凸形曲线，则定义 \(Cost(h_\theta(x^{(i)}),y^{(i)})\)：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x)),&\mbox{if $y=1$}\\-log(1-h_\theta(x)),&\mbox{if $y=0$}\end{cases}</script><p>该函数等价于：</p><script type="math/tex; mode=display">\begin{align*}Cost(h_\theta(x),y) &=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \\&= (\,log\,(g(X\theta))^Ty+(\,log\,(1-g(X\theta))^T(1-y)\end{align*}</script><p>代价函数随预测值 hθ(x)hθ(x) 的变化如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.png" alt=""> </p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.png" alt=""></p><h2 id="手推LR"><a href="#手推LR" class="headerlink" title="手推LR"></a>手推LR</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR1.png" alt="link"></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR2.png" alt="link"></p><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</p><p>如下例所示，我们将 θ3 及 θ4 减小（惩罚）到趋近于 00，原本过拟合的曲线就变得更加平滑，趋近于一条二次曲线（在本例中，二次曲线显然更能反映住房面积和房价的关系），也就能够更好的根据住房面积来预测房价。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98.png" alt="link"></p><p>直观来看，如果我们想解决这个例子中的过拟合问题，最好能将的影响消除，也就是让。假设我们对进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大惩罚项，例如：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%982.png" alt=""></p><p>这样在最小化Cost函数的时候，。正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%983.png" alt=""></p><p>lambda是正则项系数：</p><p>1.如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；</p><p>2.如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。</p><p>正则化后的梯度下降中θ的更新变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%984.png" alt=""></p><p>正则化后的线性回归的Normal Equation的公式为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%985.png" alt=""></p><hr><h1 id="scikit-learn-逻辑回归类"><a href="#scikit-learn-逻辑回归类" class="headerlink" title="scikit-learn 逻辑回归类"></a>scikit-learn 逻辑回归类</h1><h2 id="带L1与L2正则的逻辑回归损失函数"><a href="#带L1与L2正则的逻辑回归损失函数" class="headerlink" title="带L1与L2正则的逻辑回归损失函数"></a>带L1与L2正则的逻辑回归损失函数</h2><p>scikit-learn在<code>LogisticRegression</code>的<code>sklearn.linear_model.LogisticRegression</code>类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p><p>作为优化问题，带 L2 正则的二分类 logistic 回归要最小化以下代价函数（cost function）：</p><script type="math/tex; mode=display">\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p><script type="math/tex; mode=display">\underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p> L1 范数作为正则项由以下几个用途：</p><ul><li>特征选择： 它会让模型参数向量里的元素为0的点尽量多。 因此可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以 L1 范数解决过拟合措施实际上是减少特征数量。</li><li>可解释性： 模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。 这样我们就容易解释模型的因果关系。 比如针对某个癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性成关键作用。 稀疏化后，只留下几个关键特征，就更容易看到因果关系</li></ul><p>由此可见， L1 范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。 大部分情况下，我们解决过拟合问题，还是选择 L2 单数作为正则项， 这也是 sklearn 里的默认值。</p><h2 id="优化方法参数"><a href="#优化方法参数" class="headerlink" title="优化方法参数"></a>优化方法参数</h2><p>solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</p><ul><li>liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li><li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li><li>saga：线性收敛的随机优化算法的的变重。</li></ul><div class="table-container"><table><thead><tr><th>Case</th><th>Solver</th></tr></thead><tbody><tr><td>L1正则</td><td>“liblinear” or “saga”</td></tr><tr><td>多项式损失（multinomial loss）</td><td>“lbfgs”, “sag”, “saga” or “newton-cg”</td></tr><tr><td>大数据集（n_samples）</td><td>“sag” or “saga”</td></tr></tbody></table></div><p>“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear”</p><h1 id="乳腺癌检测"><a href="#乳腺癌检测" class="headerlink" title="乳腺癌检测"></a>乳腺癌检测</h1><p>使用逻辑回归算法解决乳腺癌检测问题。 我们需要先采集肿瘤病灶造影图片， 然后对图片进行分析， 从图片中提取特征， 在根据特征来训练模型。 最终使用模型来检测新采集到的肿瘤病灶造影， 判断是良性还是恶性。 这个是典型的二元分类问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 加载数据</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape,&apos;\n&apos;, X[0], &apos;\n&apos;, y[0])</span><br></pre></td></tr></table></figure></p><blockquote><p>(569, 30) (569,)<br> [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01<br> 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02<br> 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01<br> 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01<br> 4.601e-01 1.189e-01]<br> 0</p></blockquote><p>实际上它只关注了 10 个特征，然后又构造出来每个特征的标准差及最大值，这样每个特征又衍生出了两个特征，所以共有30个特征。</p><p>疑问： 该方式是否直接会导致多重共线性的出现？</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">train_score = model.score(X_train, y_train)</span><br><span class="line">test_score = model.score(X_test, y_test)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">print(&quot;train_score:&quot;, train_score)</span><br><span class="line">print(&quot;test_score:&quot;, test_score)</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9626373626373627<br>test_score: 0.9473684210526315</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">from sklearn.metrics import precision_score, recall_score, f1_score</span><br><span class="line">print(&quot;查准率：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1Score：&quot;, f1_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">print(np.equal(y_pred, y_test).shape[0], y_test.shape[0]) # 输出预测匹配成功数量和测试样本的数量</span><br></pre></td></tr></table></figure><blockquote><p>查准率： 0.9358974358974359<br> 召回率： 0.9733333333333334<br>F1Score： 0.954248366013072<br>114 114</p></blockquote><p>这里数量上显示全部都预测正确，而test_score却不是1，是因为sklearn不是使用这个数据来计算得分，因为这个数据不能完全反映误差情况，而是使用预测概率来计算模型得分。</p><h2 id="查看预测自信度"><a href="#查看预测自信度" class="headerlink" title="查看预测自信度"></a>查看预测自信度</h2><p>二元分类模型会针对每个样本输出的两个概率，即0和1的概率，哪个概率高就预测器哪个类别。我们可以找出针对测试数据集，模型预测的“自信度”低于90%的样本。我们先计算出测试数据集里每个样本的预测概率数据，针对每个样本会有两个数据：一个预测为0，一个预测为1。结合找出预测为阴性和阳性的概率大于0.1的样本。我们可以看下概率数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 计算每个测试样本的预测概率：</span><br><span class="line"></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">print(&quot;自信度示例：&quot;,y_pred_proba[0])</span><br></pre></td></tr></table></figure><blockquote><p>自信度示例： [0.00452578 0.99547422]</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba_0 = y_pred_proba[:, 0] &gt; 0.1</span><br><span class="line">result = y_pred_proba[y_pred_proba_0]</span><br><span class="line"></span><br><span class="line">y_pred_proba_1 = result[:, 1] &gt; 0.1</span><br><span class="line">print(result[y_pred_proba_1])</span><br></pre></td></tr></table></figure><blockquote><p>[[0.11338788 0.88661212]<br> [0.18245824 0.81754176]<br> [0.13110396 0.86889604]<br> [0.35245276 0.64754724]<br> [0.30664405 0.69335595]<br> [0.24931118 0.75068882]<br> [0.8350464  0.1649536 ]<br> [0.44807883 0.55192117]<br> [0.74071324 0.25928676]<br> [0.43085792 0.56914208]<br> [0.13388416 0.86611584]<br> [0.33507985 0.66492015]<br> [0.53672412 0.46327588]<br> [0.11422612 0.88577388]<br> [0.42946531 0.57053469]<br> [0.69759146 0.30240854]<br> [0.25982004 0.74017996]<br> [0.12179042 0.87820958]<br> [0.88546887 0.11453113]]</p></blockquote><h2 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#这里使用Pipeline来增加多项式特征</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">def poly_model(degree=2, penalty=penalty):</span><br><span class="line">    poly_features = PolynomialFeatures(degree=degree, include_bias=False)</span><br><span class="line">    log_regr = LogisticRegression(penalty=penalty) # 注意这里是L1而不是11，指的是使用L1范式作为其正则项</span><br><span class="line">    pipeline = Pipeline([(&quot;poly_features&quot;,poly_features),(&quot;log_regr&quot;,log_regr)])</span><br><span class="line">    return pipeline</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 接着增加二阶多项式特征，创建并训练模型</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">model = poly_model(degree=2, penalty=&apos;l1&apos;)</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;train_score:&quot;,model.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score:&quot;,model.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9934065934065934<br>test_score: 0.9649122807017544</p></blockquote><p>这里要注意的是使用L1范式作为其正则项，参数为<code>penalty=l1</code>。L1范数作为其正则项，可以实现参数的稀疏化，即自动帮我买选择出哪些对模型有关联的特征。我买可以观察下有多少个特征没有被丢弃即对应的模型参数θj非0：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_regr = model.named_steps[&apos;log_regr&apos;]</span><br><span class="line">print(&quot;特征总量：&quot;,log_regr.coef_.shape[1])</span><br><span class="line">print(&quot;特征保留量：&quot;, np.count_nonzero(log_regr.coef_))</span><br></pre></td></tr></table></figure><blockquote><p>特征总量： 495<br>特征保留量： 114</p></blockquote><p>逻辑回归模型的<code>coef_</code>属性里保存的就是模型参数。 从输出结果看，增加二阶多项式特征后，输入特征由原来的30个增加到了595个，在L1范数的“惩罚”下最终只保留了92个有效特征</p><h2 id="实验：利用决策树画出原始数据对预测相关性非0对特征"><a href="#实验：利用决策树画出原始数据对预测相关性非0对特征" class="headerlink" title="实验：利用决策树画出原始数据对预测相关性非0对特征"></a>实验：利用决策树画出原始数据对预测相关性非0对特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">dtmodel = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtmodel.fit(X_train, y_train)</span><br><span class="line">print(&quot;train_score&quot;, dtmodel.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score&quot;, dtmodel.score(X_test, y_test))</span><br><span class="line">from pyecharts import Bar</span><br><span class="line">index = np.nonzero(dtmodel.feature_importances_)</span><br><span class="line">bar = Bar()</span><br><span class="line">bar.add(&quot;&quot;, cancer.feature_names[index],dtmodel.feature_importances_[index])</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><blockquote><p>train_score 0.9910875596851206<br>test_score 0.6296416546416548</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%88%A9%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%BB%E5%87%BA%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%AF%B9%E9%A2%84%E6%B5%8B%E7%9B%B8%E5%85%B3%E6%80%A7%E9%9D%9E0%E5%AF%B9%E7%89%B9%E5%BE%81.png" alt=""></p><h2 id="评估模型：画出学习曲线"><a href="#评估模型：画出学习曲线" class="headerlink" title="评估模型：画出学习曲线"></a>评估模型：画出学习曲线</h2><p>首先画出L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 5)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L1&quot;,&quot;degree:2 penalty=L1&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l1&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i], penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>L2范数作为正则项画出对应一阶和二阶多项式学习曲线<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L2&quot;,&quot;degree:2 penalty=L2&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l2&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i],penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF1.png" alt=""></p><p>从上面两个图可以看出，使用二阶多项式并使用L1范数作为正则项的模型最优，训练样本评分最高，交叉验证样本评分最高。<br>训练样本评分和交叉验证样本评分之间的间隙还比较大，这说明可以通过采集更多数据来训练模型，以便进一步优化模型.</p><p>通过时间消耗对比上可以看出利用L1范式作为正则项需要花费的时间更多，是因为<code>sklearn</code>的<code>learning_curve()</code>函数在画学习曲线的过程中要对模型进行多次训练，并计算交叉验证样本评分。同时为了让曲线更平滑，针对每个点还会进行多次计算球平均值。这个就是<code>ShufferSplit</code>类的作用。在这个实例里只有569个样本是很小的数据集。如果数据集增加100倍，拿出来画学习曲线将是场灾难。</p><p>问题是针对大数据集，怎么画学习曲线？</p><p>思路一：可以考虑从大数据集选取一小部分数据来画学习曲线，待选择好最优的模型之后，在使用全部的数据来训练模型。这时需要警惕的是，尽量保证选择出来的这部分数据的<strong>标签分布与大数据集的标签分布相同</strong>，如针对二元分类，<strong>阳性和阴性比例要一致！</strong></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1-LR中损失函数的意义是什么？"><a href="#1-LR中损失函数的意义是什么？" class="headerlink" title="1.LR中损失函数的意义是什么？"></a>1.LR中损失函数的意义是什么？</h2><p>在LR中，最大似然函数与最小化对数损失函数等价</p><h2 id="2-LR与线性回归的联系和区别"><a href="#2-LR与线性回归的联系和区别" class="headerlink" title="2. LR与线性回归的联系和区别"></a>2. LR与线性回归的联系和区别</h2><p>逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</p><h2 id="3-LR与最大熵模型"><a href="#3-LR与最大熵模型" class="headerlink" title="3.LR与最大熵模型"></a>3.LR与最大熵模型</h2><p>逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。</p><ul><li><p>指数簇分布的最大熵等价于其指数形式的最大似然。</p></li><li><p>二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；</p></li><li><p>多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。</p></li></ul><h2 id="4-LR与svm"><a href="#4-LR与svm" class="headerlink" title="4.LR与svm"></a>4.LR与svm</h2><p>不同点:</p><ol><li><p>损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss</p></li><li><p>逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。</p></li><li><p>逻辑回归对概率建模，svm对分类超平面建模</p></li><li><p>逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有</p></li><li><p>逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响</p></li><li><p>逻辑回归是统计方法，svm是几何方法</p></li></ol><h2 id="5-LR与朴素贝叶斯"><a href="#5-LR与朴素贝叶斯" class="headerlink" title="5.LR与朴素贝叶斯"></a>5.LR与朴素贝叶斯</h2><ul><li><p>相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)形式跟逻辑回归是一样的。</p></li><li><p>不同的地方在于，逻辑回归为判别模型求的是p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。</p></li></ul><h2 id="6-多分类-softmax"><a href="#6-多分类-softmax" class="headerlink" title="6. 多分类-softmax"></a>6. 多分类-softmax</h2><p>如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。</p><h2 id="7-LR模型在工业界的应用"><a href="#7-LR模型在工业界的应用" class="headerlink" title="7.LR模型在工业界的应用"></a><strong>7.LR模型在工业界的应用</strong></h2><p>常见应用场景</p><ul><li><p>预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等）</p></li><li><p>分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等）</p></li></ul><p>LR适用上述场景的原因</p><p>LR模型自身的特点具备了应用广泛性</p><ul><li><p>模型易用：LR模型建模思路清晰，容易理解与掌握；</p></li><li><p>概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上；</p></li><li><p>强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性；</p></li><li><p>简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline；</p></li></ul><p>参考文献：<br><a href="https://blog.csdn.net/joycewyj/article/details/51596797" target="_blank" rel="noopener">https://blog.csdn.net/joycewyj/article/details/51596797</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://frankblog.site/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib基本操作</title>
    <link href="http://frankblog.site/2018/06/03/Matplotlib%20%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://frankblog.site/2018/06/03/Matplotlib 基本操作/</id>
    <published>2018-06-03T03:59:17.192Z</published>
    <updated>2018-06-04T01:42:15.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">一张图胜过一千句话</font></blockquote><hr><h1 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h1><h2 id="1、设置坐标轴"><a href="#1、设置坐标轴" class="headerlink" title="1、设置坐标轴"></a>1、设置坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line"></span><br><span class="line">#使用`plt.figure`定义一个图像窗口. 使用`plt.plot`画(`x` ,`y2`)曲线. 使用`plt.plot`画(`x` ,`y1`)曲线，曲线的颜色属性(`color`)为红色;曲线的宽度(`linewidth`)为1.0；曲线的类型(`linestyle`)为虚线。</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line"></span><br><span class="line">#使用`plt.xlim`设置x坐标轴范围：(-1, 2)； 使用`plt.ylim`设置y坐标轴范围：(-2, 3)； 使用`plt.xlabel`设置x坐标轴名称：’I am x’； 使用`plt.ylabel`设置y坐标轴名称：’I am y’；</span><br><span class="line"></span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">plt.xlabel(&apos;I am x&apos;)</span><br><span class="line">plt.ylabel(&apos;I am y&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`np.linspace`定义范围以及个数：范围是(-1,2);个数是5\. 使用`print`打印出新定义的范围. 使用`plt.xticks`设置x轴刻度：范围是(-1,2);个数是5.</span><br><span class="line"></span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line"></span><br><span class="line">#使用`plt.yticks`设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用`plt.show`显示图像.</span><br><span class="line"></span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[r&apos;$really\ bad$&apos;, r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;, r&apos;$really\ good$&apos;])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%BE%E7%BD%AE%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="2、调整坐标轴"><a href="#2、调整坐标轴" class="headerlink" title="2、调整坐标轴"></a>2、调整坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[&apos;$really\ bad$&apos;, &apos;$bad$&apos;, &apos;$normal$&apos;, &apos;$good$&apos;, &apos;$really\ good$&apos;])</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.xaxis.set_ticks_position`设置x坐标刻度数字或名称的位置：`bottom`.（所有位置：`top`，`bottom`，`both`，`default`，`none`）</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：x轴；使用`.set_position`设置边框位置：y=0的位置；（位置所有属性：`outward`，`axes`，`data`）</span><br><span class="line"></span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`.yaxis.set_ticks_position`设置y坐标刻度数字或名称的位置：`left`.（所有位置：`left`，`right`，`both`，`default`，`none`）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：y轴；使用`.set_position`设置边框位置：x=0的位置；（位置所有属性：`outward`，`axes`，`data`） 使用`plt.show`显示图像。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># set line syles</span><br><span class="line">l1, = plt.plot(x, y1, label=&apos;linear line&apos;)</span><br><span class="line">l2, = plt.plot(x, y2, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;, label=&apos;square line&apos;)</span><br><span class="line"></span><br><span class="line">#参数 `loc=&apos;upper right&apos;` 表示图例将添加在图中的右上角.</span><br><span class="line">plt.legend(loc=&apos;upper right&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%AE%E6%94%B9%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="3、辅助线和标识"><a href="#3、辅助线和标识" class="headerlink" title="3、辅助线和标识"></a>3、辅助线和标识</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y = 2*x + 1</span><br><span class="line"></span><br><span class="line">#挪动坐标系</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line">#辅助线</span><br><span class="line">plt.figure(num=1, figsize=(8, 5),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line">x0 = 1</span><br><span class="line">y0 = 2*x0 + 1</span><br><span class="line">plt.plot([x0, x0,], [0, y0,], &apos;k--&apos;, linewidth=2.5)</span><br><span class="line"># set dot styles</span><br><span class="line">plt.scatter([x0, ], [y0, ], s=50, color=&apos;b&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#标注，其中参数xycoords=&apos;data&apos; 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords=&apos;offset points&apos; 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置.</span><br><span class="line">plt.annotate(r&apos;$2x+1=%s$&apos; % y0, xy=(x0, y0), xycoords=&apos;data&apos;, xytext=(+30, -30),</span><br><span class="line">             textcoords=&apos;offset points&apos;, fontsize=16,</span><br><span class="line">             arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">#注释</span><br><span class="line">plt.text(-3.7, 3, r&apos;$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$&apos;,</span><br><span class="line">         fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BE%85%E5%8A%A9%E7%BA%BF%E5%92%8C%E6%A0%87%E8%AF%86.png" alt=""></p><h2 id="4、3D图框"><a href="#4、3D图框" class="headerlink" title="4、3D图框"></a>4、3D图框</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line"># X, Y value</span><br><span class="line">X = np.arange(-4, 4, 0.25)</span><br><span class="line">Y = np.arange(-4, 4, 0.25)</span><br><span class="line">X, Y = np.meshgrid(X, Y)    # x-y 平面的网格</span><br><span class="line">R = np.sqrt(X ** 2 + Y ** 2)</span><br><span class="line"># height value</span><br><span class="line">Z = np.sin(R)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/3d%E5%9B%BE.png" alt=""></p><h1 id="绘制动态图"><a href="#绘制动态图" class="headerlink" title="绘制动态图"></a>绘制动态图</h1><p>使用matplotlib为Jupyter / IPython中的动画图创建一些选项：</p><ul><li><p><strong>在循环中使用<code>display</code></strong>使用<code>IPython.display.display(fig)</code>在输出中显示图形。 使用一个循环，你需要在显示一个新数字之前清除输出。 请注意，这种技术通常不会那么流畅。 因此我会建议使用下面的任何一个。</p></li><li><p><strong><code>%matplotlib notebook</code></strong>使用IPython magic <code>%matplotlib notebook</code>将后端设置为笔记本后端。 这样可以保持图形不会显示静态PNG文件，因此也可以显示动画。 </p></li></ul><ul><li><strong><code>%matplotlib tk</code></strong>使用IPython magic <code>%matplotlib tk</code>将后端设置为tk后端。 这将在一个新的绘图窗口中打开这个图形，这是一个互动的，因此也可以显示动画。 </li></ul><ul><li><p><strong>将动画转换为mp4视频</strong> （已提供@Perfi选项）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_html5_video())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;html5&quot;</code> 。 这需要将ffmpeg视频编解码器转换为HTML5视频。 视频然后显示在内。 因此，这与<code>%matplotlib inline</code>后端兼容。 完整的例子：</p><ul><li><p><strong>将动画转换为JavaScript</strong> ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot;</code> 。 这将使用JavaScript将动画显示为HTML。 这与大多数新浏览器以及<code>%matplotlib inline</code>后端都非常兼容。 它在matplotlib 2.1或更高版本中可用。</p><h2 id="1、sin动态点曲线"><a href="#1、sin动态点曲线" class="headerlink" title="1、sin动态点曲线"></a>1、sin动态点曲线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">animation example 2</span><br><span class="line">author: Kiterun</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">x = np.linspace(0, 2*np.pi, 200)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">l = ax.plot(x, y)</span><br><span class="line">dot, = ax.plot([], [], &apos;ro&apos;)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(0, 2*np.pi)</span><br><span class="line">    ax.set_ylim(-1, 1)</span><br><span class="line">    return l</span><br><span class="line"></span><br><span class="line">def gen_dot():</span><br><span class="line">    for i in np.linspace(0, 2*np.pi, 200):</span><br><span class="line">        newdot = [i, np.sin(i)]</span><br><span class="line">        yield newdot</span><br><span class="line"></span><br><span class="line">def update_dot(newd):</span><br><span class="line">    dot.set_data(newd[0], newd[1])</span><br><span class="line">    return dot,</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update_dot, frames = gen_dot, interval = 100, init_func=init)</span><br><span class="line">ani.save(&apos;sin_dot.gif&apos;, writer=&apos;imagemagick&apos;, fps=30)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/sin_dot%20%281%29.gif" alt=""></p><h2 id="2、动态雨点"><a href="#2、动态雨点" class="headerlink" title="2、动态雨点"></a>2、动态雨点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line"># New figure with white background</span><br><span class="line">fig = plt.figure(figsize=(6,6), facecolor=&apos;white&apos;)</span><br><span class="line"></span><br><span class="line"># New axis over the whole figure, no frame and a 1:1 aspect ratio</span><br><span class="line">ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)</span><br><span class="line"></span><br><span class="line"># Number of ring</span><br><span class="line">n = 50</span><br><span class="line">size_min = 50</span><br><span class="line">size_max = 50 ** 2</span><br><span class="line"></span><br><span class="line"># Ring position</span><br><span class="line">pos = np.random.uniform(0, 1, (n,2))</span><br><span class="line"></span><br><span class="line"># Ring colors</span><br><span class="line">color = np.ones((n,4)) * (0,0,0,1)</span><br><span class="line"># Alpha color channel geos from 0(transparent) to 1(opaque)</span><br><span class="line">color[:,3] = np.linspace(0, 1, n)</span><br><span class="line"></span><br><span class="line"># Ring sizes</span><br><span class="line">size = np.linspace(size_min, size_max, n)</span><br><span class="line"></span><br><span class="line"># Scatter plot</span><br><span class="line">scat = ax.scatter(pos[:,0], pos[:,1], s=size, lw=0.5, edgecolors=color, facecolors=&apos;None&apos;)</span><br><span class="line"></span><br><span class="line"># Ensure limits are [0,1] and remove ticks</span><br><span class="line">ax.set_xlim(0, 1), ax.set_xticks([])</span><br><span class="line">ax.set_ylim(0, 1), ax.set_yticks([])</span><br><span class="line"></span><br><span class="line">def update(frame):</span><br><span class="line">    global pos, color, size</span><br><span class="line"></span><br><span class="line">    # Every ring is made more transparnt</span><br><span class="line">    color[:, 3] = np.maximum(0, color[:,3]-1.0/n)</span><br><span class="line"></span><br><span class="line">    # Each ring is made larger</span><br><span class="line">    size += (size_max - size_min) / n</span><br><span class="line"></span><br><span class="line">    # Reset specific ring</span><br><span class="line">    i = frame % 50</span><br><span class="line">    pos[i] = np.random.uniform(0, 1, 2)</span><br><span class="line">    size[i] = size_min</span><br><span class="line">    color[i, 3] = 1</span><br><span class="line"></span><br><span class="line">    # Update scatter object</span><br><span class="line">    scat.set_edgecolors(color)</span><br><span class="line">    scat.set_sizes(size)</span><br><span class="line">    scat.set_offsets(pos)</span><br><span class="line"></span><br><span class="line">    # Return the modified object</span><br><span class="line">    return scat,</span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, update, interval=10, blit=True, frames=200)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8A%A8%E6%80%81%E9%9B%A8%E7%82%B9.png" alt=""></p><h2 id="3、阻尼摆"><a href="#3、阻尼摆" class="headerlink" title="3、阻尼摆"></a>3、阻尼摆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%98%BB%E5%B0%BC%E6%91%86.png" alt=""></p><h2 id="4、内切滚动球"><a href="#4、内切滚动球" class="headerlink" title="4、内切滚动球"></a>4、内切滚动球</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/roll.gif" alt=""></p><h2 id="5、分类超平面可视化"><a href="#5、分类超平面可视化" class="headerlink" title="5、分类超平面可视化"></a>5、分类超平面可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"># 算法可视化</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">%matplotlib notebook</span><br><span class="line">import copy</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"> </span><br><span class="line">training_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]]</span><br><span class="line">w = [0, 0]</span><br><span class="line">b = 0</span><br><span class="line">history = []</span><br><span class="line"> </span><br><span class="line">def update(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    update parameters using stochastic gradient descent</span><br><span class="line">    :param item: an item which is classified into wrong class</span><br><span class="line">    :return: nothing</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    global w, b, history</span><br><span class="line">    w[0] += 1 * item[1] * item[0][0]</span><br><span class="line">    w[1] += 1 * item[1] * item[0][1]</span><br><span class="line">    b += 1 * item[1]</span><br><span class="line">    print(w, b)</span><br><span class="line">    history.append([copy.copy(w), b])</span><br><span class="line">    # you can uncomment this line to check the process of stochastic gradient descent</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def cal(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    calculate the functional distance between &apos;item&apos; an the dicision surface. output yi(w*xi+b).</span><br><span class="line">    :param item:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    res = 0</span><br><span class="line">    for i in range(len(item[0])):</span><br><span class="line">        res += item[0][i] * w[i]</span><br><span class="line">    res += b</span><br><span class="line">    res *= item[1]</span><br><span class="line">    return res</span><br><span class="line"> </span><br><span class="line">def check():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    check if the hyperplane can classify the examples correctly</span><br><span class="line">    :return: true if it can</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    flag = False</span><br><span class="line">    for item in training_set:</span><br><span class="line">        if cal(item) &lt;= 0:</span><br><span class="line">            flag = True</span><br><span class="line">            update(item)</span><br><span class="line">    # draw a graph to show the process</span><br><span class="line">    if not flag:</span><br><span class="line">        print (&quot;RESULT: w: &quot; + str(w) + &quot; b: &quot; + str(b))</span><br><span class="line">    return flag</span><br><span class="line"> </span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    for i in range(1000):</span><br><span class="line">        if not check(): break</span><br><span class="line"> </span><br><span class="line">    # first set up the figure, the axis, and the plot element we want to animate</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = plt.axes(xlim=(0, 2), ylim=(-2, 2))</span><br><span class="line">    line, = ax.plot([], [], &apos;g&apos;, lw=2)</span><br><span class="line">    label = ax.text([], [], &apos;&apos;)</span><br><span class="line"> </span><br><span class="line">    # initialization function: plot the background of each frame</span><br><span class="line">    def init():</span><br><span class="line">        line.set_data([], [])</span><br><span class="line">        x, y, x_, y_ = [], [], [], []</span><br><span class="line">        for p in training_set:</span><br><span class="line">            if p[1] &gt; 0:</span><br><span class="line">                x.append(p[0][0])</span><br><span class="line">                y.append(p[0][1])</span><br><span class="line">            else:</span><br><span class="line">                x_.append(p[0][0])</span><br><span class="line">                y_.append(p[0][1])</span><br><span class="line"> </span><br><span class="line">        plt.plot(x, y, &apos;bo&apos;, x_, y_, &apos;rx&apos;)</span><br><span class="line">        plt.axis([-6, 6, -6, 6])</span><br><span class="line">        plt.grid(True)</span><br><span class="line">        plt.xlabel(&apos;x&apos;)</span><br><span class="line">        plt.ylabel(&apos;y&apos;)</span><br><span class="line">        plt.title(&apos;Perceptron Algorithm&apos;)</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    # animation function.  this is called sequentially</span><br><span class="line">    def animate(i):</span><br><span class="line">        global history, ax, line, label</span><br><span class="line"> </span><br><span class="line">        w = history[i][0]</span><br><span class="line">        b = history[i][1]</span><br><span class="line">        if w[1] == 0: return line, label</span><br><span class="line">        x1 = -7</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        x2 = 7</span><br><span class="line">        y2 = -(b + w[0] * x2) / w[1]</span><br><span class="line">        line.set_data([x1, x2], [y1, y2])</span><br><span class="line">        x1 = 0</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        label.set_text(history[i])</span><br><span class="line">        label.set_position([x1, y1])</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line">    # call the animator.  blit=true means only re-draw the parts that have changed.</span><br><span class="line">    print (history)</span><br><span class="line">    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=1000, repeat=True,</span><br><span class="line">                                   blit=True)</span><br><span class="line">    plt.show()</span><br><span class="line">    anim.save(&apos;perceptron.gif&apos;, fps=2, writer=&apos;imagemagick&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/perceptron1.gif" alt=""></p><h1 id="python其他可视化模块"><a href="#python其他可视化模块" class="headerlink" title="python其他可视化模块"></a>python其他可视化模块</h1><ul><li>Traits-为Python添加类型定义</li><li>TraitsUI-制作用户界面</li><li>Chaco-交互式图表</li><li>TVTK-三维可视化数据</li><li>Visual-制作3D演示动画</li><li>Mayavi-更方便的可视化</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://frankblog.site/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="Matplotlib" scheme="http://frankblog.site/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>markdown公式编辑</title>
    <link href="http://frankblog.site/2018/06/03/markdown%E5%85%AC%E5%BC%8F%E7%BC%96%E8%BE%91/"/>
    <id>http://frankblog.site/2018/06/03/markdown公式编辑/</id>
    <published>2018-06-03T03:42:27.605Z</published>
    <updated>2018-06-04T01:33:58.285Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/mathjax1.png" alt=""></p><a id="more"></a><hr><h1 id="加载mathjax"><a href="#加载mathjax" class="headerlink" title="加载mathjax"></a>加载mathjax</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>引入脚本对网页进行渲染<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="插入方式"><a href="#插入方式" class="headerlink" title="插入方式"></a>插入方式</h2><blockquote><p>这里分两种，一种是行间插入，另一种是另取一行</p></blockquote><h3 id="行内插入"><a href="#行内插入" class="headerlink" title="行内插入"></a>行内插入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(a+b\\)</span><br></pre></td></tr></table></figure><p>\(a+b\)</p><h3 id="单独一行"><a href="#单独一行" class="headerlink" title="单独一行"></a>单独一行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$a + b$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">a + b</script><h2 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h2><h3 id="上、下标"><a href="#上、下标" class="headerlink" title="上、下标"></a>上、下标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$x_1$$</span><br><span class="line"></span><br><span class="line">$$x_1^2$$</span><br><span class="line"></span><br><span class="line">$$x^2_1$$</span><br><span class="line"></span><br><span class="line">$$x_&#123;22&#125;^&#123;(n)&#125;$$ #多于一位是要加 `&#123;&#125;` 包裹的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$$x_&#123;balabala&#125;^&#123;bala&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">x_1</script><script type="math/tex; mode=display">x_1^2</script><script type="math/tex; mode=display">x^2_1</script><script type="math/tex; mode=display">x_{22}^{(n)}</script><script type="math/tex; mode=display">x_{balabala}^{bala}</script><h3 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$\frac&#123;x+y&#125;&#123;2&#125;$$</span><br><span class="line"></span><br><span class="line">$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\frac{x+y}{2}</script><script type="math/tex; mode=display">\frac{1}{1+\frac{1}{2}}</script><h3 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\sqrt{2}<\sqrt[3]{3}</script><script type="math/tex; mode=display">\sqrt{1+\sqrt[p]{1+a^2}}</script><script type="math/tex; mode=display">\sqrt{1+\sqrt[^p\!]{1+a^2}}</script><h3 id="求和、积分"><a href="#求和、积分" class="headerlink" title="求和、积分"></a>求和、积分</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$\sum_&#123;k=1&#125;^&#123;n&#125;\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class="line"></span><br><span class="line">\\(\sum_&#123;k=1&#125;^n\frac&#123;1&#125;&#123;k&#125;\\)</span><br><span class="line"></span><br><span class="line">$$\int_a^b f(x)dx$$</span><br><span class="line"></span><br><span class="line">\\(\int_a^b f(x)dx\\)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\sum_{k=1}^{n}\frac{1}{k}</script><p>\(\sum_{k=1}^n\frac{1}{k}\)</p><script type="math/tex; mode=display">\int_a^b f(x)dx</script><p>\(\int_a^b f(x)dx\)</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">紧贴 $a\\!b$</span><br><span class="line"></span><br><span class="line">没有空格 $ab$</span><br><span class="line"></span><br><span class="line">小空格 a\,b</span><br><span class="line"></span><br><span class="line">中等空格 a\;b</span><br><span class="line"></span><br><span class="line">大空格 a\ b</span><br><span class="line"></span><br><span class="line">quad空格 $a\quad b$</span><br><span class="line"></span><br><span class="line">两个quad空格 $a\qquad b$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">a\\!b</script><script type="math/tex; mode=display">{ab}</script><script type="math/tex; mode=display">a\,b</script><script type="math/tex; mode=display">a\;b</script><script type="math/tex; mode=display">a\ b</script><script type="math/tex; mode=display">a\quad b</script><script type="math/tex; mode=display">a\qquad b</script><h3 id="公式界定符"><a href="#公式界定符" class="headerlink" title="公式界定符"></a>公式界定符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\left(\sum_&#123;k=\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\frac&#123;1&#125;&#123;k&#125;\right)$$</span><br></pre></td></tr></table></figure><p>通过 <code>\left</code> 和 <code>\right</code> 后面跟界定符来对同时进行界定。</p><script type="math/tex; mode=display">\left(\sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k}\right)</script><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;pmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;vmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\left|\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;\right|$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Vmatrix&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{matrix}1 & 2\\\\3 &4\end{matrix}</script><script type="math/tex; mode=display">\begin{pmatrix}1 & 2\\\\3 &4\end{pmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}1 & 2\\\\3 &4\end{bmatrix}</script><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2\\\\3 &4\end{Bmatrix}</script><script type="math/tex; mode=display">\begin{vmatrix}1 & 2\\\\3 &4\end{vmatrix}</script><script type="math/tex; mode=display">\left|\begin{matrix}1 & 2\\\\3 &4\end{matrix}\right|</script><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2\\\\3 &4\end{Vmatrix}</script><p>类似于 left right，这里是 begin 和 end。而且里面有具体的矩阵语法，<code>&amp;</code> 区分行间元素，<code>\\\\</code> 代表换行。可以理解为 HTML 的标签之类的。</p><h3 id="排版数组"><a href="#排版数组" class="headerlink" title="排版数组"></a>排版数组</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;X&#125; =</span><br><span class="line">\left( \begin&#123;array&#125;&#123;ccc&#125;</span><br><span class="line">x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\\</span><br><span class="line">x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots</span><br><span class="line">\end&#123;array&#125; \right)</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{ccc}x\_{11} & x\_{12} & \ldots \\\\x\_{21} & x\_{22} & \ldots \\\\\vdots & \vdots & \ddots\end{array} \right)</script><h1 id="常用公式举例"><a href="#常用公式举例" class="headerlink" title="常用公式举例"></a>常用公式举例</h1><h2 id="公式组"><a href="#公式组" class="headerlink" title="公式组"></a>公式组</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">a &amp;= b+c+d \\\\</span><br><span class="line"></span><br><span class="line">x &amp;= y+z</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{align}a &= b+c+d \\\\x &= y+z\end{align}</script><h2 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">y=\begin&#123;cases&#125;</span><br><span class="line">-x,\quad x\leq 0 \\\\</span><br><span class="line">x,\quad x&gt;0</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">y=\begin{cases}-x,\quad x\leq 0 \\\\x,\quad x>0\end{cases}</script><h1 id="常用数学符号"><a href="#常用数学符号" class="headerlink" title="常用数学符号"></a>常用数学符号</h1><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line">&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|c|c|c|c|}\hline{\alpha} & {\backslash alpha} & {\theta} & {\backslash theta} & {o} & {o} & {\upsilon} & {\backslash upsilon} \\\\\hline{\beta} & {\backslash beta} & {\vartheta} & {\backslash vartheta} & {\pi} & {\backslash pi} & {\phi} & {\backslash phi} \\\\\hline{\gamma} & {\backslash gamma} & {\iota} & {\backslash iota} & {\varpi} & {\backslash varpi} & {\varphi} & {\backslash varphi} \\\\\hline{\delta} & {\backslash delta} & {\kappa} & {\backslash kappa} & {\rho} & {\backslash rho} & {\chi} & {\backslash chi} \\\\\hline{\epsilon} & {\backslash epsilon} & {\lambda} & {\backslash lambda} & {\varrho} & {\backslash varrho} & {\psi} & {\backslash psi} \\\\\hline{\varepsilon} & {\backslash varepsilon} & {\mu} & {\backslash mu} & {\sigma} & {\backslash sigma} & {\omega} & {\backslash omega} \\\\\hline{\zeta} & {\backslash zeta} & {\nu} & {\backslash nu} & {\varsigma} & {\backslash varsigma} & {} & {} \\\\\hline{\eta} & {\backslash eta} & {\xi} & {\backslash xi} & {\tau} & {\backslash tau} & {} & {} \\\\\hline{\Gamma} & {\backslash Gamma} & {\Lambda} & {\backslash Lambda} & {\Sigma} & {\backslash Sigma} & {\Psi} & {\backslash Psi} \\\\\hline{\Delta} & {\backslash Delta} & {\Xi} & {\backslash Xi} & {\Upsilon} & {\backslash Upsilon} & {\Omega} & {\backslash Omega} \\\\\hline{\Omega} & {\backslash Omega} & {\Pi} & {\backslash Pi} & {\Phi} & {\backslash Phi} & {} & {} \\\\\hline\end{array}</script><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://blog.csdn.net/xiahouzuoxin/article/details/26478179" target="_blank" rel="noopener">Markdown中插入数学公式的方法</a></li><li><a href="http://www.cnblogs.com/houkai/p/3399646.html" target="_blank" rel="noopener">LATEX数学公式基本语法</a></li><li><a href="https://liam0205.me/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/mathjax1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="写作" scheme="http://frankblog.site/categories/%E5%86%99%E4%BD%9C/"/>
    
    
      <category term="markdown" scheme="http://frankblog.site/tags/markdown/"/>
    
      <category term="LaTex" scheme="http://frankblog.site/tags/LaTex/"/>
    
  </entry>
  
  <entry>
    <title>SVM可视化</title>
    <link href="http://frankblog.site/2018/06/02/svm%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://frankblog.site/2018/06/02/svm可视化/</id>
    <published>2018-06-02T08:02:15.545Z</published>
    <updated>2018-06-06T01:54:17.664Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4]])</span><br><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Decision Boundary&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;r&apos;)</span><br><span class="line">plt.plot([3, 3], [0.5, 6], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, 0, shape=&apos;full&apos;, color=&apos;b&apos;, linestyle=&apos;--&apos;)</span><br><span class="line">plt.annotate(r&apos;margin 1&apos;,</span><br><span class="line">             xy=(3.5, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(3.1, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;margin 2&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4, 3.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(2, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(0.5, 1.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Support Vector Machine&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 0$&apos;,</span><br><span class="line">             xy=(5, 1), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(6, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 1$&apos;,</span><br><span class="line">             xy=(6, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(7, 2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = -1$&apos;,</span><br><span class="line">             xy=(3.5, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4.5, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;d&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(2, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;A&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM%E7%94%BB%E5%9B%BE1.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=140)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=100, </span><br><span class="line"> n_features=2, </span><br><span class="line"> centers=[(1, 1), (2, 2)], </span><br><span class="line"> random_state=4, </span><br><span class="line"> shuffle=False,</span><br><span class="line"> cluster_std=0.4)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 3)</span><br><span class="line">plt.ylim(0, 3)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([0.5, 2.5], [2.5, 0.5], &apos;-r&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2], [1.5, 1.5], [1.2, 1.7]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4], [5.5, 3.5], [4.5, 4.5], [2, 1.5]])</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Slack Variable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 7)</span><br><span class="line">plt.ylim(0, 7)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(2, 1.5, 2.25, 2.25, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;violate margin rule.&apos;,</span><br><span class="line"> xy=(2, 1.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(0.2, 0.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;normal sample. $\epsilon = 0$&apos;,</span><br><span class="line"> xy=(4, 5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(4.5, 5.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$\epsilon &gt; 0$&apos;,</span><br><span class="line"> xy=(3, 2.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(3, 1.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Cost&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 4)</span><br><span class="line">plt.ylim(0, 2)</span><br><span class="line">plt.xlabel(&apos;$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$&apos;)</span><br><span class="line">plt.ylabel(&apos;Cost&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [1.5, 0], &apos;-r&apos;)</span><br><span class="line">plt.plot([1, 3], [0.015, 0.015], &apos;-r&apos;)</span><br><span class="line">plt.annotate(r&apos;$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$&apos;,</span><br><span class="line">             xy=(0.7, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$&apos;,</span><br><span class="line">             xy=(1.5, 0), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1.8, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE3.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=144)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [3, 2], [4, 1], [5, 1]])</span><br><span class="line">class2 = np.array([[2.2, 4], [1.5, 5], [1.8, 4.6], [2.4, 5], [3.2, 5], [3.7, 4], [4.5, 4.5], [5.4, 3]])</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable in Low Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], np.zeros(class1[:, 0].shape[0]) + 0.05, marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], np.zeros(class2[:, 0].shape[0]) + 0.05, marker=&apos;s&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Linear Separatable in High Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">plt.ylabel(&apos;X2&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [3.8, 2], &apos;-r&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE4.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def gaussian_kernel(x, mean, sigma):</span><br><span class="line"> return np.exp(- (x - mean)**2 / (2 * sigma**2))</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 6, 500)</span><br><span class="line">mean = 1</span><br><span class="line">sigma1 = 0.1</span><br><span class="line">sigma2 = 0.3</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 3), dpi=144)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma1))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma1), &apos;r-&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma2))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma2), &apos;r-&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE5.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="SVM" scheme="http://frankblog.site/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？</title>
    <link href="http://frankblog.site/2018/06/01/%E5%A6%82%E4%BD%95%E7%94%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E6%AF%94%E5%96%BB%E6%8F%8F%E8%BF%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%EF%BC%9FHadoop%E3%80%81Hive%E3%80%81Spark%20%E4%B9%8B%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F/"/>
    <id>http://frankblog.site/2018/06/01/如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？/</id>
    <published>2018-06-01T12:06:43.304Z</published>
    <updated>2018-06-08T11:34:11.717Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？<br>这是知乎上某大神的解释：</p><p><strong>学习很重要的是能将纷繁复杂的信息进行归类和抽象。</strong><br>对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题：</p><ul><li>存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka；</li><li>计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等；</li><li>查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等；</li></ul><p><strong>大数据技术生态其实是一个江湖….</strong><br>在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…<br>这三本秘籍分别为：</p><ul><li>《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据；</li><li>《Google MapReduce》：论述了怎样快速计算海量的数据；</li><li>《Google BigTable》：论述了怎样实现海量数据的快速查询；</li></ul><p><strong>以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术；</strong></p><p>在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。<br>Hadoop包括三大部分，分别是hdfs、MapReduce和hbase：</p><ul><li>hdfs解决大数据的存储问题。</li><li>mapreduce解决大数据的计算问题。</li><li>hbase解决大数据量的查询问题。</li></ul><p>之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进：</p><ul><li>1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍；</li><li>2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。</li><li>3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。</li><li>4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼；</li></ul><p>原文地址：<a href="https://www.zhihu.com/question/27974418/answer/156227565" target="_blank" rel="noopener">知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://frankblog.site/tags/hadoop/"/>
    
      <category term="spark" scheme="http://frankblog.site/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降法总结</title>
    <link href="http://frankblog.site/2018/06/01/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://frankblog.site/2018/06/01/梯度下降法总结/</id>
    <published>2018-06-01T08:20:06.419Z</published>
    <updated>2018-06-02T07:09:29.464Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度实际上就是多变量微分的一般化。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_0.png" alt="link"></p><p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p><ul><li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li><li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2.png" alt="link"></p><p>此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！<br>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_1.png" alt="link"></p><h1 id="多元函数的梯度下降"><a href="#多元函数的梯度下降" class="headerlink" title="多元函数的梯度下降"></a>多元函数的梯度下降</h1><p>我们假设有一个目标函数</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2.png" alt="多元函数的梯度下降"></p><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_1.png" alt="image.png"></p><p>初始的学习率为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_2.png" alt="image.png"></p><p>函数的梯度为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_3.png" alt="image.png"></p><p>进行多次迭代：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_4.png" alt="image.png"></p><p>我们发现，已经基本靠近函数的最小值点</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_5.png" alt="image.png"></p><h1 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h1><h2 id="选择合适的学习速率"><a href="#选择合适的学习速率" class="headerlink" title="选择合适的学习速率"></a>选择合适的学习速率</h2><p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg" alt=""></p><p>假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果 learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。</p><p>虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。</p><h2 id="自适应学习速率"><a href="#自适应学习速率" class="headerlink" title="自适应学习速率"></a>自适应学习速率</h2><p>举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate</p><ul><li>通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate</li><li>update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate</li><li>比如 \(\eta^{t} = \eta / \sqrt{t+1}\)，t是次数。随着次数的增加，\(η_t\)减小</li></ul><h1 id="Feature-Scaling（特征缩放）"><a href="#Feature-Scaling（特征缩放）" class="headerlink" title="Feature Scaling（特征缩放）"></a>Feature Scaling（特征缩放）</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.jpg" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE1.jpg" alt=""></p><p>图左边是\(x_{1}\)的scale比的scale比\(x_{2}\)要小很多，所以当要小很多，所以当\(w_{1}\)和和\(w_{2}\)做同样的变化时，做同样的变化时，\(w_{1}\)对y的变化影响是比较小的，对y的变化影响是比较小的，\(x_{2}\)对y的变化影响是比较大的。</p><p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为\(w_{1}\)对y的变化影响比较小，所以对y的变化影响比较小，所以\(w_{1}\)对损失函数的影响比较小，对损失函数的影响比较小，\(w_{1}\)对损失函数有比较小的微分，所以对损失函数有比较小的微分，所以vw_{1}\)方向上是比较平滑的。同理方向上是比较平滑的。同理\(x_{2}\)对y的影响比较大，所以对y的影响比较大，所以\(x_{2}\)对损失函数的影响比较大，所以在对损失函数的影响比较大，所以在\(x_{2}\)方向有比较尖的峡谷。</p><p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p><p>对于左边的情况，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p><h1 id="常见的算法"><a href="#常见的算法" class="headerlink" title="常见的算法"></a>常见的算法</h1><ul><li><strong>批量梯度下降</strong>：批量梯度下降每次更新使用了所有的训练数据。<strong>如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，</strong>但是缺点是如果样本值很大的话，更新速度会很慢。</li><li><strong>随机梯度下降</strong>：随机也就是说用一个样本的梯度来近似所有的样本，来调整θ，这样会大大加快训练数据，但是有可能由于训练数据的噪声点较多。<strong>每一次利用噪声点进行更新的过程中，不一定是朝着极小值方向更新，但是由于多次迭代，整体方向还是大致朝着极小值方向更新，提高了速度。</strong></li><li><strong>小批量梯度下降</strong>：小批量梯度下降法是<strong>为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的</strong>。</li></ul><h2 id="批量梯度下降代码"><a href="#批量梯度下降代码" class="headerlink" title="批量梯度下降代码"></a>批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line">#这里更新权重的时候所有的样本点都用上了</span><br><span class="line">    for i in range (3):</span><br><span class="line"> pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line"> theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"></span><br><span class="line">iters_count 219</span><br><span class="line">iters_count 220</span><br><span class="line">iters_count 221</span><br><span class="line">iters_count 222</span><br><span class="line">iters_count 223</span><br><span class="line">iters_count 224</span><br><span class="line">iters_count 225</span><br><span class="line">theta: [3.0027765778748003, 3.997918297015663]</span><br><span class="line">final loss: 9.68238055213e-05</span><br><span class="line">iters: 225</span><br><span class="line">[Finished in 0.2s]</span><br></pre></td></tr></table></figure><h2 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码"></a>随机梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 每次选取一个值,随机一个点更新 θ</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"> #每一次选取随机的一个点进行权重的更新</span><br><span class="line">    i = random.randint(0,3)</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#output:</span><br><span class="line">iters_count 1226</span><br><span class="line">iters_count 1227</span><br><span class="line">iters_count 1228</span><br><span class="line">iters_count 1229</span><br><span class="line">iters_count 1230</span><br><span class="line">iters_count 1231</span><br><span class="line">iters_count 1232</span><br><span class="line">theta: [3.002441488688225, 3.9975844154600226]</span><br><span class="line">final loss: 9.989420302e-05</span><br><span class="line">iters: 1232</span><br><span class="line">[Finished in 0.3s]</span><br></pre></td></tr></table></figure><h2 id="小批量梯度下降代码"><a href="#小批量梯度下降代码" class="headerlink" title="小批量梯度下降代码"></a>小批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 这里用2个样本点</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"></span><br><span class="line">    i = random.randint(0,3) #注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span><br><span class="line">    j = (i+1)%4</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[0]*input_x[j][0]+theta[1]*input_x[j][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[j]) * input_x[j][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[j]) * input_x[j][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line">iters_count 543</span><br><span class="line">iters_count 544</span><br><span class="line">iters_count 545</span><br><span class="line">iters_count 546</span><br><span class="line">iters_count 547</span><br><span class="line">iters_count 548</span><br><span class="line">iters_count 549</span><br><span class="line">theta: [3.0023012574840764, 3.997553282857357]</span><br><span class="line">final loss: 9.81717138358e-05</span><br><span class="line">iters: 549</span><br></pre></td></tr></table></figure><h1 id="梯度下降的局限"><a href="#梯度下降的局限" class="headerlink" title="梯度下降的局限"></a>梯度下降的局限</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%B1%80%E9%99%90.jpg" alt=""></p><ul><li>容易陷入局部极值</li><li>还有可能卡在不是极值，但微分值是0的地方</li><li>还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</li></ul><p>参考：<a href="https://zhuanlan.zhihu.com/qinlibo-ml" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/qinlibo-ml</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="优化算法" scheme="http://frankblog.site/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之Tensorflow(一)</title>
    <link href="http://frankblog.site/2018/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BTensorflow(%E4%B8%80)/"/>
    <id>http://frankblog.site/2018/06/01/深度学习之Tensorflow(一)/</id>
    <published>2018-06-01T07:35:47.712Z</published>
    <updated>2018-06-01T09:10:04.104Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/tensorflow.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="Tensorflow简单示例"><a href="#Tensorflow简单示例" class="headerlink" title="Tensorflow简单示例"></a>Tensorflow简单示例</h1><h2 id="1-基本运算"><a href="#1-基本运算" class="headerlink" title="1. 基本运算"></a>1. 基本运算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#定义一个常量</span><br><span class="line">a = tf.constant([3,3])</span><br><span class="line">#定义一个变量</span><br><span class="line">x = tf.Variable([1,2])</span><br><span class="line"></span><br><span class="line">#定义一个加法op</span><br><span class="line">add = tf.add(a,x)</span><br><span class="line">#定义一个减法</span><br><span class="line">sub = tf.subtract(a,x)</span><br><span class="line">#定义一个乘法op</span><br><span class="line">mul = tf.multiply(a,x)</span><br><span class="line">#定义初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">#定义多个操作</span><br><span class="line">add2 = tf.add(a,add)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">print(&quot;加法：&quot;,sess.run(add)) #执行加法</span><br><span class="line">print(&quot;减法：&quot;,sess.run(sub)) #执行减法</span><br><span class="line">print(&quot;乘法：&quot;,sess.run(mul)) #执行乘法</span><br><span class="line">#同时执行乘法op和加法op</span><br><span class="line">result = sess.run([add,add2,sub,mul])</span><br><span class="line">print(&quot;执行多个：&quot;,result)</span><br></pre></td></tr></table></figure><h2 id="2-使用占位符"><a href="#2-使用占位符" class="headerlink" title="2. 使用占位符"></a>2. 使用占位符</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#Feed：先定义占位符，等需要的时候再传入数据</span><br><span class="line">#创建占位符</span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">#定义乘法op</span><br><span class="line">output = tf.multiply(input1,input2)</span><br><span class="line">add = tf.add(input1,input2)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">#feed的数据以字典的形式传入</span><br><span class="line">print(sess.run(add, feed_dict=&#123;input1:[8.],input2:[2.]&#125;))</span><br></pre></td></tr></table></figure><h1 id="Tensorflow简单回归模型"><a href="#Tensorflow简单回归模型" class="headerlink" title="Tensorflow简单回归模型"></a>Tensorflow简单回归模型</h1><h2 id="1-最简单的线性回归模型"><a href="#1-最简单的线性回归模型" class="headerlink" title="1. 最简单的线性回归模型"></a>1. 最简单的线性回归模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#使用numpy生成100个随机点</span><br><span class="line">#样本点</span><br><span class="line">x_data = np.random.rand(100)</span><br><span class="line">y_data = x_data*0.1 + 0.2</span><br><span class="line"></span><br><span class="line">#构造一个线性模型</span><br><span class="line">d = tf.Variable(1.1)</span><br><span class="line">k = tf.Variable(0.5)</span><br><span class="line">y = k*x_data + d</span><br><span class="line"></span><br><span class="line">#二次代价函数&lt;均方差&gt;</span><br><span class="line">loss = tf.losses.mean_squared_error(y_data,y)</span><br><span class="line">#定义一个梯度下降法来进行训练的优化器</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.3)</span><br><span class="line">#最小化代价函数</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line"> for step in range(1000):</span><br><span class="line"> sess.run(train)</span><br><span class="line"> if step%100 ==0:</span><br><span class="line"> print(step,sess.run([k,d]))</span><br></pre></td></tr></table></figure><h2 id="2-非线性回归的问题"><a href="#2-非线性回归的问题" class="headerlink" title="2. 非线性回归的问题"></a>2. 非线性回归的问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data = np.linspace(-0.5,0.5,200).reshape(-1,1)</span><br><span class="line">noise = np.random.normal(0,0.015,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder，列数为1，行数未知</span><br><span class="line">x = tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 = tf.Variable(tf.random_normal([1,20])) # 初始化1行20列权值</span><br><span class="line">biases_L1 = tf.Variable(tf.zeros([1,20])) # 初始化1行20列偏置</span><br><span class="line">Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 # 计算神经元信号</span><br><span class="line">L1 = tf.nn.tanh(Wx_plus_b_L1) # 使用激活函数计算神经元输出信号</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 = tf.Variable(tf.random_normal([20,1]))</span><br><span class="line">biases_L2 = tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction = tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"># prediction = Wx_plus_b_L2</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line">#使用梯度下降法最小化代价函数训练</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">#变量初始化</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">for _ in range(1000):</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line"></span><br><span class="line">#获得预测值</span><br><span class="line">prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;)</span><br><span class="line">#画图</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.plot(x_data,prediction_value,&apos;r-&apos;,lw=5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这里思考下中间层和输出层的激活函数的选取问题。</span><br><span class="line">1\. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。</span><br><span class="line">2\. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。</span><br><span class="line">3\. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。</span><br><span class="line">4\. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。</span><br><span class="line">5\. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。</span><br></pre></td></tr></table></figure><h1 id="Tensorflow分类模型"><a href="#Tensorflow分类模型" class="headerlink" title="Tensorflow分类模型"></a>Tensorflow分类模型</h1><p>本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。</p><h2 id="1-简单的MNIST数据集分类"><a href="#1-简单的MNIST数据集分类" class="headerlink" title="1. 简单的MNIST数据集分类"></a>1. 简单的MNIST数据集分类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">W = tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b = tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction) </span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中。</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率。</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">#epoch：所有数据训练一次，就是一个epoch周期</span><br><span class="line">for epoch in range(21):</span><br><span class="line">#batch：一般为32，64个数据</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(acc))</span><br></pre></td></tr></table></figure><h2 id="2-过拟合解决及梯度下降优化器"><a href="#2-过拟合解决及梯度下降优化器" class="headerlink" title="2. 过拟合解决及梯度下降优化器"></a>2. 过拟合解决及梯度下降优化器</h2><p>Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。<br>在神经网络中它们之间也可以结合使用，dropout应用较多些。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义三个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 784-1000-500-10</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([784,1000],stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.zeros([1000])+0.1)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([1000,500],stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.zeros([500])+0.1)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([500,10],stddev=0.1))</span><br><span class="line">b3 = tf.Variable(tf.zeros([10])+0.1)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line">#同样这里也可以使用正则项</span><br><span class="line">#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"></span><br><span class="line">#正则后的交叉熵代价函数</span><br><span class="line">#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率</span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)</span><br><span class="line"></span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">for epoch in range(31):</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.5&#125;) #这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了</span><br><span class="line"></span><br><span class="line">test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(test_acc) +&quot;,Training Accuracy &quot; + str(train_acc))</span><br></pre></td></tr></table></figure></p><h2 id="3-神经网络优化"><a href="#3-神经网络优化" class="headerlink" title="3. 神经网络优化"></a>3. 神经网络优化</h2><p>这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义三个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line">lr = tf.Variable(0.001, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"># 784-500-300-10</span><br><span class="line">#创建一个神经网络</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.zeros([500])+0.1)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.zeros([300])+0.1)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob)</span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))</span><br><span class="line">b3 = tf.Variable(tf.zeros([10])+0.1)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line">#训练</span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">for epoch in range(21):</span><br><span class="line">sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:1.0&#125;)</span><br><span class="line"></span><br><span class="line">learning_rate = sess.run(lr)</span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc) + &quot;, Learning Rate= &quot; + str(learning_rate))</span><br></pre></td></tr></table></figure></p><h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a>CNN卷积神经网络</h1><p>以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。</p><ul><li>CNN通过<strong>局部感受野</strong>和<strong>权值共享</strong>减少了神经网络需要训练的参数（权值）的个数。</li><li><p>卷积核/滤波器<br><img src="https://ihoge.cn/2018/media/15273889821782.jpg" alt=""></p></li><li><p>卷积Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li><li>池化<ul><li>max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数</li><li>mean-pooling</li><li>随机-pooling<br><img src="https://ihoge.cn/2018/media/15273890234010.jpg" alt=""></li></ul></li><li><p>池化Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li></ul><p>下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line">#定义两个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])#28*28</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#初始化权值</span><br><span class="line">def weight_variable(shape):</span><br><span class="line">initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布</span><br><span class="line">return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">#初始化偏置</span><br><span class="line">def bias_variable(shape):</span><br><span class="line">initial = tf.constant(0.1,shape=shape)</span><br><span class="line">return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">#卷积层</span><br><span class="line">def conv2d(x,W):</span><br><span class="line">#x input tensor of shape `[batch, in_height, in_width, in_channels]`</span><br><span class="line">#W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]</span><br><span class="line">#`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长</span><br><span class="line">#padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`</span><br><span class="line">return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=&apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">#池化层</span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">#ksize [1,x,y,1]</span><br><span class="line">return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`</span><br><span class="line">x_image = tf.reshape(x,[-1,28,28,1])</span><br><span class="line"></span><br><span class="line">#初始化第一个卷积层的权值和偏置</span><br><span class="line">W_conv1 = weight_variable([5,5,1,32])#5*5的采样窗口，32个卷积核从1个平面抽取特征</span><br><span class="line">b_conv1 = bias_variable([32])#每一个卷积核一个偏置值</span><br><span class="line"></span><br><span class="line">#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling</span><br><span class="line"></span><br><span class="line">#初始化第二个卷积层的权值和偏置</span><br><span class="line">W_conv2 = weight_variable([5,5,32,64])#5*5的采样窗口，64个卷积核从32个平面抽取特征</span><br><span class="line">b_conv2 = bias_variable([64])#每一个卷积核一个偏置值</span><br><span class="line"></span><br><span class="line">#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling</span><br><span class="line"></span><br><span class="line">#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14</span><br><span class="line">#第二次卷积后为14*14，第二次池化后变为了7*7</span><br><span class="line">#进过上面操作后得到64张7*7的平面</span><br><span class="line"></span><br><span class="line">#初始化第一个全连接层的权值</span><br><span class="line">W_fc1 = weight_variable([7*7*64,1024])#上一层有7*7*64个神经元，全连接层有1024个神经元</span><br><span class="line">b_fc1 = bias_variable([1024])#1024个节点</span><br><span class="line"></span><br><span class="line">#把池化层2的输出扁平化为1维</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])</span><br><span class="line">#求第一个全连接层的输出</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">#keep_prob用来表示神经元的输出概率</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line">#初始化第二个全连接层</span><br><span class="line">W_fc2 = weight_variable([1024,10])</span><br><span class="line">b_fc2 = bias_variable([10])</span><br><span class="line"></span><br><span class="line">#计算输出</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line">#使用AdamOptimizer进行优化</span><br><span class="line">train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">#结果存放在一个布尔列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">for epoch in range(21):</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line"></span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/tensorflow.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://frankblog.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://frankblog.site/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础（二）</title>
    <link href="http://frankblog.site/2018/05/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://frankblog.site/2018/05/30/机器学习基础（二）/</id>
    <published>2018-05-30T06:05:51.455Z</published>
    <updated>2018-06-16T04:11:35.745Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/ai%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="1-数据类型比较"><a href="#1-数据类型比较" class="headerlink" title="1.数据类型比较"></a>1.数据类型比较</h2><p>Scalar: 标量，可以看成一个数<br>Vector: 向量，可以看成一个一维数组<br>Matrix: 矩阵，可以看成二维数组<br>Tensor: 张量，三维或三维以上的数组的统称，维度不定</p><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。<br>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%B8%89%E9%98%B6%E5%BC%A0%E9%87%8F.jpg" alt=""></p><ul><li><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p></li><li><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p></li></ul><h2 id="2-线性代数概念"><a href="#2-线性代数概念" class="headerlink" title="2.线性代数概念"></a>2.线性代数概念</h2><p><strong>  逆矩阵</strong>：对于n阶矩阵A，如果有一个n阶矩阵B，使 AB = BA = E， 则说矩阵A是可逆的，并把矩阵B称为A的逆矩阵。当 |A| =0 时，A称为奇异矩阵。可逆矩阵一定是非奇异矩阵，因为矩阵可逆的充分必要条件是 |A|不为0。</p><p><strong> 矩阵的秩</strong>：矩阵A的行阶梯形中非零行的行数，就是矩阵A的秩。 对于n阶矩阵A，由于A的n阶子式只有一个|A|，故当 |A|不为0时R(A)=n，当|A|=0时R(A)&lt;n。因此，可逆矩阵的秩 = 矩阵的阶数，不可逆矩阵的秩 &lt; 矩阵的阶数。那么秩有什么实际意义吗？答案是肯定的。在做矩阵SVD分解的时候用于降噪，如果矩阵秩远小于样本维数（即矩阵列数），那么这些样本相当于只生活在外围空间中的一个低维子空间，这样就能实施降维操作。再者，如果把矩阵看成线性映射，那么秩就是象空间的的维数。</p><p><strong> 线性方程组的解</strong>：矩阵在数学上的基本的应用就是解线性方程组，这也是贯穿整个课本的样例。一个复杂的线性方程组可以表示为Ax=b，其中x，b是向量。通过求A的秩可以判定方程组的解：无解的充分必要条件是R(A) &lt; R(A,b)；有唯一解的充分必要条件是R(A) = R(A,b) = n；有无限多解的充分必要条件是R(A) = R(A,b) &lt; n。</p><p><strong>   向量空间的基</strong>：设V是一个向量空间，V上有r个向量a1,a2…ar，并且满足 a1,a2…ar线性无关，并且V中的任一向量都可以由a1,a2…ar线性表示，那么向量组a1,a2…ar就称为向量空间V的一个基，r是向量空间的维数，并称V为r维向量空间。可以这么理解，把向量空间看做是向量组，那么基就是一个极大线性无关组，可以用来表示其他向量的最小组合，维数就是向量组的秩。</p><p><strong>  特征值与特征向量</strong>：对于n阶矩阵A，如果数 λ 和 n 维非零列向量 x 使关系式 Ax=λ x 成立，那么 λ 称为矩阵A的特征值，向量 x 就是A的对应于λ 的特征向量。那么如何理解特征值与特征向量呢？</p><p> 我们知道，矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。实际上，这段话既讲了矩阵变换特征值及特征向量的几何意义（图形变换）也讲了其物理含义。物理的含义就是运动的图景：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。</p><p> <strong>相似矩阵</strong>：设A，B都是n阶矩阵，若有可逆矩阵P，使P-1 AP=B,则B是A的相似矩阵。</p><p> <strong>对角阵</strong>：是一个主对角线之外的元素皆为 0 的矩阵。对角线上的元素可以为 0 或其他值。</p><p> <strong>二次型</strong>：含有n个变量的二次齐次函数叫做二次型。只含平方项叫做二次型的标准型。用矩阵表示二次型就是f = xT Ax，A为对称矩阵。</p><h2 id="3-新视角看待矩阵运算"><a href="#3-新视角看待矩阵运算" class="headerlink" title="3.新视角看待矩阵运算"></a>3.新视角看待矩阵运算</h2><h3 id="1-矩阵"><a href="#1-矩阵" class="headerlink" title="1 矩阵"></a>1 矩阵</h3><p>矩阵<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97.png" alt="">构成行视图的方程组为 2x-y=1;x+y=5，表示的是二维平面的两条直线，方程组的解就是两条直线的交点。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%971.png" alt=""> </p><p>如果用列视图表达，就是<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%972.png" alt="">，表示了两个向量之间的关系，如下图，有向量a（2,1）和向量b（-1,1）,2a+3b=（1,5）。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%973.png" alt=""></p><p>这里我们来探究一下“维度”。数学中的维度和物理研究的维度不可一概而论。数学的维度表示的是独立参数的数目，物理学的维度指的是独立的时空坐标的数目。根据爱因斯坦的理论，我们生活的空间是四维空间，包含了三维的空间+时间。</p><h3 id="2-线性相关和线性无关"><a href="#2-线性相关和线性无关" class="headerlink" title="2 线性相关和线性无关"></a>2 线性相关和线性无关</h3><p>我们先看线性相关和线性无关的定义。在向量空间V的一组向量A，如果存在不全为零的数 k1, k2, ···,km , 使<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%974.png" alt="">则称向量组A是线性相关的，否则数 k1, k2, ···,km全为0时，称它是线性无关。线性无关可以这么理解，如果是二维空间，这组向量的任意两个向量不共线；如果是三维空间，则任意三个向量不共面，如果共面了就可以用另外两个表示第三个向量了不是么。</p><h3 id="3-向量空间的基"><a href="#3-向量空间的基" class="headerlink" title="3 向量空间的基"></a>3 向量空间的基</h3><p>基：向量空间V的一组向量若满足(1)线性无关，(2)V中任一向量可由此向量线性表出，则称该组向量V中的一个基（亦称基底）。</p><h3 id="4-四个基本的子空间"><a href="#4-四个基本的子空间" class="headerlink" title="4 四个基本的子空间"></a>4 四个基本的子空间</h3><p>列空间：是包含所有列的线性组合。列向量是m维的，所以C(A)在Rm里。维数上，A的主列就是列空间的一组基，dim(C(A))=Rank(A)=r，维数就是秩的大小。</p><p>   零空间：n维向量，是Ax=0的所有解的集合，所以N(A)在Rn里。零空间有可能不存在。维数上，一组基就是一组特殊解，r是主变量的个数，n-r是自由变量的个数，零空间的维数等于n-r。</p><p>   行空间：是包含所有行的线性组合。A的行的所有线性组合，即A转置的列的线性组合（因为我们不习惯处理行向量），C(AT)在Rn里。维数上，有一个重要的性质：行空间和列空间维数相同，都等于秩的大小。</p><p>   左零空间：和列空间垂直的空间，交于一个零点，维数为m-r。</p><p>   可以画出四个子空间如下，行空间和零空间在Rn里，他们的维数加起来等于n，列空间和左零空间在Rm里，他们的维数加起来等于m。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%975.png" alt=""></p><h2 id="4-矩阵导数"><a href="#4-矩阵导数" class="headerlink" title="4.矩阵导数"></a>4.矩阵导数</h2><ul><li>分子、分母布局：<br>分子布局：分子为列向量，或者分母为行向量；<br>分母布局：分母为列向量，或者分子为行向量；</li><li>运算规则：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E5%88%86%E5%AD%90%E5%88%86%E6%AF%8D%E5%B8%83%E5%B1%80%E8%BF%90%E7%AE%97.png" alt=""></li><li>需要注意的规则：<br>以下公式默认在分子布局下的结果<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%99.png" alt="">；<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%991.png" alt="">；<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%992.png" alt=""></li><li>常用公式<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E6%B1%82%E5%AF%BC%E6%96%B9%E5%BC%8F.png" alt=""></li></ul><h2 id="5-特征分解"><a href="#5-特征分解" class="headerlink" title="5.特征分解"></a>5.特征分解</h2><h3 id="1-特征分解"><a href="#1-特征分解" class="headerlink" title="1 特征分解"></a>1 特征分解</h3><ul><li>特征分解的性质：对于Ax=λ x,如果所有的特征值都不相同，则相应的所有的特征向量线性无关，此时X可以被对角化为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3.jpg" alt="">。</li></ul><ul><li>对称矩阵的特征分解：如果一个对称矩阵的特征值不同，则其相应的所有的特征向量正交。</li><li>二次型：用于判定一个矩阵是正定矩阵、半正定矩阵、负定矩阵还是不定矩阵。如果矩阵的特征值都大于0，就是正定矩阵。下图中是两个二次型图形的例子，左边是凸函数，右边是非凸函数。之后会详细讲凸函数的优化问题。</li><li><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A31.jpg" alt=""></li></ul><h3 id="2-PCA"><a href="#2-PCA" class="headerlink" title="2 PCA"></a>2 PCA</h3><p>PCA是特征分解的一个有效应用。在进行图像的特征提取的过程中，提取的特征维数太多经常会导致特征匹配时过于复杂，消耗系统资源，不得不采用特征降维的方法。所谓特征降维，即采用一个低纬度的特征来表示高纬度。特征降维一般有两类方法：特征选择和特征抽取。特征选择即从高纬度的特征中选择其中的一个子集来作为新的特征；而特征抽取是指将高纬度的特征经过某个函数映射至低纬度作为新的特征。常用的特征抽取方法就是PCA（Principal Component Analysis）。降维过程举例：比如有矩阵X，希望把它由两行降成一行。</p><p>矩阵X：<img src="http://p4rlzrioq.bkt.clouddn.com/PCA.jpg" alt=""></p><p>1.计算协方差矩阵<img src="http://p4rlzrioq.bkt.clouddn.com/PCA1.jpg" alt=""></p><p>2.计算Cx的特征值为：λ1=2，λ2=2/5。特征值对应的特征向量为<img src="http://p4rlzrioq.bkt.clouddn.com/PCA2.jpg" alt=""></p><p>3.降维：特征向量的转置*X。<img src="http://p4rlzrioq.bkt.clouddn.com/PCA3.jpg" alt="">得到一行的矩阵。</p><h2 id="6-奇异值分解（Singular-Value-Decomposition，SVD）"><a href="#6-奇异值分解（Singular-Value-Decomposition，SVD）" class="headerlink" title="6.奇异值分解（Singular Value Decomposition，SVD）"></a>6.<strong>奇异值分解（Singular Value Decomposition，SVD）</strong></h2><p>  PCA的实现一般有两种方式，分别是特征值分解和SVD分解。SVD奇异值分解可以将一个复杂的矩阵用几个更小的子矩阵表示，每个子矩阵代表了原矩阵的重要特性。 SVD奇异值分解：任何秩为r的矩阵，都可以特征分解为以下公式：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A30.jpg" alt=""> </p><p>其中U和V是正交矩阵。公式表示了SVD与子空间的关系。公式(13)便于分析，但并不计算有效；公式(14)计算有效，但有时候不方便分析；公式(15)方便展开，用于低秩矩阵的计算。另外SVD还提供了计算四个子空间正交基的一种快速方法。</p><p>SVD应用之一图像压缩：给定一幅图像，256<em>512像素，考虑用低秩矩阵近似的方式，存储奇异向量，若保留一个奇异向量k=1，压缩比大致是 (256</em>512) / (256+512) = 170。但是k太小图像的质量也损失较大，实际中k不会这么小，下面四个图分别是原图、k=1、k=10、k=80 时图片的表现。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3.jpg" alt=""></p><blockquote><p><strong>矩阵的特征值分解和奇异值分解有什么区别？</strong></p></blockquote><p>首先，特征值只能作用在一个m<em>m的正方矩阵上，而奇异值分解则可以作用在一个m</em>n的长方矩阵上。其次，奇异值分解同时包含了<strong>旋转</strong>、<strong>缩放</strong>和<strong>投影</strong>三种作用，(1)式中，U和V都起到了对A旋转的作用，而ΣΣ起到了对A缩放的作用。特征值分解只有缩放的效果。</p><h2 id="7-常见距离"><a href="#7-常见距离" class="headerlink" title="7.常见距离"></a>7.常见距离</h2><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p><p>设有两个n维变量\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)和\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)，则一些常用的距离公式定义如下：</p><h3 id="1-闵可夫斯基距离"><a href="#1-闵可夫斯基距离" class="headerlink" title="1.闵可夫斯基距离"></a>1.<strong>闵可夫斯基距离</strong></h3><ul><li>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</li></ul><script type="math/tex; mode=display">d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } }</script><ul><li>实际上，当p=1时，就是<strong>曼哈顿距离</strong>；当p=2时，就是<strong>欧式距离</strong>。当p=无穷时，就是<strong>切比雪夫距离</strong></li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%98%8E%E5%BC%8F%E8%B7%9D%E7%A6%BB.png" alt=""></p><ul><li>切比雪夫距离Python实现如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure><h3 id="2-向量内积"><a href="#2-向量内积" class="headerlink" title="2. 向量内积"></a>2. 向量内积</h3><p>向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的<strong>余弦相似度</strong>（Cosine similarity）：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt=""></p><p>余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度（<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">TF-IDF</a>）和图片相似性（<a href="http://www.ruanyifeng.com/blog/2013/03/similar_image_search_part_ii.html" target="_blank" rel="noopener">histogram</a>）计算上都有它的身影。需要注意一点的是，余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。怎样才能实现平移不变性？这就是下面要说的<strong>皮尔逊相关系数</strong>（Pearson correlation），有时候也直接叫<strong>相关系数</strong>:</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0.png" alt=""></p><p>皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%B3%BB%E6%95%B0.png" alt=""></p><p>由于皮尔逊系数具有的良好性质，在各个领域都应用广泛，例如，在推荐系统根据为某一用户查找喜好相似的用户,进而提供推荐，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。</p><h3 id="3-分类数据点间的距离"><a href="#3-分类数据点间的距离" class="headerlink" title="3. 分类数据点间的距离"></a>3. 分类数据点间的距离</h3><p><strong>汉明距离</strong>（Hamming distance）是指，两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB.png" alt=""></p><p>还可以用简单的<strong>_匹配系数_</strong>来表示两点之间的相似度——匹配字符数/总字符数。</p><p>在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影（两个都是 0），并不能说明两者相似。反而言之，如果两个用户都看过某一部电影（序列中都是 1），则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的<strong>杰卡德相似系数</strong>（Jaccard similarity）。</p><p>在上面的例子中，用 M11 表示两个用户都看过的电影数目，M10 表示用户 A 看过，用户 B 没看过的电影数目，M01 表示用户 A 没看过，用户 B 看过的电影数目，M00 表示两个用户都没有看过的电影数目。Jaccard 相似性系数可以表示为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/Jaccard%20%E7%9B%B8%E4%BC%BC%E6%80%A7%E7%B3%BB%E6%95%B0.png" alt=""></p><hr><h1 id="概率论和统计"><a href="#概率论和统计" class="headerlink" title="概率论和统计"></a>概率论和统计</h1><h2 id="1-统计量"><a href="#1-统计量" class="headerlink" title="1.统计量"></a>1.统计量</h2><h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE1.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE2.png" alt=""></p><h3 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a><strong>Pearson相关系数</strong></h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Pearson%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0.png" alt=""></p><h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a><strong>协方差矩阵</strong></h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt=""></p><h2 id="2-独立与不相关"><a href="#2-独立与不相关" class="headerlink" title="2.独立与不相关"></a>2.<strong>独立与不相关</strong></h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%8B%AC%E7%AB%8B%E4%B8%8E%E4%B8%8D%E7%9B%B8%E5%85%B3.png" alt=""> </p><h2 id="3-贝叶斯公式"><a href="#3-贝叶斯公式" class="headerlink" title="3.贝叶斯公式"></a>3.<strong>贝叶斯公式</strong></h2><p>先看看什么是“先验概率”和“后验概率”，以一个例子来说明：</p><p>假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： <strong>P(患病) = 0.1%</strong>；即：在没有做检验之前，我们预计的患病率为<strong>P(患病)=0.1%</strong>，这个就叫作<strong>“先验概率”</strong>。</p><p>再假设现在有一种该病的检测方法，其检测的准确率为<strong>95%</strong>；即：如果真的得了这种病，该检测法有<strong>95%</strong>的概率会检测出阳性，但也有<strong>5%</strong>的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有<strong>95%</strong>的概率检测出阴性，但也有<strong>5%</strong>的概率检测为阳性。用概率条件概率表示即为：<strong>P(显示阳性|患病)=95%</strong></p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率<strong>P(患病|显示阳性)</strong>，这个其实就称为<strong>“后验概率”。</strong></p><p>而这个叫贝叶斯的人其实就是为我们提供了一种可以<strong>利用先验概率计算后验概率</strong>的方法，我们将其称为<strong>“贝叶斯公式”。</strong></p><p>这里先了解<strong>条件概率公式</strong>：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}</script><p>由条件概率可以得到<strong>乘法公式</strong>：</p><script type="math/tex; mode=display">P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)</script><p>将条件概率公式和乘法公式结合可以得到：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}</script><p>再由<strong>全概率公式</strong>：</p><script type="math/tex; mode=display">P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}</script><p>代入可以得到<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }</script><h2 id="4-常见分布函数"><a href="#4-常见分布函数" class="headerlink" title="4.常见分布函数"></a>4.<strong>常见分布函数</strong></h2><h3 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a><strong>0-1分布</strong></h3><p>0-1分布是单个二值型离散随机变量的分布，其概率分布函数为：</p><script type="math/tex; mode=display">P\left( X=1 \right) =p</script><script type="math/tex; mode=display">P\left( X=0 \right) =1-p</script><h3 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a><strong>几何分布</strong></h3><p>几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为：</p><script type="math/tex; mode=display">P\left( X=k \right) =\left( 1-p \right) ^{k-1} p</script><p>性质：<br>\(E\left( X \right) =\frac{1}{p}\)<br>\(Var\left( X \right) =\frac{1-p}{p^{2} }\)</p><h3 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a><strong>二项分布</strong></h3><p>二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：</p><script type="math/tex; mode=display">P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}</script><p>性质：<br>\(E\left( X \right) =np\)<br>\(Var\left( X \right) =np\left( 1-p \right)\)</p><h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a><strong>高斯分布</strong></h3><p>高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：<br><img src="http://p4rlzrioq.bkt.clouddn.com/488px-Normal_Distribution_PDF.svg.png" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.png" alt=""></p><p>若随机变量X服从一个数学期望为\(\mu\)，方差为\(\sigma ^{2}\)的正态分布，则我们将其记为：\(N\left( \mu ,\sigma^{2} \right)\)决定了正态分布的位置，其标准差\(\sigma\)（方差的开方）决定了正态分布的幅度。</p><h3 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a><strong>指数分布</strong></h3><p>指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了t小时，它总共使用至少t+s小时的条件概率，与从开始使用时算起它使用至少s小时的概率相等。下面这些都属于指数分布：</p><ul><li>婴儿出生的时间间隔</li><li>网站访问的时间间隔</li><li>奶粉销售的时间间隔</li></ul><p>指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间t，就等同于t之内没有任何婴儿出生，即：</p><script type="math/tex; mode=display">P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t}</script><p>则：</p><script type="math/tex; mode=display">P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}</script><p>如：接下来15分钟，会有婴儿出生的概率为：</p><script type="math/tex; mode=display">P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53</script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83.jpg" alt=""></p><h3 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a><strong>泊松分布</strong></h3><p>日常生活中，大量事件是有固定频率的，比如：</p><ul><li>某医院平均每小时出生3个婴儿</li><li>某网站平均每分钟有2次访问</li><li>某超市平均每小时销售4包奶粉</li></ul><p>它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p><p><strong>泊松分布就是描述某段时间内，事件具体的发生概率。</strong>其概率函数为：</p><script type="math/tex; mode=display">P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!}</script><p>其中：</p><p>P表示概率，N表示某种函数关系，t表示时间，n表示数量，1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) ；λ 表示事件的频率。</p><p>还是以上面医院平均每小时出生3个婴儿为例，则\(\lambda =3\)；</p><p>那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：</p><script type="math/tex; mode=display">P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025</script><p>同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：</p><script type="math/tex; mode=display">P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8</script><h2 id="5-常见的分布总结"><a href="#5-常见的分布总结" class="headerlink" title="5.常见的分布总结"></a>5.常见的分布总结</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83.jpg" alt="常见分布"></p><h2 id="6-极大似然估计（MLE）"><a href="#6-极大似然估计（MLE）" class="headerlink" title="6.极大似然估计（MLE）"></a>6.极大似然估计（MLE）</h2><p>极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p><p>求极大似然函数估计值的一般步骤：</p><p>（1） 写出似然函数，即每个随机实验出现概率相乘，为这个抽样出现的概率。<br>（2） 对似然函数取对数，为了方便求导；<br>（3） 对参数求导数。<br>（4） 令导数=0，即求解极值，由实际情况知，该极值为极大值。解似然方程。</p><hr><h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><h2 id="1-导数与梯度"><a href="#1-导数与梯度" class="headerlink" title="1.导数与梯度"></a>1.导数与梯度</h2><ul><li><strong>导数</strong>：一个一元函数函数在某一点的导数描述了这个函数在这一点附近的变化率。</li><li><strong>梯度</strong>:多元函数的导数就是梯度。<ul><li>一阶导数，即梯度（gradient）</li><li>二阶导数，Hessian矩阵</li></ul></li></ul><h2 id="2-泰勒展开"><a href="#2-泰勒展开" class="headerlink" title="2.泰勒展开"></a>2.<strong>泰勒展开</strong></h2><ul><li>一元函数的泰勒展开：<br><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d0070da6cb78cda75d9ff9521f85702c97862673" alt="link"></li><li>基尼指数的图像、熵、分类误差率三者之间的关系。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80.jpg" alt="link"></li></ul><h2 id="3-Lagrange乘子法"><a href="#3-Lagrange乘子法" class="headerlink" title="3.Lagrange乘子法"></a>3.<strong>Lagrange乘子法</strong></h2><p>对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是<strong>把约束项添加到原函数上，然后对构造的新函数求导</strong>。</p><p>对于一个要求极值的函数\(f\left( x,y \right)\)，图上的蓝圈就是这个函数的等高图，就是说 \(f\left( x,y \right) =c_{1} ,c_{2} ,…,c_{n}\)分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组\(\left( x,y \right)\)，使它的\(c_{i}\)值越大越好，但是这点必须满足约束条件\(g\left( x,y \right)\)（在黄线上）。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0.jpg" alt=""></p><p>也就是说\(f(x,y)\)相切，或者说它们的梯度▽ \({f}\)和▽\({g}\)平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为\(\lambda\)倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。</p><p>在<strong>支持向量机模型（SVM）</strong>的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。</p><hr><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.<strong>信息熵</strong></h2><p>熵的概念最早由统计热力学引入。</p><p>信息熵是由信息论之父香农提出来的，它用于随机变量的不确定性度量，先上信息熵的公式。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="link"><br>信息是用来减少随机不确定性的东西（即不确定性的减少）。</p><p>我们可以用log ( 1/P )来衡量不确定性。P是一件事情发生的概率，概率越大，不确定性越小。</p><p>可以看到信息熵的公式，其实就是log ( 1/P )的期望，就是不确定性的期望，它代表了一个系统的不确定性，信息熵越大，不确定性越大。</p><p>注意这个公式有个默认前提，就是X分布下的随机变量x彼此之间相互独立。还有log的底默认为2，实际上底是多少都可以，但是在信息论中我们经常讨论的是二进制和比特，所以用2。</p><p>信息熵在联合概率分布的自然推广，就得到了联合熵</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B51.png" alt=""></p><p>当X, Y相互独立时，H(X, Y) = H(X) + H(Y)</p><h3 id="信息熵的实例解释"><a href="#信息熵的实例解释" class="headerlink" title="信息熵的实例解释"></a><strong>信息熵的实例解释</strong></h3><p>举个例子说明信息熵的作用。</p><p>例子是知乎上看来的，我觉得讲的挺好的。</p><blockquote><p>比如赌马比赛，有4匹马{ A, B, C, D}，获胜概率分别为{ 1/2, 1/4, 1/8, 1/8 }，将哪一匹马获胜视为随机变量X属于 { A, B, C, D } 。</p><p>假定我们需要用尽可能少的二元问题来确定随机变量 X 的取值。</p><p>例如，问题1：A获胜了吗？　问题2：B获胜了吗？　问题3：C获胜了吗？</p><p>最后我们可以通过最多3个二元问题，来确定取值。</p><p>如果X = A，那么需要问1次（问题1：是不是A？），概率为1/2 </p><p>如果X = B，那么需要问2次（问题1：是不是A？问题2：是不是B？），概率为1/4 </p><p>如果X = C，那么需要问3次（问题1，问题2，问题3），概率为1/8 </p><p>如果X = D，那么需要问3次（问题1，问题2，问题3），概率为1/8 </p><p>那么为确定X取值的二元问题的数量为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B52.png" alt=""></p><p>回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B52.png" alt=""> </p><p>在二进制计算机中，一个比特为0或1，其实就代表了一个二元问题的回答。也就是说，在计算机中，我们给哪一匹马夺冠这个事件进行编码，所需要的平均码长为1.75个比特。</p><p>很显然，为了尽可能减少码长，我们要给发生概率 p(x)较大的事件，分配较短的码长 l(x)。这个问题深入讨论，可以得出霍夫曼编码的概念。</p><p>霍夫曼编码就是利用了这种大概率事件分配短码的思想，而且可以证明这种编码方式是最优的。我们可以证明上述现象：</p><ul><li>为了获得信息熵为 H(X) 的随机变量 X 的一个样本，平均需要抛掷均匀硬币（或二元问题） H(X)次（参考猜赛马问题的案例）</li><li>信息熵是数据压缩的一个临界值（参考码长部分的案例）</li></ul></blockquote><p>所以，信息熵H(X)可以看做，对X中的样本进行编码所需要的编码长度的期望值。</p><h2 id="2-相对熵-交叉熵-K-L散度"><a href="#2-相对熵-交叉熵-K-L散度" class="headerlink" title="2.相对熵/交叉熵/K-L散度"></a>2.<strong>相对熵/交叉熵/K-L散度</strong></h2><p>这里可以引申出交叉熵的理解，现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p。</p><p>按照真实分布p来编码样本所需的编码长度的期望为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B51.png" alt="">，这就是上面说的<strong>信息熵H( p )</strong></p><p>按照不真实分布q来编码样本所需的编码长度的期望为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B52.png" alt="">，这就是所谓的<strong>交叉熵H( p,q )</strong><br>交叉熵和熵，相当于，协方差和方差</p><p>这里引申出<strong>KL散度D(p||q)</strong> = H(p,q) - H(p) = <img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B53.png" alt="">，也叫做<strong>相对熵</strong>，它表示两个分布的差异，差异越大，相对熵越大。</p><p>机器学习中，我们用非真实分布q去预测真实分布p，因为真实分布p是固定的，D(p||q) = H(p,q) - H(p) 中 H(p) 固定，也就是说交叉熵H(p,q)越大，相对熵D(p||q)越大，两个分布的差异越大。</p><p>所以交叉熵用来做损失函数就是这个道理，它衡量了真实分布和预测分布的差异性。</p><p>用图像形象化的表示二者之间的关系可以如下图：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B54.png" alt="这里写图片描述"><br>上面是q所含的信息量/平均编码长度H(p)<br>第二行是cross-entropy，即用q来编码p所含的信息量/平均编码长度|或者称之为q对p的cross-entropy<br>第三行是上面两者之间的差值即为q对p的KL距离</p><p>从编码的角度，可以这样简单总结，信息熵是最优编码（最短的平均码长），交叉熵是非最优编码（大于最短的平均码长），KL散度是两者的差异（距离最优编码的差距）。</p><h2 id="3-互信息（信息增益）"><a href="#3-互信息（信息增益）" class="headerlink" title="3.互信息（信息增益）"></a>3.<strong>互信息（信息增益）</strong></h2><p>互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量。<br><strong>决策树中的信息增益就是互信息</strong>，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%92%E4%BF%A1%E6%81%AF.png" alt=""></p><p>可以通过简单的计算得到：</p><p>H(X|Y) = H(X) - I(X, Y), </p><p>互信息为0，则随机变量X和Y是互相独立的。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%92%E4%BF%A1%E6%81%AF1.png" alt=""></p><h2 id="信息论与机器学习的关系"><a href="#信息论与机器学习的关系" class="headerlink" title="信息论与机器学习的关系"></a>信息论与机器学习的关系</h2><div class="table-container"><table><thead><tr><th>信息论视角</th><th>机器学习视角</th></tr></thead><tbody><tr><td>接受信号</td><td>特征</td></tr><tr><td>信源</td><td>标签</td></tr><tr><td>平均互信息</td><td>特征有效性分析</td></tr><tr><td>最大熵模型</td><td>极大似然法</td></tr><tr><td>交叉熵</td><td>逻辑回归损失函数</td></tr></tbody></table></div><h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a><strong>最大熵模型</strong></h2><p>最大熵模型的原则：</p><ul><li>承认已知事物（知识）；</li><li>对未知事物不做任何假设，没有任何偏见。</li></ul><p>对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。</p><p>因为这时概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”（Maximum Entropy）。</p><p>Logistic回归是统计学习中的经典分类方法，可以用于二类分类也可以用于多类分类。</p><p>最大熵模型由最大熵原理推导出来，最大熵原理是概率模型学习或估计的一个准则，最大熵原理认为在所有可能的概率模型的集合中，熵最大的模型是最好的模型，最大熵模型也可以用于二类分类和多类分类。</p><p>Logistic回归模型与最大熵模型都属于对数线性模型。</p><p>逻辑回归跟最大熵模型<strong>没有本质区别</strong>。逻辑回归是最大熵对应类别为<strong>二类</strong>时的特殊情况</p><p><strong>指数簇分布的最大熵</strong>等价于其<strong>指数形式的最大似然</strong>。</p><p><strong>二项式</strong>分布的最大熵解等价于二项式指数形式(<strong>sigmoid</strong>)的最大似然；<br><strong>多项式</strong>分布的最大熵等价于多项式分布指数形式(<strong>softmax</strong>)的最大似然。</p><h2 id="熵总结"><a href="#熵总结" class="headerlink" title="熵总结"></a>熵总结</h2><ul><li><strong>熵：不确定性的度量；</strong></li><li><strong>似然：与知识的吻合程度；</strong></li><li><strong>最大熵模型：对不确定度的无偏分配；</strong></li><li><strong>最大似然估计：对知识的无偏理解。</strong></li></ul><h2 id="2-上溢和下溢"><a href="#2-上溢和下溢" class="headerlink" title="2.上溢和下溢"></a>2.<strong>上溢和下溢</strong></h2><p>在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。</p><p>一种特别毁灭性的舍入误差是<strong>下溢</strong>。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除<strong>。</strong></p><p>另一个极具破坏力的数值错误形式是<strong>上溢(overflow)</strong>。当大量级的数被近似为\(\varpi\)时发生上溢。进一步的运算通常将这些无限值变为非数字。</p><p>必须对上溢和下溢进行数值稳定的一个例子是<strong>softmax 函数</strong>。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%B8%8A%E6%BA%A2%E5%92%8C%E4%B8%8B%E6%BA%A2.jpg" alt=""></p><p>softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 \(x_{i}\) 的类别标签 \(y_{i}\)属于K个类别的概率，最后判别 \(y_{i}\)所属类别时就是将其归为对应概率最大的那一个。</p><p>当式中的\(w_{k} x_{i} +b\)都是很小的负数时，\(e^{w_{k} x_{i} +b }\)就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的\(x_{w_{k} x_{i} +b}\)是很大的正数时，\(e^{w_{k} x_{i} +b }\)就会发生上溢导致结果是未定的。</p><hr><h1 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a><strong>凸优化</strong></h1><h2 id="1-凸集-Convex-Sets"><a href="#1-凸集-Convex-Sets" class="headerlink" title="1.凸集(Convex Sets)"></a>1.凸集(Convex Sets)</h2><p>集合C是凸的，如果对于所有的\(x,y\in C\)和\(\theta\in\mathbb{R},0\leq\theta\leq 1\)有：</p><script type="math/tex; mode=display">\theta x+(1-\theta)y\in C</script><p>可以这样理解：在集合C中任选两点，在这两点的连线上的所有点都属于集合C。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E9%9B%86.jpg" alt=""></p><h2 id="2-凸函数-Convex-Fuctions"><a href="#2-凸函数-Convex-Fuctions" class="headerlink" title="2.凸函数(Convex Fuctions)"></a>2.凸函数(Convex Fuctions)</h2><p>如果函数的定义域\({\cal D}(f)\)是一个凸集，并且对于所有的\(x,y\in {\cal D}(f)\)和\(\theta\in\mathbb{R},0\leq\theta\leq1\)，都有：<br>\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\)<br><img src="http://oddpnmpll.bkt.clouddn.com/2016-10-05-01%3A23%3A27.jpg" alt=""></p><h3 id="凸函数的一阶条件"><a href="#凸函数的一阶条件" class="headerlink" title="凸函数的一阶条件"></a>凸函数的一阶条件</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E6%9D%A1%E4%BB%B6.png" alt=""><br>直观上可以这样理解，在函数上随便挑一个点，该点的切线必然在函数的下方<br><img src="http://oddpnmpll.bkt.clouddn.com/2016-10-05-01%3A27%3A53.jpg" alt=""></p><h3 id="凸性质的二阶条件"><a href="#凸性质的二阶条件" class="headerlink" title="凸性质的二阶条件"></a>凸性质的二阶条件</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%BA%8C%E9%98%B6%E6%9D%A1%E4%BB%B6.png" alt=""></p><h3 id="琴生不等式-Jensen’s-Inequality"><a href="#琴生不等式-Jensen’s-Inequality" class="headerlink" title="琴生不等式(Jensen’s Inequality)"></a>琴生不等式(Jensen’s Inequality)</h3><p>假设凸函数的基本定义为:<br>\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\ \ \ \text{for} \ \ \ 0\leq\theta\leq1\)<br>上述等式可以扩展到多个点:<br>\(f\left(\sum_{i=1}^k\theta_ix_i\right)\leq\sum_{i=1}^k\theta_if(x_i)\ \ \ \text{for}\ \ \ \sum_{i=1}^k\theta_i=1,\theta_i\geq0 \ \ \forall i\)<br>再将上述等式扩展到积分形式:<br>\(f\left(\int p(x)xdx\right)\leq\int p(x)f(x)dx\ \ \ \text{for}\ \ \ \int p(x)dx=1,p(x)\leq0\ \ \forall x\)<br>由于\(p(x)\)积分为1，我们可以把\(p(x)\)看作是一个概率密度函数，所以尚属等式可以用以下形式表达：<br>\(f(\mathbb{E}[x])\leq\mathbb{E}[f(x)]\)<br>最后一条等式就是著名的<strong>琴生不等式</strong>。</p><h2 id="3-凸优化问题-Convex-Optimization-Problems"><a href="#3-凸优化问题-Convex-Optimization-Problems" class="headerlink" title="3.凸优化问题(Convex Optimization Problems)"></a>3.凸优化问题(Convex Optimization Problems)</h2><p>在凸优化问题中，一个最关键的点就是<strong>对于一个凸优化问题，所有的局部最优解(locally optimal)都是全局最优解(globally optimal)</strong>。<br>最优化的基本数学模型如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98.jpg" alt=""><br>它有三个基本要素，即：</p><ul><li>设计变量：x是一个实数域范围内的n维向量，被称为决策变量或问题的解；</li><li>目标函数：f(x)为目标函数；</li><li>约束条件：\(h_{i} \left( x \right) =0\)称为等式约束，\(g_{i} \left( x \right) \leq 0\)为不等式约束，\(i=0,1,2,……\)</li></ul><h2 id="4-牛顿法"><a href="#4-牛顿法" class="headerlink" title="4.牛顿法"></a>4.牛顿法</h2><h3 id="牛顿法介绍"><a href="#牛顿法介绍" class="headerlink" title="牛顿法介绍"></a><strong>牛顿法介绍</strong></h3><p><strong>牛顿法</strong>也是求解<strong>无约束最优化</strong>问题常用的方法，<strong>最大的优点是收敛速度快</strong>。</p><p>从本质上去看，<strong>牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快</strong>。<strong>通俗地说</strong>，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%95.jpg" alt=""></p><p>或者从几何上说，<strong>牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面</strong>，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><h3 id="牛顿法的推导"><a href="#牛顿法的推导" class="headerlink" title="牛顿法的推导"></a><strong>牛顿法的推导</strong></h3><p>将目标函数\(f\left( x \right)\) 在\(x_{k}\)处进行二阶泰勒展开，可得：</p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k} \right) +f^{'} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{''}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}</script><p>因为目标函数\(f\left( x \right)\)有极值的必要条件是在极值点处一阶导数为0，即：\(f^{‘} \left( x \right) =0\)</p><p>所以对上面的展开式两边同时求导（注意\({x}\)才是变量，\(x_{k}\)是常量\(\Rightarrow f^{‘} \left( x_{k} \right) ,f^{‘’} \left( x_{k} \right)\)都是常量），并令\(f^{‘} \left( x \right) =0\)可得：</p><script type="math/tex; mode=display">f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0</script><p>即：</p><script type="math/tex; mode=display">x=x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>于是可以构造如下的迭代公式：</p><script type="math/tex; mode=display">x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>这样，我们就可以利用该迭代式依次产生的序列逐渐逼近\(f\left( x \right)\)的极小值点了。</p><p>牛顿法的迭代示意图如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%95%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%A4%BA%E6%84%8F%E5%9B%BE%E5%A6%82%E4%B8%8B.jpg" alt=""></p><p>上面讨论的是2维情况，<strong>高维情况</strong>的牛顿迭代公式是：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%AB%98%E7%BB%B4%E6%83%85%E5%86%B5%E7%9A%84%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E5%85%AC%E5%BC%8F.jpg" alt=""></p><p>式中， ▽\({f}\)是\(f\left( x \right)\)的梯度，即：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%953.jpg" alt=""></p><p>H是Hessen矩阵，即：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hessen%E7%9F%A9%E9%98%B5.jpg" alt=""></p><h3 id="牛顿法的过程"><a href="#牛顿法的过程" class="headerlink" title="牛顿法的过程"></a><strong>牛顿法的过程</strong></h3><ul><li>1、给定初值\(x_{0} ]\)和精度阈值\(\varepsilon\)，并令\(k=0\)；</li><li>2、计算\(x_{k}\)和\(H_{k}\)；</li><li>3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)；</li><li>4、计算新的迭代点：\(x_{k+1} =x_{k} +d_{k}\)；</li><li>5、令\(k=k+1\)，转至2。</li></ul><h2 id="5-阻尼牛顿法"><a href="#5-阻尼牛顿法" class="headerlink" title="5.阻尼牛顿法"></a>5.<strong>阻尼牛顿法</strong></h2><h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a><strong>引入</strong></h3><p>注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现\(f\left( x_{k+1} \right) &gt;f\left( x_{k} \right)\)的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p><p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是\(x_{k}\)，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子\(\lambda _{k}\)，即：</p><p>\(\lambda _{k} = minf\left( x_{k} +\lambda d_{k} \right)\)</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a><strong>算法过程</strong></h3><ul><li>1、给定初值\(x_{0}\)和精度阈值\(\varepsilon\)，并令\(k=0\)；</li><li>2、计算\(g_{k}\)（\(f\left( x \right)\)在\(x_{k}\)处的梯度值）和\(H_{k}\)；</li><li>3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)；</li><li>4、利用\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)得到步长\(\lambda _{k}\)，并令\(x_{k+1} =x_{k} +\lambda _{k} d_{k}\)</li><li>5、令\(k=k+1\)，转至2。</li></ul><h2 id="6-拟牛顿法"><a href="#6-拟牛顿法" class="headerlink" title="6.拟牛顿法"></a>6.<strong>拟牛顿法</strong></h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>由于<strong>牛顿法</strong>每一步都要求解目标函数的<strong>Hessen矩阵的逆矩阵</strong>，<strong>计算量比较大</strong>（求矩阵的逆运算量比较大），因此提出一种<strong>改进方法</strong>，即<strong>通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程</strong>，改进后的方法称为<strong>拟牛顿法</strong>。</p><h3 id="拟牛顿法的推导"><a href="#拟牛顿法的推导" class="headerlink" title="拟牛顿法的推导"></a><strong>拟牛顿法的推导</strong></h3><p>先将目标函数在\(x_{k+1}\)处展开，得到：</p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k+1} \right) +f^{'} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{''}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}</script><p>两边同时取梯度，得：</p><script type="math/tex; mode=display">f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>取上式中的\(x=x_{k}\)，得：</p><script type="math/tex; mode=display">f^{'}\left( x_{k} \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>即：</p><script type="math/tex; mode=display">g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)</script><p>可得：</p><script type="math/tex; mode=display">H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}</script><p>上面这个式子称为<strong>“拟牛顿条件”</strong>，由它来对Hessen矩阵做约束。</p><hr><h1 id="计算复杂性与NP问题"><a href="#计算复杂性与NP问题" class="headerlink" title="计算复杂性与NP问题"></a><strong>计算复杂性与NP问题</strong></h1><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>表明问题规模扩大后，程序需要的时间长度增长得有多快。程序的时间复杂度一般可以分为两种级别：</p><ul><li>多项式级的复杂度，如O(1)，O(log(n))、O（n^a）等，</li><li>非多项式级的，如O(a^n)、O(n!)等。后者的复杂度计算机往往不能承受。</li></ul><h2 id="约化-Reducibility"><a href="#约化-Reducibility" class="headerlink" title="约化(Reducibility)"></a>约化(Reducibility)</h2><p>简单的说，一个问题A可以约化为问题B的含义是，可以用问题B的解法解决问题A。（个人感觉也就是说，问题A是B的一种特殊情况。）标准化的定义是，如果能找到一个变化法则，对任意一个A程序的输入，都能按照这个法则变换成B程序的输入，使两程序的输出相同，那么我们说，问题A可以约化为问题B。</p><p>例如求解一元一次方程这个问题可以约化为求解一元二次方程，即可以令对应项系数不变，二次项的系数为0，将A的问题的输入参数带入到B问题的求解程序去求解。</p><p>另外，约化还具有传递性，A可以化约为B，B可以约化为C，那么A也可以约化为C。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="P-Problem"><a href="#P-Problem" class="headerlink" title="P Problem"></a>P Problem</h3><p>假设有 n 个数要排序。一个初级的冒泡排序算法所需时间可能与 n2 成正比，快一点的算法所需时间与 nlog（n） 成正比。在某些条件下，桶排序算法所需时间甚至只和 n 成正比。最不实用的算法就是输入的数字随机排列，直到出现完全有序的情况为止……记前三个算法的时间复杂度分别记为 O(n2)、O(nlogn) 和 O(n)，最后的<a href="http://zh.wikipedia.org/wiki/Bogo%E6%8E%92%E5%BA%8F" target="_blank" rel="noopener">“猴子排序”(Bogosort)算法</a>平均时间复杂度则达到了 O(n*n!)。</p><p>在上面的例子中，前三种算法的复杂度是 n 的多项式函数；最后一种算法的复杂度是 n 的阶乘，根据斯特林公式，n! 相当于指数级别的增长。当 n 特别小时，多项式级的算法已经快过指数级的算法。当 n 非常大时，人类根本看不到指数级复杂度算法结束的那天。自然的，大家会对多项式级别的算法抱有好感，希望对每一个问题都能找到多项式级别的算法。问题是——每个问题都能找到想要的多项式级别的算法吗？</p><p>在一个由问题构成的集合中，如果每个问题都存在多项式级复杂度的算法，这个集合就是 P 类问题（Polynomial）。</p><h3 id="NP-Nondeterministic-Polynomial-问题"><a href="#NP-Nondeterministic-Polynomial-问题" class="headerlink" title="NP (Nondeterministic Polynomial)问题"></a>NP (Nondeterministic Polynomial)问题</h3><p>NP 类问题指的是，能在多项式时间内<strong>检验</strong>一个解是否正确的问题。比如我的机器上存有一个密码文件，于是就能在多项式时间内验证另一个字符串文件是否等于这个密码，所以“破译密码”是一个 NP 类问题。NP 类问题也等价为能在多项式时间内<strong>猜出</strong>一个解的问题。这里的“猜”指的是如果有解，那每次都能在很多种可能的选择中运气极佳地选择正确的一步。</p><p>不妨举个例子：给出 n 个城市和两两之间的距离，求找到一个行走方案，使得到达每个城市一次的总路程最短。我们可以这样来“猜测”它的解：先求一个总路程不超过 100 的方案，假设我们可以依靠极好的运气“猜出”一个行走路线，使得总长度确实不超过 100，那么我们只需要每次猜一条路一共猜 n 次。接下来我们再找总长度不超过 50 的方案，找不到就将阈值提高到75…… 假设最后找到了总长度为 90 的方案，而找不到总长度小于 90 的方案。我们最终便在多项式时间内“猜”到了这个旅行商问题的解是一个长度为 90 的路线。它是一个 NP 类的问题。</p><p>也就是说，NP 问题能在多项式时间内“解决”，只不过需要好运气。显然，P 类问题肯定属于 NP 类问题。所谓“P=NP”，就是问——是不是所有的 NP 问题，都能找到多项式时间的确定性算法？</p><h3 id="NPC-Problem"><a href="#NPC-Problem" class="headerlink" title="NPC Problem"></a>NPC Problem</h3><p>在与数不尽的问题搏斗的过程中，人们有时候会发现，解决问题 A 的算法可以同时用来解决问题 B。例如问题 A 是对学生的姓名与所属班级同时排序，问题 B 是对人们按照姓名做排序。这时候，我们只需要让班级全都相同，便能照搬问题 A 的算法来解决问题 B。这种情况下，数学家就说，问题 B 能归约为问题 A。</p><p>人们发现，不同的 NP 问题之间也会出现可归约的关系，甚至存在这么一类（不只是一个）问题，使得任何其它的 NP 问题都能归约到它们上。也就是说，能够解决它们的算法就能够解决所有其它的 NP 问题。这一类问题就是 NPC 问题。这样的问题人们已经找到了几千个，如果我们给其中任何一个找到了多项式级别的算法，就相当于证明了 P=NP。</p><p>但NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。</p><h3 id="P-NP？"><a href="#P-NP？" class="headerlink" title="P=NP？"></a>P=NP？</h3><p>证明 P=NP 的一个主要方法就是，给某一个 NPC 问题找到一个快速算法。但是，也不排除有人给出一个“存在性”而非“构造性”的证明，只是告诉大家存在符合要求的算法，但没法详细描述出来。如果 P=NP 被人以这种方式证明出来了，我们也没法依葫芦画瓢地把这个神奇的算法在电脑上写出来，所以对破解密码仍然没有帮助。</p><p>退一步说，假如有人构造出可以运用的多项式算法，以此证明了这个问题。这个算法恐怕也很复杂（毕竟这么难找），它的多项式级别的复杂度也可能会非常慢。假设这个算法的复杂度达到了 O(n10)，那我们依然面临着不小的麻烦。即使 n=100，运算时间也会增长到非常巨大的地步。</p><p>再退一步，假设人类的运气好到 P=NP 是真的，并且找到了复杂度不超过 O(n3) 的算法。如果到了这一步，我们就会有一个算法，能够很快算出某个帐号的密码。《基本演绎法》里面所想象的可能就要成真了，所有的加密系统都会失去效果——应该说，所有会把密码变成数字信息的系统都会失去效果，因为这个数字串很容易被“金钥匙”计算出来。</p><p>除此之外，我们需要担心或期许的事情还有很多：</p><ul><li>一大批耳熟能详的游戏，如扫雷、俄罗斯方块、超级玛丽等，人们将为它们编写出高效的AI，使得电脑玩游戏的水平无人能及。</li><li>整数规划、旅行商问题等许多运筹学中的难题会被高效地解决，这个方向的研究将提升到前所未有的高度。</li><li>蛋白质的折叠问题也是一个 NPC 问题，新的算法无疑是生物与医学界的一个福音。</li></ul><p>参考文献：<br><a href="http://www.junnanzhu.com/?p=141" target="_blank" rel="noopener">http://www.junnanzhu.com/?p=141</a><br><a href="https://www.zybuluo.com/frank-shaw/note/139175" target="_blank" rel="noopener">https://www.zybuluo.com/frank-shaw/note/139175</a><br><a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-09-Visual-Information/</a><br><a href="https://www.guokr.com/article/437662/" target="_blank" rel="noopener">https://www.guokr.com/article/437662/</a><br><a href="https://www.zhihu.com/question/22178202" target="_blank" rel="noopener">https://www.zhihu.com/question/22178202</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/ai%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础（一）</title>
    <link href="http://frankblog.site/2018/05/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <id>http://frankblog.site/2018/05/18/机器学习基础/</id>
    <published>2018-05-18T04:57:16.128Z</published>
    <updated>2018-06-19T16:33:37.609Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="机器学习应用"><a href="#机器学习应用" class="headerlink" title="机器学习应用"></a>机器学习应用</h1><h2 id="1、计算机视觉"><a href="#1、计算机视觉" class="headerlink" title="1、计算机视觉"></a>1、计算机视觉</h2><p>典型的应用包括：<strong>人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索</strong>等等。</p><h2 id="2、自然语言处理"><a href="#2、自然语言处理" class="headerlink" title="2、自然语言处理"></a>2、自然语言处理</h2><p>典型的应用包括：<strong>搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译</strong>等等。</p><h2 id="3、社会网络分析"><a href="#3、社会网络分析" class="headerlink" title="3、社会网络分析"></a>3、社会网络分析</h2><p>典型的应用包括：<strong>用户画像、网络关联分析、欺诈作弊发现、热点发现</strong>等等。</p><h2 id="4、推荐系统"><a href="#4、推荐系统" class="headerlink" title="4、推荐系统"></a>4、推荐系统</h2><p>典型的应用包括：<strong>虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”</strong>等等。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt=""></p><h2 id="数据挖掘流程"><a href="#数据挖掘流程" class="headerlink" title="数据挖掘流程"></a>数据挖掘流程</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%B5%81%E7%A8%8B.jpg" alt=""></p><h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><h2 id="数据分类"><a href="#数据分类" class="headerlink" title="数据分类"></a>数据分类</h2><p>正例(positive example)<br>反例(negative example)<br>训练集(training set)<br>验证集(validation set)：用作超参数验证<br>测试集(test set)<br>类别不平衡数据集（class-imbalanced data set）</p><h2 id="采样方式"><a href="#采样方式" class="headerlink" title="采样方式"></a>采样方式</h2><h3 id="1、分层采样-stratified-sampling"><a href="#1、分层采样-stratified-sampling" class="headerlink" title="1、分层采样(stratified sampling)"></a>1、分层采样(stratified sampling)</h3><p>保留类别比例的采样方式通常称为分层采样</p><h3 id="2、留出法（hold-out）"><a href="#2、留出法（hold-out）" class="headerlink" title="2、留出法（hold-out）"></a>2、留出法（hold-out）</h3><p>直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p><h3 id="3、k折交叉验证（k-fold-cross-validation）"><a href="#3、k折交叉验证（k-fold-cross-validation）" class="headerlink" title="3、k折交叉验证（k-fold cross validation）"></a>3、k折交叉验证（k-fold cross validation）</h3><p>交叉验证先将数据集D划分为k个大小相似的互斥子集，每个子集从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。</p><h3 id="4、自助法-bootstrapping"><a href="#4、自助法-bootstrapping" class="headerlink" title="4、自助法(bootstrapping)"></a>4、自助法(bootstrapping)</h3><p>对数据集D有放回的随机采样m次后，一个样本不在样本集D1出现的概率：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%87%AA%E5%8A%A9%E6%B3%95.png" alt=""></p><p>当n足够大时，大约有36.8%的样本不会被采到，用没采到的部分做测试集，也是包外估计（out-of-bag-estimate）。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。</p><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="1、缺失值处理"><a href="#1、缺失值处理" class="headerlink" title="1、缺失值处理"></a>1、缺失值处理</h2><p>1.直接删除——适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况。<br>2.使用一个全局常量填充—-譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用<br>3.使用均值或中位数代替——优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好。<br>4.插补法<br>    1）随机插补法——从总体中随机抽取某个样本代替缺失样本<br>    2）多重插补法——通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理<br>    3）热平台插补——指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。<br>优点：简单易行，准去率较高<br>缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补<br>    4)拉格朗日差值法和牛顿插值法<br>5.建模法<br>可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。</p><p><strong>使用sklearn进行插补：</strong><br>其实sklearn里也有一个工具<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer" target="_blank" rel="noopener">Imputer</a>可以对缺失值进行插补。Imputer类可以对缺失值进行均值插补、中位数插补或者某行/列出现的频率最高的值进行插补，也可以对不同的缺失值进行编码。<strong>并且支持稀疏矩阵。</strong></p><p>在稀疏矩阵中，缺失值被编码为0存储为矩阵中，这种格式是适合于缺失值比非缺失值多得多的情况。此外，Imputer类也可以用于Pipeline中。</p><blockquote><p>Imputor类的参数：_class _<code>sklearn.preprocessing.``Imputer</code>(_missing_values=’NaN’_, _strategy=’mean’_, _axis=0_, _verbose=0_, _copy=True_)</p><p><strong>missing_values</strong> : int或”NaN”,默认NaN（String类型）<br><strong>strategy</strong> : string, 默认为mean，可选则mean、median、most_frequent<br><strong>axis</strong> :int, 默认为0（axis = 0，对列进行插值；axis= 1，对行进行插值）<br><strong>verbose</strong> : int, 默认为0<br><strong>copy</strong> : boolean, 默认为True<br>　　True：会创建一个X的副本<br>　　False：在任何合适的地方都会进行插值。<br>　　但是以下四种情况，计算设置的copy = Fasle，也会创建一个副本：<br>　　1.X不是浮点型数组<br>　　2.X是稀疏矩阵，而且miss_value = 0<br>　　3.axis= 0，X被编码为CSR矩阵<br>　　4.axis= 1，X被编码为CSC矩阵</p></blockquote><p>p.s.：LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。</p><h2 id="2、异常点处理"><a href="#2、异常点处理" class="headerlink" title="2、异常点处理"></a>2、异常点处理</h2><p>1.简单的统计分析<br>　　拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。<br>2.3∂原则<br>　　如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。</p><p>3.箱型图分析<br>　　箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。</p><p>4.基于模型检测<br>　　首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象</p><p>优缺点：1.有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；2.对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。</p><p>5.基于距离<br>　　通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象</p><p>优缺点：1.简单；2.缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；3.该方法对参数的选择也是敏感的；4.不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。</p><p>6.基于密度<br>　　当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。</p><p>优缺点：1.给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；2.与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；3.参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。</p><p>7.基于聚类：<br>　　基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。</p><p>优缺点：1.基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；2.簇的定义通常是离群点的补，因此可能同时发现簇和离群点；3.产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；4.聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。</p><p><strong>处理方法：</strong></p><p>1.删除异常值——明显看出是异常且数量较少可以直接删除<br>2.不处理—-如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。<br>3.平均值替代——损失信息小，简单高效。<br>4.视为缺失值——可以按照处理缺失值的方法来处理<br>5.标准化——如果你的数据有离群点，对数据进行均差和方差的标准化效果并不好。这种情况你可以使用<code>sklearn</code>中的<code>robust_scale</code>和 <code>RobustScaler</code> 作为替代。</p><h2 id="3、去重处理"><a href="#3、去重处理" class="headerlink" title="3、去重处理"></a>3、去重处理</h2><h3 id="dataframe格式"><a href="#dataframe格式" class="headerlink" title="dataframe格式"></a>dataframe格式</h3><p>1、DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行<br>2、drop_duplicates方法用于返回一个移除了重复行的DataFrame<br>3、data.drop_duplicates([‘v1’]) #只判断某列</p><h3 id="list格式"><a href="#list格式" class="headerlink" title="list格式"></a>list格式</h3><p>1、使用set()<br>2、{}.fromkeys().keys()<br>3、set()+sort()<br>4、排序后比较相邻2个元素的数据，重复的删除</p><h2 id="4、噪音处理"><a href="#4、噪音处理" class="headerlink" title="4、噪音处理"></a>4、<strong>噪音处理</strong></h2><p> 噪音，是被测量变量的随机误差或方差。</p><h3 id="噪音与离群点"><a href="#噪音与离群点" class="headerlink" title="噪音与离群点"></a>噪音与离群点</h3><blockquote><p>离群点： 你正在从口袋的零钱包里面穷举里面的钱，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。这个100元就是个离群点，因为并不应该常出现在口袋里..</p><p>噪声： 你晚上去三里屯喝的酩酊大醉，很需要买点东西清醒清醒，这时候你开始翻口袋的零钱包，嘛，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。但是你突然眼晕，把那三个一角看成了三个1元…这样错误的判断使得数据集中出现了噪声。</p></blockquote><h3 id="噪音处理方法"><a href="#噪音处理方法" class="headerlink" title="噪音处理方法"></a>噪音处理方法</h3><p><strong>1.分箱法</strong><br>分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。</p><ul><li>用箱均值光滑：箱中每一个值被箱中的平均值替换。</li><li>用箱中位数平滑：箱中的每一个值被箱中的中位数替换。</li><li>用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。</li></ul><p>一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用.</p><p><strong>2.  回归法</strong><br>　　可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。</p><h2 id="5、其他实用小技巧"><a href="#5、其他实用小技巧" class="headerlink" title="5、其他实用小技巧"></a>5、其他实用小技巧</h2><p>1.<strong>去掉文件中多余的空行</strong><br>空行主要指的是（\n,\r,\r\n,\n\r等），在python中有个strip()的方法，该方法可以去掉字符串两端多余的“空白”，此处的空白主要包括空格，制表符(\t)，换行符。不过亲测以后发现，strip()可以匹配掉\n,\r\n,\n\r等，但是过滤不掉单独的\r。为了万无一失，我还是喜欢用麻烦的办法。</p><p>2.<strong>如何判断文件的编码格式</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import chardet</span><br><span class="line">if chardet.detect(data)[&apos;encoding&apos;] != &apos;utf-8&apos;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">批量处理编码格式转换的代码已上传到github上</span><br></pre></td></tr></table></figure><h1 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h1><h2 id="1、离散型"><a href="#1、离散型" class="headerlink" title="1、离散型"></a>1、离散型</h2><p>1、<strong>one-hot 编码</strong><br>编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。</p><blockquote><p>sklearn.preprocessing.OneHotEncoder(_n_values=’auto’_, _categorical_features=’all’_, _dtype=<class 'float'="">_, _sparse=True_, _handle_unknown=’error’_)</class></p><p><strong>n_values</strong> : ‘auto’, int or array of ints 每个特征的数量</p><blockquote><p>auto : 从训练数据的范围中得到<br>     int : 所有特征的最大值（number）<br>     array : 每个特征的最大值（number）</p></blockquote><p><strong>categorical_features: “all” or array of indices or mask</strong> :确定哪些特征是类别特征</p><blockquote><p>all (默认): 所有特征都是类别特征，意味着所有特征都要进行OneHot编码<br>    array of indices: 类别特征的数组索引<br>    mask: n_features 长度的数组，切dtype = bool<br>非类别型特征通常会放到矩阵的右边</p></blockquote><p><strong>dtype</strong> : number type, default=np.float<br>输出数据的类型<br><strong>sparse</strong> : boolean, default=True<br>设置True会返回稀疏矩阵，否则返回数组<br><strong>handle_unknown</strong> : str, ‘error’ or ‘ignore’<br>当一个不明类别特征出现在变换中时，报错还是忽略</p></blockquote><p>２、<strong>Hash编码成词向量</strong>：<br><img src="http://p4rlzrioq.bkt.clouddn.com/hash%E7%BC%96%E7%A0%81.jpg" alt=""></p><h2 id="2、文本型"><a href="#2、文本型" class="headerlink" title="2、文本型"></a>2、文本型</h2><p>１. <strong>词袋</strong>：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．<br>２. 把词袋中的词扩充到<strong>n-gram</strong>：n-gram代表n个词的组合。比如“我喜欢你”、“你喜欢我”这两句话如果用词袋表示的话，分词后包含相同的三个词，组成一样的向量：“我 喜欢 你”。显然两句话不是同一个意思，用n-gram可以解决这个问题。如果用2-gram，那么“我喜欢你”的向量中会加上“我喜欢”和“喜欢你”，“你喜欢我”的向量中会加上“你喜欢”和“喜欢我”。这样就区分开来了。<br>３. 使用<strong>TF-IDF</strong>特征：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)，IDF(t) = ln(总文档数/ 含t的文档数)，TF-IDF权重 = TF(t) * IDF(t)。自然语言处理中经常会用到。</p><h2 id="3、数值型"><a href="#3、数值型" class="headerlink" title="3、数值型"></a>3、数值型</h2><h3 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%BD%92%E4%B8%80%E5%8C%96.svg" alt="link"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])</span><br><span class="line">min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line">#将上述得到的scale参数应用至测试数据</span><br><span class="line">X_test = np.array([[ -3., -1., 4.]]) </span><br><span class="line">X_test_minmax = min_max_scaler.transform(X_test)</span><br></pre></td></tr></table></figure></p><h3 id="区间缩放（scaling）"><a href="#区间缩放（scaling）" class="headerlink" title="区间缩放（scaling）"></a>区间缩放（scaling）</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BC%A9%E6%94%BE.svg" alt="link"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">                    [ 2.,  0.,  0.],</span><br><span class="line">                    [ 0.,  1., -1.]])</span><br><span class="line">max_abs_scaler = preprocessing.MaxAbsScaler() </span><br><span class="line">X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line">X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line">X_test_maxabs = max_abs_scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h3 id="标准化（Standardization）"><a href="#标准化（Standardization）" class="headerlink" title="标准化（Standardization）"></a>标准化（Standardization）</h3><h4 id="适用情况"><a href="#适用情况" class="headerlink" title="适用情况"></a>适用情况</h4><p>看模型是否具有伸缩不变性。<br> 不是所有的模型都一定需要标准化，有些模型对量纲不同的数据比较敏感，譬如SVM等。当各个维度进行不均匀伸缩后，最优解与原来不等价，这样的模型，除非原始数据的分布范围本来就不叫接近，否则<strong>必须</strong>进行标准化，以免模型参数被分布范围较大或较小的数据主导。<br>但是如果模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression等，对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。<br>所以对于具有伸缩不变性的模型，<strong>最好</strong>也进行数据标准化。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A0%87%E5%87%86%E5%8C%96.svg" alt="link"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing  </span><br><span class="line">import numpy as np </span><br><span class="line">X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])  </span><br><span class="line">X_scaled = preprocessing.scale(X)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler().fit(X) </span><br><span class="line">#测试将该scaler用于输入数据，变换之后得到的结果同上</span><br><span class="line">scaler.transform(X)</span><br></pre></td></tr></table></figure><h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a><strong>二值化</strong></h3><p><strong>1.特征二值化</strong><br>特征二值化是把数值特征转化成布尔值的过程。这个方法对符合多变量伯努利分布的输入数据进行预测概率参数很有效。详细可以见这个例子<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM" target="_blank" rel="noopener">sklearn.neural_network.BernoulliRBM</a>.</p><p>对于 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer" target="_blank" rel="noopener">Normalizer</a>，<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer" title="sklearn.preprocessing.Binarizer" target="_blank" rel="noopener">Binarizer</a>工具类通常是在Pipeline阶段（<a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline" target="_blank" rel="noopener">sklearn.pipeline.Pipeline</a>）的前期过程会用到。</p><h2 id="4、比赛实际场景"><a href="#4、比赛实际场景" class="headerlink" title="4、比赛实际场景"></a>4、比赛实际场景</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AF%94%E8%B5%9B%E5%9C%BA%E6%99%AF1.jpg" alt="link"><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AF%94%E8%B5%9B%E5%9C%BA%E6%99%AF2.jpg" alt="link"><br>可见，选手需要进行制定规则、数据清洗、各个种类的特征处理等，对特征的研究是非常细化的。</p><h1 id="特征衍生"><a href="#特征衍生" class="headerlink" title="特征衍生"></a>特征衍生</h1><h2 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h2><p>1. 拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。 </p><ul><li>user_id&amp;&amp;category: 10001&amp;&amp;女裙 10002&amp;&amp;男士牛仔 </li><li>user_id&amp;&amp;style: 10001&amp;&amp;蕾丝 10002&amp;&amp;全棉　　<br>2. 模型特征组合： </li><li>用GBDT产出特征组合路径 </li><li>组合特征和原始特征一起放进LR训练</li></ul><h2 id="生成多项式特征"><a href="#生成多项式特征" class="headerlink" title="生成多项式特征"></a>生成多项式特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(9).reshape(3, 3)</span><br><span class="line">#只需要特征的交叉项，可以设置interaction_only=True</span><br><span class="line">poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class="line">poly.fit_transform(X)</span><br></pre></td></tr></table></figure><p>此方法经常用于核方法中</p><h2 id="自定义特征"><a href="#自定义特征" class="headerlink" title="自定义特征"></a>自定义特征</h2><p>1.想用对数据取对数，可以自己用 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer" target="_blank" rel="noopener">FunctionTransformer</a>自定义一个转化器,并且可以在Pipeline中使用`</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import FunctionTransformer </span><br><span class="line">transformer = FunctionTransformer(np.log1p)#括号内的就是自定义函数</span><br><span class="line">X = np.array([[0, 1], [2, 3]]) </span><br><span class="line">transformer.transform(X)</span><br></pre></td></tr></table></figure><p>2.如果你在做一个分类任务时，发现第一主成分与这个不相关，你可以用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer" target="_blank" rel="noopener">FunctionTransformer</a>把第一列除去，剩下的列用PCA</p><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p><strong>特征选择</strong>，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。 </p><h2 id="1-过滤型"><a href="#1-过滤型" class="headerlink" title="1 过滤型"></a>1 过滤型</h2><ul><li>方法：  评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。 </li><li>评价方式：Pearson相关系数， 互信息， 距离相关度。 </li><li>缺点：只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。 </li><li>python包：SelectKBest指定过滤个数、SelectPercentile指定过滤百分比。</li></ul><h2 id="2-包裹型"><a href="#2-包裹型" class="headerlink" title="2 包裹型"></a>2 包裹型</h2><ul><li>方法：把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果。 </li><li>典型算法：“递归特征删除算法”。 </li><li>应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。 </li><li>python包：RFE </li></ul><h2 id="3-嵌入型"><a href="#3-嵌入型" class="headerlink" title="3 嵌入型"></a>3 嵌入型</h2><ul><li>方法：根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。 </li><li>举例：最早在电商用LR做CTR预估， 在3-5亿维的系数特征上用L1正则化的LR模型。上一篇介绍了L1正则化有截断作用，剩余2-3千万的feature， 意味着其他的feature重要度不够。 </li><li>python包：feature_selection.SelectFromModel选出权重不为0的特征。</li></ul><h1 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h1><pre><code>       在数据处理中，经常会遇到特征维度比样本数量多得多的情况，如果拿到实际工程中去跑，效果不一定好。    一是因为冗余的特征会带来一些噪音，影响计算的结果；    二是因为无关的特征会加大计算量，耗费时间和资源。所以我们通常会对数据重新变换一下，再跑模型。数据变换的目的不仅仅是降维，还可以消除特征之间的相关性，并发现一些潜在的特征变量。</code></pre><h2 id="PCA-过程"><a href="#PCA-过程" class="headerlink" title="PCA 过程"></a>PCA 过程</h2><p>1.去掉数据的类别特征（label），将去掉后的d维数据作为样本<br>2.计算d维的均值向量（即所有数据的每一维向量的均值）<br>3.计算所有数据的散布矩阵（或者协方差矩阵）<br>4.计算特征值（e1,e2,…,ed）以及相应的特征向量（lambda1,lambda2,…,lambda d）<br>5.按照特征值的大小对特征向量降序排序，选择前k个最大的特征向量，组成d<em>k维的矩阵W（其中每一列代表一个特征向量）<br>6.运用d</em>K的特征向量矩阵W将样本数据变换成新的子空间。（用数学式子表达就是<img src="http://p4rlzrioq.bkt.clouddn.com/pca%E9%99%8D%E7%BB%B4%E5%A4%84%E7%90%86.png" alt="">，其中x是d<em>1维的向量，代表一个样本，y是K</em>1维的在新的子空间里的向量）</p><p>python里有已经写好的模块，可以直接拿来用，但是我觉得不管什么模块，都要懂得它的原理是什么。matplotlib有<a href="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml" target="_blank" rel="noopener">matplotlib.mlab.PCA()</a>，sklearn也有专门一个模块<a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions" target="_blank" rel="noopener">Dimensionality reduction</a>专门讲PCA，包括传统的PCA，也就是我上文写的，以及增量PCA，核PCA等等，除了PCA以外，还有ZCA白化等等，在图像处理中也经常会用到。</p><p>　　推荐一个博客，动态展示了PCA的过程：<a href="http://setosa.io/ev/principal-component-analysis/" target="_blank" rel="noopener">http://setosa.io/ev/principal-component-analysis/</a>  写的也很清楚，可以看一下；再推荐一个维基百科的，讲的真的是详细啊<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><ul><li>线性回归的L1正则化通常称为Lasso回归，α来调节损失函数的均方差项和正则化项的权重。</li><li>Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0，故具有特征选择的功能，增强了模型的泛化能力。</li></ul><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><ul><li>线性回归的L2正则化通常称为Ridge回归。</li><li>Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。</li><li>Ridge回归的求解比较简单，一般用最小二乘法。</li></ul><p><strong>L1正则化产生稀疏的权值, 具有特征选择的作用；L2正则化产生平滑的权值</strong>。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/L1,L2%E6%AD%A3%E5%88%99%E5%8C%96.png" alt=""></p><h2 id="最小二乘法的局限性"><a href="#最小二乘法的局限性" class="headerlink" title="最小二乘法的局限性"></a>最小二乘法的局限性</h2><ol><li>最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了</li><li>当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。</li><li>如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用</li><li>当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。</li></ol><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="决策树构建中的分裂准则"><a href="#决策树构建中的分裂准则" class="headerlink" title="决策树构建中的分裂准则"></a>决策树构建中的分裂准则</h2><p>决策树可以通过一系列规则递归地分割特征空间</p><h3 id="信息增益（information-gain）"><a href="#信息增益（information-gain）" class="headerlink" title="信息增益（information gain）"></a><strong>信息增益（information gain）</strong></h3><p>属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合</p><h3 id="增益率（gain-ratio）"><a href="#增益率（gain-ratio）" class="headerlink" title="增益率（gain ratio）"></a><strong>增益率（gain ratio）</strong></h3><p>信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。</p><h3 id="基尼指数-Gini-index"><a href="#基尼指数-Gini-index" class="headerlink" title="基尼指数(Gini index)"></a><strong>基尼指数(Gini index)</strong></h3><p>基尼指数是另外一种数据的不纯度的度量方法，其定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B01.jpg" alt="">　　</p><p>其中的m仍然表示数据集D中类别C的个数，Pi表示D中任意一个记录属于Ci的概率，计算时Pi=(D中属于Ci类的集合的记录个数/|D|)。如果所有的记录都属于同一个类中，则P1=1，Gini(D)=0，此时不纯度最低。<br>在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树，对每个属性都会枚举其属性的非空真子集，以属性R分裂后的基尼系数为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B02.jpg" alt=""></p><p>D1为D的一个非空真子集，D2为D1在D的补集，即D1+D2=D，对于属性R来说，有多个真子集，即GiniR(D)有多个值，但我们选取最小的那么值作为R的基尼指数。最后：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B03.jpg" alt=""></p><p>对于二类分类，基尼系数和熵之半的曲线如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/40658-24f62052d3f57559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代</p><h2 id="常用决策树模型"><a href="#常用决策树模型" class="headerlink" title="常用决策树模型"></a>常用决策树模型</h2><h3 id="决策树模型总结"><a href="#决策树模型总结" class="headerlink" title="决策树模型总结"></a>决策树模型总结</h3><div class="table-container"><table><thead><tr><th>算法</th><th>支持模型</th><th>树结构</th><th>特征选择</th><th>连续值处理</th><th>缺失值处理</th><th>剪枝</th></tr></thead><tbody><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td><td>不支持</td><td>不支持</td><td>不支持</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>CART</td><td>分类，回归</td><td>二叉树</td><td>基尼系数，均方差</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table></div><h3 id="CART决策树属性分裂方法"><a href="#CART决策树属性分裂方法" class="headerlink" title="CART决策树属性分裂方法"></a>CART决策树属性分裂方法</h3><ol><li>m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。</li><li>对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。</li></ol><h2 id="决策树优化方法"><a href="#决策树优化方法" class="headerlink" title="决策树优化方法"></a>决策树优化方法</h2><h3 id="后剪枝（postpruning）"><a href="#后剪枝（postpruning）" class="headerlink" title="后剪枝（postpruning）"></a>后剪枝（postpruning）</h3><p>先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。</p><h3 id="预剪枝（prepruning）"><a href="#预剪枝（prepruning）" class="headerlink" title="预剪枝（prepruning）"></a>预剪枝（prepruning）</h3><p>在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。</p><h3 id="抑制单颗决策树的复杂度的方法"><a href="#抑制单颗决策树的复杂度的方法" class="headerlink" title="抑制单颗决策树的复杂度的方法"></a>抑制单颗决策树的复杂度的方法</h3><ol><li>限制树的最大深度</li><li>限制叶子节点的最少样本数量</li><li>限制节点分裂时的最少样本数量</li><li>吸收 bagging 的思想对训练样本采样，在学习单颗决策树时只使用一部分训练样本</li><li>借鉴随机森林的思路在学习单颗决策树时只采样一部分特征，在目标函数中添加正则项惩罚复杂的树结。</li></ol><h2 id="决策树算法的优点"><a href="#决策树算法的优点" class="headerlink" title="决策树算法的优点"></a>决策树算法的优点</h2><ol><li>基本不需要预处理，不需要提前归一化，处理缺失值。</li><li>使用决策树预测的代价是O(log2m)。 m为样本数。</li><li>既可以处理离散值也可以处理连续值。很多算法只是4.专注于离散值或者连续值。</li><li>可以处理多维度输出的分类问题。</li><li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li><li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li><li>对于异常点的容错能力好，健壮性高。</li></ol><h2 id="决策树算法的缺陷"><a href="#决策树算法的缺陷" class="headerlink" title="决策树算法的缺陷"></a>决策树算法的缺陷</h2><ol><li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li><li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li><li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li><li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li><li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li></ol><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。</p><h2 id="梯度下降法的超参数"><a href="#梯度下降法的超参数" class="headerlink" title="梯度下降法的超参数"></a>梯度下降法的超参数</h2><ul><li>步长（step size）<br>学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" alt=""></li></ul><h2 id="梯度下降的算法调优"><a href="#梯度下降的算法调优" class="headerlink" title="梯度下降的算法调优"></a>梯度下降的算法调优</h2><ol><li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p></li><li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p></li><li><p>标准化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据标准化，这样特征的新期望为0，新方差为1，迭代次数可以大大加快。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E4%BC%98%E5%8A%BF.png" alt=""></p></li></ol><h2 id="梯度下降方法总结"><a href="#梯度下降方法总结" class="headerlink" title="梯度下降方法总结"></a>梯度下降方法总结</h2><h3 id="批梯度下降-batch-gradient-descent-BGD"><a href="#批梯度下降-batch-gradient-descent-BGD" class="headerlink" title="批梯度下降(batch gradient descent/BGD)"></a>批梯度下降(batch gradient descent/BGD)</h3><p>求梯度的时候就用了所有m个样本的梯度数据。</p><h3 id="随机梯度下降（stochastic-gradient-descent-SGD）"><a href="#随机梯度下降（stochastic-gradient-descent-SGD）" class="headerlink" title="随机梯度下降（stochastic gradient descent/SGD）"></a>随机梯度下降（stochastic gradient descent/SGD）</h3><p>随机梯度下降法由于每次仅仅采用一个样本来迭代。优点是速度快以及可以跳出局部最优解，缺点是导致迭代方向变化很大，不能很快的收敛到局部最优解。</p><h3 id="小批量随机梯度下降（mini-batch-stochastic-gradient-descent）"><a href="#小批量随机梯度下降（mini-batch-stochastic-gradient-descent）" class="headerlink" title="小批量随机梯度下降（mini-batch stochastic gradient descent）"></a>小批量随机梯度下降（mini-batch stochastic gradient descent）</h3><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。</p><h2 id="梯度下降法与最小二乘法"><a href="#梯度下降法与最小二乘法" class="headerlink" title="梯度下降法与最小二乘法"></a>梯度下降法与最小二乘法</h2><ul><li>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。</li><li>梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</li></ul><h1 id="分类模型指标"><a href="#分类模型指标" class="headerlink" title="分类模型指标"></a>分类模型指标</h1><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p><img src="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png" alt="link"></p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p><strong>准确率</strong>是预测和标签一致的样本在所有样本中所占的比例</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg" alt="link"></p><h2 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h2><p><strong>精确率</strong>是你预测为正类的数据中，有多少确实是正类</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg" alt="link"></p><h2 id="查全率（Recall）"><a href="#查全率（Recall）" class="headerlink" title="查全率（Recall）"></a>查全率（Recall）</h2><p><strong>查全率</strong>是所有正类的数据中，你预测为正类的数据占比</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg" alt="link"></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg" alt="link"></p><p>不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg" alt="link"></p><h2 id="F-beta-Score"><a href="#F-beta-Score" class="headerlink" title="F-beta Score"></a>F-beta Score</h2><p>有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg" alt="link"></p><h2 id="ROC-（receiver-operating-characteristic-curve）"><a href="#ROC-（receiver-operating-characteristic-curve）" class="headerlink" title="ROC （receiver operating characteristic curve）"></a>ROC （receiver operating characteristic curve）</h2><p>绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。<br>ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</p><p><img src="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif" alt=""></p><h2 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h2><p>ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。</p><p><img src="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p><h1 id="模型选择与评估"><a href="#模型选择与评估" class="headerlink" title="模型选择与评估"></a>模型选择与评估</h1><h2 id="算法选择"><a href="#算法选择" class="headerlink" title="算法选择"></a>算法选择</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/sklearn%20%E4%B8%AD%E6%96%87.png" alt=""></p><h2 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h2><p>偏差-方差分解（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。令：<br>x为测试样本<br>yD：x在数据集中的标记<br>y：x的真实标记<br>f(x;D):训练集D上学得的模型f在x上的预测输出<br>f(x)=E[f(x;D)] :期望预测<br>使用样本数相同的不同训练集产生的方差（variance）为：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE0.png" alt=""><br>方差描述的是预测值作为随机变量的离散程度。度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。<br>期望输出与真实标记的差别称为<strong>偏差（bias）</strong>,偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。偏差公式如下：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE.png" alt=""><br><strong>噪声</strong>为：（表示若不为0，就是标记错了）<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE1.png" alt=""><br>噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。<br><strong>算法的期望泛化误差</strong>为：<br><a href="http://p4rlzrioq.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE0.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE0.png" alt=""></a><br>即泛化误差可分解为偏差、方差与噪声之和。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E4%B8%8E%E7%A1%AE.jpeg" alt="link"></p><ul><li>有一些算法天生是高方差的算法。如KNN、决策树。非参数学习通常是高方差算法，对数据较为敏感，因为不对数据进行任何假设。</li><li>有一些算法天生就是高偏差算法。如线性回归。参数学习通常是高偏差算法，因为对数据具有极强的假设。</li><li>机器学习的主要挑战来自于方差，解决高方差的通常手段有：<ul><li>1.降低模型复杂度</li><li>2.减少数据维度；降噪</li><li>3.增加样本数</li><li>4.使用验证集</li><li>5.模型正则化</li></ul></li></ul><h2 id="泛化能力、欠拟合和过拟合"><a href="#泛化能力、欠拟合和过拟合" class="headerlink" title="泛化能力、欠拟合和过拟合"></a>泛化能力、欠拟合和过拟合</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/overfitting.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
