<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frank&#39;s Blog</title>
  
  <subtitle>Enjoy everything fun and challenging</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://frankblog.site/"/>
  <updated>2018-06-25T15:28:30.556Z</updated>
  <id>http://frankblog.site/</id>
  
  <author>
    <name>FGY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据挖掘面试总结(一)</title>
    <link href="http://frankblog.site/2018/06/25/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93(%E4%B8%80)/"/>
    <id>http://frankblog.site/2018/06/25/数据挖掘面试总结(一)/</id>
    <published>2018-06-25T15:10:12.926Z</published>
    <updated>2018-06-25T15:28:30.556Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>栈(stack)又称之为堆栈是一个特殊的有序表，其插入和删除操作都在栈顶进行操作，并且按照先进后出，后进先出的规则进行运作。</p><h2 id="栈的接口"><a href="#栈的接口" class="headerlink" title="栈的接口"></a>栈的接口</h2><p>如果你创建了一个栈，那么那么应该具有以下接口来进行对栈的操作</p><div class="table-container"><table><thead><tr><th>接口</th><th>描述</th></tr></thead><tbody><tr><td>push()</td><td>入栈</td></tr><tr><td>pop()</td><td>出栈</td></tr><tr><td>isEmpty()</td><td>判断是否为空栈</td></tr><tr><td>length()</td><td>获取栈的长度</td></tr><tr><td>getTop()</td><td>取栈顶的元素，元素不出栈</td></tr></tbody></table></div><p>知道栈需要上述的接口后，那么在Python中，列表就类似是一个栈，提供接口如下：</p><div class="table-container"><table><thead><tr><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>s = []</td><td>创建一个栈</td></tr><tr><td>s.append(x)</td><td>往栈内添加一个元素</td></tr><tr><td>s.pop()</td><td>在栈内删除一个元素</td></tr><tr><td>not s</td><td>判断是否为空栈</td></tr><tr><td>len(s)</td><td>获取栈内元素的数量</td></tr><tr><td>s[-1]</td><td>获取栈顶的元素</td></tr></tbody></table></div><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。</p><p>查找表（Search Table）：由同一类型的数据元素（或记录）构成的集合<br>关键字（Key）：数据元素中某个数据项的值，又称为键值。<br>主键（Primary Key）：可唯一地标识某个数据元素或记录的关键字。</p><p>查找表按照操作方式可分为：</p><ul><li>静态查找表（Static Search Table）：只做查找操作的查找表。它的主要操作是：</li><li>查询某个“特定的”数据元素是否在表中</li><li>检索某个“特定的”数据元素和各种属性</li><li>动态查找表（Dynamic Search Table）：在查找中同时进行插入或删除等操作：</li><li>查找时插入数据</li><li>查找时删除数据</li></ul><h2 id="无序表查找"><a href="#无序表查找" class="headerlink" title="无序表查找"></a>无序表查找</h2><p>算法分析：最好情况是在第一个位置就找到了，此为O(1)；最坏情况在最后一个位置才找到，此为O(n)；所以平均查找次数为(n+1)/2。<strong>最终时间复杂度为O(n)</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 最基础的遍历无序列表的查找算法</span><br><span class="line"># 时间复杂度O(n)</span><br><span class="line">def sequential_search(lis,  key):</span><br><span class="line">    length  =  len(lis)</span><br><span class="line">    for  i  in  range(length):</span><br><span class="line">        if  lis[i]  ==  key:</span><br><span class="line">            return  i</span><br><span class="line">        else:</span><br><span class="line">            return  False</span><br><span class="line">if  __name__  ==  &apos;__main__&apos;:</span><br><span class="line">    LIST  =  [1,  5,  8,  123,  22,  54,  7,  99,  300,  222]</span><br><span class="line">    result  =  sequential_search(LIST,  123)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></p><h2 id="有序表查找"><a href="#有序表查找" class="headerlink" title="有序表查找"></a>有序表查找</h2><h3 id="1-二分查找-Binary-Search"><a href="#1-二分查找-Binary-Search" class="headerlink" title="1. 二分查找(Binary Search)"></a>1. 二分查找(Binary Search)</h3><p>算法核心：在查找表中不断取中间元素与查找值进行比较，以二分之一的倍率进行表范围的缩小。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 针对有序查找表的二分查找算法</span><br><span class="line"># 时间复杂度O(log(n))</span><br><span class="line">def  binary_search(lis,  key):</span><br><span class="line">    low  =  0</span><br><span class="line">    high  =  len(lis)  -  1</span><br><span class="line">    time  =  0</span><br><span class="line">    while  low  &lt;  high:</span><br><span class="line">        time  +=  1</span><br><span class="line">        mid  =  int((low  +  high)  /  2)</span><br><span class="line">        if  key  &lt;  lis[mid]:</span><br><span class="line">            high  =  mid  -  1</span><br><span class="line">        elif  key  &gt;  lis[mid]:</span><br><span class="line">            low  =  mid  +  1</span><br><span class="line">        else:</span><br><span class="line">            # 打印折半的次数</span><br><span class="line">            print(&quot;times: %s&quot;  %  time)</span><br><span class="line">            return  mid</span><br><span class="line">    print(&quot;times: %s&quot;  %  time)</span><br><span class="line">    return  False</span><br><span class="line">if  __name__  ==  &apos;__main__&apos;:</span><br><span class="line">    LIST  =  [1,  5,  7,  8,  22,  54,  99,  123,  200,  222,  444]</span><br><span class="line">    result  =  binary_search(LIST,  99)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></p><h3 id="2-插值查找"><a href="#2-插值查找" class="headerlink" title="2. 插值查找"></a>2. 插值查找</h3><p>二分查找法虽然已经很不错了，但还有可以优化的地方。<br>有的时候，对半过滤还不够狠，要是每次都排除十分之九的数据岂不是更好？选择这个值就是关键问题，插值的意义就是：以更快的速度进行缩减。</p><p>插值的核心就是使用公式：<br>value = (key – list[low])/(list[high] – list[low])</p><p>用这个value来代替二分查找中的1/2。<br>上面的代码可以直接使用，只需要改一句。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 插值查找算法</span><br><span class="line"># 时间复杂度O(log(n))</span><br><span class="line">def  binary_search(lis,  key):</span><br><span class="line">    low  =  0</span><br><span class="line">    high  =  len(lis)  -  1</span><br><span class="line">    time  =  0</span><br><span class="line">    while  low  &lt;  high:</span><br><span class="line">        time  +=  1</span><br><span class="line">        # 计算mid值是插值算法的核心代码</span><br><span class="line">        mid  =  low  +  int((high  -  low)  *  (key  -  lis[low])/(lis[high]  -  lis[low]))</span><br><span class="line">        print(&quot;mid=%s, low=%s, high=%s&quot;  %  (mid,  low,  high))</span><br><span class="line">        if  key  &lt;  lis[mid]:</span><br><span class="line">            high  =  mid  -  1</span><br><span class="line">        elif  key  &gt;  lis[mid]:</span><br><span class="line">            low  =  mid  +  1</span><br><span class="line">        else:</span><br><span class="line">            # 打印查找的次数</span><br><span class="line">            print(&quot;times: %s&quot;  %  time)</span><br><span class="line">            return  mid</span><br><span class="line">    print(&quot;times: %s&quot;  %  time)</span><br><span class="line">    return  False</span><br><span class="line">if  __name__  ==  &apos;__main__&apos;:</span><br><span class="line">    LIST  =  [1,  5,  7,  8,  22,  54,  99,  123,  200,  222,  444]</span><br><span class="line">    result  =  binary_search(LIST,  444)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></p><p>插值算法的总体时间复杂度仍然属于O(log(n))级别的。其优点是，对于表内数据量较大，且关键字分布比较均匀的查找表，使用插值算法的平均性能比二分查找要好得多。反之，对于分布极端不均匀的数据，则不适合使用插值算法。</p><h2 id="线性索引查找"><a href="#线性索引查找" class="headerlink" title="线性索引查找"></a>线性索引查找</h2><p>对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。<br>索引就是把一个关键字与它相对应的记录进行关联的过程。<br>一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。<br>索引按照结构可以分为：线性索引、树形索引和多级索引。<br>线性索引：将索引项的集合通过线性结构来组织，也叫索引表。<br>线性索引可分为：稠密索引、分块索引和倒排索引</p><ol><li><p>稠密索引<br>稠密索引指的是在线性索引中，为数据集合中的每个记录都建立一个索引项。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E7%B4%A2%E5%BC%95%E6%9F%A5%E6%89%BE.png" alt="image_1b2cl8r0dk1v1u0ssf0rmk8o29.png-157.4kB"><br>这其实就相当于给无序的集合，建立了一张有序的线性表。其索引项一定是按照关键码进行有序的排列。<br>这也相当于把查找过程中需要的排序工作给提前做了。</p></li><li><p>分块索引<br>给大量的无序数据集合进行分块处理，使得块内无序，块与块之间有序。<br>这其实是有序查找和无序查找的一种中间状态或者说妥协状态。因为数据量过大，建立完整的稠密索引耗时耗力，占用资源过多；但如果不做任何排序或者索引，那么遍历的查找也无法接受，只能折中，做一定程度的排序或索引。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E7%B4%A2%E5%BC%95%E6%9F%A5%E6%89%BE1.png" alt="image_1b2clkecf3mt1j7a8hn3v5vbrm.png-136.6kB"><br>分块索引的效率比遍历查找的O(n)要高一些，但与二分查找的O(logn)还是要差不少。</p></li><li><p>倒排索引<br>不是由记录来确定属性值，而是由属性值来确定记录的位置，这种被称为倒排索引。其中记录号表存储具有相同次关键字的所有记录的地址或引用（可以是指向记录的指针或该记录的主关键字）。<br>倒排索引是最基础的搜索引擎索引技术。</p></li></ol><h2 id="二叉树遍历"><a href="#二叉树遍历" class="headerlink" title="二叉树遍历"></a>二叉树遍历</h2><p> 二叉树是有限个元素的集合，该集合或者为空、或者有一个称为根节点（root）的元素及两个互不相交的、分别被称为左子树和右子树的二叉树组成。</p><ul><li>二叉树的每个结点至多只有二棵子树(不存在度大于2的结点)，二叉树的子树有左右之分，次序不能颠倒。</li><li>二叉树的第i层至多有2^{i-1}个结点</li><li>深度为k的二叉树至多有2^k-1个结点；</li><li>对任何一棵二叉树T，如果其终端结点数为N0，度为2的结点数为N2，则N0=N2+1</li></ul><p>首先构建二叉树：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Node:    </span><br><span class="line">def __init__(self,value=None,left=None,right=None)         </span><br><span class="line">self.value=value          </span><br><span class="line">self.left=left    #左子树</span><br><span class="line">self.right=right  #右子树</span><br></pre></td></tr></table></figure></p><p>下面给出二叉树的前序遍历／中序遍历／后序遍历<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def preTraverse(root):  </span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    前序遍历</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    if root==None:  </span><br><span class="line">        return  </span><br><span class="line">    print(root.value)  </span><br><span class="line">    preTraverse(root.left)  </span><br><span class="line">    preTraverse(root.right)  </span><br><span class="line"></span><br><span class="line">def midTraverse(root): </span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    中序遍历</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    if root==None:  </span><br><span class="line">        return  </span><br><span class="line">    midTraverse(root.left)  </span><br><span class="line">    print(root.value)  </span><br><span class="line">    midTraverse(root.right)  </span><br><span class="line">  </span><br><span class="line">def afterTraverse(root):  </span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    后序遍历</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    if root==None:  </span><br><span class="line">        return  </span><br><span class="line">    afterTraverse(root.left)  </span><br><span class="line">    afterTraverse(root.right)  </span><br><span class="line">    print(root.value)</span><br></pre></td></tr></table></figure></p><p>下面给出一个例子，验证一下程序</p><p> <img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%8C%E5%8F%89%E6%A0%91.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line">root=Node(&apos;D&apos;,Node(&apos;B&apos;,Node(&apos;A&apos;),Node(&apos;C&apos;)),Node(&apos;E&apos;,right=Node(&apos;G&apos;,Node(&apos;F&apos;))))</span><br><span class="line">    print(&apos;前序遍历：&apos;)</span><br><span class="line">    preTraverse(root)</span><br><span class="line">    print(&apos;\n&apos;)</span><br><span class="line">    print(&apos;中序遍历：&apos;)</span><br><span class="line">    midTraverse(root)</span><br><span class="line">    print(&apos;\n&apos;)</span><br><span class="line">    print(&apos;后序遍历：&apos;)</span><br><span class="line">    afterTraverse(root)</span><br><span class="line">    print(&apos;\n&apos;)</span><br></pre></td></tr></table></figure><h2 id="散列表（哈希表）"><a href="#散列表（哈希表）" class="headerlink" title="散列表（哈希表）"></a>散列表（哈希表）</h2><h3 id="散列函数的构造方法"><a href="#散列函数的构造方法" class="headerlink" title="散列函数的构造方法"></a>散列函数的构造方法</h3><p>好的散列函数：计算简单、散列地址分布均匀</p><ol><li>直接定址法<br>例如取关键字的某个线性函数为散列函数：<br>f(key) = a*key + b (a,b为常数）</li><li>数字分析法<br>抽取关键字里的数字，根据数字的特点进行地址分配</li><li>平方取中法<br>将关键字的数字求平方，再截取部分</li><li>折叠法<br>将关键字的数字分割后分别计算，再合并计算，一种玩弄数字的手段。</li><li>除留余数法<br>最为常见的方法之一。<br>对于表长为m的数据集合，散列公式为：<br>f(key) = key mod p (p&lt;=m)<br>mod：取模（求余数）<br>该方法最关键的是p的选择，而且数据量较大的时候，冲突是必然的。一般会选择接近m的质数。</li><li>随机数法<br>选择一个随机数，取关键字的随机函数值为它的散列地址。<br>f(key) = random(key)</li></ol><p>总结，实际情况下根据不同的数据特性采用不同的散列方法，考虑下面一些主要问题：</p><ul><li>计算散列地址所需的时间</li><li>关键字的长度</li><li>散列表的大小</li><li>关键字的分布情况</li><li>记录查找的频率</li></ul><h3 id="处理散列冲突"><a href="#处理散列冲突" class="headerlink" title="处理散列冲突"></a>处理散列冲突</h3><ul><li>开放定址法</li></ul><p>就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</p><p>公式是：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%A3%E5%88%97%E8%A1%A81.png" alt=""><br>这种简单的冲突解决办法被称为线性探测，无非就是自家的坑被占了，就逐个拜访后面的坑，有空的就进，也不管这个坑是不是后面有人预定了的。<br>线性探测带来的最大问题就是冲突的堆积，你把别人预定的坑占了，别人也就要像你一样去找坑。</p><p>改进的办法有二次方探测法和随机数探测法。</p><ul><li>再散列函数法<br>发生冲突时就换一个散列函数计算，总会有一个可以把冲突解决掉，它能够使得关键字不产生聚集，但相应地增加了计算的时间。</li><li>链接地址法<br>碰到冲突时，不更换地址，而是将所有关键字为同义词的记录存储在一个链表里，在散列表中只存储同义词子表的头指针，如下图：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%A3%E5%88%97%E8%A1%A82.png" alt="image_1b3gig3eu1uh3rujcvuli1qspm.png-59.3kB"></li></ul><p>这样的好处是，不怕冲突多；缺点是降低了散列结构的随机存储性能。本质是用单链表结构辅助散列结构的不足。</p><ul><li>公共溢出区法<br>其实就是为所有的冲突，额外开辟一块存储空间。如果相对基本表而言，冲突的数据很少的时候，使用这种方法比较合适。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%A3%E5%88%97%E8%A1%A83.png" alt="image_1b3gim8dp1m4hd0015su1jvm15mg13.png-56.8kB"></li></ul><p><a href="https://blog.csdn.net/dongrixinyu/article/details/78775057" target="_blank" rel="noopener">python 数据结构刷题</a></p><h1 id="手推LR和SVM"><a href="#手推LR和SVM" class="headerlink" title="手推LR和SVM"></a>手推LR和SVM</h1><h2 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8lr01.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8lr02.jpg" alt=""></p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm1_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/svm2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/svm3_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/svm4_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/svm5_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/svm6_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><h1 id="xgboost调参"><a href="#xgboost调参" class="headerlink" title="xgboost调参"></a>xgboost调参</h1><h2 id="XGBoost的参数"><a href="#XGBoost的参数" class="headerlink" title="XGBoost的参数"></a><strong>XGBoost的参数</strong></h2><p>XGBoost的作者把所有的参数分成了三类：</p><ol><li>通用参数：宏观函数控制。</li><li>Booster参数：控制每一步的booster(tree/regression)。</li><li>学习目标参数：控制训练目标的表现。</li></ol><p>在这里我会类比GBM来讲解，所以作为一种基础知识，强烈推荐先阅读<a href="http://blog.csdn.net/han_xiaoyang/article/details/52663170" target="_blank" rel="noopener">这篇文章</a>。</p><h3 id="通用参数"><a href="#通用参数" class="headerlink" title=" 通用参数"></a><strong> 通用参数</strong></h3><p>这些参数用来控制XGBoost的宏观功能。</p><h4 id="booster-默认gbtree"><a href="#booster-默认gbtree" class="headerlink" title="booster[默认gbtree]"></a><strong>booster[默认gbtree]</strong></h4><ul><li>选择每次迭代的模型，有两种选择：<br>gbtree：基于树的模型<br>gbliner：线性模型</li></ul><h4 id="silent-默认0"><a href="#silent-默认0" class="headerlink" title="silent[默认0]"></a><strong>silent[默认0]</strong></h4><ul><li>当这个参数值为1时，静默模式开启，不会输出任何信息。</li><li>一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li></ul><h4 id="nthread-默认值为最大可能的线程数"><a href="#nthread-默认值为最大可能的线程数" class="headerlink" title="nthread[默认值为最大可能的线程数]"></a><strong>nthread[默认值为最大可能的线程数]</strong></h4><ul><li>这个参数用来进行多线程控制，应当输入系统的核数。</li><li>如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。</li></ul><p>还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。</p><h3 id="booster参数"><a href="#booster参数" class="headerlink" title=" booster参数"></a><strong> booster参数</strong></h3><p>尽管有两种booster可供选择，我这里只介绍<strong>tree booster</strong>，因为它的表现远远胜过<strong>linear booster</strong>，所以linear booster很少用到。</p><h4 id="eta-默认0-3"><a href="#eta-默认0-3" class="headerlink" title="eta[默认0.3]"></a><strong>eta[默认0.3]</strong></h4><ul><li>和GBM中的 learning rate 参数类似。</li><li>通过减少每一步的权重，可以提高模型的鲁棒性。</li><li>典型值为0.01-0.2。</li></ul><h4 id="min-child-weight-默认1"><a href="#min-child-weight-默认1" class="headerlink" title="min_child_weight[默认1]"></a><strong>min_child_weight[默认1]</strong></h4><ul><li>决定最小叶子节点样本权重和。</li><li>和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小_样本权重的和_，而GBM参数是最小_样本总数_。</li><li>这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。</li><li>但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</li></ul><h4 id="max-depth-默认6"><a href="#max-depth-默认6" class="headerlink" title="max_depth[默认6]"></a><strong>max_depth[默认6]</strong></h4><ul><li>和GBM中的参数相同，这个值为树的最大深度。</li><li>这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。</li><li>需要使用CV函数来进行调优。</li><li>典型值：3-10</li></ul><h4 id="max-leaf-nodes"><a href="#max-leaf-nodes" class="headerlink" title="max_leaf_nodes"></a><strong>max_leaf_nodes</strong></h4><ul><li>树上最大的节点或叶子的数量。</li><li>可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2n2个叶子。</li><li>如果定义了这个参数，GBM会忽略max_depth参数。</li></ul><h4 id="gamma-默认0"><a href="#gamma-默认0" class="headerlink" title="gamma[默认0]"></a><strong>gamma[默认0]</strong></h4><ul><li>在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。</li><li>这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</li></ul><h4 id="max-delta-step-默认0"><a href="#max-delta-step-默认0" class="headerlink" title="max_delta_step[默认0]"></a><strong>max_delta_step[默认0]</strong></h4><ul><li>这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。</li><li>通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。</li><li>这个参数一般用不到，但是你可以挖掘出来它更多的用处。</li></ul><h4 id="subsample-默认1"><a href="#subsample-默认1" class="headerlink" title="subsample[默认1]"></a><strong>subsample[默认1]</strong></h4><ul><li>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。</li><li>减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。</li><li>典型值：0.5-1</li></ul><h4 id="colsample-bytree-默认1"><a href="#colsample-bytree-默认1" class="headerlink" title="colsample_bytree[默认1]"></a><strong>colsample_bytree[默认1]</strong></h4><ul><li>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。</li><li>典型值：0.5-1</li></ul><h4 id="colsample-bylevel-默认1"><a href="#colsample-bylevel-默认1" class="headerlink" title="colsample_bylevel[默认1]"></a><strong>colsample_bylevel[默认1]</strong></h4><ul><li>用来控制树的每一级的每一次分裂，对列数的采样的占比。</li><li>我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</li></ul><h4 id="lambda-默认1"><a href="#lambda-默认1" class="headerlink" title="lambda[默认1]"></a><strong>lambda[默认1]</strong></h4><ul><li>权重的L2正则化项。(和Ridge regression类似)。</li><li>这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。</li></ul><h4 id="alpha-默认1"><a href="#alpha-默认1" class="headerlink" title="alpha[默认1]"></a><strong>alpha[默认1]</strong></h4><ul><li>权重的L1正则化项。(和Lasso regression类似)。</li><li>可以应用在很高维度的情况下，使得算法的速度更快。</li></ul><h4 id="scale-pos-weight-默认1"><a href="#scale-pos-weight-默认1" class="headerlink" title="scale_pos_weight[默认1]"></a><strong>scale_pos_weight[默认1]</strong></h4><ul><li>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。</li></ul><h3 id="学习目标参数"><a href="#学习目标参数" class="headerlink" title="学习目标参数"></a><strong>学习目标参数</strong></h3><p>这个参数用来控制理想的优化目标和每一步结果的度量方法。</p><h4 id="objective-默认reg-linear"><a href="#objective-默认reg-linear" class="headerlink" title="objective[默认reg:linear]"></a><strong>objective[默认reg:linear]</strong></h4><ul><li><p>这个参数定义需要被最小化的损失函数。最常用的值有： </p><ul><li>binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。</li><li><p>multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 </p><ul><li>在这种情况下，你还需要多设一个参数：num_class(类别数目)。</li></ul></li><li>multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li></ul></li></ul><h4 id="eval-metric-默认值取决于objective参数的取值"><a href="#eval-metric-默认值取决于objective参数的取值" class="headerlink" title="eval_metric[默认值取决于objective参数的取值]"></a><strong>eval_metric[默认值取决于objective参数的取值]</strong></h4><ul><li>对于有效数据的度量方法。</li><li>对于回归问题，默认值是rmse，对于分类问题，默认值是error。</li><li><p>典型值有： </p><ul><li>rmse 均方根误差(\(\sqrt \frac{\sum_{i=1}^N \epsilon^2}{N}\)</li><li>mae 平均绝对误差(\(\frac{\sum_{i=1}^N |\epsilon|}{N}\)</li><li>logloss 负对数似然函数值</li><li>error 二分类错误率(阈值为0.5)</li><li>merror 多分类错误率</li><li>mlogloss 多分类logloss损失函数</li><li>auc 曲线下面积</li></ul></li></ul><h4 id="seed-默认0"><a href="#seed-默认0" class="headerlink" title="seed(默认0)"></a><strong>seed(默认0)</strong></h4><ul><li>随机数的种子</li><li>设置它可以复现随机数据的结果，也可以用于调整参数</li></ul><p>如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是：</p><p>1、<strong>eta -&gt; learning_rate</strong><br>2、<strong>lambda -&gt; reg_lambda</strong><br>3、<strong>alpha -&gt; reg_alpha</strong></p><h3 id="参数调优的一般方法"><a href="#参数调优的一般方法" class="headerlink" title="参数调优的一般方法"></a><strong>参数调优的一般方法</strong></h3><p>我们会使用和GBM中相似的方法。需要进行如下步骤：</p><ol><li><p>选择较高的<strong>学习速率(learning rate)</strong>。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择<strong>对应于此学习速率的理想决策树数量</strong>。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。</p></li><li><p>对于给定的学习速率和决策树数量，进行<strong>决策树特定参数调优</strong>(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。</p></li><li><p>xgboost的<strong>正则化参数</strong>的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。</p></li><li><p>降低学习速率，确定理想参数。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="面经" scheme="http://frankblog.site/categories/%E9%9D%A2%E7%BB%8F/"/>
    
    
      <category term="面经" scheme="http://frankblog.site/tags/%E9%9D%A2%E7%BB%8F/"/>
    
  </entry>
  
  <entry>
    <title>浅谈Hbase</title>
    <link href="http://frankblog.site/2018/06/25/%E6%B5%85%E8%B0%88Hbase/"/>
    <id>http://frankblog.site/2018/06/25/浅谈Hbase/</id>
    <published>2018-06-24T17:08:58.673Z</published>
    <updated>2018-06-24T17:12:06.958Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/hadoop-ecosystem%20%281%29.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>概述</li><li>HBase访问借口</li><li>HBase数据模型</li><li>HBase实现原理</li><li>HBase运行机制</li><li>HBase应用方案</li></ol><hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>HBase是一个高可靠、高性能、面向咧、可伸缩的分布式数据库。是谷歌<code>BigTable</code>的开源实现，主要用来存储<code>非结构化</code>和<code>半结构化</code>的松散数据。<br><a href="http://p4rlzrioq.bkt.clouddn.com/Hbase.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase.jpg" alt=""></a><br><a href="http://p4rlzrioq.bkt.clouddn.com/Hbase2.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase2.jpg" alt=""></a></p><h3 id="为什么要HBase？"><a href="#为什么要HBase？" class="headerlink" title="为什么要HBase？"></a>为什么要HBase？</h3><ol><li>受限于Hadoop MR编程框架的高延迟数据处理机制，<code>无法满足大规模数据实时处理的需求</code>。</li><li>HDFS不能随机访问</li><li>传统关系型数据无法应对剧增的海量数据</li><li>传统关系型数据库在数据结构变化时需要停机维护；孔裂浪费存储空间</li></ol><p>因此，业界出现了一类面相半结构化数据存储和处理的高可扩展、低写入、查询延迟的系统。如键值数据库、文档数据库和<code>列族</code>数据库（如BitTable和HBase）。如今HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中。</p><h3 id="HBase与传统关系数据库的对比"><a href="#HBase与传统关系数据库的对比" class="headerlink" title="HBase与传统关系数据库的对比"></a>HBase与传统关系数据库的对比</h3><div class="table-container"><table><thead><tr><th>特性</th><th>传统关系数据库</th><th>HBase</th></tr></thead><tbody><tr><td>数据类型</td><td>关系模式，具有丰富的数据类型后和存储方式</td><td>采用更简洁的数据模型，吧数据存储为未经解释的字符串</td></tr><tr><td>数据操作</td><td>包含丰富的操作，涉及复杂的夺标链接</td><td>不存在复杂的表于表之间的关系，只有简单的增、删、查、清空等，避免表于表间复杂关系</td></tr><tr><td>存储模式</td><td>基于行存储</td><td>基于列存储，每个列族有几个文件保存，不同列族的文件是分离的</td></tr><tr><td>数据索引</td><td>通过针对不同列构建复杂多个索引以提高访问性能</td><td>只有一个索引（行键），访问方法为或行键访问或行键扫面，通过巧妙的设计，速度不会慢下来</td></tr><tr><td>数据维护</td><td>更新操作用最新的数据覆盖旧的</td><td>更新操作生成一个新的版本，久的版本仍然保留</td></tr><tr><td>可伸缩性</td><td>很难实现横向拓展，纵向拓展空间也有限</td><td>可以轻易通过在集群中增加或减少硬件数量实现性能伸缩</td></tr></tbody></table></div><h3 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h3><p><a href="http://p4rlzrioq.bkt.clouddn.com/Hbase3.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase3.jpg" alt=""></a></p><h2 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h2><ol><li>数据模型概述</li><li>数据模型相关概念</li><li>概念视图</li><li>物理视图</li><li>面向列的存储</li></ol><h3 id="数据模型概述"><a href="#数据模型概述" class="headerlink" title="数据模型概述"></a>数据模型概述</h3><ul><li>HBase是一个<code>稀疏</code>、<code>多维度</code>、排序的<code>映射表</code>，这张表的索引时<code>行键</code>、<code>列族</code>、<code>列限定符</code>、、<code>时间戳</code>。</li><li>每一个值是一个未经解释的字符串，没有数据类型。</li><li>每一行都有一个可排序的行键和任意多的列</li><li>表在水平方向由一个或者多个列族组成，一个列族可以包含任意多个列，同一个列族里面的数据存储在一起</li><li>列族支持动态扩展，轻松的添加列族或列，无预先定义列的数量和类型。所有列均以字符串形式存储，用户需自行进行数据类型转换。</li><li>HBase中执行更新操作时，生成新版本，保留旧版本，查询时默认返回最新版本。创建时可以设置最多保留版本数量。</li></ul><h3 id="数据模型相关概念"><a href="#数据模型相关概念" class="headerlink" title="数据模型相关概念"></a>数据模型相关概念</h3><ul><li>表：HBase采用表来组织数据，表由行和列组成，列换分为若干个列族，避免夺标链接操作，追求分析效率。</li><li>行：每个HBAse表由若干行组成，每个行由行键（Row Key）来标示。不给行键所在的列进行命名，让其拥有纵向可拓展性。</li><li>列族：一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元，也是基本存储单元。</li><li>列限定符：相当于列名</li><li>单元格：在HBaae表中，通过行、列族和列限定符确定一个“单元格”，（时间戳定义其版本）单元格中存储的数据没有数据类型，总被视为字节数组byte[]</li><li>HBase中需要根据行键、列族、列限定符和时间戳确定一个单元格，因此是一个“四维”坐标。<br>💡：上文中提到HBase数据只有一个索引（行键），</li></ul><p><a href="http://p4rlzrioq.bkt.clouddn.com/Hbase4.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase4.jpg" alt=""></a></p><h3 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase5.jpg" alt=""></p><h3 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase6.jpg" alt=""></p><h3 id="面向列的存储"><a href="#面向列的存储" class="headerlink" title="面向列的存储"></a>面向列的存储</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase7.jpg" alt=""></p><p>💡<strong>不同存储模型优劣对比</strong></p><ul><li><p>面向行存储的数据库主要采用NSM（N-ary Storage Model）存储模型，即一个元组（行）会被连续存储在磁盘页中，数据是一行行进行存储，读取也是一行行进行读取。当要选取某属性进行分析时，也需要首先扫面完整元组内容。</p><ul><li>优点：适用于联机事务性数据处理，即将分布于不同地理位置的数据利用网络进行连接，进而进行统一的存储和管理。</li><li>缺点：鄙视和分析性操作</li></ul></li><li><p>面向列存储的数据库主要采用DSM（Decompostion Storage Model）存储模型，该模型会对关系进行垂直分解，并为每个属性分配一个子关系，每个子关系单独存储。</p><ul><li>优点：在批处理和即兴查询等分析操作中能够直接定位目标列，能够有有效I/O开销；同一列数据类型相同，存储过程能够拥有很高的数据压缩旅，从而节省存储空间。</li><li>缺点：执行连续操作时要付出昂贵的元组重构代价。</li></ul></li></ul><p>💡 <strong>总结：NSM存储模型更加适合事务型应用，DSM存储模型更加适合分析性应用</strong></p><h2 id="HBase实现原理"><a href="#HBase实现原理" class="headerlink" title="HBase实现原理"></a>HBase实现原理</h2><h3 id="HBase功能组件"><a href="#HBase功能组件" class="headerlink" title="HBase功能组件"></a>HBase功能组件</h3><ul><li>库函数</li><li>一个Master主服务器</li><li>许多个Region服务器</li></ul><p>Master负责管理和维护Hbase表的分区信息，维护Region服务器列表，分配Region，负责均衡，和Namenode功能类似。</p><p>Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求，和Datanode功能类似。</p><p>客户端并不是直接从Master主服务器读取数据，而是在获得Region的存储位置后，直接从Region服务器上读取数据。</p><p>客户端并不依赖Master，而是通过<code>Zookeeper</code>来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计是的Master负载很小。</p><h3 id="表和Region"><a href="#表和Region" class="headerlink" title="表和Region"></a>表和Region</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase8.jpg" alt=""></p><ul><li>一个表包含多个Region</li><li>开始只有一个Region，后来不断分裂</li><li><p>Regin拆分操作非常快（开始只是修改文件指向），接近瞬间。因为拆分之后的Region读取的仍然是原存储文件，知道“合并”过程吧存储文件异步写到独立的文件之后，才会读取新文件。<br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase9.jpg" alt=""></p></li><li><p>每个Region默认大小是100MB到200MB（2006之前）</p><ul><li>每个Region的最佳大小取决于丹台服曲奇的有效处理能力</li><li>目前每个Region的最佳大小建议1G～2G（2013以后）</li></ul></li><li>每个Region不会被分拆到多个Region服务器（Region最小不可分）</li><li>每个Region服务器存储10～1000个Region<br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase10.jpg" alt=""></li></ul><h3 id="Region的定位"><a href="#Region的定位" class="headerlink" title="Region的定位"></a>Region的定位</h3><ul><li>元数据表，又名<code>META</code>表，存储了<code>Region</code>和<code>Region</code>服务器的映射关系。</li><li>当HBase表很大时，<code>META</code>表也会被分裂成多个<code>Region</code></li><li>根数据表，有明<code>ROOT</code>表，记录所有元数据的具体位置</li><li><code>ROOT</code>表只有唯一一个<code>Region</code>，名字是在程序中被写死的</li><li><p><code>Zookeeper</code>文件记录了<code>ROOT</code>表的位置<br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase11.jpg" alt=""></p></li><li><p>为了加快访问速度，<code>META</code>表的全部<code>Region</code>都会被保存在内存中</p></li><li><p>假设<code>META</code>表的每行（一个映射条目）在内存中大约占用1KB，每个<code>Region</code>限制为128MB，那么上面三层结构可以曹村的用户数据表的<code>Region</code>数目的计算方法是：</p><ul><li><code>ROOT</code>表能够寻址<code>META</code>表的<code>Region</code>个数 X 每个<code>META</code>表能寻址的个数</li></ul></li><li>一个<code>ROOT</code>表最多只能有一个<code>Region</code>大小，也就是最多只能有<code>128MB</code>，按照每行占用1KB内存计算，128MB空间可以容纳128MB÷1kb=217128MB÷1kb=217。也就是一个ROOT可以寻址217217个.META表的Region。</li><li>同理每个<code>META</code>表的Region可以存之的用户数据表的Region个数也是217217个</li></ul><p>客户访问数据时的“三级寻址”：</p><ul><li>为了加速寻址，客户端会缓存位置信息，同时需要解决缓存失效的问题</li><li>寻址过程客户端需要询问<code>Zookeeper</code>服务器，不需要链接<code>Master</code>服务器。</li></ul><h2 id="HBase运行机制"><a href="#HBase运行机制" class="headerlink" title="HBase运行机制"></a>HBase运行机制</h2><ol><li>HBase系统架构</li><li>Region服务器工作原理</li><li>Store工作原理</li><li>HLog工作原理</li></ol><h3 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase12.jpg" alt=""></p><ul><li>客户端<br>客户端包含访问HBase的接口,同时在缓存中维护者已经好访问过的Region位置信息,用来加快后续数据访问的过程</li><li>Zookeeper服务器<br>Zookeeper可以帮助选出一个Master作为集群的总管,并保证在任何时刻总有唯一一个Master在运行,这就避免了Master“单点失效”问题.</li></ul><p>💡<strong>Zookeeper是一个很好的集群管理工具,被大量用于分布式计算,提供配置维护、域名服务、分布式同步、组服务等</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase13.jpg" alt=""></p><ul><li>Master服务器: 主服务器主要负责和Region的管理工作<ul><li>管理用户对表的增删改查</li><li>实现不同Region服务器之间的负载均衡</li><li>在Region分裂或合并后,负责重新调整Region的分布</li><li>对发生故障时晓得Region服务器上的Region进行迁移</li></ul></li><li>Region服务器:<ul><li>Region服务器时HBase最核心的模块,负责维护分配给自己的Region,并响应用户的读写请求</li></ul></li></ul><h3 id="Region服务器的工作原理"><a href="#Region服务器的工作原理" class="headerlink" title="Region服务器的工作原理"></a>Region服务器的工作原理</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase14.jpg" alt=""><br><strong>1.用户读写数据过程</strong></p><ul><li>用户写入数据时，被分配到相应Region服务器去执行</li><li>用户数据首先被写入到MemStore和Hlog中</li><li>只有当操作写入Hlog之后，commit()调用才会将其返回给客户端</li><li>当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找</li></ul><p><strong>2.缓存的刷新</strong></p><ul><li>系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记</li><li><p>每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件</p></li><li><p>每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务</p></li></ul><p><strong>3.StoreFile的合并</strong></p><ul><li>每次刷写都生成一个新的StoreFile，数量太多，影响查找速度</li><li>调用Store.compact()把多个合并成一个</li><li>合并操作比较耗费资源，只有数量达到一个阈值才启动合并</li></ul><p>💡<code>这么做是为了尽量一次性刷到磁盘,以此提高速度.但是如果StoreFile数量太多影响查找速度</code></p><h3 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h3><ul><li>Store是Region服务器的核心</li><li>多个StoreFile合并成一个</li><li>单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region<br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase15.jpg" alt=""></li></ul><h3 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h3><ul><li>分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复</li><li>HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）</li><li>用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘</li><li>Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master</li><li>Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录</li><li>系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器</li><li>Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复</li><li>共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志</li></ul><h2 id="应用方案"><a href="#应用方案" class="headerlink" title="应用方案"></a>应用方案</h2><ol><li>HBase实际应用中的性能优化方法</li><li>HBase性能监视</li><li>在HBase之上构建SQL引擎</li><li>构建HBase二级索引</li></ol><h3 id="HBase实际应用中的性能优化方法"><a href="#HBase实际应用中的性能优化方法" class="headerlink" title="HBase实际应用中的性能优化方法"></a>HBase实际应用中的性能优化方法</h3><p><strong>行键</strong><br>行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。<br>举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。<br><strong>InMemory</strong><br>创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。<br><strong>Max Version</strong><br>创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。<br><strong>Time to Live</strong><br>创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 _24 _60 * 60)。</p><h3 id="HBase性能监视"><a href="#HBase性能监视" class="headerlink" title="HBase性能监视"></a>HBase性能监视</h3><ol><li>Master-status(自带)</li><li>Ganglia</li><li>OpenTSDB</li><li>Ambari</li></ol><p><strong>Master-status</strong><br>HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是<a href="http://master.foo.com:60010%EF%BC%8C%E7%94%A8%E6%88%B7%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87Web%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5%E8%BF%99%E4%B8%AA%E5%9C%B0%E5%9D%80%E6%9F%A5%E7%9C%8B%E8%AF%A5%E9%A1%B5%E9%9D%A2/" target="_blank" rel="noopener">http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面</a><br>可以查看HBase集群的当前状态<br><strong>Ganglia</strong><br>Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能<br><strong>OpenTSDB</strong><br>OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等<br><strong>Ambari</strong><br>Ambari 的作用就是创建、管理、监视 Hadoop 的集群 (推荐HDP CDH)<br><img src="http://p4rlzrioq.bkt.clouddn.com/Hbase16.jpg" alt=""></p><h3 id="在HBase之上构建SQL引擎"><a href="#在HBase之上构建SQL引擎" class="headerlink" title="在HBase之上构建SQL引擎"></a>在HBase之上构建SQL引擎</h3><p>NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：</p><p>1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。<br>2.减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。　</p><p>方案：<br>1.Hive整合HBase<br>2.Phoenix</p><h2 id="HIVE和HBASE区别"><a href="#HIVE和HBASE区别" class="headerlink" title="HIVE和HBASE区别"></a>HIVE和HBASE区别</h2><ol><li><p>两者分别是什么？<br>Apache Hive是一个构建在Hadoop基础设施之上的数据仓库。通过Hive可以使用HQL语言查询存放在HDFS上的数据。HQL是一种类SQL语言，这种语言最终被转化为Map/Reduce. 虽然Hive提供了SQL查询功能，但是Hive不能够进行交互查询—因为它只能够在Haoop上批量的执行Hadoop。<br>Apache HBase是一种Key/Value系统，它运行在HDFS之上。和Hive不一样，Hbase的能够在它的数据库上实时运行，而不是运行MapReduce任务。Hive被分区为表格，表格又被进一步分割为列簇。列簇必须使用schema定义，列簇将某一类型列集合起来（列不要求schema定义）。例如，“message”列簇可能包含：“to”, ”from” “date”, “subject”, 和”body”. 每一个 key/value对在Hbase中被定义为一个cell，每一个key由row-key，列簇、列和时间戳。在Hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。Hbase利用Hadoop的基础设施，可以利用通用的设备进行水平的扩展。</p></li><li><p>两者的特点<br>Hive帮助熟悉SQL的人运行MapReduce任务。因为它是JDBC兼容的，同时，它也能够和现存的SQL工具整合在一起。运行Hive查询会花费很长时间，因为它会默认遍历表中所有的数据。虽然有这样的缺点，一次遍历的数据量可以通过Hive的分区机制来控制。分区允许在数据集上运行过滤查询，这些数据集存储在不同的文件夹内，查询的时候只遍历指定文件夹（分区）中的数据。这种机制可以用来，例如，只处理在某一个时间范围内的文件，只要这些文件名中包括了时间格式。<br>HBase通过存储key/value来工作。它支持四种主要的操作：增加或者更新行，查看一个范围内的cell，获取指定的行，删除指定的行、列或者是列的版本。版本信息用来获取历史数据（每一行的历史数据可以被删除，然后通过Hbase compactions就可以释放出空间）。虽然HBase包括表格，但是schema仅仅被表格和列簇所要求，列不需要schema。Hbase的表格包括增加/计数功能。</p></li><li><p>限制<br>Hive目前不支持更新操作。另外，由于hive在hadoop上运行批量操作，它需要花费很长的时间，通常是几分钟到几个小时才可以获取到查询的结果。Hive必须提供预先定义好的schema将文件和目录映射到列，并且Hive与ACID不兼容。<br>HBase查询是通过特定的语言来编写的，这种语言需要重新学习。类SQL的功能可以通过Apache Phonenix实现，但这是以必须提供schema为代价的。另外，Hbase也并不是兼容所有的ACID特性，虽然它支持某些特性。最后但不是最重要的—为了运行Hbase，Zookeeper是必须的，zookeeper是一个用来进行分布式协调的服务，这些服务包括配置服务，维护元信息和命名空间服务。</p></li><li><p>应用场景<br>Hive适合用来对一段时间内的数据进行分析查询，例如，用来计算趋势或者网站的日志。Hive不应该用来进行实时的查询。因为它需要很长时间才可以返回结果。<br>Hbase非常适合用来进行大数据的实时查询。Facebook用Hbase进行消息和实时的分析。它也可以用来统计Facebook的连接数。</p></li><li><p>总结<br>Hive和Hbase是两种基于Hadoop的不同技术—Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/hadoop-ecosystem%20%281%29.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://frankblog.site/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hbase" scheme="http://frankblog.site/tags/Hbase/"/>
    
      <category term="数据库" scheme="http://frankblog.site/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>常用正则表达式</title>
    <link href="http://frankblog.site/2018/06/24/%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://frankblog.site/2018/06/24/常用正则表达式/</id>
    <published>2018-06-24T15:35:14.289Z</published>
    <updated>2018-06-24T16:03:21.677Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AD%A3%E5%88%991.jpg" alt=""></p><a id="more"></a><hr><p>正则在线检测: <a href="https://link.jianshu.com?t=http://www.regexpal.com/" target="_blank" rel="noopener">http://www.regexpal.com/</a></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AD%A3%E5%88%99.png" alt=""></p><p>正则表达式.png</p><h2 id="一、校验数字的表达式"><a href="#一、校验数字的表达式" class="headerlink" title="一、校验数字的表达式"></a><strong>一、校验数字的表达式</strong></h2><p>1.数字：<code>^[0-9]*$</code><br>2.n位的数字：<code>^\d{n}$</code><br>3.至少n位的数字：<code>^\d{n,}$</code><br>4.m-n位的数字：<code>^\d{m,n}$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. \d 数字:[0-9]</span><br><span class="line">2. &#123;m,n&#125; 匹配前一个字符m至n次</span><br><span class="line">   &#123;，n&#125; 匹配前一个字符0至n次</span><br><span class="line">   &#123;m,&#125; 匹配前一个字符m至无穷次</span><br><span class="line">3. * 匹配前一个字符0或无限次</span><br><span class="line">4. ^ 匹配字符串开头</span><br><span class="line">    $ 匹配字符串结尾</span><br></pre></td></tr></table></figure><p>5.零和非零开头的数字：<code>^(0|[1-9][0-9]*)$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1\. 左右表达式任意一个</span><br></pre></td></tr></table></figure><p>6.非零开头的最多带两位小数的数字：<code>^([1-9][0-9]*)+(.[0-9]{1,2})?$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1\. ()被括起来的表达式将作为分组</span><br><span class="line">2\. ？匹配前一个字符0次或1次</span><br></pre></td></tr></table></figure><p>7 带1-2位小数的正数或负数：<code>^(-)?\d+(.\d{1,2})?$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1\. \转义字符</span><br><span class="line">2\. ？重复0次或1次</span><br></pre></td></tr></table></figure><p>8 正数、负数、和小数：<code>^(-|+)?\d+(.\d+)?$</code><br>9 有两位小数的正实数：<code>^[0-9]+(.[0-9]{2})?$</code><br>10 有1~3位小数的正实数：<code>^[0-9]+(.[0-9]{1,3})?$</code><br>11 非零的正整数：<code>^[1-9]\d_$</code><br>12 非零的负整数： <code>^-[1-9]\d_$</code><br>13 非负整数：<code>^\d+$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1\. 匹配前一个字符1次或无限次</span><br></pre></td></tr></table></figure><p>14 非正整数：<code>^((-\d+)|(0+))$</code><br>15 非负浮点数：<code>^\d+(.\d+)?$</code><br>16 非正浮点数：<code>^((-\d+(.\d+)?)|(0+(.0+)?))$</code><br>17 正浮点数：<code>^[0-9]\d_.\d_$</code></p><h2 id="二、校验字符的表达式"><a href="#二、校验字符的表达式" class="headerlink" title="二、校验字符的表达式"></a><strong>二、校验字符的表达式</strong></h2><p>1 汉字：<code>^[\u4e00-\u9fa5]{0,}$</code>, 对应的是中文的unicode编码</p><p>2 英文和数字：<code>^[A-Za-z0-9]+$</code><br>3 长度为3-20的所有字符：<code>^.{3,20}$</code><br>4 由26个英文字母组成的字符串：<code>^[A-Za-z]+$</code><br>5 由26个大写英文字母组成的字符串：<code>^[A-Z]+$</code><br>6 由26个小写英文字母组成的字符串：<code>^[a-z]+$</code><br>7 由数字和26个英文字母组成的字符串：<code>^[A-Za-z0-9]+$</code><br>8 由数字、26个英文字母或者下划线组成的字符串：<code>^\w+$</code><br>9 中文、英文、数字包括下划线：<code>^[\u4E00-\u9FA5A-Za-z0-9_]+$</code><br>10 中文、英文、数字但不包括下划线等符号：<code>^[\u4E00-\u9FA5A-Za-z0-9]+$</code><br>11 可以输入含有<code>^%&amp;&#39;,;=?$&quot;</code> :<br><code>[^%&amp;&#39;,;=?$]+</code></p><p>12 禁止输入含有的字符：<code>[^\x22]+</code></p><h2 id="三、特殊需求表达式"><a href="#三、特殊需求表达式" class="headerlink" title="三、特殊需求表达式"></a><strong>三、特殊需求表达式</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1 Email地址：^\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*$</span><br><span class="line">2 域名：[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;(/.[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;)+/.?</span><br><span class="line">3 InternetURL：[a-zA-z]+://[^\s]* </span><br><span class="line">4 手机号码:^(13[0-9]|14[5|7]|15[0-9])\d&#123;8&#125;$</span><br><span class="line">5 电话号码 ^(\(\d&#123;3,4&#125;-)|\d&#123;3.4&#125;-)?\d&#123;7,8&#125;$ </span><br><span class="line">6 国内电话号码(0511-4405222、021-87888822)：\d&#123;3&#125;-\d&#123;8&#125;|\d&#123;4&#125;-\d&#123;7&#125;</span><br><span class="line">7 身份证号(15位、18位数字)：^\d&#123;15&#125;|\d&#123;18&#125;$</span><br><span class="line">8 短身份证号码(数字、字母x结尾)：^([0-9])&#123;7,18&#125;(x|X)?$ 或 ^\d&#123;8,18&#125;|[0-9x]&#123;8,18&#125;|[0-9X]&#123;8,18&#125;?$</span><br><span class="line">9 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]&#123;4,15&#125;$</span><br><span class="line">10 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\w&#123;5,17&#125;$</span><br><span class="line">11 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$  </span><br><span class="line">12 日期格式：^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span><br><span class="line">13 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$</span><br><span class="line">14 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ </span><br><span class="line">15 空白行的正则表达式：\n\s*\r    (可以用来删除空白行)</span><br><span class="line">16 首尾空白字符的正则表达式：^\s*|\s*$</span><br><span class="line">17腾讯QQ号：[1-9][0-9]&#123;4,&#125;    (腾讯QQ号从10000开始)</span><br><span class="line">18 中国邮政编码：[1-9]\d&#123;5&#125;(?!\d)    (中国邮政编码为6位数字)</span><br><span class="line">19 IP地址：\d+\.\d+\.\d+\.\d+    (提取IP地址时有用)</span><br></pre></td></tr></table></figure><p>参考：<a href="http://www.cnblogs.com/zxin/archive/2013/01/26/2877765.html" target="_blank" rel="noopener">http://www.cnblogs.com/zxin/archive/2013/01/26/2877765.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%AD%A3%E5%88%991.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="正则表达式" scheme="http://frankblog.site/categories/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
    
      <category term="正则表达式" scheme="http://frankblog.site/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之风控评分卡模型</title>
    <link href="http://frankblog.site/2018/06/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%A3%8E%E6%8E%A7%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B/"/>
    <id>http://frankblog.site/2018/06/20/机器学习之风控评分卡模型/</id>
    <published>2018-06-19T17:20:35.600Z</published>
    <updated>2018-06-24T16:20:57.571Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7%E5%B0%81%E9%9D%A2.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p>一般来说风控领域在意的是前两个模型种类，排序类以及决策类。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7.jpg" alt="link"><br>其中：巴塞尔协议定义了金融风险类型：市场风险、作业风险、信用风险。信用风险ABC模型有进件申请评分、行为评分、催收评分。</p><div class="table-container"><table><thead><tr><th style="text-align:left">模型</th><th style="text-align:left">解释</th><th style="text-align:left">应用场景</th></tr></thead><tbody><tr><td style="text-align:left">Logistics回归</td><td style="text-align:left">影响程度大小与显著性，解释力度强，但只是线性，没有顾及到非线性，预测精度较低</td><td style="text-align:left">申请评分、流失预测</td></tr><tr><td style="text-align:left">决策树</td><td style="text-align:left">1、描述性，重建用户场景，可做变量提取与用户画像 2、树的结构不稳定，可以得出变量重要性，可以作为变量筛选</td><td style="text-align:left">流失模式识别</td></tr><tr><td style="text-align:left">随机森林</td><td style="text-align:left">随机森林比决策树在变量筛选中，变量排序比较优秀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">神经网络</td><td style="text-align:left">1、不可解释，内部使用，预测精度较高。可以作为初始模型的金模型（用以评估在给定数据条件下，逻辑回归可达到的最精确程度）2、线性（逻辑回归）+非线性关系，可用于行为评分的预测模型（行为评分对模型可解释性不强），可用于申请评分的金模型3、使用场景：先做一个神经网络，让预测精度（AUC）达到最大时，再用逻辑回归</td><td style="text-align:left">申请评分的金模型；行为评分的预测模型</td></tr></tbody></table></div><h1 id="监控模型指标"><a href="#监控模型指标" class="headerlink" title="监控模型指标"></a>监控模型指标</h1><p>决策类：准确率/误分率、利润/成本<br>排序类：ROC指标（一致性）、Gini指数、KS统计量、提升度</p><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p><img src="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png" alt="link"></p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p><strong>准确率</strong>是预测和标签一致的样本在所有样本中所占的比例</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg" alt="link"></p><h2 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h2><p><strong>精确率</strong>是你预测为正类的数据中，有多少确实是正类</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg" alt="link"></p><h2 id="查全率（Recall）"><a href="#查全率（Recall）" class="headerlink" title="查全率（Recall）"></a>查全率（Recall）</h2><p><strong>查全率</strong>是所有正类的数据中，你预测为正类的数据占比</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg" alt="link"></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg" alt="link"></p><p>不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg" alt="link"></p><h2 id="F-beta-Score"><a href="#F-beta-Score" class="headerlink" title="F-beta Score"></a>F-beta Score</h2><p>有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg" alt="link"></p><h2 id="ROC-（receiver-operating-characteristic-curve）"><a href="#ROC-（receiver-operating-characteristic-curve）" class="headerlink" title="ROC （receiver operating characteristic curve）"></a>ROC （receiver operating characteristic curve）</h2><p>绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。<br>ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</p><p><img src="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif" alt=""></p><h2 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h2><p>ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。</p><p><img src="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p><h2 id="Lift提升图"><a href="#Lift提升图" class="headerlink" title="Lift提升图"></a>Lift提升图</h2><p><strong>Lift</strong> =[TP/(TP+FP)] / [(TP+FN)/(TP+FP+FN+TN)] = PV_plus / pi1，它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。</p><p>不利用模型，我们只能利用“正例的比例是(TP+FN)/(TP+FP+FN+TN)”这个样本信息来估计正例的比例（baseline model），而利用模型之后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集TP+FP中挑选正例，这时预测的准确率PV_plus(Precision)为TP/(TP+FP)。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/lift%E8%AF%84%E4%BC%B0.png" alt=""></p><p>上图的纵坐标是lift，横坐标是正例集百分比。随着阈值的减小，更多的客户就会被归为正例，也就是预测成正例的比例变大。当阈值设得够大，只有一小部分观测值会归为正例，但这一小部分一定是最具有正例特征的观测值集合（用前面银行向客户推荐信用卡的例子来看，这一部分人群对推荐的反应最为活跃），所以在这个设置下，对应的lift值最大。同样，当阈值设定得足够的小，那么几乎所有的观测值都会被归为正例（占比几乎为100%）——这时分类的效果就跟baseline model差不多了，相对应的lift值就接近于1。</p><blockquote><p>ROC曲线和lift曲线都能够评价逻辑回归模型的效果：类似信用评分的场景，希望能够尽可能完全地识别出有违约风险的客户，选择ROC曲线及相应的AUC作为指标；</p><p>类似数据库精确营销的场景，希望能够通过对全体消费者的分类而得到具有较高响应率的客户群从而提高投入产出比，选择lift曲线作为指标；</p></blockquote><h2 id="Gain增益图"><a href="#Gain增益图" class="headerlink" title="Gain增益图"></a>Gain增益图</h2><p>Gains(增益) 与 Lift （提升）类似：Lift 曲线是不同阈值下Lift和Depth的轨迹，Gain曲线则是不同阈值下PV_plus和Depth的轨迹，而PV_plus=Lift*pi1= TP/TP+FP，所以它们显而易见的区别就在于纵轴刻度的不同。</p><p>增益图是描述整体精准率的指标。按照模型预测出的概率从高到低排列，将每一个百分位数内的精准率指标标注在图形区域内，就形成了非累积的增益图。如果对每一个百分位及其之前的精准率求和，并将值标注在图形区域内，则形成累积的增益图。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B0%E8%AF%84%E4%BB%B7.png" alt="Gain图"></p><h2 id="K-S图"><a href="#K-S图" class="headerlink" title="K-S图"></a>K-S图</h2><p>正样本洛伦兹曲线记为f(x)，负样本洛伦兹曲线记为g(x)，K-S曲线实际上是f(x)与g(x)的差值曲线。K-S曲线的最高点（最大值）定义为KS值，KS值越大，模型分值的区分度越好，KS值为0代表是最没有区分度的随机模型。准确的来说，K-S是用来度量阳性与阴性分类区分程度的。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/ks%E5%9B%BE%E8%AF%84%E4%BC%B0.png" alt=""></p><h2 id="PSI-群体稳定性指标-population-stability-index"><a href="#PSI-群体稳定性指标-population-stability-index" class="headerlink" title="PSI 群体稳定性指标(population stability index)"></a>PSI 群体稳定性指标(population stability index)</h2><p>psi = sum(（实际占比-预期占比）* ln(实际占比/预期占比))</p><p>一般认为psi小于0.1时候模型稳定性很高，0.1-0.25一般，大于0.25模型稳定性差，建议重做。</p><h1 id="常用特征"><a href="#常用特征" class="headerlink" title="常用特征"></a>常用特征</h1><ul><li><p>个人信息：学历 性别 收入</p></li><li><p>负债能力：在申请的金融机构或者其他金融机构的负债情况（例如月还债金额超过月收入的60%，说明负债较高），例如多头借贷信息等</p></li><li>消费能力：商品购买记录，出境游，奢侈品消费</li><li>历史信用记录：历史逾期行为</li><li>其他数据：个人交际、网络足迹、个人财务等</li></ul><p>备注：客户还款能力*还款意愿 = 还款等级</p><h1 id="非平衡样本的处理方法"><a href="#非平衡样本的处理方法" class="headerlink" title="非平衡样本的处理方法"></a>非平衡样本的处理方法</h1><ul><li>过采样：优点方法简单，缺点容易造成模型过拟合。</li><li>欠采样：优点和过采样类似，缺点是容易造成模型的欠拟合；</li><li>SMOTE：优点是不易过拟合，能够保留大量的信息，缺点是不能对缺失值和类别变量做处理。</li></ul><h2 id="SMOTE算法"><a href="#SMOTE算法" class="headerlink" title="SMOTE算法"></a>SMOTE算法</h2><ol><li>采样最近邻算法，计算出每个少数类样本的K个同类近邻；</li><li>从K个同类近邻中随机挑选N个样本进行随机线性插值；</li><li>构造新的少数类样本：<br>New=Xi+rand(0,1)∗(yj−xi),j=1,2,3,4…..N<br>其中Xi为少类中的一个观测点，Yj为K个近邻中随机抽取的样本</li><li><p>将新样本与原数据合成，产生新的训练集</p><p>例子：选取了一个X1为年龄为22岁，月收入为8000元，则X1=（22，8000），选取了一个近邻点为X2，X2=(28,5000)，随机系数为0.5，计算逻辑为22+(28−22)∗0.5=25,8000+(5000−8000)∗0.5=6500，这样得到的一个新的X3点为(25,6500)。</p></li></ol><h1 id="构建申请评分卡"><a href="#构建申请评分卡" class="headerlink" title="构建申请评分卡"></a>构建申请评分卡</h1><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h3 id="对于类别型变量"><a href="#对于类别型变量" class="headerlink" title="对于类别型变量"></a>对于类别型变量</h3><ol><li>删除缺失率超过50%的变量</li><li>缺失值作为一种新的状态</li></ol><h3 id="对于连续型变量"><a href="#对于连续型变量" class="headerlink" title="对于连续型变量"></a>对于连续型变量</h3><ol><li>删除缺失率超过80%的变量</li><li>均值，众数，预测填充等</li></ol><h2 id="特征的分箱"><a href="#特征的分箱" class="headerlink" title="特征的分箱"></a>特征的分箱</h2><h3 id="1-分箱的重要性"><a href="#1-分箱的重要性" class="headerlink" title="1. 分箱的重要性"></a>1. 分箱的重要性</h3><ol><li>稳定性：避免特征中无意义的波动对评分带来的波动</li><li>健壮性：避免了极端值的影响</li></ol><h3 id="2-分箱的优势"><a href="#2-分箱的优势" class="headerlink" title="2. 分箱的优势"></a>2. 分箱的优势</h3><ol><li>可以将缺失作为独立的一个箱带入模型中</li><li>将所有变量变换到相似的尺度上</li></ol><h3 id="3-分箱的限制"><a href="#3-分箱的限制" class="headerlink" title="3. 分箱的限制"></a>3. 分箱的限制</h3><ol><li>分箱后需要编码</li><li>计算量大</li></ol><h3 id="4-分箱的方法"><a href="#4-分箱的方法" class="headerlink" title="4. 分箱的方法"></a>4. 分箱的方法</h3><ul><li>有监督的分箱</li></ul><ol><li>Best—KS</li><li>Chimerge</li></ol><ul><li>无监督的分箱</li></ul><ol><li>等频分箱</li><li>等距分箱</li><li>聚类分箱</li></ol><h3 id="5-有监督的分箱—最小熵法分箱"><a href="#5-有监督的分箱—最小熵法分箱" class="headerlink" title="5. 有监督的分箱—最小熵法分箱"></a>5. 有监督的分箱—最小熵法分箱</h3><p>(1) 假设因变量为分类变量，可取值1，… ，J。令pij表示第i个分箱内因变量取值为j的观测的比例，i=1，…，k，j=1，…，J；那么第i个分箱的熵值为∑Jj=0−pij×logpij。如果第i个分箱内因变量各类别的比例相等，即p11=p12=p1J=1/J，那么第i个分箱的熵值达到最大值；如果第i个分箱内因变量只有一种取值，即某个pij等于1而其他类别的比例等于0，那么第i个分箱的熵值达到最小值。</p><p>(2) 令ri表示第i个分箱的观测数占所有观测数的比例；那么总熵值为∑ki=0∑Jj=0(−pij×logpij)。需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。</p><h3 id="6-卡方分箱法-ChiMerge"><a href="#6-卡方分箱法-ChiMerge" class="headerlink" title="6. 卡方分箱法(ChiMerge)"></a>6. 卡方分箱法(ChiMerge)</h3><p>自底向上的(即基于合并的)数据离散化方法。</p><p>它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。</p><p>基本思想:<br>对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%A1%E6%96%B9%E5%88%86%E7%AE%B1.png" alt=""><br>根据显著性水平和自由度得到卡方值,自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度（10%显著性水平)下，卡方的值为4.6。</p><p>类别和属性独立时，有90%的可能性，计算得到的卡方值会小于4.6，这样，大于阈值的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大，区间合并就会进行很多次，离散后的区间数量少、区间大。<br>【注】：</p><ol><li>ChiMerge算法推荐使用0.90、0.95、0.99置信度，最大区间数取10到15之间.</li><li>也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限，最多几个区间，最少几个区间。</li><li>对于类别型变量，需要分箱时需要按照某种方式进行排序</li></ol><h3 id="无监督分箱法"><a href="#无监督分箱法" class="headerlink" title="无监督分箱法"></a>无监督分箱法</h3><ul><li>等距分箱<br>从最小值到最大值之间，均分为 N 等份， 这样， 如果 A,B 为最小最大值， 则每个区间的</li></ul><p>长度为 W=(B−A)/N , 则区间边界值为 A+W,A+2W,….A+(N−1)W .</p><ul><li><p>等频分箱<br>区间的边界值要经过选择，使得每个区间包含大致相等的实例数量。比如说 N=10 ，每个区间应该包含大约10%的实例。</p></li><li><p>两种算法的弊端<br>比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。</p></li></ul><h3 id="分箱的注意点"><a href="#分箱的注意点" class="headerlink" title="分箱的注意点"></a>分箱的注意点</h3><ul><li>对于连续型变量，</li></ul><ol><li>使用ChiMerge进行分箱(默认分成5个箱)</li><li>检查分箱后的bad  rate单调性；倘若不满足，需要进行相邻两箱的合并，直到bad rate为止</li><li>上述过程是收敛的，因为当箱数为2时，bad rate自然单调</li><li>分箱必须覆盖所有训练样本外可能存在的值！</li></ol><ul><li>对于类别型变量</li></ul><ol><li>当类别数较少时，原则上不需要分箱</li><li>当某个或者几个类别的bad rate为0时，需要和最小的非0bad rate的箱进行合并</li><li>当该变量可以完全区分目标变量时，需要认真检查该变量的合理性</li><li>例如：“该申请者在本机构历史信用行为”把客群的好坏样本完全区分时，需要检查该变量的合理性(有可能是事后变量)</li></ol><h2 id="woe编码-amp-IV值"><a href="#woe编码-amp-IV值" class="headerlink" title="woe编码&amp;IV值"></a>woe编码&amp;IV值</h2><h3 id="1-woe编码的定义"><a href="#1-woe编码的定义" class="headerlink" title="1.woe编码的定义"></a>1.woe编码的定义</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/woe%E7%BC%96%E7%A0%81.png" alt=""></p><p>其中，pyi是这个组中响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）占所有样本中所有响应客户的比例，pni是这个组中未响应客户占样本中所有未响应客户的比例，#yi是这个组中响应客户的数量，#ni是这个组中未响应客户的数量，#yT是样本中所有响应客户的数量，#nT是样本中所有未响应客户的数量。</p><p>从这个公式中我们可以体会到，WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。</p><h3 id="2-woe-编码的优劣"><a href="#2-woe-编码的优劣" class="headerlink" title="2.woe 编码的优劣"></a>2.woe 编码的优劣</h3><p>一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值<br>优势：将特征的值规范到相近的尺度上(经验上讲，WOE的绝对值波动范围在0.1～3之间)<br>缺点：需要每箱中同时包含好、坏两个类别</p><h3 id="3-WOE编码的意义"><a href="#3-WOE编码的意义" class="headerlink" title="3.WOE编码的意义"></a>3.WOE编码的意义</h3><ul><li>符号与好样本比例相关</li><li>要求回归模型的系数为负</li></ul><h3 id="4-IV-Information-Value"><a href="#4-IV-Information-Value" class="headerlink" title="4. IV(Information Value)"></a>4. IV(Information Value)</h3><p>IV去衡量变量的预测能力：我们假设在一个分类问题中，目标变量的类别有两类：Y1，Y2。对于一个待预测的个体A，要判断A属于Y1还是Y2，我们是需要一定的信息的，假设这个信息总量是I，而这些所需要的信息，就蕴含在所有的自变量C1，C2，C3，……，Cn中，那么，对于其中的一个变量Ci来说，其蕴含的信息越多，那么它对于判断A属于Y1还是Y2的贡献就越大，Ci的信息价值就越大，Ci的IV就越大，它就越应该进入到入模变量列表中。<br><img src="http://p4rlzrioq.bkt.clouddn.com/iv%E5%80%BC.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/iv%E5%80%BC%E9%A2%84%E6%B5%8B.png" alt=""></p><h3 id="5-特征信息度的计算和意义"><a href="#5-特征信息度的计算和意义" class="headerlink" title="5. 特征信息度的计算和意义"></a>5. 特征信息度的计算和意义</h3><h4 id="挑选变量"><a href="#挑选变量" class="headerlink" title="挑选变量"></a>挑选变量</h4><ol><li>非负指标</li><li>高IV表示该特征和目标变量的关联度高</li><li>目标变量只能是二分类</li><li>过高的IV，可能有潜在的风险</li><li>特征分箱越细，IV越高</li></ol><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="1-特征衍生"><a href="#1-特征衍生" class="headerlink" title="1.特征衍生"></a>1.特征衍生</h3><p>特征衍生是指利用现有的特征进行某种组合生成新的特征。</p><p>1.时间切片<br>2.比例数据（负责比例，收入支出比例等）<br>3.平均数据（月均收入）</p><h3 id="2-特征抽象"><a href="#2-特征抽象" class="headerlink" title="2.特征抽象"></a>2.特征抽象</h3><p>特征抽象是指将数据转换成算法可以理解的数据。</p><ol><li><p>分类型变量转换数值型变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loans[&apos;delinq_2yrs&apos;] = loans[&apos;delinq_2yrs&apos;].apply(lambda x: float(x))</span><br><span class="line">loans[&apos;total_acc&apos;] = loans[&apos;total_acc&apos;].apply(lambda x: float(x))</span><br><span class="line">loans[&apos;revol_bal&apos;] = loans [&apos;revol_bal&apos;].apply(lambda x: float(x))</span><br></pre></td></tr></table></figure></li><li><p>有序特征的映射<br>A &lt;B &lt;C &lt; D &lt; E &lt; F &lt; G ; 信用风险从低到高排序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;grade&quot;:&#123;</span><br><span class="line">&quot;A&quot;: 1,</span><br><span class="line">&quot;B&quot;: 2,</span><br><span class="line">&quot;C&quot;: 3,</span><br><span class="line">&quot;D&quot;: 4,</span><br><span class="line">&quot;E&quot;: 5,</span><br><span class="line">&quot;F&quot;: 6,</span><br><span class="line">&quot;G&quot;: 7</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>独热编码（one-hot encoding）<br>pandas的get_dummies( )方法创建虚拟特征，虚拟特征的每一列各代表变量属性的一个分类</p></li><li><p>特征缩放<br>特征缩放本质是一个去量纲的过程，同时可以加快算法收敛的速度。目前，将不同变量缩放到相同的区间有两个常用的方法：归一化（normalization）和标准化（standardization）</p></li></ol><h2 id="建模后续"><a href="#建模后续" class="headerlink" title="建模后续"></a>建模后续</h2><p>我们将客户违约的概率表示为p，则正常的概率为1-p<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B1.png" alt=""></p><p>评分卡设定的分值刻度可以通过将分值表示为比率对数的线性表达式来定义<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B2.png" alt=""><br>A和B是常数。式中的负号可以使得违约概率越低，得分越高。通常情况下，这是分值的理想变动方向，即高分值代表低风险，低分值代表高风险。<br>式中的常数A、B的值可以通过将两个已知或假设的分值带入计算得到。通常情况下，需要设定两个假设：</p><p>（1）给某个特定的比率设定特定的预期分值；<br>（2）确定比率翻番的分数（PDO）</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B3.png" alt=""><br>最后，进行分数分级<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E6%95%B0%E5%88%86%E7%BA%A7.jpg" alt=""></p><p>更多有关机器学习的总结可以阅读：<br><a href="http://frankblog.site/2018/05/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">机器学习基础(一)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E9%A3%8E%E6%8E%A7%E5%B0%81%E9%9D%A2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="风控" scheme="http://frankblog.site/tags/%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>Spark生态体系</title>
    <link href="http://frankblog.site/2018/06/13/Spark%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB/"/>
    <id>http://frankblog.site/2018/06/13/Spark生态体系/</id>
    <published>2018-06-13T12:45:29.856Z</published>
    <updated>2018-06-13T12:49:59.646Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/spark%E6%A1%86%E6%9E%B6.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="Spark的基本架构"><a href="#Spark的基本架构" class="headerlink" title="Spark的基本架构"></a>Spark的基本架构</h1><p>当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。<br>Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark.jpg" alt=""></a></p><h2 id="1-Spark-Application"><a href="#1-Spark-Application" class="headerlink" title="1. Spark Application"></a>1. Spark Application</h2><p>Spark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。<br>Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark1.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark1.jpg" alt=""></a></p><p>群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。<br>在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。</p><ul><li>Spark有一些集群管理器，负责调度可用资源。</li><li>驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。</li></ul><h2 id="2-Spark’s-Languge-APIs"><a href="#2-Spark’s-Languge-APIs" class="headerlink" title="2. Spark’s Languge APIs"></a>2. Spark’s Languge APIs</h2><p>尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark2.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark2.jpg" alt=""></a></p><p><code>Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。</code></p><h2 id="3-SparkSession"><a href="#3-SparkSession" class="headerlink" title="3. SparkSession"></a>3. SparkSession</h2><p>我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark3.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark3.jpg" alt=""></a></p><h2 id="4-Dataframe"><a href="#4-Dataframe" class="headerlink" title="4. Dataframe"></a>4. Dataframe</h2><p>DataFrame是最常见的<code>Structured API</code>（结构化API），只是表示有类型的<code>包含行和列的数据表</code>。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark4.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark4.jpg" alt=""></a></p><p>DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的<code>Spark’s Language APIs</code>，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。</p><p><strong>注意</strong><br>Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。<strong>以下概念适用于所有的核心抽象。</strong></p><h2 id="5-Partitions"><a href="#5-Partitions" class="headerlink" title="5. Partitions"></a>5. Partitions</h2><p>为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。<br>值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark5.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark5.jpg" alt=""></a></p><h2 id="6-Transformations"><a href="#6-Transformations" class="headerlink" title="6. Transformations"></a>6. Transformations</h2><p>在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为<code>转换</code>。<br>转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。</p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark6.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark6.jpg" alt=""></a><br>宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。</p><h2 id="7-Lazy-Evaluation"><a href="#7-Lazy-Evaluation" class="headerlink" title="7. Lazy Evaluation"></a>7. Lazy Evaluation</h2><p>Lazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。</p><h2 id="8-Actions"><a href="#8-Actions" class="headerlink" title="8. Actions"></a>8. Actions</h2><p>转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。<br>在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI，UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。</p><h2 id="9-Dataframe-amp-SQL"><a href="#9-Dataframe-amp-SQL" class="headerlink" title="9. Dataframe &amp; SQL"></a>9. Dataframe &amp; SQL</h2><p>Spark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。<br>Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。</p><p>现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。<br>通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。</p><p><strong>With SQl</strong><br><a href="http://p4rlzrioq.bkt.clouddn.com/spark7.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark7.jpg" alt=""></a><br><strong>With DataFrame</strong><br><a href="http://p4rlzrioq.bkt.clouddn.com/spark8.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark8.jpg" alt=""></a></p><p><a href="http://p4rlzrioq.bkt.clouddn.com/spark9.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark9.jpg" alt=""></a></p><p>现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。</p><ol><li>第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation）</li><li>第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前<strong>指定聚合</strong>。</li><li>因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。</li><li>第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！</li><li>第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。</li><li>第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。</li><li>最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。<br><a href="http://p4rlzrioq.bkt.clouddn.com/spark10.jpg" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/spark10.jpg" alt=""></a><br>虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/spark%E6%A1%86%E6%9E%B6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://frankblog.site/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>GBDT&amp;XGBOOST（一）</title>
    <link href="http://frankblog.site/2018/06/12/GBDT&amp;XGBOOST%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://frankblog.site/2018/06/12/GBDT&amp;XGBOOST（一）/</id>
    <published>2018-06-12T14:03:50.675Z</published>
    <updated>2018-06-13T13:29:03.096Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/boosting2.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p><h1 id="GBDT（梯度提升决策树）"><a href="#GBDT（梯度提升决策树）" class="headerlink" title="GBDT（梯度提升决策树）"></a>GBDT（梯度提升决策树）</h1><p>GBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终结果.</p><p>要理解 GBDT，就要先理解 GB ，然后才是 DT，而且这里的 DT 是回归树，而不是分类树.</p><h2 id="DT（决策回归树）"><a href="#DT（决策回归树）" class="headerlink" title="DT（决策回归树）"></a><strong>DT（决策回归树）</strong></h2><p>这里先补充一下什么是回归树，因为之前所讲的决策树都是属于分类树.</p><p>先回顾下分类树:</p><blockquote><p>我们知道 分类树在每次分枝时，是穷举每一个 feature 的每一个阈值，找到使得按照 <code>feature &lt;= 阈值</code>和<code>feature &gt; 阈值</code>分成两枝的熵最大的 feature 和阈值，按照该标准分枝得到的两个新节点，按同样的方法递归分裂下去，直到所有样本都被分入唯一的叶子节点，或达到预设的终止条件（如果叶子节点的样本不唯一，则以多数类作为叶子节点的分类结果）.</p></blockquote><p>回归树大致流程类似:</p><blockquote><p>不过在每个节点（不一定是叶子节点）都会得到一个预测值，以人的年龄为例，该预测值等于属于这个节点的所有人的年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差—即（每个人的年龄-预测年龄）平方的总和除以 N（总人数），或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。直到每个叶子节点上人的年龄都唯一，或者达到预设终止条件，若最终叶子节点上的人的年龄不唯一，就以该节点上所有人的平均年龄作为该叶子节点的预测值.</p><p>就是分类树是按多数投票，以多数类的类别标号作为叶子节点的分类类别；回归树是按叶子节点的样本平均值作为该节点的预测值.</p></blockquote><h2 id="GBDT-实例"><a href="#GBDT-实例" class="headerlink" title="GBDT 实例"></a><strong>GBDT 实例</strong></h2><p>GBDT的核心就在于，<strong>_每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量_</strong>。</p><p>比如以年龄预测的例子来说明，A 的真实年龄是 18 岁，但第一棵树的预测年龄是 12 岁，差了 6 岁，即残差为 6 岁。那么在第二棵树里我们把 A 的年龄设为 6 岁去学习，如果第二棵树真的能把 A 分到 6 岁的叶子节点，那累加两棵树的结论就是 A 的真实年龄；如果第二棵树的结论是 5 岁，则 A 仍然存在 1 岁的残差，第三棵树里 A 的年龄就变成 1 岁，继续学。</p><p>我们来详细的说一下这个例子，为简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图1所示结果：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gbdt1_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt="gbdt1"></p><p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gbdt2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt="gbdt2"></p><p>在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。</p><p>此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是 16-15=1（注意，A 的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是 15，如果还有树则需要都累加起来作为 A 的预测值）。进而得到 A,B,C,D 的残差分别为 (-1,1,-1,1)。</p><p>然后我们拿残差替代 A,B,C,D 的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值 1 和 -1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。</p><p>换句话说，现在 A,B,C,D 的预测值都和真实年龄一致了。Perfect!：</p><p>A: 14 岁高一学生，购物较少，经常问学长问题；预测年龄 A = 15 + (– 1) = 14</p><p>B: 16 岁高三学生，购物较少，经常被学弟问问题；预测年龄 B= 15 + (1) = 16</p><p>C: 24 岁应届毕业生，购物较多，经常问师兄问题；预测年龄 C = 25 + (-1) = 24</p><p>D: 26 岁工作两年员工，购物较多，经常被师弟问问题；预测年龄 D = 25 + (– 1) = 26</p><blockquote><p>那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量 (-1, 1, -1, 1) 都是它的全局最优方向，这就是Gradient。</p><p>其实 GBDT 大致的过程就是这例子中所讲的，下面我们再做一些深入的理解</p></blockquote><h2 id="GBDT-深入理解"><a href="#GBDT-深入理解" class="headerlink" title="GBDT 深入理解"></a><strong>GBDT 深入理解</strong></h2><p>前面说了，GBDT 是先有 GB（梯度提升），再有 DT（决策树），所以我们先从 GB 讲起.</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="_Boosting_"></a><strong>_Boosting_</strong></h3><p>boosting 在前面讲 AdaBoost提升算法的时候讲了，就是通过训练多个弱分类器来组合成一个强分类器，形式如下:</p><script type="math/tex; mode=display">F_m(x) = f_0 + \alpha_1 f_1(x) + \alpha_2 f_2(x) + \cdots + \alpha_m f_m(x)</script><p>其中，\(f_i(x),i = 1,2,\cdots,m\),是弱分类器，比如在 AdaBoost提升中是 C4.5决策树；\(F_m(x)\)是最终得到的强分类器。</p><h3 id="Gradient-Boosting-Modeling"><a href="#Gradient-Boosting-Modeling" class="headerlink" title="_Gradient Boosting Modeling_"></a><strong>_Gradient Boosting Modeling_</strong></h3><p>给定一个问题，我们如何构造这些弱分类器呢？</p><p>Gradient Boosting Modeling （梯度提升模型） 就是构造这些弱分类器的一种方法。它指的不是某个具体的算法，而是一种思想.</p><p>我们先从普通的梯度优化问题入手来理解:</p><script type="math/tex; mode=display">find \ \hat{x} = arg \min_{x} f(x)</script><p>针对这种问题，有个经典的算法叫 <strong>_Steepest Gradient Descent_</strong>，也就是最深梯度下降法。算法的大致过程是:</p><ol><li>给定一个起始点 \(x_0\)</li><li>对 \(i = 1,2,\cdots,K\)分别做如下迭代:<br>\(\qquad x_i = x_{i-1} + \gamma_{i-1} \times g_{i-1}\)<br>其中 \(g_{i-1} = -\frac{\partial f}{\partial x} |_{x = x_{i-1}}\)表示  f在 \(x_{i-1}\)点的梯度</li><li>直到 \(|g_{i-1}\)足够小，或者是 \(|x_i - x_{i-1}|\)足够小</li></ol><p>以上迭代过程可以理解为: <strong>_整个寻优的过程就是小步快跑的过程，每跑一小步，都往函数当前下降最快的那个方向走一点，直到达到可接受的点_</strong>.</p><p>我们将这个迭代过程展开得到寻优的结果:</p><script type="math/tex; mode=display">x_k = x_0 + \gamma_1 g_1 + \gamma_2 g_2 + \cdots + \gamma_k g_k</script><p>这个形式是不是与最开始我们要求的\(F_m(x)\)类似；构造\(F_m(x)\) 本身也是一个寻优的过程，只不过我们寻找的不是一个最优点，而是一个最优的函数。</p><p>寻找最优函数这个目标，也是定义一个损失函数来做:</p><script type="math/tex; mode=display">find \ F_m = arg \ \min_{F} L(F) = arg \  \min_{F} \sum_{i=0}^N Loss(F(x_i),y_i)</script><p>其中，\(Loss(F(x_i),y_i)\)表示损失函数\(Loss()\)  在第i个样本上的损失值， \(x_i,y_i\)分别表示第i个样本的特征和目标值。<br>类似最速梯度下降法，我们可以通过梯度下降法来构造弱分类器 \(f_1,f_2,\cdots,f_m\)，只不过每次迭代时，令:</p><script type="math/tex; mode=display">g_i = -\frac{\partial L}{\partial F}|_{F=F_{i-1}}</script><p>即损失函数L()对F求取梯度。</p><p>但是函数对函数求导不好理解，而且通常都无法通过上述公式直接求解。于是就采取一个近似的方法，把函数  理解成在所有样本上的离散的函数值，即:\(F_{i-1}\)理解成在所有样本上的离散的函数值，即:</p><script type="math/tex; mode=display">\left[ F_{i-1}(x_1),F_{i-1}(x_2),\cdots,F_{i-1}(x_N) \right]</script><p>这是一个N 维向量，然后计算:</p><script type="math/tex; mode=display">\hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N</script><p>这是一个函数对向量的求导，得到的也是一个梯度向量。注意，这里求导时的变量还是函数F，不是样本Xk ，只不过对F(Xk)求导时，其他的 Xi 都可以看成常数。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/GBM%E8%BF%87%E7%A8%8B.png" alt=""></p><h3 id="Gradient-Boosting-Decision-Tree"><a href="#Gradient-Boosting-Decision-Tree" class="headerlink" title="_Gradient Boosting Decision Tree_"></a><strong>_Gradient Boosting Decision Tree_</strong></h3><p>在上述算法过程中，如何通过 \(\hat{g}_{i-1}(x_j),j = 1,2,\cdots,N\)构造拟合函数\(g_{i-1}\)呢，这里我们用的就是 Decision Tree（决策树）了.</p><p>所以理解 GBDT，重点是先理解 Gradient Boosting，其次才是 Decision Tree；也就是说 GBDT 是 Gradient Boosting 的一种具体实现，这个拟合函数也可以改用其他的方法，只不过决策树好用一点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="_损失函数_"></a><strong>_损失函数_</strong></h3><p>谈到 GBDT 常听到的一种描述是 <code>先构造一个(决策)树，然后不断在已有模型和实际样本输出的残差上再构造一棵树，依次迭代</code>；其实这个说法不全面，尽管在本篇开头的那个 GBDT 的例子中是这样描述的，但拟合残差只是 GDBT 的一种特殊情况，下面对损失函数进行解释就清楚了.</p><p>从对 GBM 的描述里可以看到 Gradient Boosting 过程和具体用什么样的弱分类器是完全独立的，可以任意组合，因此这里不再刻意强调用决策树来构造弱分类器，转而我们来仔细看看弱分类器拟合的目标值，即梯度| \(\hat{g}_{i-1}(x_j)\)，之前我们已经提到过</p><script type="math/tex; mode=display">\hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N</script><p>因此  \(\frac{\partial L}{\partial F(x_k)}\)很重要，以平方差损失函数为例，得:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = 2(F_{i-1}(x_k) - y_k)</script><p>忽略 2 倍，后面括号中正是当前已经构造好的函数\(F_{i-1}\)在样本上和目标值Yk之间的差值.<br>如果我们换一个损失函数，比如绝对差:</p><script type="math/tex; mode=display">Loss(F(x_i),y_i) = |F(x_i) - y_i|</script><p>这个损失函数的梯度是个符号函数:</p><script type="math/tex; mode=display">{\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = sign(F_{i-1}(x_k) - y_k)}</script><p>由此可以看到，只有当损失函数为平方差函数时，才能说 GBDT 是通过拟合残差来构造弱分类器的，比如上面说的对残差不断的用决策回归树来拟合。</p><h1 id="知乎上关于xgboost-gbdt讨论的经典问答"><a href="#知乎上关于xgboost-gbdt讨论的经典问答" class="headerlink" title="知乎上关于xgboost/gbdt讨论的经典问答"></a>知乎上关于xgboost/gbdt讨论的经典问答</h1><p>【问】xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？<br>  用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？<br>【答】<br>  这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。<br>  一句话的解释，来自周志华老师的机器学习教科书（ 机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。<br>  随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。<br>  Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。<br>  其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X^2)-[E(X)]^2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。<br>  如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。</p><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p11.png" alt=""></p><p>  当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。</p><p>  也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。<br>  对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。<br>  对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p><p>【问】机器学习算法中GBDT和XGBOOST的区别有哪些？<br>【答】<br>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</p><p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p><p>列抽样（column subsampling）即特征抽样。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p><p>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p><p>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<br>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p><p>多种语言封装支持。</p><p>【问】为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？<br>【答】<br>作者：马超<br>链接：<a href="https://www.zhihu.com/question/51818176/answer/127637712" target="_blank" rel="noopener">https://www.zhihu.com/question/51818176/answer/127637712</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。我主要从三个方面来回答楼主这个问题。</p><ol><li>理论模型 （站在 vc-dimension 的角度）</li><li>实际数据</li><li>系统的实现 （主要基于 xgboost）<br>通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。</li></ol><p>（1）站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件：</p><ol><li>模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。</li><li>模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit.当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off.</li></ol><p>好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis.</p><p>在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？</p><p>区别就在于 “模型的可控性”。</p><p>先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。</p><p>我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.</p><p>（2）站在数据的角度</p><p>除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。</p><p>除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。</p><p>（3）站在系统实现的角度</p><p>除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征：</p><ol><li>正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。</li><li>系统具有灵活、深度的定制功能。</li><li>系统简单易用。</li><li>系统具有可扩展性, 可以从容处理更大的数据。</li></ol><p>到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。</p><p>在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。</p><p>在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。</p><p>最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。</p><p>有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。</p><p>综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/boosting2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://frankblog.site/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="boosting" scheme="http://frankblog.site/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>pyecharts链接mysql进行数据可视化</title>
    <link href="http://frankblog.site/2018/06/12/Pyecharts%E8%BF%9E%E6%8E%A5Mysql%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://frankblog.site/2018/06/12/Pyecharts连接Mysql进行可视化/</id>
    <published>2018-06-12T12:34:52.054Z</published>
    <updated>2018-06-12T12:42:33.911Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/echarts.png" alt=""></p><a id="more"></a><hr><h2 id="Pandas常用读取方式"><a href="#Pandas常用读取方式" class="headerlink" title="Pandas常用读取方式"></a>Pandas常用读取方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># 读取Mysql整个表为DataFrame：</span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">query = &quot;select * from table&quot;</span><br><span class="line">df = pd.read_sql(query,conn) # conn对象创建参考下文</span><br><span class="line"></span><br><span class="line">conn.close()</span><br><span class="line"># 读取json文件</span><br><span class="line">json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号</span><br><span class="line">js = pd.read_json(json_str)</span><br><span class="line"></span><br><span class="line"># 读取html</span><br><span class="line">url = &quot;http://quote.stockstar.com&quot;</span><br><span class="line">dfs = pd.read_html(url)</span><br><span class="line">dfs[0]</span><br><span class="line"></span><br><span class="line">dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定table</span><br><span class="line">dfs1[0]</span><br><span class="line"></span><br><span class="line"># 读取粘贴板文件称DataFrame</span><br><span class="line"></span><br><span class="line">df = pd.read_clipboard(sep=&apos;,&apos;, header=None)</span><br><span class="line"></span><br><span class="line"># 读取Mysql整个表为DataFrame：</span><br><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">query = &quot;select * from table&quot;</span><br><span class="line">df = pd.read_sql(query,conn) # conn对象创建参考下文</span><br><span class="line"></span><br><span class="line">conn.close()</span><br><span class="line"># 读取json文件</span><br><span class="line">json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号</span><br><span class="line">js = pd.read_json(json_str)</span><br><span class="line"></span><br><span class="line"># 读取html</span><br><span class="line">url = &quot;http://quote.stockstar.com&quot;</span><br><span class="line">dfs = pd.read_html(url)</span><br><span class="line">dfs[0]</span><br><span class="line"></span><br><span class="line">dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定table</span><br><span class="line">dfs1[0]</span><br><span class="line"></span><br><span class="line"># 读取粘贴板文件称DataFrame</span><br><span class="line"></span><br><span class="line">df = pd.read_clipboard(sep=&apos;,&apos;, header=None)</span><br></pre></td></tr></table></figure><h2 id="每日交易额汇总"><a href="#每日交易额汇总" class="headerlink" title="每日交易额汇总"></a>每日交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select day_date from day_sum order by day_date&quot;)</span><br><span class="line">tran_data = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select day_sum from day_sum order by day_date&quot;)</span><br><span class="line">transum = cursor.fetchall()</span><br><span class="line">tran_sum = [*map(lambda x :x[0]/100,list(transum))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br><span class="line">conn.colse()</span><br><span class="line"></span><br><span class="line">tran_add = [0]</span><br><span class="line">for i in range (len(tran_sum)):</span><br><span class="line"> if i &gt; 0:</span><br><span class="line"> tran_add.append((tran_sum[i] - tran_sum[i-1]) / tran_sum[i-1] * 100)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Bar, Line, Overlap</span><br><span class="line"></span><br><span class="line">attr1 = tran_data</span><br><span class="line">v1 = tran_sum</span><br><span class="line">v2 = tran_add</span><br><span class="line">bar1 = Bar(&quot;每日交易信息汇总&quot;)</span><br><span class="line">bar1.add(&quot;日期 /金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)</span><br><span class="line">line1 = Line()</span><br><span class="line">line1.add(&apos;环比增长率(%)&apos;,attr1, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True )</span><br><span class="line"></span><br><span class="line">line2 = Line()</span><br><span class="line">line2.add(&quot;日期/ 金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)</span><br><span class="line"></span><br><span class="line">overlap = Overlap()</span><br><span class="line">overlap.add(bar1)</span><br><span class="line"># overlap.add(bar2)</span><br><span class="line">overlap.add(line1)</span><br><span class="line">overlap.render()</span><br><span class="line">overlap</span><br><span class="line"></span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">#bar.render()</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts.png" alt="png"></a></p><h2 id="各出口交易金额"><a href="#各出口交易金额" class="headerlink" title="各出口交易金额"></a>各出口交易金额</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">#2016-04-29每个出口id交易额信息</span><br><span class="line">cursor.execute(&quot;select in_id from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)</span><br><span class="line">in_plazaid = cursor.fetchall()</span><br><span class="line">#2016-04-29每个出口交易额</span><br><span class="line">cursor.execute(&quot;select in_sum from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)</span><br><span class="line">in_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr2 = in_plazaid</span><br><span class="line">v2 = in_transum</span><br><span class="line">bar2 = Bar(&quot;2016-4-29日各出口交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar2.add(&quot;出口ID/金额（元）&quot;, attr2, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar2.render()</span><br><span class="line">bar2</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts1.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts1.png" alt="png"></a></p><h2 id="入出口交易金额"><a href="#入出口交易金额" class="headerlink" title="入出口交易金额"></a>入出口交易金额</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select ent_plazaid from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)</span><br><span class="line">ent_plazaid = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)</span><br><span class="line">ent_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr3 = ent_plazaid</span><br><span class="line">v3 = ent_transum</span><br><span class="line">bar3 = Bar(&quot;2016-4-29日各入口交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar3.add(&quot;入口ID/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar3.render()</span><br><span class="line">bar3</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts2.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts2.png" alt="png"></a></p><h2 id="某入口交易额汇总"><a href="#某入口交易额汇总" class="headerlink" title="某入口交易额汇总"></a>某入口交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select trans_date from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)</span><br><span class="line">ent_date = cursor.fetchall()</span><br><span class="line">cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)</span><br><span class="line">transum_ = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr3 = ent_date</span><br><span class="line">v3 = transum_</span><br><span class="line">bar3 = Bar(&quot;某出口每日交易额&quot;,&quot;&quot;)</span><br><span class="line">bar3.add(&quot;出口/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar3.render()</span><br><span class="line">bar3</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts3.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts3.png" alt="png"></a></p><h2 id="金额突增原因分析"><a href="#金额突增原因分析" class="headerlink" title="金额突增原因分析"></a>金额突增原因分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Funnel</span><br><span class="line"></span><br><span class="line">#根据2016-4-29出入口及各车型的交易额信息，每个组选取前4个贡献率最高的样本。</span><br><span class="line">#利用除均操作对这三种因素的影响率进行标准化。</span><br><span class="line">#下图可以看出2016-4-29日，这三种影响因素的前四个样本分别占改组贡献率的比例</span><br><span class="line">#100108红门主站出京入口、100861榆垡南出京出口、车型-1对该日的收费的贡献率最大</span><br><span class="line"></span><br><span class="line">attr = [&apos;100861榆垡南出京出&apos;,&apos;100158璃河南出京出&apos;,&apos;100109红门主站进京出&apos;,&apos;100135六里桥站进京出&apos;,&apos;100108红门主站出京入&apos;,&apos;100134六里桥站出京入&apos;,&apos;100862榆垡南进京入&apos;,&apos;100175西红门南桥出京入&apos;,&apos;车型-1&apos;,&apos;车型-4&apos;,&apos;车型-3&apos;,&apos;车型-2&apos;]</span><br><span class="line">value = [6.24, 6.01, 4.72, 4.71, 16.75, 15.52, 10.85, 8.08, 4.46, 0.29, 0.13, 0.1]</span><br><span class="line">funnel = Funnel(&quot;&quot;)</span><br><span class="line">funnel.add(&quot;因素&quot;, attr, value, is_label_show=True,</span><br><span class="line"> label_pos=&quot;inside&quot;, label_text_color=&quot;#fff&quot;)</span><br><span class="line">funnel.render()</span><br><span class="line">funnel</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts4.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts4.png" alt="png"></a></p><h2 id="各车型交易汇总"><a href="#各车型交易汇总" class="headerlink" title="各车型交易汇总"></a>各车型交易汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select distinct vehtype from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)</span><br><span class="line">type_id = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select distinct trans_sum from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)</span><br><span class="line">type_sum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]</span><br><span class="line">cursor.close()</span><br><span class="line">#========================</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">attr4 = type_id</span><br><span class="line">v4 = type_sum</span><br><span class="line">bar4 = Bar(&quot;2016-4-29日各车型交易额汇总&quot;,&quot;&quot;)</span><br><span class="line">bar4.add(&quot;车型/金额（元）&quot;, attr4, v4, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])</span><br><span class="line">bar4.render()</span><br><span class="line">bar4</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts5.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts5.png" alt="png"></a></p><h2 id="全日期各车型交易额汇总"><a href="#全日期各车型交易额汇总" class="headerlink" title="全日期各车型交易额汇总"></a>全日期各车型交易额汇总</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select * from tran_vehtype_sum&quot;)</span><br><span class="line">typedata = cursor.fetchall()</span><br><span class="line">cursor.close()</span><br><span class="line"></span><br><span class="line">typedata = pd.DataFrame(list(typedata))</span><br><span class="line">typedata.columns = [&apos;type&apos;,&apos;date&apos;,&apos;value&apos;]</span><br><span class="line"></span><br><span class="line">data=pd.pivot_table(typedata,index=&quot;date&quot;,columns=&quot;type&quot;,values=&quot;value&quot;,aggfunc=np.sum,fill_value=0)</span><br><span class="line"># data.to_csv(&quot;data.csv&quot;)</span><br><span class="line"></span><br><span class="line">from pyecharts import Bar</span><br><span class="line"></span><br><span class="line">y, x1, x2, x3, x4, x5 = [], [], [], [], [], []</span><br><span class="line">for i in range(len(data)):</span><br><span class="line"> y.append(data.index[i])</span><br><span class="line"> x1.append(data[1][i])</span><br><span class="line"> x2.append(data[2][i])</span><br><span class="line"> x3.append(data[3][i])</span><br><span class="line"> x4.append(data[4][i])</span><br><span class="line"> x5.append(data[5][i])</span><br><span class="line"></span><br><span class="line">bar = Bar(&quot;各车型每日交易额&quot;)</span><br><span class="line">bar.add(&quot;车型-1&quot;, y, x1, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-2&quot;, y, x2, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-3&quot;, y, x3, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-4&quot;, y, x4, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.add(&quot;车型-5&quot;, y, x5, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)</span><br><span class="line">bar.render()</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><p><a href="http://p4rlzrioq.bkt.clouddn.com/pyecharts6.png" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/pyecharts6.png" alt="png"></a></p><h2 id="PyMysql使用"><a href="#PyMysql使用" class="headerlink" title="PyMysql使用"></a>PyMysql使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">#创建连接</span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line"># 创建游标</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回收影响行数</span><br><span class="line">effect_row = cursor.execute(&quot;select * from tran_day_sum&quot;)</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回受影响行数</span><br><span class="line">#effect_row = cursor.execute(&quot;update tb7 set pass = &apos;123&apos; where nid = %s&quot;, (11,))</span><br><span class="line"></span><br><span class="line"># 执行SQL，并返回受影响行数,执行多次</span><br><span class="line">#effect_row = cursor.executemany(&quot;insert into tb7(user,pass,licnese)values(%s,%s,%s)&quot;, [(&quot;u1&quot;,&quot;u1pass&quot;,&quot;11111&quot;),(&quot;u2&quot;,&quot;u2pass&quot;,&quot;22222&quot;)])</span><br><span class="line"></span><br><span class="line"># 提交，不然无法保存新建或者修改的数据</span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"># 关闭游标</span><br><span class="line">cursor.close()</span><br><span class="line"># 关闭连接</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#分析每个入口的交易笔数和交易总额</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(&quot;select ent_plazaid from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_id = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select count(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_count = [*map(lambda x :x[0],list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;select sum(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)</span><br><span class="line">tran_sum = [*map(lambda x :int(x[0]),list(cursor.fetchall()))]</span><br><span class="line"></span><br><span class="line">cursor.close()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/echarts.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://frankblog.site/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="pyecharts" scheme="http://frankblog.site/tags/pyecharts/"/>
    
  </entry>
  
  <entry>
    <title>linux常用命令</title>
    <link href="http://frankblog.site/2018/06/11/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://frankblog.site/2018/06/11/linux常用命令/</id>
    <published>2018-06-11T06:01:21.977Z</published>
    <updated>2018-06-11T07:02:37.438Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/linux.jpg" alt=""></p><a id="more"></a><hr><p>日常Linux操作记录，用于查询不考虑编写顺序</p><p><code>[root@localhost ~]# root</code>：当前登录用户<br><code>localhost</code>：主机名<br><code>~</code>：当前所在目录（家目录）<br><code>#</code>：超级用户提示符 ==&gt; 家目录(/root)<br><code>$</code>：普通用户提示符 ==&gt; 家目录(/home/user01)<br>文件类型（- 文件；d 目录；l 快捷方式）</p><p>命令格式：<br>命令 [选项] [参数]<br>例：查询目录内容<br><code>ls</code> [选项] [文件或目录]<br><code>ls -a</code> 显示所有文件，包括隐藏文件(.开头)<br><code>ls -l</code> 显示详细信息(也可用ll)</p><p>下载器：<br>更新系统（不建议使用apt-get dist-upgrade，会导致文件不匹配卸载的情况）<br><code>apt-get update</code> &amp;&amp; <code>apt-get upgrade</code><br>下载安装文件<br><code>apt-get install</code> [文件名]<br>安装过程中存在依赖关系<br><code>apt-get install -f</code></p><p>知道网址下载方法： wget [粘贴网址]</p><p>不同类型文件解压：<br><code>tar -xvf file.tar</code>//解压 tar包<br><code>tar -xzvf file.tar.gz</code>//解压tar.gz<br><code>tar -xjvf file.tar.bz2</code> //解压 tar.bz2<br><code>tar -xZvf file.tar.Z</code>//解压tar.Z<br><code>unrar e file.rar</code> //解压rar<br><code>unzip file.zip</code>//解压zip</p><p>未完待续。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/linux.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="操作系统" scheme="http://frankblog.site/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://frankblog.site/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>浅谈推荐系统</title>
    <link href="http://frankblog.site/2018/06/10/%E6%B5%85%E8%B0%88%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://frankblog.site/2018/06/10/浅谈推荐系统/</id>
    <published>2018-06-10T13:03:49.318Z</published>
    <updated>2018-06-11T07:00:22.418Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p><h1 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h1><h2 id="1-为什么需要推荐系统"><a href="#1-为什么需要推荐系统" class="headerlink" title="1. 为什么需要推荐系统"></a>1. 为什么需要推荐系统</h2><p><strong>结论是，为了解决互联网时代下的信息超载问题。</strong></p><p>正如《大数据时代》中作者所言，这仅仅是一个开始，人们与世界的交流方式，从原来对因果关系的渴求，转变为现在对相关关系的发现和使用上。</p><h2 id="2-搜索引擎与推荐系统"><a href="#2-搜索引擎与推荐系统" class="headerlink" title="2. 搜索引擎与推荐系统"></a>2. 搜索引擎与推荐系统</h2><p>众所周知，解决信息过载问题，最有代表性的解决方案是【分类目录】和【搜索引擎】，这两种解决方案分别催生了互联网领域的两家著名公司—雅虎和谷歌。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%92%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.jpeg" alt=""></p><p>推荐系统，和搜索引擎一样，是一种帮助用户快速发展有用信息的工具。通过分析用户的历史行为，给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。</p><p>并且，推荐系统能够很好的发掘物品的长尾，挑战传统的2/8原则（80%的销售额来自20%的热门品牌）。</p><p>从技术角度来看，搜索引擎和推荐系统的区别在于：<br>1）搜索引擎，注重搜索结果之间的关系和排序；<br>2）推荐系统，需要研究用户的兴趣模型，利用社交网络的信息进行个性化的计算；<br>3）搜索引擎，由用户主导，需要输入关键词，自行选择结果。如果结果不满意，需要修改关键词，再次搜索；<br>4）推荐系统，由系统主导，根据用户的浏览顺序，引导用户发现自己感兴趣的信息；</p><h1 id="推荐算法详述"><a href="#推荐算法详述" class="headerlink" title="推荐算法详述"></a>推荐算法详述</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p><h2 id="1-基于内容的推荐"><a href="#1-基于内容的推荐" class="headerlink" title="1 基于内容的推荐"></a>1 基于内容的推荐</h2><p>原理是基于用户感兴趣的物品A，找到和A内容信息相近的物品B。<br>利用用户和物品本身的内容特征，如用户的地理位置、性别、年龄，电影物品的导演、演员、发布时间等。<br>所以提取推荐对象的特征，是内容推荐算法的关键。但是对于多媒体内容，如视频、音乐，很难找到它们之间的特性关联性。</p><p><strong>基于内容的推荐的优点如下：</strong><br>(1) 简单、有效，推荐结果直观，容易理解，不需要领域知识。<br>(2) 不需要用户的历史数据，如对对象的评价等。<br>(3) 没有关于新推荐对象出现的冷启动问题。<br>(4) 没有稀疏问题。<br>(5) 算法成熟，如数据挖掘、聚类分析等。</p><p><strong>基于内容的推荐的缺点如下：</strong><br>(1) 受到了推荐对象特征提取能力的限制。<br>比如图像、视频，没有有效的特征提取方法。即便是文本资源，特征提取也只能反应一部分内容，难以提取内容质量，会影响用户满意度。<br>(2) 很难出现新的推荐结果。<br>根据用户兴趣的喜好进行推荐，很难出现惊喜。对于时间敏感的内容，如新闻，推荐内容基本相同，体验度较差。<br>(3)存在新用户出现时的冷启动问题。<br>当新用户出现时， 系统较难获得该用户的兴趣偏好，无法进行有效推荐。<br>(4) 推荐对象内容分类方法需要的数据量较大。</p><h2 id="2-协同过滤算法"><a href="#2-协同过滤算法" class="headerlink" title="2 协同过滤算法"></a>2 协同过滤算法</h2><p>仅仅基于用户行为数据设计的推荐算法，称为协同过滤算法。此方法主要根据用户对物品的历史行为，寻找用户或物品的近邻集合，以此计算用户对物品的偏好。</p><h3 id="基于用户的协同过滤算法（UserCF）"><a href="#基于用户的协同过滤算法（UserCF）" class="headerlink" title="基于用户的协同过滤算法（UserCF）"></a>基于用户的协同过滤算法（UserCF）</h3><p>算法的关键是计算两个用户的兴趣相似度。协同过滤计算用户兴趣相似度是利用用户行为的相似度。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A41.jpeg" alt=""></p><p>关键在于计算用户与用户之间的兴趣相似度。</p><ul><li>计算用户相似度的方法有3种：<ul><li>余弦相似性</li><li>相关相似性（皮尔森系数相关）</li><li>修正的余弦相似性<br>这里主要使用余弦相似度来计算：</li></ul></li></ul><script type="math/tex; mode=display">w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|| N(v)|}}</script><p>\(w_{uv}\)代表用户 u 与 v 之间的兴趣相似度，N(u)表示用户 u 曾经喜欢过的物品集合, N(v) 表示用户 v 曾经喜欢过的物品集合。</p><p>根据上述核心思想，可以有如下算法步骤：</p><ol><li>建立物品-用户的倒排表</li><li>用户与用户之间的共现矩阵 C[u][v]，表示用户u与v喜欢相同物品的个数</li><li>用户与用户之间的相似度矩阵 W[u][v]，根据上述相似度计算公式计算。</li><li>用上面的相似度矩阵来给用户推荐和他兴趣相似的用户喜欢的物品。用户 u 对物品 i 的兴趣程度可以估计为</li></ol><p><a href="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A42.gif" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A42.gif" alt=""></a></p><p>S(u,K) 为和用户 u 兴趣最接近的 K 个用户， N(i) 为对物品 i 有正反馈的用户集合， W[u][v] 为用户 u 和用户 v 的兴趣相似度，rvi 为用户 v 对物品 i 的兴趣。</p><h3 id="基于物品的协同过滤算法（ItemCF）"><a href="#基于物品的协同过滤算法（ItemCF）" class="headerlink" title="基于物品的协同过滤算法（ItemCF）"></a>基于物品的协同过滤算法（ItemCF）</h3><p>这种算法给用户推荐和他之前喜欢的物品相似的物品。<br>该算法是目前业界应用最多的算法，如亚马逊、Netflix、YouTube，都是以该算法为基础。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A43.gif" alt=""></p><p>物品之间的相似度可以使用如下公式计算：</p><script type="math/tex; mode=display">w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)|| N(j)|}}</script><p>从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。</p><ul><li>用户活跃度对物品相似度的影响<br>IUF（Inverse User Frequence），用户活跃度对数的倒数的参数。<br>论文提出的观点是，活跃用户对物品相似度的贡献应该小于不活跃的用户。用IUF修正物品相似度的计算。</li><li>物品相似度的归一化<br>研究表明，将ItemCF的相似度矩阵按最大值归一，可以提高推荐的准确率。</li></ul><p>根据上述核心思想，可以有如下算法步骤：</p><ol><li>建立用户-物品的倒排表</li><li>物品与物品之间的共现矩阵 C[i][j]，表示物品 i 与 j 共同被多少用户所喜欢。</li><li>用户与用户之间的相似度矩阵 W[i][j] ， 根据上述相似度计算公式计算。</li><li>用上面的相似度矩阵来给用户推荐与他所喜欢的物品相似的其他物品。用户 u 对物品 j 的兴趣程度可以估计为</li></ol><p><a href="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4.gif" target="_blank" rel="noopener"><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A44.gif" alt=""></a></p><p>S(j,K) 为和物品 j 最相似的前 K 个物品， N(u) 为对用户 u 所喜欢的物品集合， W[j][i] 为物品 j 和物品 i 之间的相似度， rui 为用户 u 对物品 i 的兴趣。</p><p>ItemCF 与 基于内容的推荐算法的区别</p><ul><li>基于内容的推荐算法，计算的是物品内容属性之间的相似度。如，电影的导演是不是同一个人；</li><li>ItemCF是通过用户的行为计算物品之间的相似度。如，物品A、B具有很大相似度，是因为喜欢物品A的用户也大都喜欢物品B。</li></ul><ul><li>UserCF 与 ItemCF 的优缺点</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A45.jpeg" alt=""></p><h2 id="3-基于关联规则的推荐"><a href="#3-基于关联规则的推荐" class="headerlink" title="3 基于关联规则的推荐"></a>3 基于关联规则的推荐</h2><p>关联规则分析中的关键概念包括：<strong>支持度(Support)、置信度(Confidence)与提升度(Lift)</strong>。首先，我们简单温故下这3个关键指标~</p><p><strong>1、支持度 (Support)：</strong>支持度是两件商品（A∩B）在总销售笔数(N)中出现的概率，<strong>即A与B同时被购买的概率。</strong>类似于中学学的交集，需要原始同时满足条件。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99.jpg" alt=""></p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%991.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>比如某超市2016年有100w笔销售，顾客购买可乐又购买薯片有20w笔，顾客购买可乐又购买面包有10w笔，那可乐和薯片的关联规则的支持度是20%，可乐和面包的支持度是10%。</p><p><strong>2、置信度 (Confidence)：置信度是购买A后再购买B的条件概率。</strong>简单来说就是交集部分C在A中比例，如果比例大说明购买A的客户很大期望会购买B商品。</p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%992.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>某超市2016年可乐购买次数40w笔，购买可乐又购买了薯片是30w笔，顾客购买可乐又购买面包有10w笔，则购买可乐又会购买薯片的置信度是75%，购买可乐又购买面包的置信度是25%，这说明买可乐也会买薯片的关联性比面包强，营销上可以做一些组合策略销售。</p><p><strong>3、提升度 (Lift)：提升度表示先购买A对购买B的概率的提升作用</strong>，用来判断规则是否有实际价值，即使用规则后商品在购物车中出现的次数是否高于商品单独出现在购物车中的频率。如果大于1说明规则有效，小于1则无效。</p><p><strong>公式：</strong></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%993.jpg" alt=""></p><p><strong>例子说明：</strong></p><p>可乐和薯片的关联规则的支持度是20%，购买可乐的支持度是3%，购买薯片的支持度是5%，则提升度是1.33&gt;1, A-B规则对于商品B有提升效果。</p><h2 id="4-隐语义模型"><a href="#4-隐语义模型" class="headerlink" title="4 隐语义模型"></a>4 隐语义模型</h2><p>待学习</p><h1 id="推荐系统评测"><a href="#推荐系统评测" class="headerlink" title="推荐系统评测"></a>推荐系统评测</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E8%AF%84%E6%B5%8B.png" alt="link"></p><h2 id="1-实验方法"><a href="#1-实验方法" class="headerlink" title="1. 实验方法"></a>1. 实验方法</h2><p>获得评测指标的实验方法，通常分3种：</p><ul><li>离线实验（offline experiment）</li><li>用户调查（user study）</li><li>在线实验（online experiment）</li></ul><p>我们分别介绍3种实验方法的优缺点。</p><h3 id="1）离线实验"><a href="#1）离线实验" class="headerlink" title="1）离线实验"></a>1）离线实验</h3><p>离线实验的方法的步骤如下：<br>a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；<br>b）将数据集按照一定的规则分成训练集和测试集；<br>c）在训练集上训练用户兴趣模型，在测试集上进行预测；<br>d）通过事先定义的离线指标，评测算法在测试集上的预测结果。</p><p>从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。</p><p><strong>离线实验的优点是：</strong></p><ul><li>不需要有对实际系统的控制权；</li><li>不需要用户参与实践；</li><li>速度快，可以测试大量算法；</li></ul><p><strong>缺点是：</strong></p><ul><li>数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果；</li><li>评价结果的客观性，无法得到用户主观性的评价；</li><li>难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系；</li></ul><h3 id="2）用户调查"><a href="#2）用户调查" class="headerlink" title="2）用户调查"></a>2）用户调查</h3><p>用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。</p><p>最后，我们通过分析他们的行为和答案，了解测试系统的性能。</p><p><strong>用户调查的优点是：</strong></p><ul><li>可以获得用户主观感受的指标，出错后容易弥补；</li></ul><p><strong>缺点是：</strong></p><ul><li>招募测试用户代价较大；</li><li>无法组织大规模的测试用户，统计意义不足；</li></ul><h3 id="3）在线实验"><a href="#3）在线实验" class="headerlink" title="3）在线实验"></a>3）在线实验</h3><p>在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。</p><p>在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。</p><p>它的核心思想是:<br>a) 多个方案并行测试;<br>b) 每个方案只有一个变量不同;<br>c) 以某种规则优胜劣汰。</p><p>其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。<br>对于推荐系统的评价中，唯一变量就是—推荐算法。</p><blockquote><p>有个很棒的网站，<a href="https://link.jianshu.com/?t=http%3A%2F%2Fwww.abtests.com" target="_blank" rel="noopener">http://www.abtests.com</a>，里面有很多通过实际AB测试提高网站用户满意度的例子。</p></blockquote><p><strong>AB测试的优点是：</strong></p><ul><li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标；</li></ul><p><strong>缺点是：</strong></p><ul><li>周期较长，必须进行长期的实验才能得到可靠的结果；</li></ul><p>大型网站做AB测试，可能会因为不同团队同时进行各种测试对结果造成干扰，所以切分流量是AB测试中的关键。</p><h3 id="4）总结"><a href="#4）总结" class="headerlink" title="4）总结"></a>4）总结</h3><p>一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。</p><ul><li>首先，通过离线实验证明它在很多离线指标上优于现有的算法；</li><li>其次，通过用户调查确定用户满意度不低于现有的算法；</li><li>最后，通过在线AB测试确定它在我们关心的指标上优于现有的算法；</li></ul><h2 id="2-评测指标"><a href="#2-评测指标" class="headerlink" title="2. 评测指标"></a>2. 评测指标</h2><p>评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。</p><h3 id="1）用户满意度"><a href="#1）用户满意度" class="headerlink" title="1）用户满意度"></a>1）用户满意度</h3><p>用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。</p><p>调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。</p><p>在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。</p><p>一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。</p><h3 id="2）预测准确度"><a href="#2）预测准确度" class="headerlink" title="2）预测准确度"></a>2）预测准确度</h3><p>预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。</p><p>大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。</p><p>由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。</p><h2 id="a）预测评分准确度"><a href="#a）预测评分准确度" class="headerlink" title="a）预测评分准确度"></a>a）预测评分准确度</h2><p>预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。<br>这针对于一些需要用户给物品评分的网站。</p><p>预测评分的准确度指标，一般通过以下指标计算：</p><ul><li>平均绝对误差（MAE）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E6%B5%8B%E5%87%86%E7%A1%AE%E5%BA%A6png.png" alt=""></p><p>MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。</p><p>所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。</p><ul><li>均方根误差（RMSE）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E6%B5%8B%E5%87%86%E7%A1%AE%E5%BA%A62.png" alt=""></p><p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。<br>研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。</p><h2 id="b）TopN推荐"><a href="#b）TopN推荐" class="headerlink" title="b）TopN推荐"></a>b）TopN推荐</h2><p>网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。</p><p>TopN推荐的预测准确率，一般通过2个指标度量：</p><ul><li>准确率（precision）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/topN.jpeg" alt=""></p><ul><li>召回率（recall）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/topN1.jpeg" alt=""></p><p>R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。</p><p>TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。</p><h3 id="3）覆盖率"><a href="#3）覆盖率" class="headerlink" title="3）覆盖率"></a>3）覆盖率</h3><p>覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。<br>最简单的定义是，推荐系统推荐出来的物品占总物品的比例。</p><p>假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为：</p><p><img src="https://upload-images.jianshu.io/upload_images/610388-dc450d844bcc84be.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/229" alt=""></p><p>覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。</p><p>除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。</p><p>如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。</p><p>信息论和经济学中有两个著名指标，可以定义覆盖率：</p><ul><li>信息熵</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B5.jpeg" alt=""></p><p>p(i)是物品i的流行度除以所有物品流行度之和。</p><ul><li>基尼系数（Gini Index）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%88Gini%20Index%EF%BC%89.jpeg" alt=""></p><p>p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%88Gini%20Index%EF%BC%892.jpeg" alt=""></p><ul><li>评测马太效应</li></ul><p>马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。</p><p>但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。</p><p>如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。</p><h3 id="4）多样性"><a href="#4）多样性" class="headerlink" title="4）多样性"></a>4）多样性</h3><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。</p><p>多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E6%A0%B7%E6%80%A71.jpeg" alt=""></p><p>推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E6%A0%B7%E6%80%A72.jpeg" alt=""></p><h3 id="5）新颖性"><a href="#5）新颖性" class="headerlink" title="5）新颖性"></a>5）新颖性</h3><p>新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。</p><p>评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。</p><p>此计算比较粗糙，需要配合用户调查准确统计新颖度。</p><h3 id="6）惊喜度"><a href="#6）惊喜度" class="headerlink" title="6）惊喜度"></a>6）惊喜度</h3><p>推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。</p><p>目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。</p><h3 id="7）信任度"><a href="#7）信任度" class="headerlink" title="7）信任度"></a>7）信任度</h3><p>如果用户信任推荐系统，就会增加用户和推荐系统的交互。</p><p><strong>提高信任度的方式有两种：</strong></p><ul><li><p>增加系统透明度<br>提供推荐解释，让用户了解推荐系统的运行机制。</p></li><li><p>利用社交网络，通过好友信息给用户做推荐<br>通过好友进行推荐解释</p></li></ul><p>度量信任度的方式，只能通过问卷调查。</p><h3 id="8）实时性"><a href="#8）实时性" class="headerlink" title="8）实时性"></a>8）实时性</h3><p>推荐系统的实时性，包括两方面：</p><ul><li>实时更新推荐列表满足用户新的行为变化；</li><li>将新加入系统的物品推荐给用户；</li></ul><h3 id="9）健壮性"><a href="#9）健壮性" class="headerlink" title="9）健壮性"></a>9）健壮性</h3><p>任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。</p><p>健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。</p><blockquote><p>2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。<br>就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。</p></blockquote><p><strong>评测算法的健壮性，主要利用模拟攻击：</strong></p><p>a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；<br>b）用常用的攻击方法向数据集中注入噪声数据；<br>c）利用算法在有噪声的数据集上再次生成推荐列表；<br>d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。</p><p><strong>提高系统健壮性的方法：</strong></p><ul><li>选择健壮性高的算法；</li><li>选择代价较高的用户行为，如购买行为比浏览行为代价高；</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h2 id="3-评测维度"><a href="#3-评测维度" class="headerlink" title="3. 评测维度"></a>3. 评测维度</h2><p>增加评测维度的目的，就是知道一个算法在什么情况下性能最好。</p><p>一般评测维度分3种：</p><ul><li>用户维度<br>主要包括用户的人口统计学信息、活跃度以及是不是新用户等；</li><li>物品维度<br>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等；</li><li>时间维度<br>包括季节，是工作日还是周末，白天还是晚上等；</li></ul><p>如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。</p><h1 id="冷启动问题"><a href="#冷启动问题" class="headerlink" title="冷启动问题"></a>冷启动问题</h1><p><strong>1 解决方案：</strong></p><ul><li>提供非个性化推荐，如热门排行。等有了数据之后再推荐。</li><li>利用用户注册信息，做粗粒度的个性化。</li><li>利用用户的社交网络账号，导入用户的好友，推荐好友喜欢的物品。</li><li>用户初次登录时，对一些物品进行反馈，根据这些信息做个性化。</li><li>对于新上线的物品，利用内容信息，推荐给喜欢类似物品的用户。</li><li>系统冷启动，可以引入外部资源，如专家知识，建立起物品的相关度。</li></ul><p><strong>2 冷启动，启动用户兴趣的物品需要具有以下特点：</strong></p><ul><li>比较热门</li><li>具有代表性和区分性</li><li>启动物品集合需要有多样性</li></ul><p><strong>3 选择启动物品集合的系统</strong></p><p>如何设计一个选择启动物品集合的系统？Nadav Golbandi在论文中提出用一个决策树解决。</p><p>首先，给定一群用户，用这群用户对物品评分的方差度量这群用户兴趣的一致程度。如果方差很小，说明这一群用户的兴趣不太一致，也就是物品具有比较大的区分度，反之则说明这群用户的兴趣比较一致。</p><p>再根据用户的评分方差计算物品的区分度。</p><p>也就是说，对于物品i，将用户分为3类—喜欢物品i的用户，不喜欢物品i的用户和不知道物品i的用户。如果这3类用户集合内的用户对其他的物品兴趣很不一致，说明物品i具有较高的区分度。</p><p>算法首先从所有用户中找到具有最高区分度的物品i，然后将用户分成3类。然后在每类用户中再找到最具区分度的物品，然后将每一类用户又各自分为3类，也就是将总用户分为9类，然后继续这样下去，最终可以通过对一系列物品的看法将用户进行分类。</p><p>在冷启动时，从根节点开始询问用户对该节点物品的看法，然后根据用户的选择将用户放到不同的分枝，直到进入最后的叶子节点，此时对用户的兴趣有了比较清楚的了解，从而可以开始对用户进行比较准确地个性化推荐。</p><p><strong>4 利用物品的内容信息</strong></p><p>就是基于内容的推荐，很适合解决物品冷启动问题。</p><p>物品冷启动对诸如新闻网站等时效性很强的网站的推荐非常重要，因为那些网站中时时刻刻都有新加入的物品，而且每个物品必须能够在第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。</p><p>一般来说，物品的内容可以通过向量空间模型表示，该模型会将物品表示成一个关键词向量。</p><p>如果物品的内容是诸如导演、演员等实体，可以直接将实体作为关键词。<br>如果内容是文本，需要引入自然语言的技术抽取关键词。如何建立文章、话题和关键词的关系是话题模型研究的重点，代表性的话题模型有LDA。</p><p>最后推荐几篇博文</p><ul><li>基于内容和用户画像的推荐：此种算法，可见之前的一篇文章：<a href="http://www.rowkey.me/blog/2016/04/07/up-recommend/" target="_blank" rel="noopener">http://www.rowkey.me/blog/2016/04/07/up-recommend/</a>。</li><li>基于矩阵分解的推荐: 基于SVD/ALS算法对用户进行内容推荐。相比起SVD，ALS更加适合解决稀疏矩阵的问题。Spark mlib中已经集成了对als算法的实现，需要做的就是在etl-1中把数据转换为als需要的数据格式以及调整als算法的各种参数。这里有一篇文章比较具体地描述了如何使用spark来做基于ALS的推荐：<a href="http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/" target="_blank" rel="noopener">http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/</a>。</li><li>用户&amp;物品协同过滤推荐：包括UserBased CF和ItemBased CF。对于这两者，需要根据业务的不同来选择不同的算法。当用户非常多的时候，考虑到维护用户矩阵的成本，一般是不推荐选择用户协同过滤的，而对于候选item很多的时候，则不推荐使用物品协同过滤</li></ul><p>参考文献：<br>项亮《推荐系统实践》<br>参考论文：<br><a href="http://t.cn/RjXktmC" target="_blank" rel="noopener">http://t.cn/RjXktmC</a><br><a href="http://t.cn/RjXkiFP" target="_blank" rel="noopener">http://t.cn/RjXkiFP</a><br><a href="http://blog.csdn.net/qingqingpiaoguo/article/details/60882309" target="_blank" rel="noopener">http://blog.csdn.net/qingqingpiaoguo/article/details/60882309</a> <a href="https://www.zhihu.com/question/27141495/answer/161027882" target="_blank" rel="noopener">https://www.zhihu.com/question/27141495/answer/161027882</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://frankblog.site/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>浅谈分布式</title>
    <link href="http://frankblog.site/2018/06/07/%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://frankblog.site/2018/06/07/浅谈分布式/</id>
    <published>2018-06-07T12:15:05.369Z</published>
    <updated>2018-06-08T02:01:56.248Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="分布式基础"><a href="#分布式基础" class="headerlink" title="分布式基础"></a>分布式基础</h2><p>分布式，按字面意思就是并行计算，对于一些串行的计算可以运用。一些本来就并行的就不可以。</p><p>之前聊起这个，就会说mapreduce，就是把任务分拆开来，最后计算的时候再合并。但后来发现这样的回答太水了，看了几篇介绍分布式计算的PPT，特此写下来分布式到底是怎么一回事。</p><p>我们用一个矩阵运算作为例子：</p><p>对于两个矩阵，A和B：</p><p>A = \(\begin{pmatrix}1 &amp; 2 &amp; 3\\\\  4 &amp; 5 &amp;0 \\\\  7 &amp; 8&amp;9\\\\  10 &amp; 11&amp;12\end{pmatrix}\)</p><p>B =  \(\begin{pmatrix}10 &amp; 15\\\\  0 &amp;2 \\\\ 11 &amp;9\end{pmatrix}\)</p><p>有</p><p>C = AB = \(\begin{pmatrix}43 &amp; 46\\\\  40 &amp;70 \\\\ 169 &amp;202\\\\ 232 &amp;280\end{pmatrix}\)</p><p>我们要改变矩阵存储方式，只存储那些非零的数值。</p><p>所以矩阵A可以表示成</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/A%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>其中，每一行第一个字段为行标签i，第二个字段为列标签j，第三个字段是A[i,j],即矩阵对应的值。</p><p>同理，对于矩阵B，可以变换为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/B%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>下图中，对于矩阵A，因为矩阵B一共两列，所以p=2，所以key会有（1，1）和（1，2）。而对于每个key，每一行中每一个位置的数据都得变成value，即value=”a:j,aij”</p><p>同理，对于矩阵B，k就在key的前面，因为是每一列拿出来做相乘。Value中也变成按行遍历。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce%201_%E7%9C%8B%E5%9B%BE%E7%8E%8B_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><p>在shuffle阶段，就把相同的key放在一起<br>在reduce阶段，在同一个key中，value第一维相同的匹配一起，其第二维相乘，最后把所有相加（如key（1，1）中，\(1<em>10+3</em>11\)，因为a:2没有匹配的，所以去掉a:2）</p><p>最后把每一个key相加就可以得到结果了。</p><center>  <img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce3.png" width="120"> </center><p>整体的流程实现如下图：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/MapReduceWordCountOverview1.png" alt=""></p><h2 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a><strong>Python代码实现</strong></h2><p>对于wordcount，如果是串行的话，等于是对于每个word进行一次循环。而并行的话可以多个word一起遍历。</p><p>map代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import sys </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    word_list = line.strip().split(&apos; &apos;)    </span><br><span class="line">    for word in word_list:</span><br><span class="line">        print &apos;\t&apos;.join([word.strip(), str(1)])</span><br></pre></td></tr></table></figure><p>reduce代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">cur_word = None</span><br><span class="line">sum = 0    </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    ss = line.strip().split(&apos;\t&apos;)        </span><br><span class="line">    if len(ss) &lt; 2:</span><br><span class="line">        continue    </span><br><span class="line">    word = ss[0].strip()</span><br><span class="line">    count = ss[1].strip()    </span><br><span class="line">    if cur_word == None:</span><br><span class="line">        cur_word = word    </span><br><span class="line">    if cur_word != word:</span><br><span class="line">        print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">        cur_word = word</span><br><span class="line">        sum = 0        </span><br><span class="line">    sum += int(count)    </span><br><span class="line">print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">sum = 0</span><br></pre></td></tr></table></figure><p>然后在linux上运行以下代码：</p><p>src.txt为自己随便找的文本文件txt格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat src.txt | python map.py | sort -k 1 | python reduce.py</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/python%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.jpg" alt=""></p><p>还算比较简单的，就不过多解释了。本意就是map中算出每一行中的word，然后传到reduce中处理。</p><h2 id="实战中怎么运用并行化处理"><a href="#实战中怎么运用并行化处理" class="headerlink" title="实战中怎么运用并行化处理"></a><strong>实战中怎么运用并行化处理</strong></h2><p>Python中有几个package可以做并行，如joblib，pp，multiprocessing等，</p><p>此处仅介绍joblib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from joblib import Parallel, delayed</span><br><span class="line">from math import sqrt</span><br><span class="line">Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))</span><br></pre></td></tr></table></figure><p>得到：</p><p>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</p><p>本质就是你要定义好一个子函数，这个子函数是并行任务中每个任务的输出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="分布式" scheme="http://frankblog.site/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="mapreduce" scheme="http://frankblog.site/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之聚类</title>
    <link href="http://frankblog.site/2018/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/"/>
    <id>http://frankblog.site/2018/06/07/机器学习之聚类/</id>
    <published>2018-06-07T06:48:23.561Z</published>
    <updated>2018-06-08T03:50:40.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>无监督学习方法在现实场景中还是有很多应用的，例如在金融和计算广告等反欺诈场景中，我们是不可能能够获取大量的标签数据的，因为欺诈用户不会告诉你他是欺诈用户的。这时候，如果想要利用机器学习方法检测出他们来，只能使用无监督方法。</p><h1 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>下面具体看到该算法的步骤：</p><p>（1）根据设定的聚类数 K，随机地选择 K 个聚类中心（Cluster Centroid）</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>（2）评估各个样本到聚类中心的距离，如果样本距离第 i 个聚类中心更近，则认为其属于第 ii 簇</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%92%E5%BD%92%E5%88%B0%E7%B0%87.png" alt=""></p><p>（3）计算每个簇中样本的<strong>平均（Mean）</strong>位置，将聚类中心移动至该位置</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%A7%BB%E5%8A%A8%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>重复以上步骤直至各个聚类中心的位置不再发生改变。</p><h2 id="变量标准化"><a href="#变量标准化" class="headerlink" title=".变量标准化"></a>.变量标准化</h2><p>在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。</p><p>常用的方法有：</p><p><strong>正态标准化</strong>：\(x_i=\frac {x_i-mean(X)}{std(X}\)<br><strong>归一化</strong>：\(x_i=\frac {x_i-min(X)}{max(X)-min(X)}\)</p><h2 id="如何确定聚类数"><a href="#如何确定聚类数" class="headerlink" title="如何确定聚类数"></a>如何确定聚类数</h2><p>实际上，一开始是很难确定聚类数的，下图的两种聚类数似乎都是可行的：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B01.png" alt=""><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B02.png" alt=""></p><p>但是，也存在一种称之为<strong>肘部法则（Elbow Method）</strong>的方法来选定适当的K值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png" alt=""></p><p>上图曲线类似于人的手肘，“肘关节”部分对应的 K 值就是最恰当的 K值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B2%A1%E6%9C%89%E8%82%98.png" alt=""></p><p>一般来说，K-Means 得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择 K 值。在下面这个例子中，假定我们的衣服想要是分为 S,M,L 三个尺码，就设定 K=3，如果我们想要 XS、S、M、L、XL 5 个衣服的尺码，就设定 K=5：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%8D%E9%A5%B0%E5%A4%A7%E5%B0%8F.png" alt=""></p><h2 id="质心的目标函数"><a href="#质心的目标函数" class="headerlink" title="质心的目标函数"></a>质心的目标函数</h2><p>聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性</p><p>常见的邻近度、质心和目标函数组合</p><div class="table-container"><table><thead><tr><th>邻近度函数</th><th>质心</th><th>目标函数</th></tr></thead><tbody><tr><td>曼哈顿距离</td><td>中位数</td><td>最小化对象与质心的绝对误差和SAE</td></tr><tr><td>平方欧几里得距离</td><td>均值</td><td>最小化对象与质心的误差平方和SSE</td></tr><tr><td>余弦</td><td>均值</td><td>最大化对象与质心的余弦相似度和</td></tr><tr><td>Bregman散度</td><td>均值</td><td>最小化对象到质心的Bregman散度和</td></tr></tbody></table></div><p><code>Bregman散度</code>实际上是一类紧邻性度量，包括平方欧几里得距离。<code>Bregman散度函数</code>的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。</p><h2 id="K-means优缺点"><a href="#K-means优缺点" class="headerlink" title="K-means优缺点"></a>K-means优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>简单并且可以用于各种数据类型；</li><li>具备适合的空间复杂度和计算负责度，适用于大样本数据；</li><li>K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1. 属于“硬聚类” ， 每个样本只能有一个类别。 其他的一些聚类方法(GMM或者模糊K-means允许“软聚类”)。<br>2. K-means对异常点的“免疫力” 很差， 我们可以通过一些调整，比如中心不直接取均值， 而是找均值最近的样本点代替——k-medoids算法。<br>3. 对于团状的数据点集区分度好， 对于带状(环绕)等“非凸”形状不太好。 (用谱聚类或者做特征映射)</p><h1 id="密度聚类方法"><a href="#密度聚类方法" class="headerlink" title="密度聚类方法"></a>密度聚类方法</h1><h2 id="基本名词"><a href="#基本名词" class="headerlink" title="基本名词"></a>基本名词</h2><ul><li><strong>核心点</strong>： 如果该点满足给定的邻域内（半径为EpsEps的范围内）的点的个数超过给定的阈值MinptsMinpts，则该点为满足该条件下的核心点。</li><li><strong>边界点</strong>： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。</li><li><p><strong>噪声点</strong>： 噪声点既非核心点，也不是边界点</p></li><li><p><strong>ϵ 邻域</strong></p></li></ul><p>对于样本集 D 中样本点 xi，它的 ϵ 邻域定义为与 xi 距离不大于 ϵ 的样本的集合，即 Nϵ(xi)={x∈D|dist(x,xi)≤ϵ}；</p><ul><li><strong>核心对象</strong></li></ul><p>如果样本 x 的 ϵ 邻域内至少包含 mps 个样本，即 |Nϵ(xi)|≥mps，则称 x 为核心对象；</p><ul><li><strong>密度直达</strong></li></ul><p>如果 xi 是一个核心对象，并且 xj 位于它的 ϵ邻域内，那么我们称 xj 由 xi 密度直达；</p><ul><li><strong>密度可达</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在这样的样本序列 p1,p2,⋯,pn，其中 p1=xi,pn=xj且\(p_{i+1}\)由pi密度直达，则称 xi 与 xj 密度可达；</p><ul><li><strong>密度相连</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在第三个样本点 xk 使得 xi与 xj均由 xk密度可达，则称 xi与 xj密度相连。</p><h2 id="DBSCAN算法步骤"><a href="#DBSCAN算法步骤" class="headerlink" title="DBSCAN算法步骤"></a>DBSCAN算法步骤</h2><blockquote><p><1> 将所有点标记为核心点、边界点或噪音点</1></p><p><2> 删除噪音点</2></p><p><3> 为距离在 EpsEps 之内的所有核心点之间赋予一条边</3></p><p><4> 每组联通的核心点形成一个簇</4></p><p><5> 将每个边界点指派到一个与之关联的核心点的簇中</5></p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB.jpg" alt="link"></p><h2 id="选择-DBSCAN-参数"><a href="#选择-DBSCAN-参数" class="headerlink" title="选择 DBSCAN 参数"></a>选择 DBSCAN 参数</h2><ul><li><strong>k−距离</strong>：如何选择 Eps 和 MinPts 参数，基本的方法是观察点到它的第 k 个最近邻的距离。<br>考虑下，如何 k不大于簇个数的话， k−距离相对较小，反之对于不在簇中的点（噪音点或异常值）则k距离较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 k (一般为4)，计算所有点的k−距离，并递增排序，画出曲线图，则我们会看到k−距离的变化，并依照此图选择出合适的 MinPts参数，即对应拐点的位置。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB2.jpg" alt=""></p><h2 id="优点和缺点"><a href="#优点和缺点" class="headerlink" title="优点和缺点"></a>优点和缺点</h2><ul><li>对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。</li><li>当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。</li></ul><h1 id="层次聚类方法"><a href="#层次聚类方法" class="headerlink" title="层次聚类方法"></a>层次聚类方法</h1><p>有两种产生层次聚类的基本方法：</p><ul><li>凝聚型： 从点作为个体簇开始，每一步合并两个最接近的簇</li><li>分裂型： 从包含所有的点某个簇开始，每一步分裂一个簇，直到成为单点簇<br>到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。</li></ul><h2 id="簇之间的临近性"><a href="#簇之间的临近性" class="headerlink" title="簇之间的临近性"></a>簇之间的临近性</h2><ul><li><strong>MIN</strong>：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做<strong>单链（sigle link）</strong></li><li><strong>MAX</strong>：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做<strong>全链（complete link）</strong></li><li><strong>组平均</strong>：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。</li></ul><p><img src="https://img-blog.csdn.net/20160624104427613" alt="link"></p><h2 id="层次聚类的主要问题"><a href="#层次聚类的主要问题" class="headerlink" title="层次聚类的主要问题"></a>层次聚类的主要问题</h2><h3 id="处理不同大小簇的能力"><a href="#处理不同大小簇的能力" class="headerlink" title="处理不同大小簇的能力"></a>处理不同大小簇的能力</h3><p>对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法：</p><ul><li><strong>非加权</strong>：平等的对待所有簇，赋予不同大小的簇中点不同的权值</li><li><strong>加权</strong>： 赋予不同大小簇中点相同的权值</li></ul><h3 id="合并不可逆"><a href="#合并不可逆" class="headerlink" title="合并不可逆"></a>合并不可逆</h3><p>对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。</p><p>有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。</p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>众所周知，对于有监督学习方法的分类预测结果，我们有很多种不同的评价指标来度量分类效果的好坏。例如，召回率、精准率、准确率、F1-Score 以及 AUC 值等等。但是，由于无监督学习方法与有监督学习不同，绝大多数情况下，我们根本不知道它的真实类别标签，所以我们不可能完全依据有监督学习方法的评价指标来度量聚类算法。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.jpg" alt=""></p><p>无监督聚类算法的评价指标大致可以分为两大类：一类是聚类的结果具有某个参考模型作为人为基准进行比较，称之为<strong>外部指标</strong>；第二种是直接考察聚类结果而不参考任何模型，称之为内部指标。</p><h2 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h2><p>对数据集 \(D=\{x_1, x_2,\cdots,x_m\}\)，假定通过聚类算法将样本聚为 \(C=\{C_1,C_2,\cdots,C_k\}\)，参考模型给出的簇划分为 \(C^<em>=\{C_1^</em>,C_2^<em>,\cdots,C_s^</em>\}\)。相应的，令 λ 与 \(λ^∗\) 分别表示与 C 与 \(C^∗\)对应的簇标记向量。我们将样本两两配对考虑，定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%8711.png" alt=""></p><p>其中集合 S1包含了在 C 中属于相同的簇并且在 C∗中也属于相同的簇的样本； S2 包含了在 C 中属于相同的簇并且在 C∗中不属于相同的簇的样本……以此类推。对每个样本对 (xi,xj) (i &lt; j) 仅能出现在一个集合中，因此有 a+b+c+d=m(m−1)/2。</p><p>基于以上定义，对无监督聚类算法的聚类结果，我们有如下的性能度量指标：</p><ul><li>Jaccard 系数（简记为 JCI）</li></ul><script type="math/tex; mode=display">\begin{align} JCI = \frac{a}{a+b+c} \end{align}</script><ul><li>FM 指数（简记为FMI）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/fm%E6%8C%87%E6%95%B0.png" alt=""></p><ul><li>Rand 指数（简记为 RI）</li></ul><script type="math/tex; mode=display">\begin{align} RI=\frac{2(a+d)}{m(m-1)} \end{align}</script><p>很显然，上述指数值都在 <code>[0,1]</code> 之间，并且值越大越好。</p><h2 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h2><p>对于聚类结果\(C=\{C_1,C_2,\cdots,C_k\}\)，作如下定义：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%86%85%E9%83%A8%E6%8C%87%E6%A0%87.png" alt="内部指标"></p><p>其中，\(dist(x_i,x_j)\)用于计算两个样本间的距离 \(μ_i\)代表类 \(C_j\) 的样本中心。基于以上定义如下内部指标：</p><ul><li><strong>DB 指数（简称 DBI）</strong><script type="math/tex; mode=display">\begin{align} DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\bigl(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\bigl) \end{align}</script></li><li><strong>Dunn 指数（简称 DI）</strong></li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/dunn%E6%8C%87%E6%95%B02.png" alt=""><br>显然，DBI 的值越小越好，而 DI 值越大越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类算法" scheme="http://frankblog.site/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之KNN</title>
    <link href="http://frankblog.site/2018/06/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN/"/>
    <id>http://frankblog.site/2018/06/06/机器学习之KNN/</id>
    <published>2018-06-06T03:48:14.506Z</published>
    <updated>2018-06-06T04:05:38.617Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h1><p>KNN采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h2 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/kd%E6%A0%91.jpg" alt=""></p><p>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h2 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h2><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h1 id="KNN算法原理"><a href="#KNN算法原理" class="headerlink" title="KNN算法原理"></a>KNN算法原理</h1><p>KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别：</p><p>假设X_test为未标记的数据样本，X_train为已标记类别的样本，算法原理伪代码如下：</p><ul><li>遍历X_train中所有样本，计算每个样本与X_test的距离，并保存在Distance数组中</li><li>对Distance数组进行排序，取距离最近的k个点，记为X_knn</li><li>在X_knn中统计每个类别的个数</li><li>代表记得样本的类别，就是在X_knn中样本最多的类别</li></ul><h2 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h2><p><strong>优点：</strong>准确性高，对异常值和噪声有较高的容忍度</p><p><strong>缺点：</strong>计算量大，对内存的需求也较大</p><h2 id="算法参数（k）"><a href="#算法参数（k）" class="headerlink" title="算法参数（k）"></a>算法参数（k）</h2><p><strong>k越大：模型偏差越大，对噪声越不敏感。过大是造成欠拟合</strong></p><p><strong>k越小：模型的方差就会越大。太小是会造成过拟合</strong></p><h2 id="算法的变种"><a href="#算法的变种" class="headerlink" title="算法的变种"></a>算法的变种</h2><p><strong>增加邻居的权重：</strong>默认情况下X_knn的权重相等，我们可以指定算法的<code>weights</code>参数调整成距离越近权重越大</p><p><strong>使用一定半径内的点取代距离最近的kk个点</strong>，<code>RadiusNeighborsClassifier</code>类实现了这个算法</p><h1 id="使用KNN作分类"><a href="#使用KNN作分类" class="headerlink" title="使用KNN作分类"></a>使用KNN作分类</h1><ul><li><code>sklearn.neighbors.KNeighborsClassifier</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">k = 5</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># X_test = np.array([0, 2]).reshape(1,-1)</span><br><span class="line">X_test = [[0,2]]</span><br><span class="line">y_test = clf.predict(X_test)</span><br><span class="line">neighbors = clf.kneighbors(X_test, return_distance=False)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)  # 样本</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;) # 中心点</span><br><span class="line">plt.scatter(X_test[0][0], X_test[0][1], marker=&apos;x&apos;, s=50, c=&apos;blue&apos;) # 中心点</span><br><span class="line"></span><br><span class="line">for i in neighbors[0]:</span><br><span class="line">    plt.plot([X[i][0], X_test[0][0]], [X[i][1], X_test[0][1]],&apos;k--&apos;, linewidth=0.5)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB.png" alt=""></p><h1 id="KNN回归拟合"><a href="#KNN回归拟合" class="headerlink" title="KNN回归拟合"></a>KNN回归拟合</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">import numpy as np</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">n = 50</span><br><span class="line">X = 5 * np.random.rand(n ,1)</span><br><span class="line">y = np.cos(X).ravel()</span><br><span class="line"># 添加一些噪声</span><br><span class="line">y += 0.2 * np.random.rand(n) - 0.1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = 5</span><br><span class="line">knn = KNeighborsRegressor(k)</span><br><span class="line">knn.fit(X, y)</span><br></pre></td></tr></table></figure><blockquote><p>KNeighborsRegressor(algorithm=’auto’, leaf_size=30,metric=’minkowski’,metric_params=None, n_jobs=1,n_neighbors=5, p=2,weights=’uniform’)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T = np.linspace(0,5, 500)[:, np.newaxis]</span><br><span class="line">y_pred = knn.predict(T)</span><br><span class="line">knn.score(X,y)</span><br></pre></td></tr></table></figure><blockquote><p>0.9909058023770559</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">plt.scatter(X, y, label=&apos;data&apos;, s=10)</span><br><span class="line">plt.scatter(T, y_pred, label=&apos;prediction&apos;, lw=4, s=0.1)</span><br><span class="line">plt.axis(&apos;tight&apos;)</span><br><span class="line">plt.title(&quot;KNeighborsRegressor (k=%i)&quot; % k)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/kneighborsRegressor.png" alt=""></p><h1 id="糖尿病预测"><a href="#糖尿病预测" class="headerlink" title="糖尿病预测"></a>糖尿病预测</h1><p>总共有768个数据、8个特征，其中Outcome为标记值（1表示有糖尿病）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br></pre></td></tr></table></figure></p><h2 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h2><ul><li>分别使用普通KNN，加权重KNN，和指定权重的KNN分别对数据拟合计算评分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=10)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(</span><br><span class="line">n_neighbors=10, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=10, radius=500.0)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    results.append((name, model.score(X_test, y_test)))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; score:&#123;&#125;&quot;.format(results[i][0], results[i][1]))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; score:0.7207792207792207<br>name:KNN + weights; score:0.6818181818181818<br>name:Radius Neighbors; score:0.6558441558441559</p></blockquote><ul><li>此时单从得分上看，普通的KNN性能是最好的，但是我们的训练样本和测试样本是随机分配的，不同的训练集、测试集会造成不同得分。</li><li>为了消除随机样本集对得分结果可能的影响，scikit-learn提供了<code>KFold和cross_val_score()</code>函数来处理这个问题</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name , model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X, y, cv=kfold) # 这里要给模型全部的样本集</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; cross_val_score:&#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; cross_val_score:0.74865003417635<br>name:KNN + weights; cross_val_score:0.7330485304169514<br>name:Radius Neighbors; cross_val_score:0.6497265892002735</p></blockquote><h2 id="用查准率和召回率以及F1对该模型进行评估："><a href="#用查准率和召回率以及F1对该模型进行评估：" class="headerlink" title="用查准率和召回率以及F1对该模型进行评估："></a>用查准率和召回率以及F1对该模型进行评估：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(10)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;该模型查准率为：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型召回率为：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型F1_score为：&quot;, f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><blockquote><p>该模型查准率为： 0.6086956521739131<br>该模型召回率为： 0.5283018867924528<br>该模型F1_score为： 0.5656565656565657</p></blockquote><h2 id="模型的训练及分析-–-学习曲线"><a href="#模型的训练及分析-–-学习曲线" class="headerlink" title="模型的训练及分析 – 学习曲线"></a>模型的训练及分析 – 学习曲线</h2><p>下面就选择用普通KNN算法模型对数据集进行训练，并查看训练样本的拟合情况及对策测试样本的预测准确性：</p><p>输入参数：</p><blockquote><p>estimator : 你用的分类器。<br>title : 表格的标题。<br>X : 输入的feature，numpy类型<br>y : 输入的target vector<br>ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点<br>cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)<br>n_jobs : 并行的的任务数(默认1))</p></blockquote><p>输出参数：</p><blockquote><p>train_sizes_abs :训练样本数<br>train_scores:训练集上准确率<br>test_scores:交叉验证集上的准确率) </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=2)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">train_score = knn.score(X_train, y_train)</span><br><span class="line">test_score = knn.score(X_test, y_test)</span><br><span class="line">print(&apos;训练集得分：&apos;,train_score)</span><br><span class="line">print(&apos;测试集得分：&apos;,test_score)</span><br></pre></td></tr></table></figure><blockquote><p>训练集得分： 0.8517915309446255<br>测试集得分： 0.6948051948051948</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line"># from common.utils import plot_learning_curve</span><br><span class="line"></span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 10)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">         plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plot_learn_curve(KNeighborsClassifier(2),&quot;KNN score&quot;,X, y, ylim=(0.5, 1), cv=cv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%8A%E5%88%86%E6%9E%90%20%E2%80%93%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>当训练集和测试集的误差收敛但却很高时，为高偏差。<br>左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。<br>我们可以增加模型参数，比如，构建更多的特征，减小正则项。<br>此时通过增加数据量是不起作用的。</p><p>当训练集和测试集的误差之间有大的差距时，为高方差。<br>当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。<br>右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。<br>我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。</p><p>理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。</p><h2 id="特征选择及数据可视化"><a href="#特征选择及数据可视化" class="headerlink" title="特征选择及数据可视化"></a>特征选择及数据可视化</h2><p><strong>使用sklearn.feature_selection.SelectKBest选择相关性最大的两个特征</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.neighborse import KN</span><br><span class="line"></span><br><span class="line">selector = SelectKBest(k=2)</span><br><span class="line">X_new = selector.fit_transform(X,y)</span><br><span class="line">X_new[0:5] #把相关性最大的两个特征放到X_new里并查看前5个数据样本</span><br></pre></td></tr></table></figure><blockquote><p>array([[148. ,  33.6],[ 85. ,  26.6],[183. ,  23.3],[ 89. ,  28.1],[137. ,  43.1]])</p></blockquote><ul><li>使用相关性最大的两个特征，对3种不同的KNN算法进行检验</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=5)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(n_neighbors=5, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=5, radius=500.0)))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X_new, y, cv=kfold)</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name: &#123;&#125;; cross_val_score: &#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name: KNN; cross_val_score: 0.7369104579630894<br>name: KNN + weights; cross_val_score: 0.7199419002050581<br>name: Radius Neighbors; cross_val_score: 0.6510252904989747</p></blockquote><p>从输出结果来看，还是普通KNN的准确性更高，与所有特征放到一起训练的准确性差不多，这也侧面证明了SelectKNBest特征选取的准确性。</p><p>回到目标上来，我们是想看看为什么KNN不能很好的拟合训练样本。现在我们至于2个特征可以很方便的在二维坐标上画出所有的训练样本，观察这些数据分布情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plt.ylabel(&quot;BMI&quot;)</span><br><span class="line">plt.xlabel(&quot;Glucose&quot;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_new[y==0][:,0], X_new[y==0][:,1], marker=&apos;o&apos;, s=10)</span><br><span class="line">plt.scatter(X_new[y==1][:,0], X_new[y==1][:,1], marker=&apos;^&apos;, s=10)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt=""></p><p>横坐标是血糖值，纵坐标是BMI值反应身体肥胖情况。在数据密集的区域，代表糖尿病的阴性和阳性的样本几乎重叠到了一起。这样就很直观的看到，KNN在糖尿病预测的这个问题上无法达到很高的预测准确性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="KNN" scheme="http://frankblog.site/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>决策树之泰坦之灾</title>
    <link href="http://frankblog.site/2018/06/05/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E6%B3%B0%E5%9D%A6%E4%B9%8B%E7%81%BE/"/>
    <id>http://frankblog.site/2018/06/05/决策树之泰坦之灾/</id>
    <published>2018-06-05T10:17:19.653Z</published>
    <updated>2018-06-06T02:29:11.443Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/titanic.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><ul><li>筛选特征值，丢掉不需要的特征数据</li><li>对性别进行二值化处理（转换为0和1）</li><li>港口转换成数值型数据</li><li>处理缺失值（如年龄，有很多缺失值）</li></ul><h2 id="1、首先读取数据"><a href="#1、首先读取数据" class="headerlink" title="1、首先读取数据"></a>1、首先读取数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def read_dataset(fname):</span><br><span class="line">#     指定第一列作为行索引</span><br><span class="line">    data = pd.read_csv(fname, index_col=0)</span><br><span class="line">#     丢弃无用数据</span><br><span class="line">    data.drop([&apos;Name&apos;, &apos;Ticket&apos;, &apos;Cabin&apos;], axis=1, inplace=True)</span><br><span class="line">#     处理性别数据</span><br><span class="line">    lables = data[&apos;Sex&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Sex&apos;] = [*map(lambda x: lables.index(x) , data[&apos;Sex&apos;])]</span><br><span class="line">#     处理登船港口数据</span><br><span class="line">    lables = data[&apos;Embarked&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Embarked&apos;] = data[&apos;Embarked&apos;].apply(lambda n: lables.index(n))</span><br><span class="line">#     处理缺失数据填充0</span><br><span class="line">    data = data.fillna(0)</span><br><span class="line">    return data</span><br><span class="line">train = read_dataset(&apos;code/datasets/titanic/train.csv&apos;)</span><br></pre></td></tr></table></figure><h2 id="2、拆分数据集"><a href="#2、拆分数据集" class="headerlink" title="2、拆分数据集"></a>2、拆分数据集</h2><p>把<code>Survived</code>列提取出来作为标签，然后在元数据集中将其丢弃。同时拆分数据集和交叉验证数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">y = train[&apos;Survived&apos;].values</span><br><span class="line">X = train.drop([&apos;Survived&apos;], axis=1).values</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">print(&quot;X_train_shape:&quot;, X_train.shape, &quot; y_train_shape:&quot;, y_train.shape)</span><br><span class="line">print(&quot;X_test_shape:&quot;, X_test.shape,&quot;  y_test_shape:&quot;, y_test.shape)</span><br></pre></td></tr></table></figure></p><blockquote><p>X_train_shape: (712, 7)  y_train_shape: (712,)<br>X_test_shape: (179, 7)   y_test_shape: (179,)</p></blockquote><h2 id="3、拟合数据集"><a href="#3、拟合数据集" class="headerlink" title="3、拟合数据集"></a>3、拟合数据集</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(&quot;train score:&quot;, clf.score(X_train, y_train))</span><br><span class="line">print(&quot;test score:&quot;, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.9845505617977528<br>test score: 0.7597765363128491</p></blockquote><h1 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h1><h2 id="1、通过max-depth参数来优化模型"><a href="#1、通过max-depth参数来优化模型" class="headerlink" title="1、通过max_depth参数来优化模型"></a>1、通过<code>max_depth</code>参数来优化模型</h2><p>从以上输出数据可以看出，针对训练样本评分很高，但针对测试数据集评分较低。很明显这是过拟合的特征。解决决策树过拟合的方法是剪枝，包括前剪枝和后剪枝。但是<code>sklearn</code>不支持后剪枝，这里通过<code>max_depth</code>参数限定决策树深度，在一定程度上避免过拟合。</p><p>这里先创建一个函数使用不同的模型深度训练模型，并计算评分数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def cv_score(d):</span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=d)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return(clf.score(X_train, y_train), clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">scores = [cv_score(d) for d in depths]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line"># 找出交叉验证数据集评分最高的索引</span><br><span class="line">tr_best_index = np.argmax(tr_scores)</span><br><span class="line">te_best_index = np.argmax(te_scores)</span><br><span class="line"></span><br><span class="line">print(&quot;bestdepth:&quot;, te_best_index+1, &quot; bestdepth_score:&quot;, te_scores[te_best_index], &apos;\n&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>bestdepth: 5  bestdepth_score: 0.8603351955307262 </p></blockquote><p><strong>这里由于以上<code>train_test_split</code>方法对数据切分是随机打散的，造成每次用不同的数据集训练模型总得到不同的最佳深度。</strong>这里写个循环反复测试，最终验证这里看到最佳的分支深度为5出现的频率最高，初步确定5为深度模型最佳。</p><p>把模型参数和对应的评分画出来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&apos;max depth of decison tree&apos;)</span><br><span class="line">plt.ylabel(&apos;Scores&apos;)</span><br><span class="line">plt.plot(depths, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(depths, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0.jpg" alt=""></p><h2 id="2、通过min-impurity-decrease来优化模型"><a href="#2、通过min-impurity-decrease来优化模型" class="headerlink" title="2、通过min_impurity_decrease来优化模型"></a>2、通过<code>min_impurity_decrease</code>来优化模型</h2><p>这个参数用来指定信息墒或者基尼不纯度的阈值，当决策树分裂后，其信息增益低于这个阈值时则不再分裂。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line"></span><br><span class="line">def minsplit_score(val):</span><br><span class="line">    clf = DecisionTreeClassifier(criterion=&apos;gini&apos;, min_impurity_decrease=val)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return (clf.score(X_train, y_train), clf.score(X_test, y_test), )</span><br><span class="line"></span><br><span class="line"># 指定参数范围，分别训练模型并计算得分</span><br><span class="line"></span><br><span class="line">vals = np.linspace(0, 0.2, 100)</span><br><span class="line">scores = [minsplit_score(v) for v in vals]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line">bestmin_index = np.argmax(te_scores)</span><br><span class="line">bestscore = te_scores[bestmin_index]</span><br><span class="line">print(&quot;bestmin:&quot;, vals[bestmin_index])</span><br><span class="line">print(&quot;bestscore:&quot;, bestscore)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&quot;min_impurity_decrease&quot;)</span><br><span class="line">plt.ylabel(&quot;Scores&quot;)</span><br><span class="line">plt.plot(vals, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(vals, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><blockquote><p>bestmin: 0.00202020202020202<br>bestscore: 0.7988826815642458</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%A5%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B.jpg" alt=""></p><p><strong>问题：每次使用不同随机切割的数据集得出最佳参数为0.002很接近0，该怎么解读？</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">至此为我们找到了两个参数,最佳深度depth=5 和最佳min_impurity_decrease=0.002，下面我来用两个参数简历模型进行测试：</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn import metrics </span><br><span class="line"></span><br><span class="line">model = DecisionTreeClassifier(max_depth=5, min_impurity_decrease=0.002)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;tees_score:&quot;, model.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;查准率:&quot;,metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率:&quot;,metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1_score:&quot;,metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure></p><blockquote><p>tees_score: 0.7821229050279329<br>查准率: 0.8461538461538461<br>召回率: 0.5866666666666667<br>F1_score: 0.6929133858267718</p></blockquote><h1 id="模型参数选择工具包"><a href="#模型参数选择工具包" class="headerlink" title="模型参数选择工具包"></a>模型参数选择工具包</h1><p>至此发现以上两种模型优化方法有两问题：</p><ul><li><p>1、数据不稳定：–&gt; 每次重新分配训练集测试集，原参数就不是最优了。 解决办法是多次计算求平均值。</p></li><li><p>2、不能一次选择多个参数：–&gt; 想考察max_depth和min_impurity_decrease两者结合起来的最优参数就没法实现。</p></li></ul><p>所幸<code>scikit-learn</code>在<code>sklearn.model_selection</code>包提供了大量的模型选择和评估的工具供我们使用。针对该问题可以使用<code>GridSearchCV</code>类来解决。</p><h2 id="利用GridSearchCV求最优参数"><a href="#利用GridSearchCV求最优参数" class="headerlink" title="利用GridSearchCV求最优参数"></a>利用<code>GridSearchCV</code>求最优参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;:thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(&quot;best_parms:&#123;0&#125;\nbest_score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best_parms:{‘min_impurity_decrease’: 0.00816326530612245}<br>best_score:0.8114478114478114</p></blockquote><p>模型解读：<br>1、关键字参数<code>param_grid</code>是一个字典，字典的关键字对应的值是一个列表。<code>GridSearchCV</code>会枚举列表里所有值来构建模型多次计算训练模型，并计算模型评分，最终得出指定参数值的平均评分及标准差。</p><p>2、关键参数<code>sv</code>，用来指定交叉验证数据集的生成规则。这里sv=5表示每次计算都把数据集分成5份，拿其中一份作为交叉验证数据集，其他作为训练集。最终得出最优参数及最优评分保存在<code>clf.best_params_</code>和<code>clf.best_score_</code>里。</p><p>3、此外<code>clf.cv_results_</code>里保存了计算过程的所有中间结果。</p><h2 id="画出学习曲线："><a href="#画出学习曲线：" class="headerlink" title="画出学习曲线："></a>画出学习曲线：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def plot_curve(train_sizes, cv_results, xlabel):</span><br><span class="line">    train_scores_mean = cv_results[&apos;mean_train_score&apos;]</span><br><span class="line">    train_scores_std = cv_results[&apos;std_train_score&apos;]</span><br><span class="line">    test_scores_mean = cv_results[&apos;mean_test_score&apos;]</span><br><span class="line">    test_scores_std = cv_results[&apos;std_test_score&apos;]</span><br><span class="line">    plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line">    plt.title(&apos;parameters turning&apos;)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(&apos;score&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &apos;.--&apos;, color=&quot;r&quot;,label=&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, &apos;.-&apos;, color=&quot;g&quot;,</span><br><span class="line">    label=&quot;Cross-validation score&quot;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&quot;best&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line"># Set the parameters by cross-validation</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;: thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param: &#123;0&#125;\nbest score: &#123;1&#125;&quot;.format(clf.best_params_, </span><br><span class="line"> clf.best_score_))</span><br><span class="line"></span><br><span class="line"># plot_curve(thresholds, clf.cv_results_, xlabel=&apos;gini thresholds&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>best param: {‘min_impurity_decrease’: 0.00816326530612245}<br>best score: 0.8114478114478114</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%EF%BC%9A.jpg" alt=""></p><h2 id="多组参数之间选择最优参数："><a href="#多组参数之间选择最优参数：" class="headerlink" title="多组参数之间选择最优参数："></a>多组参数之间选择最优参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">entropy_thresholds = np.linspace(0, 1, 100)</span><br><span class="line">gini_thresholds = np.linspace(0, 0.2, 100)</span><br><span class="line">#设置参数矩阵：</span><br><span class="line">param_grid = [&#123;&apos;criterion&apos;: [&apos;entropy&apos;], &apos;min_impurity_decrease&apos;:entropy_thresholds&#125;,&#123;&apos;criterion&apos;: [&apos;gini&apos;], &apos;min_impurity_decrease&apos;: gini_thresholds&#125;,&#123;&apos;max_depth&apos;: np.arange(2,10)&#125;,&#123;&apos;min_samples_split&apos;: np.arange(2,30,2)&#125;]</span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param:&#123;0&#125;\nbest score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best param:{‘min_impurity_decrease’: 0.00816326530612245}<br>best score:0.8114478114478114</p></blockquote><p>结果1、{‘criterion’: ‘gini’, ‘min_impurity_decrease’: 0.00816326530612245} -&gt;6</p><p>结果2、{‘min_samples_split’: 22} -&gt;10</p><p>结果3、{‘min_samples_split’: 20} -&gt;4</p><p><strong>结果波动很大，这里做了20次测试，对应结果1出现6次，结果2出现10次，结果3出现4次。</strong></p><p><strong>代码解读</strong>：<br>关键部分还是<code>param_grid</code>参数，他是一个列表。很对列表的第一个字典，选择信息墒<code>（entropy）</code>作为判断标准，取值0～1范围50等分；</p><p>第二个字典选择基尼系数，<code>min_impurity_decrease</code>取值0～0.2范围50等分。</p><p><code>GridSearchCV</code>会针对列表中的每个字典进行迭代，最终比较列表中每个字典所对应的参数组合，选择出最优的参数。</p><h2 id="生成决策树图形"><a href="#生成决策树图形" class="headerlink" title="生成决策树图形"></a>生成决策树图形</h2><p>下面代码可以生成.dot文件，需要电脑上安装<code>graphviz</code>才能把文件转换成图片格式。</p><p><code>Mac</code>上可以使用<code>brew install graphviz</code>命令来安装，它会同时安装8个依赖包。这里一定注意<code>Mac</code>环境下的权限问题：由于<code>Homebrew</code>默认是安装在<code>/usr/local</code>下，而<code>Mac</code>有强制保护不支持<code>sudo chown -R uname local</code>对<code>local</code>文件夹进行权限修改。</p><p>这里的解决方式是把<code>local</code>下<code>bin</code>,<code>lib</code>,<code>Cellar</code>等所需单个文件夹下进行赋权，即可成功安装。</p><ol><li>在电脑上安装 graphviz</li><li>运行 <code>dot -Tpng tree.dot -o filename.png</code></li><li>在当前目录查看生成的决策树 filename.png</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier </span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier(min_samples_split=22)</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line">train_score = clf.score(X_train, y_train)</span><br><span class="line">test_score = clf.score(X_test, y_test)</span><br><span class="line">print(&apos;train score: &#123;0&#125;; test score: &#123;1&#125;&apos;.format(train_score, test_score))</span><br><span class="line"></span><br><span class="line"># 导出 titanic.dot 文件</span><br><span class="line">with open(&quot;tree.dot&quot;, &apos;w&apos;) as f:</span><br><span class="line">    f = tree.export_graphviz(clf, out_file=f)</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.8834269662921348; test score: 0.8268156424581006</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E7%BB%84%E5%8F%82%E6%95%B0%E4%B9%8B%E9%97%B4%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%8F%82%E6%95%B0%EF%BC%9A.jpg" alt=""></p><h1 id="模型调参注意事项："><a href="#模型调参注意事项：" class="headerlink" title="模型调参注意事项："></a>模型调参注意事项：</h1><ul><li>当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型</li><li>如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。</li><li>推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</li><li>在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。</li><li>决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。</li><li>如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/titanic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="http://frankblog.site/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>sklearn之数据预处理和创建模型</title>
    <link href="http://frankblog.site/2018/06/05/sklearn%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B/"/>
    <id>http://frankblog.site/2018/06/05/sklearn之数据预处理和创建模型/</id>
    <published>2018-06-05T07:43:32.158Z</published>
    <updated>2018-06-06T04:14:14.934Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"></span><br><span class="line">data = np.array([[ 3, -1.5, 2, -5.4],</span><br><span class="line">[ 0, 4, -0.3, 2.1],</span><br><span class="line">[ 1, 3.3, -1.9, -4.3]])</span><br></pre></td></tr></table></figure><h2 id="均值移除-mean-removal"><a href="#均值移除-mean-removal" class="headerlink" title="均值移除 mean removal"></a>均值移除 mean removal</h2><ul><li>“通常我们会把每个特征的平均值移除，以保证特征均值为0（即标准化处理）。这样做可以消除特征彼此间的偏差（bias）”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_standardized = preprocessing.scale(data)</span><br><span class="line">print (&quot;\nMean特征均值 =&quot;, data_standardized.mean(axis=0))</span><br><span class="line">print (&quot;Std deviation标准偏差 =&quot;, data_standardized.std(axis=0))</span><br></pre></td></tr></table></figure><blockquote><p>Mean特征均值 = [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]<br>Std deviation标准偏差 = [1. 1. 1. 1.]</p></blockquote><h2 id="范围缩放-min-max-scaling"><a href="#范围缩放-min-max-scaling" class="headerlink" title="范围缩放 min max scaling"></a>范围缩放 min max scaling</h2><ul><li>“数据点中每个特征的数值范围可能变化很大，因此，有时将特征的数值范围缩放到合理的大小是非常重要的。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data_scaled = data_scaler.fit_transform(data)</span><br><span class="line">print (&quot;\nMin max scaled data范围缩放数据:\n&quot;, data_scaled)</span><br></pre></td></tr></table></figure><blockquote><p>Min max scaled data范围缩放数据:<br> [[1.         0.         1.         0.        ]<br> [0.         1.         0.41025641 1.        ]<br> [0.33333333 0.87272727 0.         0.14666667]]</p></blockquote><h2 id="归一化-normalization"><a href="#归一化-normalization" class="headerlink" title="归一化 normalization"></a>归一化 normalization</h2><ul><li>“数据归一化用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放到相同的数值范围。机器学习中最常用的归一化形式就是将特征向量调整为L1范数，使特征向量的数值之和为1。”</li><li>“这个方法经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级，提高不同特征数据的可比性。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_normalized = preprocessing.normalize(data, norm=&apos;l1&apos;)</span><br><span class="line">print (&quot;\nL1 normalized data归一化后数据:\n&quot;, data_normalized)</span><br></pre></td></tr></table></figure><blockquote><p>L1 normalized data归一化后数据:<br> [[ 0.25210084 -0.12605042  0.16806723 -0.45378151]<br> [ 0.          0.625      -0.046875    0.328125  ]<br> [ 0.0952381   0.31428571 -0.18095238 -0.40952381]]</p></blockquote><h2 id="二值化-binarization"><a href="#二值化-binarization" class="headerlink" title="二值化 binarization"></a>二值化 binarization</h2><ul><li>“二值化用于将数值特征向量转换为布尔类型向量。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)</span><br><span class="line">print (&quot;\n二值化 data:\n&quot;, data_binarized)</span><br></pre></td></tr></table></figure><blockquote><p>二值化 data:<br> [[1. 0. 1. 0.]<br> [0. 1. 0. 1.]<br> [0. 1. 0. 0.]]</p></blockquote><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><ul><li><p>one hot encoding独热编码<br>“通常，需要处理的数值都是稀疏地、散乱地分布在空间中，然而，我们并不需要存储这些大数值，这时就需要使用独热编码（One-Hot Encoding）。可以把独热编码看作是一种收紧 （tighten）特征向量的工具。它把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k 的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，这样可以更加有效地表示空间。例如，我们需要处理4维向量空间，当给一个特性向量的第n 个特征进行编码时，编码器会遍历每个特征向量的第n 个特征，然后进行非重复计数。如果非重复计数的值是K ，那么就把这个特征转换为只有一个值是1其他值都是0的K 维向量。”</p></li><li><p>“在下面的示例中，观察一下每个特征向量的第三个特征，分别是1 、5 、2 、4 这4个不重复的值，也就是说独热编码向量的长度是4。如果你需要对5 进行编码，那么向量就是[0, 1, 0, 0] 。向量中只有一个值是1。第二个元素是1，对应的值是5 。”</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = preprocessing.OneHotEncoder()</span><br><span class="line">encoder.fit([[0, 2, 1, 12], [1, 3, 5, 3], [2, 3, 2, 12], [1, 2, 4, 3]])</span><br><span class="line">encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()</span><br><span class="line">print (&quot;\n编码矢量:\n&quot;, encoded_vector)</span><br></pre></td></tr></table></figure><blockquote><p>编码矢量:<br> [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]</p></blockquote><h2 id="标记编码方法"><a href="#标记编码方法" class="headerlink" title="标记编码方法"></a>标记编码方法</h2><p>在监督学习中，经常需要处理各种各样的标记。这些标记可能是数字，也可能是单词。如果标记是数字，那么算法可以直接使用它们，但是，许多情况下，标记都需要以人们可理解的形式存在，因此，人们通常会用单词标记训练数据集。标记编码就是要把单词标记转换成数值形式，让算法懂得如何操作标记。接下来看看如何标记编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line"># 定义一个标记编码器</span><br><span class="line">label_encoder = preprocessing.LabelEncoder()</span><br><span class="line"></span><br><span class="line"># label_encoder对象知道如何理解单词标记，接下来创建标记</span><br><span class="line">input_classes = [&apos;audi&apos;, &apos;ford&apos;, &apos;audi&apos;, &apos;toyota&apos;, &apos;ford&apos;, &apos;bmw&apos;]</span><br><span class="line"># 开始标记</span><br><span class="line">label_encoder.fit(input_classes)</span><br><span class="line">print(&quot;Classes mapping: 结果显示单词背转换成从0开始的索引值&quot;)</span><br><span class="line">for i, item in enumerate(lable_encoder.classes_):</span><br><span class="line">print(item, &apos;--&gt;&apos;, i)</span><br></pre></td></tr></table></figure><blockquote><p>Classes mapping: 结果显示单词背转换成从0开始的索引值<br>audi —&gt; 0<br>bmw —&gt; 1<br>ford —&gt; 2<br>toyota —&gt; 3</p></blockquote><p>这时，如果遇到一组数据就可以轻松的转换它们了。（如药品数据的药品名）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">labels = [&apos;toyota&apos;, &apos;f</span><br><span class="line">ord&apos;, &apos;audi&apos;]</span><br><span class="line">encoded_labels = label_encoder.transform(labels)</span><br><span class="line">print (&quot;\nLabels =&quot;, labels)</span><br><span class="line">print (&quot;Encoded labels =&quot;, list(encoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>Labels = [‘toyota’, ‘ford’, ‘audi’]<br>Encoded labels = [3, 2, 0]</p></blockquote><p>还可以数字反转回单词（或字符串）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoded_labels = [2,1,0,3,1]</span><br><span class="line">decoded_labels = label_encoder.inverse_transform(encoded_labels)</span><br><span class="line">print(encoded_labels)</span><br><span class="line">print(list(decoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>[2, 1, 0, 3, 1]<br>[‘ford’, ‘bmw’, ‘audi’, ‘toyota’, ‘bmw’]</p></blockquote><h1 id="创建线性回归"><a href="#创建线性回归" class="headerlink" title="创建线性回归"></a>创建线性回归</h1><p>回归是估计输入数据与连续值输出数据之间关系的过程。数据通常是实数形式的，我们的目标是<strong>估计满足输入到输出映射关系的基本函数。</strong></p><p>线性回归的目标是提取输入变量与输出变量的关联线性模型，这就要求实际输出与线性方程预测的输出的残差平方和（sum of squares of differences）最小化。这种方法被称为普通最小二乘法 （Ordinary Least Squares，OLS）。</p><p>你可能觉得用一条曲线对这些点进行拟合效果会更好，但是线性回归不允许这样做。线性回归的主要优点就是方程简单。如果你想用非线性回归，可能会得到更准确的模型，但是拟合速度会慢很多。线性回归模型就像前面那张图里显示的，<strong>用一条直线近似数据点的趋势</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line"># 加载数据</span><br><span class="line">filename = sys.argv[1]</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_singlevar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import linear_model</span><br><span class="line">linear_regr = linear_model.LinearRegression()</span><br><span class="line">linear_regr.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>我们利用训练数据集训练了线性回归器。向fit 方法提供输入数据即可训练模型。用下面的代码看看它如何拟合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(&apos;训练集拟合效果&apos;)</span><br><span class="line">y_train_pred = linear_regr.predict(X_train)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X_train, y_train)</span><br><span class="line">plt.plot(X_train, y_train_pred, color=&apos;green&apos;, linewidth=2)</span><br><span class="line">plt.title(&apos;Training Data&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%861.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = linear_regr.predict(X_test)</span><br><span class="line">print(&quot;测试集拟合效果&quot;)</span><br><span class="line">plt.scatter(X_test, y_test)</span><br><span class="line">plt.plot(X_test, y_test_pred, color=&apos;green&apos;)</span><br><span class="line">plt.title(&quot;Test Data&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%862.png" alt=""></p><h1 id="计算回归准确性"><a href="#计算回归准确性" class="headerlink" title="计算回归准确性"></a>计算回归准确性</h1><p>现在已经建立了回归器，接下来最重要的就是如何评价回归器的拟合效果。在模型评价的相关内容中，用误差 （error）表示实际值与模型预测值之间的差值。</p><p>下面快速了解几个衡量回归器拟合效果的重要指标（metric）。回归器可以用许多不同的指标进行衡量，部分指标如下所示。</p><ul><li><p><strong>平均绝对误差（mean absolute error）</strong> ：这是给定数据集的所有数据点的绝对误差平均值。</p></li><li><p><strong>均方误差（mean squared error）</strong> ：这是给定数据集的所有数据点的误差的平方的平均值。这是最流行的指标之一。</p></li><li><p><strong>中位数绝对误差（median absolute error）</strong> ：这是给定数据集的所有数据点的误差的中位数。这个指标的主要优点是可以消除异常值（outlier）的干扰。测试数据集中的单个坏点不会影响整个误差指标，均值误差指标会受到异常点的影响。</p></li><li><p><strong>解释方差分（explained variance score）</strong> ：这个分数用于衡量我们的模型对数据集波动的解释能力。如果得分1.0分，那么表明我们的模型是完美的。</p></li><li><p><strong>R方得分（R2 score）</strong> ：这个指标读作“R方”，是指确定性相关系数，用于衡量模型对未知样本预测的效果。最好的得分是1.0，值也可以是负数。</p></li></ul><p>“每个指标都描述得面面俱到是非常乏味的，因此只选择一两个指标来评估我们的模型。通常的做法是尽量保证均方误差最低，而且解释方差分最高”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 0.54<br>均方误差（mean squared error） ： 0.38<br>中位数绝对误差（median absolute error） ： 0.54<br>解释方差分（explained variance score） ： 0.68<br>R方得分（R2 score） ： 1.0</p></blockquote><h1 id="保存模型数据"><a href="#保存模型数据" class="headerlink" title="保存模型数据"></a>保存模型数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">regr = pickle.dumps(linear_regr) # 保存</span><br><span class="line">regr1 = pickle.loads(regr) # 加载</span><br><span class="line">regr1.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p></blockquote><p>在scikit的具体情况下，使用 joblib 替换 pickle（ joblib.dump &amp; joblib.load ）可能会更有趣，这对大数据更有效，但只能序列化 (pickle) 到磁盘而不是字符串变量:</p><p>之后，您可以加载已保存的模型（可能在另一个 Python 进程中）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.externals import joblib</span><br><span class="line">joblib.dump(linear_regr, &apos;regr.pkl&apos;) </span><br><span class="line">regr2 = joblib.load(&apos;regr.pkl&apos;) </span><br><span class="line">regr2.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p><h1 id="创建岭回归"><a href="#创建岭回归" class="headerlink" title="创建岭回归"></a>创建岭回归</h1></blockquote><p>线性回归的主要问题是对异常值敏感。在真实世界的数据收集过程中，经常会遇到错误的度量结果。而线性回归使用的普通最小二乘法，其目标是使平方误差最小化。这时，由于异常值误差的绝对值很大，因此会引起问题，从而破坏整个模型。</p><p>普通最小二乘法在建模时会考虑每个数据点的影响，因此，最终模型就会瘦异常值影响较大。显然，我们发现这个模型不是最优的。为了避免这个问题，我们引入正则化项 的系数作为阈值来消除异常值的影响。这个方法被称为岭回归 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_multivar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,3)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 3)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p>alpha 参数控制回归器的复杂程度。当alpha 趋于0 时，岭回归器就是用普通最小二乘法的线性回归器。因此，如果你希望模型对异常值不那么敏感，就需要设置一个较大的alpha 值。这里把alpha 值设置为0.01 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">rid = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)</span><br><span class="line"></span><br><span class="line">rid.fit(X_train, y_train)</span><br><span class="line">y_test_pred = rid.predict(X_test)</span><br><span class="line"></span><br><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 3.95<br>均方误差（mean squared error） ： 23.15<br>中位数绝对误差（median absolute error） ： 3.69<br>解释方差分（explained variance score） ： 0.84<br>R方得分（R2 score） ： 1.0</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Line</span><br><span class="line">line = Line(&quot;期望值测试对比&quot;)</span><br><span class="line">line.add(&apos;测试目标值&apos;, np.linspace(-20,40,len(y_test)), y_test, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line.add(&apos;实际测试值&apos;, np.linspace(-20,40,len(y_test)),  y_test_pred, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line</span><br><span class="line"></span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%9B%E5%BB%BA%E5%B2%AD%E5%9B%9E%E5%BD%92.png" alt=""></p><h1 id="创建多项式回归器（重点）"><a href="#创建多项式回归器（重点）" class="headerlink" title="创建多项式回归器（重点）"></a>创建多项式回归器（重点）</h1><p>数据点本身的模式中带有自然的曲线，而线性模型是不能捕捉到这一点的。多项式回归模型的曲率是由多项式的次数决定的。随着模型曲率的增加，模型变得更准确。但是，增加曲率的同时也增加了模型的复杂性，因此拟合速度会变慢。当我们对模型的准确性的理想追求与计算能力限制的残酷现实发生冲突时，就需要综合考虑了。</p><p>下面使用岭回归的数据，注意和简单线性回归的区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">#将曲线的多项式次数初始值设置为3</span><br><span class="line">poly = PolynomialFeatures(degree = 20)</span><br><span class="line"># “其中，X_train_transformed 表示多项式形式的输入，与线性回归模型是一样的。”</span><br><span class="line">X_train_transformed = poly.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line">#测试一下</span><br><span class="line">dp = X_train[0].reshape(1,-1)</span><br><span class="line">poly_dp = poly.fit_transform(dp)</span><br><span class="line"></span><br><span class="line">poly_liner = linear_model.LinearRegression()</span><br><span class="line">poly_liner.fit(X_train_transformed, y_train) #这里注意输入转换后的X_train</span><br><span class="line"></span><br><span class="line">print (&quot;\nLinear regression:&quot;, rid.predict(dp)[0])</span><br><span class="line">print (&quot;\nPolynomial regression:&quot;, poly_liner.predict(poly_dp)[0]) ##这输入转换后的X_test</span><br></pre></td></tr></table></figure><blockquote><p>Linear regression: -11.058646635286552<br>Polynomial regression: -8.070076359128953</p></blockquote><ul><li>多项式次数为1时 返回预测结果为：-11.058729498335897，欠拟合</li><li>多项式次数为10时 返回预测结果为：-8.206005341193759，这里与真实值-8.07已经非常接近了</li><li>多项式次数为20时 返回预测结果为：-8.070076359128953，针对这个值的预测最完美</li><li>多项式次数为100时 返回预测结果为：10.01397529328105，说明出现过拟合</li></ul><h1 id="AdaBoost算法估算房屋价格"><a href="#AdaBoost算法估算房屋价格" class="headerlink" title="AdaBoost算法估算房屋价格"></a>AdaBoost算法估算房屋价格</h1><p><strong>利用AdaBoost算法的决策树回归器<code>（decision tree regreessor）</code>来估算房屋价格</strong></p><p>决策树是一个树状模型，每个节点都做出一个决策，从而影响最终结果。叶子节点表示输出数值，分支表示根据输入特征做出的中间决策。<code>AdaBoost</code>算法是指自适应增强（<code>adaptive boosting</code>）算法，这是一种利用其他系统增强模型准确性的技术。这种技术是将不同版本的算法结果进行组合，用加权汇总的方式获得最终结果，被称为弱学习器 （<code>weak learners</code>）。<code>AdaBoost</code>算法在每个阶段获取的信息都会反馈到模型中，这样学习器就可以在后一阶段重点训练难以分类的样本。这种学习方式可以增强系统的准确性。</p><p>首先使用<code>AdaBoost</code>算法对数据集进行回归拟合，再计算误差，然后根据误差评估结果，用同样的数据集重新拟合。可以把这些看作是回归器的调优过程，直到达到预期的准确性。假设你拥有一个包含影响房价的各种参数的数据集，我们的目标就是估计这些参数与房价的关系，这样就可以根据未知参数估计房价了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">from sklearn.ensemble import AdaBoostRegressor</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.metrics import mean_squared_error, explained_variance_score</span><br><span class="line">from sklearn.utils import shuffle</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">hous_data = datasets.load_boston()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 利用shuffle函数把数据的顺序打乱（参数random_state用来控制如何打乱数据）</span><br><span class="line">X, y = shuffle(hous_data.data, hous_data.target, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.8 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line"># 选择最大深度为5的决策树回归模型</span><br><span class="line">dtre = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 再用带AdaBoost算法的决策树回归模型进行拟合与上面进行比较</span><br><span class="line">abre = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=400, random_state=7)</span><br><span class="line">abre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">#  看看AdaBoost算法对决策树回归器的训练效果有多大改善</span><br><span class="line">y_pred_dt = dtre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_dt)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_dt)</span><br><span class="line">print(&quot;决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;决策树-解释方差: &quot;, evs)</span><br><span class="line"></span><br><span class="line">y_pred_ab = abre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_ab)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_ab)</span><br><span class="line">print(&quot;\nAbaBoost决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;AbaBoost决策树-解释方差: &quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>决策树-均方误差:  12.74782456548819<br>决策树-解释方差:  0.8454595720920495<br>AbaBoost决策树-均方误差:  7.015648111222207<br>AbaBoost决策树-解释方差:  0.9147414844474588</p></blockquote><h1 id="计算特征的相对重要性-（如交通案例计算各出口贡献率）"><a href="#计算特征的相对重要性-（如交通案例计算各出口贡献率）" class="headerlink" title="计算特征的相对重要性 （如交通案例计算各出口贡献率）"></a>计算特征的相对重要性 （如交通案例计算各出口贡献率）</h1><p><strong>(_modle.feature__importances_)</strong></p><p>在这个案例中，我们用了13个特征，它们对模型都有贡献。但是，有一个重要的问题出现了：如何判断哪个特征更加重要？显然，所有的特征对结果的贡献是不一样的。如果需要忽略一些特征，就需要知道哪些特征不太重要。scikit-learn里面有这样的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def plot_feature_importances(feature_importances, title, feature_names):</span><br><span class="line"> # 将重要性值标准化</span><br><span class="line">    feature_importances = 100.0 * (feature_importances / max(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 将得分从高到低排序</span><br><span class="line">    index_sorted = np.flipud(np.argsort(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 让X坐标轴上的标签居中显示</span><br><span class="line">    pos = np.arange(index_sorted.shape[0]) + 0.5</span><br><span class="line"></span><br><span class="line"> # 画条形图</span><br><span class="line"> plt.figure()</span><br><span class="line"> plt.bar(pos, feature_importances[index_sorted], align=&apos;center&apos;)</span><br><span class="line"> plt.xticks(pos, feature_names[index_sorted])</span><br><span class="line"> plt.ylabel(&apos;Relative Importance&apos;)</span><br><span class="line"> plt.title(title)</span><br><span class="line"> plt.show()</span><br><span class="line"></span><br><span class="line"># 画出特征的相对重要性</span><br><span class="line">plot_feature_importances(dtre.feature_importances_, &apos;Decision Tree regressor&apos;, hous_data.feature_names)</span><br><span class="line">plot_feature_importances(abre.feature_importances_, &apos;AdaBoost regressor&apos;, hous_data.feature_names)</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A71.png" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A72.png" alt=""></p><p>上图可以看出不带AbaBoost的决策树回归器显示最重要的特征是RM，而带AbaBoost算法的决策回归器现实的最主要特征是LASTAT。现实生活中如果对这个数据集建立不同的回归器会发现最重要的特征就是LSTAT，这足以体现AbaBoost算法对决策树训练效果的改善。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;影响房价的因素分析&quot;)</span><br><span class="line">pie.add(&quot;决策树回归器&quot;, hous_data.feature_names, dtre.feature_importances_, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[35, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie.add(&quot;AbaBoost决策树&quot;, hous_data.feature_names, abre.feature_importances_,is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[75, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A73.png" alt=""></p><h1 id="随机森林评估共享单车的需求分布"><a href="#随机森林评估共享单车的需求分布" class="headerlink" title="随机森林评估共享单车的需求分布"></a>随机森林评估共享单车的需求分布</h1><p><strong>采用随机森林回归器<code>(random forest regressor)</code>估计输出结果。</strong></p><p>随机森林死一个决策树合集，它基本上就是用一组由数据集的若干子集构建的决策树构成，再用决策树平均值改善整体学习效果</p><p>我们将使用bike_day.csv文件中的数据集，它可以在 <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a> 获取。这份数据集一共16列，前两列是序列号与日期，分析的时候可以不用；最后三列数据是不同类型的输出结果；最后一列是第十四列与第十五列的和，因此建立模型时可以不考虑第十四列与第十五列。</p><p>参数<code>n_estimators</code>是指评估器<code>（estimator）</code>的数量，表示随机森林需要使用的决策树数量；<br>参数<code>max_depth</code> 是指每个决策树的最大深度；参数<code>min_samples_split</code>是指决策树分裂一个节点需要用到的最小数据样本量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from housing import plot_feature_importances   #这个方法源码参考上例</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&apos;bike_day.csv&apos;,sep=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">X = data[data.columns[2:13]]</span><br><span class="line">y = data[data.columns[-1]]</span><br><span class="line">f_name = X.columns</span><br><span class="line"></span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line">rf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=12)</span><br><span class="line">rf_regr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = rf_regr.predict(X_test)</span><br><span class="line"></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;随机森林回归器效果：&quot;)</span><br><span class="line">print(&quot;均方误差：&quot;, round(mse, 2))</span><br><span class="line">print(&quot;解释方差分：&quot;, round(evs, 2))</span><br></pre></td></tr></table></figure></p><blockquote><p>随机森林回归器效果：<br>均方误差： 368026.24<br>解释方差分： 0.89</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A6.png" alt=""></p><p><strong>利用按小时的数据计算相关性</strong></p><p>这里要用到3～14列<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(X_train)</span><br></pre></td></tr></table></figure></p><blockquote><p>15641</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hour_data = pd.read_csv(&apos;bike_hour.csv&apos;, sep=&apos;,&apos;)</span><br><span class="line">X = hour_data[hour_data.columns[2:14]]</span><br><span class="line">y = hour_data[hour_data.columns[-1]]</span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9*len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line">f_names = X.columns</span><br><span class="line"></span><br><span class="line">hrf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=10)</span><br><span class="line">hrf_regr.fit(X_train, y_train)</span><br><span class="line">y_pred = hrf_regr.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差：&quot;, mse)</span><br><span class="line">print(&quot;解释方差分：&quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>均方误差： 1884.1767363623571<br>解释方差分： 0.9414038595964176</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">attr = f_names</span><br><span class="line">v1 = hrf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, </span><br><span class="line">legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A62.png" alt=""></p><p>由图可见，其中最重要的特征是一天中的不同时间点（hr），其次重要的是温度，这完全符合人们的直觉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="sklearn" scheme="http://frankblog.site/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之逻辑回归</title>
    <link href="http://frankblog.site/2018/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://frankblog.site/2018/06/04/机器学习之逻辑回归/</id>
    <published>2018-06-04T06:31:16.158Z</published>
    <updated>2018-06-08T04:10:19.100Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="逻辑回归基础"><a href="#逻辑回归基础" class="headerlink" title="逻辑回归基础"></a>逻辑回归基础</h1><h2 id="Sigmoid预测函数"><a href="#Sigmoid预测函数" class="headerlink" title="Sigmoid预测函数"></a>Sigmoid预测函数</h2><p>在逻辑回归中，定义预测函数为：</p><script type="math/tex; mode=display">h_\theta (x) = g(z)</script><p>其中，\(z=\theta^Tx\)是<strong>分类边界</strong>，且\(g(z)=\frac{1}{1+e^{-z}}\)<br>g(z)称之为 Sigmoid Function，亦称 Logic Function，其函数图像如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/sigmoid%E5%87%BD%E6%95%B0.jpg" alt=""></p><p>可以看到，预测函数 hθ(x)被很好地限制在了 0、1 之间，并且，sigmoid 是一个非常好的阈值函数：阈值为 0.5，大于 0.5为 1 类，反之为 0 类。函数曲线过渡光滑自然，关于 0.5中心对称也极具美感。</p><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><ul><li><strong>线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><ul><li><strong>非线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><h2 id="预测代价函数"><a href="#预测代价函数" class="headerlink" title="预测代价函数"></a>预测代价函数</h2><p>下面两幅图中，左图这样犬牙差互的代价曲线（非凸函数）显然会使我们在做梯度下降的时候陷入迷茫，任何一个极小值都有可能被错认为最小值，但无法获得最优预测精度。但在右图的代价曲线中，就像滑梯一样，我们就很容易达到最小值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E4%B8%8E%E9%9D%9E%E5%87%B8%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>逻辑回归定义的代价函数为：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})</script><p>为保证代价函数呈凸形曲线，则定义 \(Cost(h_\theta(x^{(i)}),y^{(i)})\)：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x)),&\mbox{if $y=1$}\\-log(1-h_\theta(x)),&\mbox{if $y=0$}\end{cases}</script><p>该函数等价于：</p><script type="math/tex; mode=display">\begin{align*}Cost(h_\theta(x),y) &=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \\&= (\,log\,(g(X\theta))^Ty+(\,log\,(1-g(X\theta))^T(1-y)\end{align*}</script><p>代价函数随预测值 hθ(x)hθ(x) 的变化如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.png" alt=""> </p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.png" alt=""></p><h2 id="手推LR"><a href="#手推LR" class="headerlink" title="手推LR"></a>手推LR</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR1.png" alt="link"></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR2.png" alt="link"></p><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</p><p>如下例所示，我们将 θ3 及 θ4 减小（惩罚）到趋近于 00，原本过拟合的曲线就变得更加平滑，趋近于一条二次曲线（在本例中，二次曲线显然更能反映住房面积和房价的关系），也就能够更好的根据住房面积来预测房价。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98.png" alt="link"></p><p>直观来看，如果我们想解决这个例子中的过拟合问题，最好能将的影响消除，也就是让。假设我们对进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大惩罚项，例如：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%982.png" alt=""></p><p>这样在最小化Cost函数的时候，。正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%983.png" alt=""></p><p>lambda是正则项系数：</p><p>1.如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；</p><p>2.如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。</p><p>正则化后的梯度下降中θ的更新变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%984.png" alt=""></p><p>正则化后的线性回归的Normal Equation的公式为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%985.png" alt=""></p><hr><h1 id="scikit-learn-逻辑回归类"><a href="#scikit-learn-逻辑回归类" class="headerlink" title="scikit-learn 逻辑回归类"></a>scikit-learn 逻辑回归类</h1><h2 id="带L1与L2正则的逻辑回归损失函数"><a href="#带L1与L2正则的逻辑回归损失函数" class="headerlink" title="带L1与L2正则的逻辑回归损失函数"></a>带L1与L2正则的逻辑回归损失函数</h2><p>scikit-learn在<code>LogisticRegression</code>的<code>sklearn.linear_model.LogisticRegression</code>类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p><p>作为优化问题，带 L2 正则的二分类 logistic 回归要最小化以下代价函数（cost function）：</p><script type="math/tex; mode=display">\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p><script type="math/tex; mode=display">\underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p> L1 范数作为正则项由以下几个用途：</p><ul><li>特征选择： 它会让模型参数向量里的元素为0的点尽量多。 因此可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以 L1 范数解决过拟合措施实际上是减少特征数量。</li><li>可解释性： 模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。 这样我们就容易解释模型的因果关系。 比如针对某个癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性成关键作用。 稀疏化后，只留下几个关键特征，就更容易看到因果关系</li></ul><p>由此可见， L1 范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。 大部分情况下，我们解决过拟合问题，还是选择 L2 单数作为正则项， 这也是 sklearn 里的默认值。</p><h2 id="优化方法参数"><a href="#优化方法参数" class="headerlink" title="优化方法参数"></a>优化方法参数</h2><p>solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</p><ul><li>liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li><li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li><li>saga：线性收敛的随机优化算法的的变重。</li></ul><div class="table-container"><table><thead><tr><th>Case</th><th>Solver</th></tr></thead><tbody><tr><td>L1正则</td><td>“liblinear” or “saga”</td></tr><tr><td>多项式损失（multinomial loss）</td><td>“lbfgs”, “sag”, “saga” or “newton-cg”</td></tr><tr><td>大数据集（n_samples）</td><td>“sag” or “saga”</td></tr></tbody></table></div><p>“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear”</p><h1 id="乳腺癌检测"><a href="#乳腺癌检测" class="headerlink" title="乳腺癌检测"></a>乳腺癌检测</h1><p>使用逻辑回归算法解决乳腺癌检测问题。 我们需要先采集肿瘤病灶造影图片， 然后对图片进行分析， 从图片中提取特征， 在根据特征来训练模型。 最终使用模型来检测新采集到的肿瘤病灶造影， 判断是良性还是恶性。 这个是典型的二元分类问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 加载数据</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape,&apos;\n&apos;, X[0], &apos;\n&apos;, y[0])</span><br></pre></td></tr></table></figure></p><blockquote><p>(569, 30) (569,)<br> [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01<br> 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02<br> 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01<br> 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01<br> 4.601e-01 1.189e-01]<br> 0</p></blockquote><p>实际上它只关注了 10 个特征，然后又构造出来每个特征的标准差及最大值，这样每个特征又衍生出了两个特征，所以共有30个特征。</p><p>疑问： 该方式是否直接会导致多重共线性的出现？</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">train_score = model.score(X_train, y_train)</span><br><span class="line">test_score = model.score(X_test, y_test)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">print(&quot;train_score:&quot;, train_score)</span><br><span class="line">print(&quot;test_score:&quot;, test_score)</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9626373626373627<br>test_score: 0.9473684210526315</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">from sklearn.metrics import precision_score, recall_score, f1_score</span><br><span class="line">print(&quot;查准率：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1Score：&quot;, f1_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">print(np.equal(y_pred, y_test).shape[0], y_test.shape[0]) # 输出预测匹配成功数量和测试样本的数量</span><br></pre></td></tr></table></figure><blockquote><p>查准率： 0.9358974358974359<br> 召回率： 0.9733333333333334<br>F1Score： 0.954248366013072<br>114 114</p></blockquote><p>这里数量上显示全部都预测正确，而test_score却不是1，是因为sklearn不是使用这个数据来计算得分，因为这个数据不能完全反映误差情况，而是使用预测概率来计算模型得分。</p><h2 id="查看预测自信度"><a href="#查看预测自信度" class="headerlink" title="查看预测自信度"></a>查看预测自信度</h2><p>二元分类模型会针对每个样本输出的两个概率，即0和1的概率，哪个概率高就预测器哪个类别。我们可以找出针对测试数据集，模型预测的“自信度”低于90%的样本。我们先计算出测试数据集里每个样本的预测概率数据，针对每个样本会有两个数据：一个预测为0，一个预测为1。结合找出预测为阴性和阳性的概率大于0.1的样本。我们可以看下概率数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 计算每个测试样本的预测概率：</span><br><span class="line"></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">print(&quot;自信度示例：&quot;,y_pred_proba[0])</span><br></pre></td></tr></table></figure><blockquote><p>自信度示例： [0.00452578 0.99547422]</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba_0 = y_pred_proba[:, 0] &gt; 0.1</span><br><span class="line">result = y_pred_proba[y_pred_proba_0]</span><br><span class="line"></span><br><span class="line">y_pred_proba_1 = result[:, 1] &gt; 0.1</span><br><span class="line">print(result[y_pred_proba_1])</span><br></pre></td></tr></table></figure><blockquote><p>[[0.11338788 0.88661212]<br> [0.18245824 0.81754176]<br> [0.13110396 0.86889604]<br> [0.35245276 0.64754724]<br> [0.30664405 0.69335595]<br> [0.24931118 0.75068882]<br> [0.8350464  0.1649536 ]<br> [0.44807883 0.55192117]<br> [0.74071324 0.25928676]<br> [0.43085792 0.56914208]<br> [0.13388416 0.86611584]<br> [0.33507985 0.66492015]<br> [0.53672412 0.46327588]<br> [0.11422612 0.88577388]<br> [0.42946531 0.57053469]<br> [0.69759146 0.30240854]<br> [0.25982004 0.74017996]<br> [0.12179042 0.87820958]<br> [0.88546887 0.11453113]]</p></blockquote><h2 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#这里使用Pipeline来增加多项式特征</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">def poly_model(degree=2, penalty=penalty):</span><br><span class="line">    poly_features = PolynomialFeatures(degree=degree, include_bias=False)</span><br><span class="line">    log_regr = LogisticRegression(penalty=penalty) # 注意这里是L1而不是11，指的是使用L1范式作为其正则项</span><br><span class="line">    pipeline = Pipeline([(&quot;poly_features&quot;,poly_features),(&quot;log_regr&quot;,log_regr)])</span><br><span class="line">    return pipeline</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 接着增加二阶多项式特征，创建并训练模型</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">model = poly_model(degree=2, penalty=&apos;l1&apos;)</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;train_score:&quot;,model.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score:&quot;,model.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9934065934065934<br>test_score: 0.9649122807017544</p></blockquote><p>这里要注意的是使用L1范式作为其正则项，参数为<code>penalty=l1</code>。L1范数作为其正则项，可以实现参数的稀疏化，即自动帮我买选择出哪些对模型有关联的特征。我买可以观察下有多少个特征没有被丢弃即对应的模型参数θj非0：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_regr = model.named_steps[&apos;log_regr&apos;]</span><br><span class="line">print(&quot;特征总量：&quot;,log_regr.coef_.shape[1])</span><br><span class="line">print(&quot;特征保留量：&quot;, np.count_nonzero(log_regr.coef_))</span><br></pre></td></tr></table></figure><blockquote><p>特征总量： 495<br>特征保留量： 114</p></blockquote><p>逻辑回归模型的<code>coef_</code>属性里保存的就是模型参数。 从输出结果看，增加二阶多项式特征后，输入特征由原来的30个增加到了595个，在L1范数的“惩罚”下最终只保留了92个有效特征</p><h2 id="实验：利用决策树画出原始数据对预测相关性非0对特征"><a href="#实验：利用决策树画出原始数据对预测相关性非0对特征" class="headerlink" title="实验：利用决策树画出原始数据对预测相关性非0对特征"></a>实验：利用决策树画出原始数据对预测相关性非0对特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">dtmodel = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtmodel.fit(X_train, y_train)</span><br><span class="line">print(&quot;train_score&quot;, dtmodel.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score&quot;, dtmodel.score(X_test, y_test))</span><br><span class="line">from pyecharts import Bar</span><br><span class="line">index = np.nonzero(dtmodel.feature_importances_)</span><br><span class="line">bar = Bar()</span><br><span class="line">bar.add(&quot;&quot;, cancer.feature_names[index],dtmodel.feature_importances_[index])</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><blockquote><p>train_score 0.9910875596851206<br>test_score 0.6296416546416548</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%88%A9%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%BB%E5%87%BA%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%AF%B9%E9%A2%84%E6%B5%8B%E7%9B%B8%E5%85%B3%E6%80%A7%E9%9D%9E0%E5%AF%B9%E7%89%B9%E5%BE%81.png" alt=""></p><h2 id="评估模型：画出学习曲线"><a href="#评估模型：画出学习曲线" class="headerlink" title="评估模型：画出学习曲线"></a>评估模型：画出学习曲线</h2><p>首先画出L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 5)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L1&quot;,&quot;degree:2 penalty=L1&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l1&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i], penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>L2范数作为正则项画出对应一阶和二阶多项式学习曲线<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L2&quot;,&quot;degree:2 penalty=L2&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l2&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i],penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF1.png" alt=""></p><p>从上面两个图可以看出，使用二阶多项式并使用L1范数作为正则项的模型最优，训练样本评分最高，交叉验证样本评分最高。<br>训练样本评分和交叉验证样本评分之间的间隙还比较大，这说明可以通过采集更多数据来训练模型，以便进一步优化模型.</p><p>通过时间消耗对比上可以看出利用L1范式作为正则项需要花费的时间更多，是因为<code>sklearn</code>的<code>learning_curve()</code>函数在画学习曲线的过程中要对模型进行多次训练，并计算交叉验证样本评分。同时为了让曲线更平滑，针对每个点还会进行多次计算球平均值。这个就是<code>ShufferSplit</code>类的作用。在这个实例里只有569个样本是很小的数据集。如果数据集增加100倍，拿出来画学习曲线将是场灾难。</p><p>问题是针对大数据集，怎么画学习曲线？</p><p>思路一：可以考虑从大数据集选取一小部分数据来画学习曲线，待选择好最优的模型之后，在使用全部的数据来训练模型。这时需要警惕的是，尽量保证选择出来的这部分数据的<strong>标签分布与大数据集的标签分布相同</strong>，如针对二元分类，<strong>阳性和阴性比例要一致！</strong></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1-LR中损失函数的意义是什么？"><a href="#1-LR中损失函数的意义是什么？" class="headerlink" title="1.LR中损失函数的意义是什么？"></a>1.LR中损失函数的意义是什么？</h2><p>在LR中，最大似然函数与最小化对数损失函数等价</p><h2 id="2-LR与线性回归的联系和区别"><a href="#2-LR与线性回归的联系和区别" class="headerlink" title="2. LR与线性回归的联系和区别"></a>2. LR与线性回归的联系和区别</h2><p>逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</p><h2 id="3-LR与最大熵模型"><a href="#3-LR与最大熵模型" class="headerlink" title="3.LR与最大熵模型"></a>3.LR与最大熵模型</h2><p>逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。</p><ul><li><p>指数簇分布的最大熵等价于其指数形式的最大似然。</p></li><li><p>二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；</p></li><li><p>多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。</p></li></ul><h2 id="4-LR与svm"><a href="#4-LR与svm" class="headerlink" title="4.LR与svm"></a>4.LR与svm</h2><p>不同点:</p><ol><li><p>损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss</p></li><li><p>逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。</p></li><li><p>逻辑回归对概率建模，svm对分类超平面建模</p></li><li><p>逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有</p></li><li><p>逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响</p></li><li><p>逻辑回归是统计方法，svm是几何方法</p></li></ol><h2 id="5-LR与朴素贝叶斯"><a href="#5-LR与朴素贝叶斯" class="headerlink" title="5.LR与朴素贝叶斯"></a>5.LR与朴素贝叶斯</h2><ul><li><p>相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)形式跟逻辑回归是一样的。</p></li><li><p>不同的地方在于，逻辑回归为判别模型求的是p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。</p></li></ul><h2 id="6-多分类-softmax"><a href="#6-多分类-softmax" class="headerlink" title="6. 多分类-softmax"></a>6. 多分类-softmax</h2><p>如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。</p><h2 id="7-LR模型在工业界的应用"><a href="#7-LR模型在工业界的应用" class="headerlink" title="7.LR模型在工业界的应用"></a><strong>7.LR模型在工业界的应用</strong></h2><p>常见应用场景</p><ul><li><p>预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等）</p></li><li><p>分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等）</p></li></ul><p>LR适用上述场景的原因</p><p>LR模型自身的特点具备了应用广泛性</p><ul><li><p>模型易用：LR模型建模思路清晰，容易理解与掌握；</p></li><li><p>概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上；</p></li><li><p>强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性；</p></li><li><p>简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline；</p></li></ul><p>参考文献：<br><a href="https://blog.csdn.net/joycewyj/article/details/51596797" target="_blank" rel="noopener">https://blog.csdn.net/joycewyj/article/details/51596797</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://frankblog.site/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib基本操作</title>
    <link href="http://frankblog.site/2018/06/03/Matplotlib%20%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://frankblog.site/2018/06/03/Matplotlib 基本操作/</id>
    <published>2018-06-03T03:59:17.192Z</published>
    <updated>2018-06-04T01:42:15.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">一张图胜过一千句话</font></blockquote><hr><h1 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h1><h2 id="1、设置坐标轴"><a href="#1、设置坐标轴" class="headerlink" title="1、设置坐标轴"></a>1、设置坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line"></span><br><span class="line">#使用`plt.figure`定义一个图像窗口. 使用`plt.plot`画(`x` ,`y2`)曲线. 使用`plt.plot`画(`x` ,`y1`)曲线，曲线的颜色属性(`color`)为红色;曲线的宽度(`linewidth`)为1.0；曲线的类型(`linestyle`)为虚线。</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line"></span><br><span class="line">#使用`plt.xlim`设置x坐标轴范围：(-1, 2)； 使用`plt.ylim`设置y坐标轴范围：(-2, 3)； 使用`plt.xlabel`设置x坐标轴名称：’I am x’； 使用`plt.ylabel`设置y坐标轴名称：’I am y’；</span><br><span class="line"></span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">plt.xlabel(&apos;I am x&apos;)</span><br><span class="line">plt.ylabel(&apos;I am y&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`np.linspace`定义范围以及个数：范围是(-1,2);个数是5\. 使用`print`打印出新定义的范围. 使用`plt.xticks`设置x轴刻度：范围是(-1,2);个数是5.</span><br><span class="line"></span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line"></span><br><span class="line">#使用`plt.yticks`设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用`plt.show`显示图像.</span><br><span class="line"></span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[r&apos;$really\ bad$&apos;, r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;, r&apos;$really\ good$&apos;])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%BE%E7%BD%AE%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="2、调整坐标轴"><a href="#2、调整坐标轴" class="headerlink" title="2、调整坐标轴"></a>2、调整坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[&apos;$really\ bad$&apos;, &apos;$bad$&apos;, &apos;$normal$&apos;, &apos;$good$&apos;, &apos;$really\ good$&apos;])</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.xaxis.set_ticks_position`设置x坐标刻度数字或名称的位置：`bottom`.（所有位置：`top`，`bottom`，`both`，`default`，`none`）</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：x轴；使用`.set_position`设置边框位置：y=0的位置；（位置所有属性：`outward`，`axes`，`data`）</span><br><span class="line"></span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`.yaxis.set_ticks_position`设置y坐标刻度数字或名称的位置：`left`.（所有位置：`left`，`right`，`both`，`default`，`none`）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：y轴；使用`.set_position`设置边框位置：x=0的位置；（位置所有属性：`outward`，`axes`，`data`） 使用`plt.show`显示图像。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># set line syles</span><br><span class="line">l1, = plt.plot(x, y1, label=&apos;linear line&apos;)</span><br><span class="line">l2, = plt.plot(x, y2, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;, label=&apos;square line&apos;)</span><br><span class="line"></span><br><span class="line">#参数 `loc=&apos;upper right&apos;` 表示图例将添加在图中的右上角.</span><br><span class="line">plt.legend(loc=&apos;upper right&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%AE%E6%94%B9%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="3、辅助线和标识"><a href="#3、辅助线和标识" class="headerlink" title="3、辅助线和标识"></a>3、辅助线和标识</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y = 2*x + 1</span><br><span class="line"></span><br><span class="line">#挪动坐标系</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line">#辅助线</span><br><span class="line">plt.figure(num=1, figsize=(8, 5),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line">x0 = 1</span><br><span class="line">y0 = 2*x0 + 1</span><br><span class="line">plt.plot([x0, x0,], [0, y0,], &apos;k--&apos;, linewidth=2.5)</span><br><span class="line"># set dot styles</span><br><span class="line">plt.scatter([x0, ], [y0, ], s=50, color=&apos;b&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#标注，其中参数xycoords=&apos;data&apos; 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords=&apos;offset points&apos; 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置.</span><br><span class="line">plt.annotate(r&apos;$2x+1=%s$&apos; % y0, xy=(x0, y0), xycoords=&apos;data&apos;, xytext=(+30, -30),</span><br><span class="line">             textcoords=&apos;offset points&apos;, fontsize=16,</span><br><span class="line">             arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">#注释</span><br><span class="line">plt.text(-3.7, 3, r&apos;$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$&apos;,</span><br><span class="line">         fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BE%85%E5%8A%A9%E7%BA%BF%E5%92%8C%E6%A0%87%E8%AF%86.png" alt=""></p><h2 id="4、3D图框"><a href="#4、3D图框" class="headerlink" title="4、3D图框"></a>4、3D图框</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line"># X, Y value</span><br><span class="line">X = np.arange(-4, 4, 0.25)</span><br><span class="line">Y = np.arange(-4, 4, 0.25)</span><br><span class="line">X, Y = np.meshgrid(X, Y)    # x-y 平面的网格</span><br><span class="line">R = np.sqrt(X ** 2 + Y ** 2)</span><br><span class="line"># height value</span><br><span class="line">Z = np.sin(R)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/3d%E5%9B%BE.png" alt=""></p><h1 id="绘制动态图"><a href="#绘制动态图" class="headerlink" title="绘制动态图"></a>绘制动态图</h1><p>使用matplotlib为Jupyter / IPython中的动画图创建一些选项：</p><ul><li><p><strong>在循环中使用<code>display</code></strong>使用<code>IPython.display.display(fig)</code>在输出中显示图形。 使用一个循环，你需要在显示一个新数字之前清除输出。 请注意，这种技术通常不会那么流畅。 因此我会建议使用下面的任何一个。</p></li><li><p><strong><code>%matplotlib notebook</code></strong>使用IPython magic <code>%matplotlib notebook</code>将后端设置为笔记本后端。 这样可以保持图形不会显示静态PNG文件，因此也可以显示动画。 </p></li></ul><ul><li><strong><code>%matplotlib tk</code></strong>使用IPython magic <code>%matplotlib tk</code>将后端设置为tk后端。 这将在一个新的绘图窗口中打开这个图形，这是一个互动的，因此也可以显示动画。 </li></ul><ul><li><p><strong>将动画转换为mp4视频</strong> （已提供@Perfi选项）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_html5_video())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;html5&quot;</code> 。 这需要将ffmpeg视频编解码器转换为HTML5视频。 视频然后显示在内。 因此，这与<code>%matplotlib inline</code>后端兼容。 完整的例子：</p><ul><li><p><strong>将动画转换为JavaScript</strong> ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot;</code> 。 这将使用JavaScript将动画显示为HTML。 这与大多数新浏览器以及<code>%matplotlib inline</code>后端都非常兼容。 它在matplotlib 2.1或更高版本中可用。</p><h2 id="1、sin动态点曲线"><a href="#1、sin动态点曲线" class="headerlink" title="1、sin动态点曲线"></a>1、sin动态点曲线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">animation example 2</span><br><span class="line">author: Kiterun</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">x = np.linspace(0, 2*np.pi, 200)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">l = ax.plot(x, y)</span><br><span class="line">dot, = ax.plot([], [], &apos;ro&apos;)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(0, 2*np.pi)</span><br><span class="line">    ax.set_ylim(-1, 1)</span><br><span class="line">    return l</span><br><span class="line"></span><br><span class="line">def gen_dot():</span><br><span class="line">    for i in np.linspace(0, 2*np.pi, 200):</span><br><span class="line">        newdot = [i, np.sin(i)]</span><br><span class="line">        yield newdot</span><br><span class="line"></span><br><span class="line">def update_dot(newd):</span><br><span class="line">    dot.set_data(newd[0], newd[1])</span><br><span class="line">    return dot,</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update_dot, frames = gen_dot, interval = 100, init_func=init)</span><br><span class="line">ani.save(&apos;sin_dot.gif&apos;, writer=&apos;imagemagick&apos;, fps=30)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/sin_dot%20%281%29.gif" alt=""></p><h2 id="2、动态雨点"><a href="#2、动态雨点" class="headerlink" title="2、动态雨点"></a>2、动态雨点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line"># New figure with white background</span><br><span class="line">fig = plt.figure(figsize=(6,6), facecolor=&apos;white&apos;)</span><br><span class="line"></span><br><span class="line"># New axis over the whole figure, no frame and a 1:1 aspect ratio</span><br><span class="line">ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)</span><br><span class="line"></span><br><span class="line"># Number of ring</span><br><span class="line">n = 50</span><br><span class="line">size_min = 50</span><br><span class="line">size_max = 50 ** 2</span><br><span class="line"></span><br><span class="line"># Ring position</span><br><span class="line">pos = np.random.uniform(0, 1, (n,2))</span><br><span class="line"></span><br><span class="line"># Ring colors</span><br><span class="line">color = np.ones((n,4)) * (0,0,0,1)</span><br><span class="line"># Alpha color channel geos from 0(transparent) to 1(opaque)</span><br><span class="line">color[:,3] = np.linspace(0, 1, n)</span><br><span class="line"></span><br><span class="line"># Ring sizes</span><br><span class="line">size = np.linspace(size_min, size_max, n)</span><br><span class="line"></span><br><span class="line"># Scatter plot</span><br><span class="line">scat = ax.scatter(pos[:,0], pos[:,1], s=size, lw=0.5, edgecolors=color, facecolors=&apos;None&apos;)</span><br><span class="line"></span><br><span class="line"># Ensure limits are [0,1] and remove ticks</span><br><span class="line">ax.set_xlim(0, 1), ax.set_xticks([])</span><br><span class="line">ax.set_ylim(0, 1), ax.set_yticks([])</span><br><span class="line"></span><br><span class="line">def update(frame):</span><br><span class="line">    global pos, color, size</span><br><span class="line"></span><br><span class="line">    # Every ring is made more transparnt</span><br><span class="line">    color[:, 3] = np.maximum(0, color[:,3]-1.0/n)</span><br><span class="line"></span><br><span class="line">    # Each ring is made larger</span><br><span class="line">    size += (size_max - size_min) / n</span><br><span class="line"></span><br><span class="line">    # Reset specific ring</span><br><span class="line">    i = frame % 50</span><br><span class="line">    pos[i] = np.random.uniform(0, 1, 2)</span><br><span class="line">    size[i] = size_min</span><br><span class="line">    color[i, 3] = 1</span><br><span class="line"></span><br><span class="line">    # Update scatter object</span><br><span class="line">    scat.set_edgecolors(color)</span><br><span class="line">    scat.set_sizes(size)</span><br><span class="line">    scat.set_offsets(pos)</span><br><span class="line"></span><br><span class="line">    # Return the modified object</span><br><span class="line">    return scat,</span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, update, interval=10, blit=True, frames=200)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8A%A8%E6%80%81%E9%9B%A8%E7%82%B9.png" alt=""></p><h2 id="3、阻尼摆"><a href="#3、阻尼摆" class="headerlink" title="3、阻尼摆"></a>3、阻尼摆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%98%BB%E5%B0%BC%E6%91%86.png" alt=""></p><h2 id="4、内切滚动球"><a href="#4、内切滚动球" class="headerlink" title="4、内切滚动球"></a>4、内切滚动球</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/roll.gif" alt=""></p><h2 id="5、分类超平面可视化"><a href="#5、分类超平面可视化" class="headerlink" title="5、分类超平面可视化"></a>5、分类超平面可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"># 算法可视化</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">%matplotlib notebook</span><br><span class="line">import copy</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"> </span><br><span class="line">training_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]]</span><br><span class="line">w = [0, 0]</span><br><span class="line">b = 0</span><br><span class="line">history = []</span><br><span class="line"> </span><br><span class="line">def update(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    update parameters using stochastic gradient descent</span><br><span class="line">    :param item: an item which is classified into wrong class</span><br><span class="line">    :return: nothing</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    global w, b, history</span><br><span class="line">    w[0] += 1 * item[1] * item[0][0]</span><br><span class="line">    w[1] += 1 * item[1] * item[0][1]</span><br><span class="line">    b += 1 * item[1]</span><br><span class="line">    print(w, b)</span><br><span class="line">    history.append([copy.copy(w), b])</span><br><span class="line">    # you can uncomment this line to check the process of stochastic gradient descent</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def cal(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    calculate the functional distance between &apos;item&apos; an the dicision surface. output yi(w*xi+b).</span><br><span class="line">    :param item:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    res = 0</span><br><span class="line">    for i in range(len(item[0])):</span><br><span class="line">        res += item[0][i] * w[i]</span><br><span class="line">    res += b</span><br><span class="line">    res *= item[1]</span><br><span class="line">    return res</span><br><span class="line"> </span><br><span class="line">def check():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    check if the hyperplane can classify the examples correctly</span><br><span class="line">    :return: true if it can</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    flag = False</span><br><span class="line">    for item in training_set:</span><br><span class="line">        if cal(item) &lt;= 0:</span><br><span class="line">            flag = True</span><br><span class="line">            update(item)</span><br><span class="line">    # draw a graph to show the process</span><br><span class="line">    if not flag:</span><br><span class="line">        print (&quot;RESULT: w: &quot; + str(w) + &quot; b: &quot; + str(b))</span><br><span class="line">    return flag</span><br><span class="line"> </span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    for i in range(1000):</span><br><span class="line">        if not check(): break</span><br><span class="line"> </span><br><span class="line">    # first set up the figure, the axis, and the plot element we want to animate</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = plt.axes(xlim=(0, 2), ylim=(-2, 2))</span><br><span class="line">    line, = ax.plot([], [], &apos;g&apos;, lw=2)</span><br><span class="line">    label = ax.text([], [], &apos;&apos;)</span><br><span class="line"> </span><br><span class="line">    # initialization function: plot the background of each frame</span><br><span class="line">    def init():</span><br><span class="line">        line.set_data([], [])</span><br><span class="line">        x, y, x_, y_ = [], [], [], []</span><br><span class="line">        for p in training_set:</span><br><span class="line">            if p[1] &gt; 0:</span><br><span class="line">                x.append(p[0][0])</span><br><span class="line">                y.append(p[0][1])</span><br><span class="line">            else:</span><br><span class="line">                x_.append(p[0][0])</span><br><span class="line">                y_.append(p[0][1])</span><br><span class="line"> </span><br><span class="line">        plt.plot(x, y, &apos;bo&apos;, x_, y_, &apos;rx&apos;)</span><br><span class="line">        plt.axis([-6, 6, -6, 6])</span><br><span class="line">        plt.grid(True)</span><br><span class="line">        plt.xlabel(&apos;x&apos;)</span><br><span class="line">        plt.ylabel(&apos;y&apos;)</span><br><span class="line">        plt.title(&apos;Perceptron Algorithm&apos;)</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    # animation function.  this is called sequentially</span><br><span class="line">    def animate(i):</span><br><span class="line">        global history, ax, line, label</span><br><span class="line"> </span><br><span class="line">        w = history[i][0]</span><br><span class="line">        b = history[i][1]</span><br><span class="line">        if w[1] == 0: return line, label</span><br><span class="line">        x1 = -7</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        x2 = 7</span><br><span class="line">        y2 = -(b + w[0] * x2) / w[1]</span><br><span class="line">        line.set_data([x1, x2], [y1, y2])</span><br><span class="line">        x1 = 0</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        label.set_text(history[i])</span><br><span class="line">        label.set_position([x1, y1])</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line">    # call the animator.  blit=true means only re-draw the parts that have changed.</span><br><span class="line">    print (history)</span><br><span class="line">    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=1000, repeat=True,</span><br><span class="line">                                   blit=True)</span><br><span class="line">    plt.show()</span><br><span class="line">    anim.save(&apos;perceptron.gif&apos;, fps=2, writer=&apos;imagemagick&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/perceptron1.gif" alt=""></p><h1 id="python其他可视化模块"><a href="#python其他可视化模块" class="headerlink" title="python其他可视化模块"></a>python其他可视化模块</h1><ul><li>Traits-为Python添加类型定义</li><li>TraitsUI-制作用户界面</li><li>Chaco-交互式图表</li><li>TVTK-三维可视化数据</li><li>Visual-制作3D演示动画</li><li>Mayavi-更方便的可视化</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://frankblog.site/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="Matplotlib" scheme="http://frankblog.site/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>markdown公式编辑</title>
    <link href="http://frankblog.site/2018/06/03/markdown%E5%85%AC%E5%BC%8F%E7%BC%96%E8%BE%91/"/>
    <id>http://frankblog.site/2018/06/03/markdown公式编辑/</id>
    <published>2018-06-03T03:42:27.605Z</published>
    <updated>2018-06-04T01:33:58.285Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/mathjax1.png" alt=""></p><a id="more"></a><hr><h1 id="加载mathjax"><a href="#加载mathjax" class="headerlink" title="加载mathjax"></a>加载mathjax</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>引入脚本对网页进行渲染<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="插入方式"><a href="#插入方式" class="headerlink" title="插入方式"></a>插入方式</h2><blockquote><p>这里分两种，一种是行间插入，另一种是另取一行</p></blockquote><h3 id="行内插入"><a href="#行内插入" class="headerlink" title="行内插入"></a>行内插入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(a+b\\)</span><br></pre></td></tr></table></figure><p>\(a+b\)</p><h3 id="单独一行"><a href="#单独一行" class="headerlink" title="单独一行"></a>单独一行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$a + b$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">a + b</script><h2 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h2><h3 id="上、下标"><a href="#上、下标" class="headerlink" title="上、下标"></a>上、下标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$x_1$$</span><br><span class="line"></span><br><span class="line">$$x_1^2$$</span><br><span class="line"></span><br><span class="line">$$x^2_1$$</span><br><span class="line"></span><br><span class="line">$$x_&#123;22&#125;^&#123;(n)&#125;$$ #多于一位是要加 `&#123;&#125;` 包裹的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$$x_&#123;balabala&#125;^&#123;bala&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">x_1</script><script type="math/tex; mode=display">x_1^2</script><script type="math/tex; mode=display">x^2_1</script><script type="math/tex; mode=display">x_{22}^{(n)}</script><script type="math/tex; mode=display">x_{balabala}^{bala}</script><h3 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$\frac&#123;x+y&#125;&#123;2&#125;$$</span><br><span class="line"></span><br><span class="line">$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\frac{x+y}{2}</script><script type="math/tex; mode=display">\frac{1}{1+\frac{1}{2}}</script><h3 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\sqrt{2}<\sqrt[3]{3}</script><script type="math/tex; mode=display">\sqrt{1+\sqrt[p]{1+a^2}}</script><script type="math/tex; mode=display">\sqrt{1+\sqrt[^p\!]{1+a^2}}</script><h3 id="求和、积分"><a href="#求和、积分" class="headerlink" title="求和、积分"></a>求和、积分</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$\sum_&#123;k=1&#125;^&#123;n&#125;\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class="line"></span><br><span class="line">\\(\sum_&#123;k=1&#125;^n\frac&#123;1&#125;&#123;k&#125;\\)</span><br><span class="line"></span><br><span class="line">$$\int_a^b f(x)dx$$</span><br><span class="line"></span><br><span class="line">\\(\int_a^b f(x)dx\\)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\sum_{k=1}^{n}\frac{1}{k}</script><p>\(\sum_{k=1}^n\frac{1}{k}\)</p><script type="math/tex; mode=display">\int_a^b f(x)dx</script><p>\(\int_a^b f(x)dx\)</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">紧贴 $a\\!b$</span><br><span class="line"></span><br><span class="line">没有空格 $ab$</span><br><span class="line"></span><br><span class="line">小空格 a\,b</span><br><span class="line"></span><br><span class="line">中等空格 a\;b</span><br><span class="line"></span><br><span class="line">大空格 a\ b</span><br><span class="line"></span><br><span class="line">quad空格 $a\quad b$</span><br><span class="line"></span><br><span class="line">两个quad空格 $a\qquad b$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">a\\!b</script><script type="math/tex; mode=display">{ab}</script><script type="math/tex; mode=display">a\,b</script><script type="math/tex; mode=display">a\;b</script><script type="math/tex; mode=display">a\ b</script><script type="math/tex; mode=display">a\quad b</script><script type="math/tex; mode=display">a\qquad b</script><h3 id="公式界定符"><a href="#公式界定符" class="headerlink" title="公式界定符"></a>公式界定符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\left(\sum_&#123;k=\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\frac&#123;1&#125;&#123;k&#125;\right)$$</span><br></pre></td></tr></table></figure><p>通过 <code>\left</code> 和 <code>\right</code> 后面跟界定符来对同时进行界定。</p><script type="math/tex; mode=display">\left(\sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k}\right)</script><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;pmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;vmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\left|\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;\right|$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Vmatrix&#125;$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{matrix}1 & 2\\\\3 &4\end{matrix}</script><script type="math/tex; mode=display">\begin{pmatrix}1 & 2\\\\3 &4\end{pmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}1 & 2\\\\3 &4\end{bmatrix}</script><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2\\\\3 &4\end{Bmatrix}</script><script type="math/tex; mode=display">\begin{vmatrix}1 & 2\\\\3 &4\end{vmatrix}</script><script type="math/tex; mode=display">\left|\begin{matrix}1 & 2\\\\3 &4\end{matrix}\right|</script><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2\\\\3 &4\end{Vmatrix}</script><p>类似于 left right，这里是 begin 和 end。而且里面有具体的矩阵语法，<code>&amp;</code> 区分行间元素，<code>\\\\</code> 代表换行。可以理解为 HTML 的标签之类的。</p><h3 id="排版数组"><a href="#排版数组" class="headerlink" title="排版数组"></a>排版数组</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;X&#125; =</span><br><span class="line">\left( \begin&#123;array&#125;&#123;ccc&#125;</span><br><span class="line">x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\\</span><br><span class="line">x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots</span><br><span class="line">\end&#123;array&#125; \right)</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{ccc}x\_{11} & x\_{12} & \ldots \\\\x\_{21} & x\_{22} & \ldots \\\\\vdots & \vdots & \ddots\end{array} \right)</script><h1 id="常用公式举例"><a href="#常用公式举例" class="headerlink" title="常用公式举例"></a>常用公式举例</h1><h2 id="公式组"><a href="#公式组" class="headerlink" title="公式组"></a>公式组</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">a &amp;= b+c+d \\\\</span><br><span class="line"></span><br><span class="line">x &amp;= y+z</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{align}a &= b+c+d \\\\x &= y+z\end{align}</script><h2 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">y=\begin&#123;cases&#125;</span><br><span class="line">-x,\quad x\leq 0 \\\\</span><br><span class="line">x,\quad x&gt;0</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">y=\begin{cases}-x,\quad x\leq 0 \\\\x,\quad x>0\end{cases}</script><h1 id="常用数学符号"><a href="#常用数学符号" class="headerlink" title="常用数学符号"></a>常用数学符号</h1><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line">&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|c|c|c|c|}\hline{\alpha} & {\backslash alpha} & {\theta} & {\backslash theta} & {o} & {o} & {\upsilon} & {\backslash upsilon} \\\\\hline{\beta} & {\backslash beta} & {\vartheta} & {\backslash vartheta} & {\pi} & {\backslash pi} & {\phi} & {\backslash phi} \\\\\hline{\gamma} & {\backslash gamma} & {\iota} & {\backslash iota} & {\varpi} & {\backslash varpi} & {\varphi} & {\backslash varphi} \\\\\hline{\delta} & {\backslash delta} & {\kappa} & {\backslash kappa} & {\rho} & {\backslash rho} & {\chi} & {\backslash chi} \\\\\hline{\epsilon} & {\backslash epsilon} & {\lambda} & {\backslash lambda} & {\varrho} & {\backslash varrho} & {\psi} & {\backslash psi} \\\\\hline{\varepsilon} & {\backslash varepsilon} & {\mu} & {\backslash mu} & {\sigma} & {\backslash sigma} & {\omega} & {\backslash omega} \\\\\hline{\zeta} & {\backslash zeta} & {\nu} & {\backslash nu} & {\varsigma} & {\backslash varsigma} & {} & {} \\\\\hline{\eta} & {\backslash eta} & {\xi} & {\backslash xi} & {\tau} & {\backslash tau} & {} & {} \\\\\hline{\Gamma} & {\backslash Gamma} & {\Lambda} & {\backslash Lambda} & {\Sigma} & {\backslash Sigma} & {\Psi} & {\backslash Psi} \\\\\hline{\Delta} & {\backslash Delta} & {\Xi} & {\backslash Xi} & {\Upsilon} & {\backslash Upsilon} & {\Omega} & {\backslash Omega} \\\\\hline{\Omega} & {\backslash Omega} & {\Pi} & {\backslash Pi} & {\Phi} & {\backslash Phi} & {} & {} \\\\\hline\end{array}</script><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://blog.csdn.net/xiahouzuoxin/article/details/26478179" target="_blank" rel="noopener">Markdown中插入数学公式的方法</a></li><li><a href="http://www.cnblogs.com/houkai/p/3399646.html" target="_blank" rel="noopener">LATEX数学公式基本语法</a></li><li><a href="https://liam0205.me/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/mathjax1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="写作" scheme="http://frankblog.site/categories/%E5%86%99%E4%BD%9C/"/>
    
    
      <category term="markdown" scheme="http://frankblog.site/tags/markdown/"/>
    
      <category term="LaTex" scheme="http://frankblog.site/tags/LaTex/"/>
    
  </entry>
  
  <entry>
    <title>SVM可视化</title>
    <link href="http://frankblog.site/2018/06/02/svm%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://frankblog.site/2018/06/02/svm可视化/</id>
    <published>2018-06-02T08:02:15.545Z</published>
    <updated>2018-06-06T01:54:17.664Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4]])</span><br><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Decision Boundary&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;r&apos;)</span><br><span class="line">plt.plot([3, 3], [0.5, 6], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, 0, shape=&apos;full&apos;, color=&apos;b&apos;, linestyle=&apos;--&apos;)</span><br><span class="line">plt.annotate(r&apos;margin 1&apos;,</span><br><span class="line">             xy=(3.5, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(3.1, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;margin 2&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4, 3.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(2, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(0.5, 1.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Support Vector Machine&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 0$&apos;,</span><br><span class="line">             xy=(5, 1), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(6, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 1$&apos;,</span><br><span class="line">             xy=(6, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(7, 2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = -1$&apos;,</span><br><span class="line">             xy=(3.5, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4.5, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;d&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(2, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;A&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM%E7%94%BB%E5%9B%BE1.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=140)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=100, </span><br><span class="line"> n_features=2, </span><br><span class="line"> centers=[(1, 1), (2, 2)], </span><br><span class="line"> random_state=4, </span><br><span class="line"> shuffle=False,</span><br><span class="line"> cluster_std=0.4)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 3)</span><br><span class="line">plt.ylim(0, 3)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([0.5, 2.5], [2.5, 0.5], &apos;-r&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2], [1.5, 1.5], [1.2, 1.7]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4], [5.5, 3.5], [4.5, 4.5], [2, 1.5]])</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Slack Variable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 7)</span><br><span class="line">plt.ylim(0, 7)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(2, 1.5, 2.25, 2.25, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;violate margin rule.&apos;,</span><br><span class="line"> xy=(2, 1.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(0.2, 0.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;normal sample. $\epsilon = 0$&apos;,</span><br><span class="line"> xy=(4, 5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(4.5, 5.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$\epsilon &gt; 0$&apos;,</span><br><span class="line"> xy=(3, 2.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(3, 1.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Cost&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 4)</span><br><span class="line">plt.ylim(0, 2)</span><br><span class="line">plt.xlabel(&apos;$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$&apos;)</span><br><span class="line">plt.ylabel(&apos;Cost&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [1.5, 0], &apos;-r&apos;)</span><br><span class="line">plt.plot([1, 3], [0.015, 0.015], &apos;-r&apos;)</span><br><span class="line">plt.annotate(r&apos;$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$&apos;,</span><br><span class="line">             xy=(0.7, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$&apos;,</span><br><span class="line">             xy=(1.5, 0), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1.8, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE3.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=144)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [3, 2], [4, 1], [5, 1]])</span><br><span class="line">class2 = np.array([[2.2, 4], [1.5, 5], [1.8, 4.6], [2.4, 5], [3.2, 5], [3.7, 4], [4.5, 4.5], [5.4, 3]])</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable in Low Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], np.zeros(class1[:, 0].shape[0]) + 0.05, marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], np.zeros(class2[:, 0].shape[0]) + 0.05, marker=&apos;s&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Linear Separatable in High Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">plt.ylabel(&apos;X2&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [3.8, 2], &apos;-r&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE4.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def gaussian_kernel(x, mean, sigma):</span><br><span class="line"> return np.exp(- (x - mean)**2 / (2 * sigma**2))</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 6, 500)</span><br><span class="line">mean = 1</span><br><span class="line">sigma1 = 0.1</span><br><span class="line">sigma2 = 0.3</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 3), dpi=144)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma1))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma1), &apos;r-&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma2))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma2), &apos;r-&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE5.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="SVM" scheme="http://frankblog.site/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？</title>
    <link href="http://frankblog.site/2018/06/01/%E5%A6%82%E4%BD%95%E7%94%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E6%AF%94%E5%96%BB%E6%8F%8F%E8%BF%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%EF%BC%9FHadoop%E3%80%81Hive%E3%80%81Spark%20%E4%B9%8B%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F/"/>
    <id>http://frankblog.site/2018/06/01/如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？/</id>
    <published>2018-06-01T12:06:43.304Z</published>
    <updated>2018-06-08T11:34:11.717Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？<br>这是知乎上某大神的解释：</p><p><strong>学习很重要的是能将纷繁复杂的信息进行归类和抽象。</strong><br>对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题：</p><ul><li>存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka；</li><li>计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等；</li><li>查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等；</li></ul><p><strong>大数据技术生态其实是一个江湖….</strong><br>在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…<br>这三本秘籍分别为：</p><ul><li>《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据；</li><li>《Google MapReduce》：论述了怎样快速计算海量的数据；</li><li>《Google BigTable》：论述了怎样实现海量数据的快速查询；</li></ul><p><strong>以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术；</strong></p><p>在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。<br>Hadoop包括三大部分，分别是hdfs、MapReduce和hbase：</p><ul><li>hdfs解决大数据的存储问题。</li><li>mapreduce解决大数据的计算问题。</li><li>hbase解决大数据量的查询问题。</li></ul><p>之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进：</p><ul><li>1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍；</li><li>2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。</li><li>3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。</li><li>4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼；</li></ul><p>原文地址：<a href="https://www.zhihu.com/question/27974418/answer/156227565" target="_blank" rel="noopener">知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://frankblog.site/tags/hadoop/"/>
    
      <category term="spark" scheme="http://frankblog.site/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降法总结</title>
    <link href="http://frankblog.site/2018/06/01/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://frankblog.site/2018/06/01/梯度下降法总结/</id>
    <published>2018-06-01T08:20:06.419Z</published>
    <updated>2018-06-02T07:09:29.464Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度实际上就是多变量微分的一般化。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_0.png" alt="link"></p><p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p><ul><li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li><li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2.png" alt="link"></p><p>此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！<br>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_1.png" alt="link"></p><h1 id="多元函数的梯度下降"><a href="#多元函数的梯度下降" class="headerlink" title="多元函数的梯度下降"></a>多元函数的梯度下降</h1><p>我们假设有一个目标函数</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2.png" alt="多元函数的梯度下降"></p><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_1.png" alt="image.png"></p><p>初始的学习率为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_2.png" alt="image.png"></p><p>函数的梯度为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_3.png" alt="image.png"></p><p>进行多次迭代：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_4.png" alt="image.png"></p><p>我们发现，已经基本靠近函数的最小值点</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_5.png" alt="image.png"></p><h1 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h1><h2 id="选择合适的学习速率"><a href="#选择合适的学习速率" class="headerlink" title="选择合适的学习速率"></a>选择合适的学习速率</h2><p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg" alt=""></p><p>假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果 learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。</p><p>虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。</p><h2 id="自适应学习速率"><a href="#自适应学习速率" class="headerlink" title="自适应学习速率"></a>自适应学习速率</h2><p>举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate</p><ul><li>通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate</li><li>update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate</li><li>比如 \(\eta^{t} = \eta / \sqrt{t+1}\)，t是次数。随着次数的增加，\(η_t\)减小</li></ul><h1 id="Feature-Scaling（特征缩放）"><a href="#Feature-Scaling（特征缩放）" class="headerlink" title="Feature Scaling（特征缩放）"></a>Feature Scaling（特征缩放）</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.jpg" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE1.jpg" alt=""></p><p>图左边是\(x_{1}\)的scale比的scale比\(x_{2}\)要小很多，所以当要小很多，所以当\(w_{1}\)和和\(w_{2}\)做同样的变化时，做同样的变化时，\(w_{1}\)对y的变化影响是比较小的，对y的变化影响是比较小的，\(x_{2}\)对y的变化影响是比较大的。</p><p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为\(w_{1}\)对y的变化影响比较小，所以对y的变化影响比较小，所以\(w_{1}\)对损失函数的影响比较小，对损失函数的影响比较小，\(w_{1}\)对损失函数有比较小的微分，所以对损失函数有比较小的微分，所以vw_{1}\)方向上是比较平滑的。同理方向上是比较平滑的。同理\(x_{2}\)对y的影响比较大，所以对y的影响比较大，所以\(x_{2}\)对损失函数的影响比较大，所以在对损失函数的影响比较大，所以在\(x_{2}\)方向有比较尖的峡谷。</p><p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p><p>对于左边的情况，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p><h1 id="常见的算法"><a href="#常见的算法" class="headerlink" title="常见的算法"></a>常见的算法</h1><ul><li><strong>批量梯度下降</strong>：批量梯度下降每次更新使用了所有的训练数据。<strong>如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，</strong>但是缺点是如果样本值很大的话，更新速度会很慢。</li><li><strong>随机梯度下降</strong>：随机也就是说用一个样本的梯度来近似所有的样本，来调整θ，这样会大大加快训练数据，但是有可能由于训练数据的噪声点较多。<strong>每一次利用噪声点进行更新的过程中，不一定是朝着极小值方向更新，但是由于多次迭代，整体方向还是大致朝着极小值方向更新，提高了速度。</strong></li><li><strong>小批量梯度下降</strong>：小批量梯度下降法是<strong>为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的</strong>。</li></ul><h2 id="批量梯度下降代码"><a href="#批量梯度下降代码" class="headerlink" title="批量梯度下降代码"></a>批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line">#这里更新权重的时候所有的样本点都用上了</span><br><span class="line">    for i in range (3):</span><br><span class="line"> pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line"> theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"></span><br><span class="line">iters_count 219</span><br><span class="line">iters_count 220</span><br><span class="line">iters_count 221</span><br><span class="line">iters_count 222</span><br><span class="line">iters_count 223</span><br><span class="line">iters_count 224</span><br><span class="line">iters_count 225</span><br><span class="line">theta: [3.0027765778748003, 3.997918297015663]</span><br><span class="line">final loss: 9.68238055213e-05</span><br><span class="line">iters: 225</span><br><span class="line">[Finished in 0.2s]</span><br></pre></td></tr></table></figure><h2 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码"></a>随机梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 每次选取一个值,随机一个点更新 θ</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"> #每一次选取随机的一个点进行权重的更新</span><br><span class="line">    i = random.randint(0,3)</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#output:</span><br><span class="line">iters_count 1226</span><br><span class="line">iters_count 1227</span><br><span class="line">iters_count 1228</span><br><span class="line">iters_count 1229</span><br><span class="line">iters_count 1230</span><br><span class="line">iters_count 1231</span><br><span class="line">iters_count 1232</span><br><span class="line">theta: [3.002441488688225, 3.9975844154600226]</span><br><span class="line">final loss: 9.989420302e-05</span><br><span class="line">iters: 1232</span><br><span class="line">[Finished in 0.3s]</span><br></pre></td></tr></table></figure><h2 id="小批量梯度下降代码"><a href="#小批量梯度下降代码" class="headerlink" title="小批量梯度下降代码"></a>小批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 这里用2个样本点</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"></span><br><span class="line">    i = random.randint(0,3) #注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span><br><span class="line">    j = (i+1)%4</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[0]*input_x[j][0]+theta[1]*input_x[j][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[j]) * input_x[j][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[j]) * input_x[j][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line">iters_count 543</span><br><span class="line">iters_count 544</span><br><span class="line">iters_count 545</span><br><span class="line">iters_count 546</span><br><span class="line">iters_count 547</span><br><span class="line">iters_count 548</span><br><span class="line">iters_count 549</span><br><span class="line">theta: [3.0023012574840764, 3.997553282857357]</span><br><span class="line">final loss: 9.81717138358e-05</span><br><span class="line">iters: 549</span><br></pre></td></tr></table></figure><h1 id="梯度下降的局限"><a href="#梯度下降的局限" class="headerlink" title="梯度下降的局限"></a>梯度下降的局限</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%B1%80%E9%99%90.jpg" alt=""></p><ul><li>容易陷入局部极值</li><li>还有可能卡在不是极值，但微分值是0的地方</li><li>还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</li></ul><p>参考：<a href="https://zhuanlan.zhihu.com/qinlibo-ml" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/qinlibo-ml</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="优化算法" scheme="http://frankblog.site/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
