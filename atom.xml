<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frank&#39;s Blog</title>
  
  <subtitle>Enjoy everything fun and challenging</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://frankblog.site/"/>
  <updated>2018-06-08T02:01:56.248Z</updated>
  <id>http://frankblog.site/</id>
  
  <author>
    <name>FGY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅谈分布式</title>
    <link href="http://frankblog.site/2018/06/07/%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://frankblog.site/2018/06/07/浅谈分布式/</id>
    <published>2018-06-07T12:15:05.369Z</published>
    <updated>2018-06-08T02:01:56.248Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="分布式基础"><a href="#分布式基础" class="headerlink" title="分布式基础"></a>分布式基础</h2><p>分布式，按字面意思就是并行计算，对于一些串行的计算可以运用。一些本来就并行的就不可以。</p><p>之前聊起这个，就会说mapreduce，就是把任务分拆开来，最后计算的时候再合并。但后来发现这样的回答太水了，看了几篇介绍分布式计算的PPT，特此写下来分布式到底是怎么一回事。</p><p>我们用一个矩阵运算作为例子：</p><p>对于两个矩阵，A和B：</p><p>A = \(\begin{pmatrix}1 &amp; 2 &amp; 3\\  4 &amp; 5 &amp;0 \\  7 &amp; 8&amp;9\\  10 &amp; 11&amp;12\end{pmatrix}\)</p><p>B =  \(\begin{pmatrix}10 &amp; 15\\  0 &amp;2 \\ 11 &amp;9\end{pmatrix}\)</p><p>有</p><p>C = AB = \(\begin{pmatrix}43 &amp; 46\\  40 &amp;70 \\ 169 &amp;202\\ 232 &amp;280\end{pmatrix}\)</p><p>我们要改变矩阵存储方式，只存储那些非零的数值。</p><p>所以矩阵A可以表示成</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/A%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>其中，每一行第一个字段为行标签i，第二个字段为列标签j，第三个字段是A[i,j],即矩阵对应的值。</p><p>同理，对于矩阵B，可以变换为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/B%E7%9F%A9%E9%98%B5.jpg" alt=""></p><p>下图中，对于矩阵A，因为矩阵B一共两列，所以p=2，所以key会有（1，1）和（1，2）。而对于每个key，每一行中每一个位置的数据都得变成value，即value=”a:j,aij”</p><p>同理，对于矩阵B，k就在key的前面，因为是每一列拿出来做相乘。Value中也变成按行遍历。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce%201_%E7%9C%8B%E5%9B%BE%E7%8E%8B_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><p>在shuffle阶段，就把相同的key放在一起<br>在reduce阶段，在同一个key中，value第一维相同的匹配一起，其第二维相乘，最后把所有相加（如key（1，1）中，\(1<em>10+3</em>11\)，因为a:2没有匹配的，所以去掉a:2）</p><p>最后把每一个key相加就可以得到结果了。</p><center><br><img src="http://p4rlzrioq.bkt.clouddn.com/mapreduce3.png" width="120"><br></center><p>整体的流程实现如下图：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/MapReduceWordCountOverview1.png" alt=""></p><h2 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a><strong>Python代码实现</strong></h2><p>对于wordcount，如果是串行的话，等于是对于每个word进行一次循环。而并行的话可以多个word一起遍历。</p><p>map代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import sys </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    word_list = line.strip().split(&apos; &apos;)    </span><br><span class="line">    for word in word_list:</span><br><span class="line">        print &apos;\t&apos;.join([word.strip(), str(1)])</span><br></pre></td></tr></table></figure><p>reduce代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">cur_word = None</span><br><span class="line">sum = 0    </span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    ss = line.strip().split(&apos;\t&apos;)        </span><br><span class="line">    if len(ss) &lt; 2:</span><br><span class="line">        continue    </span><br><span class="line">    word = ss[0].strip()</span><br><span class="line">    count = ss[1].strip()    </span><br><span class="line">    if cur_word == None:</span><br><span class="line">        cur_word = word    </span><br><span class="line">    if cur_word != word:</span><br><span class="line">        print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">        cur_word = word</span><br><span class="line">        sum = 0        </span><br><span class="line">    sum += int(count)    </span><br><span class="line">print &apos;\t&apos;.join([cur_word, str(sum)])</span><br><span class="line">sum = 0</span><br></pre></td></tr></table></figure><p>然后在linux上运行以下代码：</p><p>src.txt为自己随便找的文本文件txt格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat src.txt | python map.py | sort -k 1 | python reduce.py</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/python%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.jpg" alt=""></p><p>还算比较简单的，就不过多解释了。本意就是map中算出每一行中的word，然后传到reduce中处理。</p><h2 id="实战中怎么运用并行化处理"><a href="#实战中怎么运用并行化处理" class="headerlink" title="实战中怎么运用并行化处理"></a><strong>实战中怎么运用并行化处理</strong></h2><p>Python中有几个package可以做并行，如joblib，pp，multiprocessing等，</p><p>此处仅介绍joblib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from joblib import Parallel, delayed</span><br><span class="line">from math import sqrt</span><br><span class="line">Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))</span><br></pre></td></tr></table></figure><p>得到：</p><p>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</p><p>本质就是你要定义好一个子函数，这个子函数是并行任务中每个任务的输出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B0%81%E9%9D%A2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="分布式" scheme="http://frankblog.site/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="mapreduce" scheme="http://frankblog.site/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之聚类</title>
    <link href="http://frankblog.site/2018/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/"/>
    <id>http://frankblog.site/2018/06/07/机器学习之聚类/</id>
    <published>2018-06-07T06:48:23.561Z</published>
    <updated>2018-06-08T03:50:40.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>无监督学习方法在现实场景中还是有很多应用的，例如在金融和计算广告等反欺诈场景中，我们是不可能能够获取大量的标签数据的，因为欺诈用户不会告诉你他是欺诈用户的。这时候，如果想要利用机器学习方法检测出他们来，只能使用无监督方法。</p><h1 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>下面具体看到该算法的步骤：</p><p>（1）根据设定的聚类数 K，随机地选择 K 个聚类中心（Cluster Centroid）</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>（2）评估各个样本到聚类中心的距离，如果样本距离第 i 个聚类中心更近，则认为其属于第 ii 簇</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%92%E5%BD%92%E5%88%B0%E7%B0%87.png" alt=""></p><p>（3）计算每个簇中样本的<strong>平均（Mean）</strong>位置，将聚类中心移动至该位置</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%A7%BB%E5%8A%A8%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83.png" alt=""></p><p>重复以上步骤直至各个聚类中心的位置不再发生改变。</p><h2 id="变量标准化"><a href="#变量标准化" class="headerlink" title=".变量标准化"></a>.变量标准化</h2><p>在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。</p><p>常用的方法有：</p><p><strong>正态标准化</strong>：\(x_i=\frac {x_i-mean(X)}{std(X}\)<br><strong>归一化</strong>：\(x_i=\frac {x_i-min(X)}{max(X)-min(X)}\)</p><h2 id="如何确定聚类数"><a href="#如何确定聚类数" class="headerlink" title="如何确定聚类数"></a>如何确定聚类数</h2><p>实际上，一开始是很难确定聚类数的，下图的两种聚类数似乎都是可行的：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B01.png" alt=""><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%81%9A%E7%B1%BB%E6%95%B02.png" alt=""></p><p>但是，也存在一种称之为<strong>肘部法则（Elbow Method）</strong>的方法来选定适当的K值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png" alt=""></p><p>上图曲线类似于人的手肘，“肘关节”部分对应的 K 值就是最恰当的 K值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B2%A1%E6%9C%89%E8%82%98.png" alt=""></p><p>一般来说，K-Means 得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择 K 值。在下面这个例子中，假定我们的衣服想要是分为 S,M,L 三个尺码，就设定 K=3，如果我们想要 XS、S、M、L、XL 5 个衣服的尺码，就设定 K=5：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%8D%E9%A5%B0%E5%A4%A7%E5%B0%8F.png" alt=""></p><h2 id="质心的目标函数"><a href="#质心的目标函数" class="headerlink" title="质心的目标函数"></a>质心的目标函数</h2><p>聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性</p><p>常见的邻近度、质心和目标函数组合</p><table><thead><tr><th>邻近度函数</th><th>质心</th><th>目标函数</th></tr></thead><tbody><tr><td>曼哈顿距离</td><td>中位数</td><td>最小化对象与质心的绝对误差和SAE</td></tr><tr><td>平方欧几里得距离</td><td>均值</td><td>最小化对象与质心的误差平方和SSE</td></tr><tr><td>余弦</td><td>均值</td><td>最大化对象与质心的余弦相似度和</td></tr><tr><td>Bregman散度</td><td>均值</td><td>最小化对象到质心的Bregman散度和</td></tr></tbody></table><p><code>Bregman散度</code>实际上是一类紧邻性度量，包括平方欧几里得距离。<code>Bregman散度函数</code>的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。</p><h2 id="K-means优缺点"><a href="#K-means优缺点" class="headerlink" title="K-means优缺点"></a>K-means优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>简单并且可以用于各种数据类型；</li><li>具备适合的空间复杂度和计算负责度，适用于大样本数据；</li><li>K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1. 属于“硬聚类” ， 每个样本只能有一个类别。 其他的一些聚类方法(GMM或者模糊K-means允许“软聚类”)。<br>2. K-means对异常点的“免疫力” 很差， 我们可以通过一些调整，比如中心不直接取均值， 而是找均值最近的样本点代替——k-medoids算法。<br>3. 对于团状的数据点集区分度好， 对于带状(环绕)等“非凸”形状不太好。 (用谱聚类或者做特征映射)</p><h1 id="密度聚类方法"><a href="#密度聚类方法" class="headerlink" title="密度聚类方法"></a>密度聚类方法</h1><h2 id="基本名词"><a href="#基本名词" class="headerlink" title="基本名词"></a>基本名词</h2><ul><li><strong>核心点</strong>： 如果该点满足给定的邻域内（半径为EpsEps的范围内）的点的个数超过给定的阈值MinptsMinpts，则该点为满足该条件下的核心点。</li><li><strong>边界点</strong>： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。</li><li><p><strong>噪声点</strong>： 噪声点既非核心点，也不是边界点</p></li><li><p><strong>ϵ 邻域</strong></p></li></ul><p>对于样本集 D 中样本点 xi，它的 ϵ 邻域定义为与 xi 距离不大于 ϵ 的样本的集合，即 Nϵ(xi)={x∈D|dist(x,xi)≤ϵ}；</p><ul><li><strong>核心对象</strong></li></ul><p>如果样本 x 的 ϵ 邻域内至少包含 mps 个样本，即 |Nϵ(xi)|≥mps，则称 x 为核心对象；</p><ul><li><strong>密度直达</strong></li></ul><p>如果 xi 是一个核心对象，并且 xj 位于它的 ϵ邻域内，那么我们称 xj 由 xi 密度直达；</p><ul><li><strong>密度可达</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在这样的样本序列 p1,p2,⋯,pn，其中 p1=xi,pn=xj且\(p_{i+1}\)由pi密度直达，则称 xi 与 xj 密度可达；</p><ul><li><strong>密度相连</strong></li></ul><p>对于任意的两个不同的样本点 xi 与 xj，如果存在第三个样本点 xk 使得 xi与 xj均由 xk密度可达，则称 xi与 xj密度相连。</p><h2 id="DBSCAN算法步骤"><a href="#DBSCAN算法步骤" class="headerlink" title="DBSCAN算法步骤"></a>DBSCAN算法步骤</h2><blockquote><p><1> 将所有点标记为核心点、边界点或噪音点</1></p><p><2> 删除噪音点</2></p><p><3> 为距离在 EpsEps 之内的所有核心点之间赋予一条边</3></p><p><4> 每组联通的核心点形成一个簇</4></p><p><5> 将每个边界点指派到一个与之关联的核心点的簇中</5></p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB.jpg" alt="link"></p><h2 id="选择-DBSCAN-参数"><a href="#选择-DBSCAN-参数" class="headerlink" title="选择 DBSCAN 参数"></a>选择 DBSCAN 参数</h2><ul><li><strong>k−距离</strong>：如何选择 Eps 和 MinPts 参数，基本的方法是观察点到它的第 k 个最近邻的距离。<br>考虑下，如何 k不大于簇个数的话， k−距离相对较小，反之对于不在簇中的点（噪音点或异常值）则k距离较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 k (一般为4)，计算所有点的k−距离，并递增排序，画出曲线图，则我们会看到k−距离的变化，并依照此图选择出合适的 MinPts参数，即对应拐点的位置。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB2.jpg" alt=""></p><h2 id="优点和缺点"><a href="#优点和缺点" class="headerlink" title="优点和缺点"></a>优点和缺点</h2><ul><li>对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。</li><li>当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。</li></ul><h1 id="层次聚类方法"><a href="#层次聚类方法" class="headerlink" title="层次聚类方法"></a>层次聚类方法</h1><p>有两种产生层次聚类的基本方法：</p><ul><li>凝聚型： 从点作为个体簇开始，每一步合并两个最接近的簇</li><li>分裂型： 从包含所有的点某个簇开始，每一步分裂一个簇，直到成为单点簇<br>到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。</li></ul><h2 id="簇之间的临近性"><a href="#簇之间的临近性" class="headerlink" title="簇之间的临近性"></a>簇之间的临近性</h2><ul><li><strong>MIN</strong>：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做<strong>单链（sigle link）</strong></li><li><strong>MAX</strong>：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做<strong>全链（complete link）</strong></li><li><strong>组平均</strong>：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。</li></ul><p><img src="https://img-blog.csdn.net/20160624104427613" alt="link"></p><h2 id="层次聚类的主要问题"><a href="#层次聚类的主要问题" class="headerlink" title="层次聚类的主要问题"></a>层次聚类的主要问题</h2><h3 id="处理不同大小簇的能力"><a href="#处理不同大小簇的能力" class="headerlink" title="处理不同大小簇的能力"></a>处理不同大小簇的能力</h3><p>对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法：</p><ul><li><strong>非加权</strong>：平等的对待所有簇，赋予不同大小的簇中点不同的权值</li><li><strong>加权</strong>： 赋予不同大小簇中点相同的权值</li></ul><h3 id="合并不可逆"><a href="#合并不可逆" class="headerlink" title="合并不可逆"></a>合并不可逆</h3><p>对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。</p><p>有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。</p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>众所周知，对于有监督学习方法的分类预测结果，我们有很多种不同的评价指标来度量分类效果的好坏。例如，召回率、精准率、准确率、F1-Score 以及 AUC 值等等。但是，由于无监督学习方法与有监督学习不同，绝大多数情况下，我们根本不知道它的真实类别标签，所以我们不可能完全依据有监督学习方法的评价指标来度量聚类算法。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.jpg" alt=""></p><p>无监督聚类算法的评价指标大致可以分为两大类：一类是聚类的结果具有某个参考模型作为人为基准进行比较，称之为<strong>外部指标</strong>；第二种是直接考察聚类结果而不参考任何模型，称之为内部指标。</p><h2 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h2><p>对数据集 \(D={x_1, x_2,\cdots,x_m}\)，假定通过聚类算法将样本聚为 \(C={C_1,C_2,\cdots,C_k}\)，参考模型给出的簇划分为 \(C^<em>={C_1^</em>,C_2^<em>,\cdots,C_s^</em>}\)。相应的，令 λ 与 \(λ^∗\) 分别表示与 C 与 \(C^∗\)对应的簇标记向量。我们将样本两两配对考虑，定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%8711.png" alt=""></p><p>其中集合 S1包含了在 C 中属于相同的簇并且在 C∗中也属于相同的簇的样本； S2 包含了在 C 中属于相同的簇并且在 C∗中不属于相同的簇的样本……以此类推。对每个样本对 (xi,xj) (i &lt; j) 仅能出现在一个集合中，因此有 a+b+c+d=m(m−1)/2。</p><p>基于以上定义，对无监督聚类算法的聚类结果，我们有如下的性能度量指标：</p><ul><li>Jaccard 系数（简记为 JCI）</li></ul><p>$$<br>\begin{align} JCI = \frac{a}{a+b+c} \end{align}<br>$$</p><ul><li>FM 指数（简记为FMI）</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/fm%E6%8C%87%E6%95%B0.png" alt=""></p><ul><li>Rand 指数（简记为 RI）</li></ul><p>$$<br>\begin{align} RI=\frac{2(a+d)}{m(m-1)} \end{align}<br>$$<br>很显然，上述指数值都在 <code>[0,1]</code> 之间，并且值越大越好。</p><h2 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h2><p>对于聚类结果\(C={C_1,C_2,\cdots,C_k}\)，作如下定义：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%86%85%E9%83%A8%E6%8C%87%E6%A0%87.png" alt="内部指标"></p><p>其中，\(dist(x_i,x_j)\)用于计算两个样本间的距离 \(μ_i\)代表类 \(C_j\) 的样本中心。基于以上定义如下内部指标：</p><ul><li><strong>DB 指数（简称 DBI）</strong><br>$$<br>\begin{align} DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\bigl(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\bigl) \end{align}<br>$$</li><li><strong>Dunn 指数（简称 DI）</strong></li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/dunn%E6%8C%87%E6%95%B02.png" alt=""><br>显然，DBI 的值越小越好，而 DI 值越大越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%9D%83%E5%8A%9B%E7%9A%84%E6%B8%B8%E6%88%8F_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类算法" scheme="http://frankblog.site/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之KNN</title>
    <link href="http://frankblog.site/2018/06/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN/"/>
    <id>http://frankblog.site/2018/06/06/机器学习之KNN/</id>
    <published>2018-06-06T03:48:14.506Z</published>
    <updated>2018-06-06T04:05:38.617Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h1><p>KNN采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h2 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/kd%E6%A0%91.jpg" alt=""></p><p>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h2 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h2><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h1 id="KNN算法原理"><a href="#KNN算法原理" class="headerlink" title="KNN算法原理"></a>KNN算法原理</h1><p>KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别：</p><p>假设X_test为未标记的数据样本，X_train为已标记类别的样本，算法原理伪代码如下：</p><ul><li>遍历X_train中所有样本，计算每个样本与X_test的距离，并保存在Distance数组中</li><li>对Distance数组进行排序，取距离最近的k个点，记为X_knn</li><li>在X_knn中统计每个类别的个数</li><li>代表记得样本的类别，就是在X_knn中样本最多的类别</li></ul><h2 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h2><p><strong>优点：</strong>准确性高，对异常值和噪声有较高的容忍度</p><p><strong>缺点：</strong>计算量大，对内存的需求也较大</p><h2 id="算法参数（k）"><a href="#算法参数（k）" class="headerlink" title="算法参数（k）"></a>算法参数（k）</h2><p><strong>k越大：模型偏差越大，对噪声越不敏感。过大是造成欠拟合</strong></p><p><strong>k越小：模型的方差就会越大。太小是会造成过拟合</strong></p><h2 id="算法的变种"><a href="#算法的变种" class="headerlink" title="算法的变种"></a>算法的变种</h2><p><strong>增加邻居的权重：</strong>默认情况下X_knn的权重相等，我们可以指定算法的<code>weights</code>参数调整成距离越近权重越大</p><p><strong>使用一定半径内的点取代距离最近的kk个点</strong>，<code>RadiusNeighborsClassifier</code>类实现了这个算法</p><h1 id="使用KNN作分类"><a href="#使用KNN作分类" class="headerlink" title="使用KNN作分类"></a>使用KNN作分类</h1><ul><li><code>sklearn.neighbors.KNeighborsClassifier</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">k = 5</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># X_test = np.array([0, 2]).reshape(1,-1)</span><br><span class="line">X_test = [[0,2]]</span><br><span class="line">y_test = clf.predict(X_test)</span><br><span class="line">neighbors = clf.kneighbors(X_test, return_distance=False)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span><br><span class="line">centers = [[-2,2], [2,2], [0,4]]</span><br><span class="line">X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)</span><br><span class="line"></span><br><span class="line"># 画出数据</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)  # 样本</span><br><span class="line">plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;) # 中心点</span><br><span class="line">plt.scatter(X_test[0][0], X_test[0][1], marker=&apos;x&apos;, s=50, c=&apos;blue&apos;) # 中心点</span><br><span class="line"></span><br><span class="line">for i in neighbors[0]:</span><br><span class="line">    plt.plot([X[i][0], X_test[0][0]], [X[i][1], X_test[0][1]],&apos;k--&apos;, linewidth=0.5)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%BF%E7%94%A8knn%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB.png" alt=""></p><h1 id="KNN回归拟合"><a href="#KNN回归拟合" class="headerlink" title="KNN回归拟合"></a>KNN回归拟合</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">import numpy as np</span><br><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">n = 50</span><br><span class="line">X = 5 * np.random.rand(n ,1)</span><br><span class="line">y = np.cos(X).ravel()</span><br><span class="line"># 添加一些噪声</span><br><span class="line">y += 0.2 * np.random.rand(n) - 0.1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = 5</span><br><span class="line">knn = KNeighborsRegressor(k)</span><br><span class="line">knn.fit(X, y)</span><br></pre></td></tr></table></figure><blockquote><p>KNeighborsRegressor(algorithm=’auto’, leaf_size=30,metric=’minkowski’,metric_params=None, n_jobs=1,n_neighbors=5, p=2,weights=’uniform’)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T = np.linspace(0,5, 500)[:, np.newaxis]</span><br><span class="line">y_pred = knn.predict(T)</span><br><span class="line">knn.score(X,y)</span><br></pre></td></tr></table></figure><blockquote><p>0.9909058023770559</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8,5), dpi=100)</span><br><span class="line">plt.scatter(X, y, label=&apos;data&apos;, s=10)</span><br><span class="line">plt.scatter(T, y_pred, label=&apos;prediction&apos;, lw=4, s=0.1)</span><br><span class="line">plt.axis(&apos;tight&apos;)</span><br><span class="line">plt.title(&quot;KNeighborsRegressor (k=%i)&quot; % k)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/kneighborsRegressor.png" alt=""></p><h1 id="糖尿病预测"><a href="#糖尿病预测" class="headerlink" title="糖尿病预测"></a>糖尿病预测</h1><p>总共有768个数据、8个特征，其中Outcome为标记值（1表示有糖尿病）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br></pre></td></tr></table></figure></p><h2 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h2><ul><li>分别使用普通KNN，加权重KNN，和指定权重的KNN分别对数据拟合计算评分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=10)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(</span><br><span class="line">n_neighbors=10, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=10, radius=500.0)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    results.append((name, model.score(X_test, y_test)))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; score:&#123;&#125;&quot;.format(results[i][0], results[i][1]))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; score:0.7207792207792207<br>name:KNN + weights; score:0.6818181818181818<br>name:Radius Neighbors; score:0.6558441558441559</p></blockquote><ul><li>此时单从得分上看，普通的KNN性能是最好的，但是我们的训练样本和测试样本是随机分配的，不同的训练集、测试集会造成不同得分。</li><li>为了消除随机样本集对得分结果可能的影响，scikit-learn提供了<code>KFold和cross_val_score()</code>函数来处理这个问题</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name , model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X, y, cv=kfold) # 这里要给模型全部的样本集</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; cross_val_score:&#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name:KNN; cross_val_score:0.74865003417635<br>name:KNN + weights; cross_val_score:0.7330485304169514<br>name:Radius Neighbors; cross_val_score:0.6497265892002735</p></blockquote><h2 id="用查准率和召回率以及F1对该模型进行评估："><a href="#用查准率和召回率以及F1对该模型进行评估：" class="headerlink" title="用查准率和召回率以及F1对该模型进行评估："></a>用查准率和召回率以及F1对该模型进行评估：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(10)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;该模型查准率为：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型召回率为：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;该模型F1_score为：&quot;, f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><blockquote><p>该模型查准率为： 0.6086956521739131<br>该模型召回率为： 0.5283018867924528<br>该模型F1_score为： 0.5656565656565657</p></blockquote><h2 id="模型的训练及分析-–-学习曲线"><a href="#模型的训练及分析-–-学习曲线" class="headerlink" title="模型的训练及分析 – 学习曲线"></a>模型的训练及分析 – 学习曲线</h2><p>下面就选择用普通KNN算法模型对数据集进行训练，并查看训练样本的拟合情况及对策测试样本的预测准确性：</p><p>输入参数：</p><blockquote><p>estimator : 你用的分类器。<br>title : 表格的标题。<br>X : 输入的feature，numpy类型<br>y : 输入的target vector<br>ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点<br>cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)<br>n_jobs : 并行的的任务数(默认1))</p></blockquote><p>输出参数：</p><blockquote><p>train_sizes_abs :训练样本数<br>train_scores:训练集上准确率<br>test_scores:交叉验证集上的准确率) </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=2)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">train_score = knn.score(X_train, y_train)</span><br><span class="line">test_score = knn.score(X_test, y_test)</span><br><span class="line">print(&apos;训练集得分：&apos;,train_score)</span><br><span class="line">print(&apos;测试集得分：&apos;,test_score)</span><br></pre></td></tr></table></figure><blockquote><p>训练集得分： 0.8517915309446255<br>测试集得分： 0.6948051948051948</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line"># from common.utils import plot_learning_curve</span><br><span class="line"></span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 10)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">         plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)</span><br><span class="line">X = data.iloc[:,0:8]</span><br><span class="line">y = data.iloc[:,8]</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plot_learn_curve(KNeighborsClassifier(2),&quot;KNN score&quot;,X, y, ylim=(0.5, 1), cv=cv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%8A%E5%88%86%E6%9E%90%20%E2%80%93%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>当训练集和测试集的误差收敛但却很高时，为高偏差。<br>左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。<br>我们可以增加模型参数，比如，构建更多的特征，减小正则项。<br>此时通过增加数据量是不起作用的。</p><p>当训练集和测试集的误差之间有大的差距时，为高方差。<br>当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。<br>右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。<br>我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。</p><p>理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。</p><h2 id="特征选择及数据可视化"><a href="#特征选择及数据可视化" class="headerlink" title="特征选择及数据可视化"></a>特征选择及数据可视化</h2><p><strong>使用sklearn.feature_selection.SelectKBest选择相关性最大的两个特征</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.neighborse import KN</span><br><span class="line"></span><br><span class="line">selector = SelectKBest(k=2)</span><br><span class="line">X_new = selector.fit_transform(X,y)</span><br><span class="line">X_new[0:5] #把相关性最大的两个特征放到X_new里并查看前5个数据样本</span><br></pre></td></tr></table></figure><blockquote><p>array([[148. ,  33.6],[ 85. ,  26.6],[183. ,  23.3],[ 89. ,  28.1],[137. ,  43.1]])</p></blockquote><ul><li>使用相关性最大的两个特征，对3种不同的KNN算法进行检验</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line">models = []</span><br><span class="line">models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=5)))</span><br><span class="line">models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(n_neighbors=5, weights=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=5, radius=500.0)))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">for name, model in models:</span><br><span class="line">    kfold = KFold(n_splits=10)</span><br><span class="line">    cv_result = cross_val_score(model, X_new, y, cv=kfold)</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name: &#123;&#125;; cross_val_score: &#123;&#125;&quot;.format(results[i][0], results[i][1].mean()))</span><br></pre></td></tr></table></figure><blockquote><p>name: KNN; cross_val_score: 0.7369104579630894<br>name: KNN + weights; cross_val_score: 0.7199419002050581<br>name: Radius Neighbors; cross_val_score: 0.6510252904989747</p></blockquote><p>从输出结果来看，还是普通KNN的准确性更高，与所有特征放到一起训练的准确性差不多，这也侧面证明了SelectKNBest特征选取的准确性。</p><p>回到目标上来，我们是想看看为什么KNN不能很好的拟合训练样本。现在我们至于2个特征可以很方便的在二维坐标上画出所有的训练样本，观察这些数据分布情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8,6), dpi=100)</span><br><span class="line">plt.ylabel(&quot;BMI&quot;)</span><br><span class="line">plt.xlabel(&quot;Glucose&quot;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_new[y==0][:,0], X_new[y==0][:,1], marker=&apos;o&apos;, s=10)</span><br><span class="line">plt.scatter(X_new[y==1][:,0], X_new[y==1][:,1], marker=&apos;^&apos;, s=10)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt=""></p><p>横坐标是血糖值，纵坐标是BMI值反应身体肥胖情况。在数据密集的区域，代表糖尿病的阴性和阳性的样本几乎重叠到了一起。这样就很直观的看到，KNN在糖尿病预测的这个问题上无法达到很高的预测准确性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/KNN_%E7%9C%8B%E5%9B%BE%E7%8E%8B.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="KNN" scheme="http://frankblog.site/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>决策树之泰坦之灾</title>
    <link href="http://frankblog.site/2018/06/05/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E6%B3%B0%E5%9D%A6%E4%B9%8B%E7%81%BE/"/>
    <id>http://frankblog.site/2018/06/05/决策树之泰坦之灾/</id>
    <published>2018-06-05T10:17:19.653Z</published>
    <updated>2018-06-06T02:29:11.443Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/titanic.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><ul><li>筛选特征值，丢掉不需要的特征数据</li><li>对性别进行二值化处理（转换为0和1）</li><li>港口转换成数值型数据</li><li>处理缺失值（如年龄，有很多缺失值）</li></ul><h2 id="1、首先读取数据"><a href="#1、首先读取数据" class="headerlink" title="1、首先读取数据"></a>1、首先读取数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def read_dataset(fname):</span><br><span class="line">#     指定第一列作为行索引</span><br><span class="line">    data = pd.read_csv(fname, index_col=0)</span><br><span class="line">#     丢弃无用数据</span><br><span class="line">    data.drop([&apos;Name&apos;, &apos;Ticket&apos;, &apos;Cabin&apos;], axis=1, inplace=True)</span><br><span class="line">#     处理性别数据</span><br><span class="line">    lables = data[&apos;Sex&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Sex&apos;] = [*map(lambda x: lables.index(x) , data[&apos;Sex&apos;])]</span><br><span class="line">#     处理登船港口数据</span><br><span class="line">    lables = data[&apos;Embarked&apos;].unique().tolist()</span><br><span class="line">    data[&apos;Embarked&apos;] = data[&apos;Embarked&apos;].apply(lambda n: lables.index(n))</span><br><span class="line">#     处理缺失数据填充0</span><br><span class="line">    data = data.fillna(0)</span><br><span class="line">    return data</span><br><span class="line">train = read_dataset(&apos;code/datasets/titanic/train.csv&apos;)</span><br></pre></td></tr></table></figure><h2 id="2、拆分数据集"><a href="#2、拆分数据集" class="headerlink" title="2、拆分数据集"></a>2、拆分数据集</h2><p>把<code>Survived</code>列提取出来作为标签，然后在元数据集中将其丢弃。同时拆分数据集和交叉验证数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">y = train[&apos;Survived&apos;].values</span><br><span class="line">X = train.drop([&apos;Survived&apos;], axis=1).values</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">print(&quot;X_train_shape:&quot;, X_train.shape, &quot; y_train_shape:&quot;, y_train.shape)</span><br><span class="line">print(&quot;X_test_shape:&quot;, X_test.shape,&quot;  y_test_shape:&quot;, y_test.shape)</span><br></pre></td></tr></table></figure></p><blockquote><p>X_train_shape: (712, 7)  y_train_shape: (712,)<br>X_test_shape: (179, 7)   y_test_shape: (179,)</p></blockquote><h2 id="3、拟合数据集"><a href="#3、拟合数据集" class="headerlink" title="3、拟合数据集"></a>3、拟合数据集</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(&quot;train score:&quot;, clf.score(X_train, y_train))</span><br><span class="line">print(&quot;test score:&quot;, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.9845505617977528<br>test score: 0.7597765363128491</p></blockquote><h1 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h1><h2 id="1、通过max-depth参数来优化模型"><a href="#1、通过max-depth参数来优化模型" class="headerlink" title="1、通过max_depth参数来优化模型"></a>1、通过<code>max_depth</code>参数来优化模型</h2><p>从以上输出数据可以看出，针对训练样本评分很高，但针对测试数据集评分较低。很明显这是过拟合的特征。解决决策树过拟合的方法是剪枝，包括前剪枝和后剪枝。但是<code>sklearn</code>不支持后剪枝，这里通过<code>max_depth</code>参数限定决策树深度，在一定程度上避免过拟合。</p><p>这里先创建一个函数使用不同的模型深度训练模型，并计算评分数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def cv_score(d):</span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=d)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return(clf.score(X_train, y_train), clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">scores = [cv_score(d) for d in depths]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line"># 找出交叉验证数据集评分最高的索引</span><br><span class="line">tr_best_index = np.argmax(tr_scores)</span><br><span class="line">te_best_index = np.argmax(te_scores)</span><br><span class="line"></span><br><span class="line">print(&quot;bestdepth:&quot;, te_best_index+1, &quot; bestdepth_score:&quot;, te_scores[te_best_index], &apos;\n&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>bestdepth: 5  bestdepth_score: 0.8603351955307262 </p></blockquote><p><strong>这里由于以上<code>train_test_split</code>方法对数据切分是随机打散的，造成每次用不同的数据集训练模型总得到不同的最佳深度。</strong>这里写个循环反复测试，最终验证这里看到最佳的分支深度为5出现的频率最高，初步确定5为深度模型最佳。</p><p>把模型参数和对应的评分画出来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">depths = np.arange(1,10)</span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&apos;max depth of decison tree&apos;)</span><br><span class="line">plt.ylabel(&apos;Scores&apos;)</span><br><span class="line">plt.plot(depths, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(depths, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0.jpg" alt=""></p><h2 id="2、通过min-impurity-decrease来优化模型"><a href="#2、通过min-impurity-decrease来优化模型" class="headerlink" title="2、通过min_impurity_decrease来优化模型"></a>2、通过<code>min_impurity_decrease</code>来优化模型</h2><p>这个参数用来指定信息墒或者基尼不纯度的阈值，当决策树分裂后，其信息增益低于这个阈值时则不再分裂。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line"></span><br><span class="line">def minsplit_score(val):</span><br><span class="line">    clf = DecisionTreeClassifier(criterion=&apos;gini&apos;, min_impurity_decrease=val)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    return (clf.score(X_train, y_train), clf.score(X_test, y_test), )</span><br><span class="line"></span><br><span class="line"># 指定参数范围，分别训练模型并计算得分</span><br><span class="line"></span><br><span class="line">vals = np.linspace(0, 0.2, 100)</span><br><span class="line">scores = [minsplit_score(v) for v in vals]</span><br><span class="line">tr_scores = [s[0] for s in scores]</span><br><span class="line">te_scores = [s[1] for s in scores]</span><br><span class="line"></span><br><span class="line">bestmin_index = np.argmax(te_scores)</span><br><span class="line">bestscore = te_scores[bestmin_index]</span><br><span class="line">print(&quot;bestmin:&quot;, vals[bestmin_index])</span><br><span class="line">print(&quot;bestscore:&quot;, bestscore)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(6,4), dpi=120)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(&quot;min_impurity_decrease&quot;)</span><br><span class="line">plt.ylabel(&quot;Scores&quot;)</span><br><span class="line">plt.plot(vals, te_scores, label=&apos;test_scores&apos;)</span><br><span class="line">plt.plot(vals, tr_scores, label=&apos;train_scores&apos;)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p><blockquote><p>bestmin: 0.00202020202020202<br>bestscore: 0.7988826815642458</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9D%A5%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B.jpg" alt=""></p><p><strong>问题：每次使用不同随机切割的数据集得出最佳参数为0.002很接近0，该怎么解读？</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">至此为我们找到了两个参数,最佳深度depth=5 和最佳min_impurity_decrease=0.002，下面我来用两个参数简历模型进行测试：</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn import metrics </span><br><span class="line"></span><br><span class="line">model = DecisionTreeClassifier(max_depth=5, min_impurity_decrease=0.002)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;tees_score:&quot;, model.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(&quot;查准率:&quot;,metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率:&quot;,metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1_score:&quot;,metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure></p><blockquote><p>tees_score: 0.7821229050279329<br>查准率: 0.8461538461538461<br>召回率: 0.5866666666666667<br>F1_score: 0.6929133858267718</p></blockquote><h1 id="模型参数选择工具包"><a href="#模型参数选择工具包" class="headerlink" title="模型参数选择工具包"></a>模型参数选择工具包</h1><p>至此发现以上两种模型优化方法有两问题：</p><ul><li><p>1、数据不稳定：–&gt; 每次重新分配训练集测试集，原参数就不是最优了。 解决办法是多次计算求平均值。</p></li><li><p>2、不能一次选择多个参数：–&gt; 想考察max_depth和min_impurity_decrease两者结合起来的最优参数就没法实现。</p></li></ul><p>所幸<code>scikit-learn</code>在<code>sklearn.model_selection</code>包提供了大量的模型选择和评估的工具供我们使用。针对该问题可以使用<code>GridSearchCV</code>类来解决。</p><h2 id="利用GridSearchCV求最优参数"><a href="#利用GridSearchCV求最优参数" class="headerlink" title="利用GridSearchCV求最优参数"></a>利用<code>GridSearchCV</code>求最优参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;:thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(&quot;best_parms:&#123;0&#125;\nbest_score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best_parms:{‘min_impurity_decrease’: 0.00816326530612245}<br>best_score:0.8114478114478114</p></blockquote><p>模型解读：<br>1、关键字参数<code>param_grid</code>是一个字典，字典的关键字对应的值是一个列表。<code>GridSearchCV</code>会枚举列表里所有值来构建模型多次计算训练模型，并计算模型评分，最终得出指定参数值的平均评分及标准差。</p><p>2、关键参数<code>sv</code>，用来指定交叉验证数据集的生成规则。这里sv=5表示每次计算都把数据集分成5份，拿其中一份作为交叉验证数据集，其他作为训练集。最终得出最优参数及最优评分保存在<code>clf.best_params_</code>和<code>clf.best_score_</code>里。</p><p>3、此外<code>clf.cv_results_</code>里保存了计算过程的所有中间结果。</p><h2 id="画出学习曲线："><a href="#画出学习曲线：" class="headerlink" title="画出学习曲线："></a>画出学习曲线：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def plot_curve(train_sizes, cv_results, xlabel):</span><br><span class="line">    train_scores_mean = cv_results[&apos;mean_train_score&apos;]</span><br><span class="line">    train_scores_std = cv_results[&apos;std_train_score&apos;]</span><br><span class="line">    test_scores_mean = cv_results[&apos;mean_test_score&apos;]</span><br><span class="line">    test_scores_std = cv_results[&apos;std_test_score&apos;]</span><br><span class="line">    plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line">    plt.title(&apos;parameters turning&apos;)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(&apos;score&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &apos;.--&apos;, color=&quot;r&quot;,label=&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, &apos;.-&apos;, color=&quot;g&quot;,</span><br><span class="line">    label=&quot;Cross-validation score&quot;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&quot;best&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(0, 0.2, 50)</span><br><span class="line"># Set the parameters by cross-validation</span><br><span class="line">param_grid = &#123;&apos;min_impurity_decrease&apos;: thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param: &#123;0&#125;\nbest score: &#123;1&#125;&quot;.format(clf.best_params_, </span><br><span class="line"> clf.best_score_))</span><br><span class="line"></span><br><span class="line"># plot_curve(thresholds, clf.cv_results_, xlabel=&apos;gini thresholds&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>best param: {‘min_impurity_decrease’: 0.00816326530612245}<br>best score: 0.8114478114478114</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%EF%BC%9A.jpg" alt=""></p><h2 id="多组参数之间选择最优参数："><a href="#多组参数之间选择最优参数：" class="headerlink" title="多组参数之间选择最优参数："></a>多组参数之间选择最优参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">entropy_thresholds = np.linspace(0, 1, 100)</span><br><span class="line">gini_thresholds = np.linspace(0, 0.2, 100)</span><br><span class="line">#设置参数矩阵：</span><br><span class="line">param_grid = [&#123;&apos;criterion&apos;: [&apos;entropy&apos;], &apos;min_impurity_decrease&apos;:entropy_thresholds&#125;,&#123;&apos;criterion&apos;: [&apos;gini&apos;], &apos;min_impurity_decrease&apos;: gini_thresholds&#125;,&#123;&apos;max_depth&apos;: np.arange(2,10)&#125;,&#123;&apos;min_samples_split&apos;: np.arange(2,30,2)&#125;]</span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(&quot;best param:&#123;0&#125;\nbest score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><blockquote><p>best param:{‘min_impurity_decrease’: 0.00816326530612245}<br>best score:0.8114478114478114</p></blockquote><p>结果1、{‘criterion’: ‘gini’, ‘min_impurity_decrease’: 0.00816326530612245} -&gt;6</p><p>结果2、{‘min_samples_split’: 22} -&gt;10</p><p>结果3、{‘min_samples_split’: 20} -&gt;4</p><p><strong>结果波动很大，这里做了20次测试，对应结果1出现6次，结果2出现10次，结果3出现4次。</strong></p><p><strong>代码解读</strong>：<br>关键部分还是<code>param_grid</code>参数，他是一个列表。很对列表的第一个字典，选择信息墒<code>（entropy）</code>作为判断标准，取值0～1范围50等分；</p><p>第二个字典选择基尼系数，<code>min_impurity_decrease</code>取值0～0.2范围50等分。</p><p><code>GridSearchCV</code>会针对列表中的每个字典进行迭代，最终比较列表中每个字典所对应的参数组合，选择出最优的参数。</p><h2 id="生成决策树图形"><a href="#生成决策树图形" class="headerlink" title="生成决策树图形"></a>生成决策树图形</h2><p>下面代码可以生成.dot文件，需要电脑上安装<code>graphviz</code>才能把文件转换成图片格式。</p><p><code>Mac</code>上可以使用<code>brew install graphviz</code>命令来安装，它会同时安装8个依赖包。这里一定注意<code>Mac</code>环境下的权限问题：由于<code>Homebrew</code>默认是安装在<code>/usr/local</code>下，而<code>Mac</code>有强制保护不支持<code>sudo chown -R uname local</code>对<code>local</code>文件夹进行权限修改。</p><p>这里的解决方式是把<code>local</code>下<code>bin</code>,<code>lib</code>,<code>Cellar</code>等所需单个文件夹下进行赋权，即可成功安装。</p><ol><li>在电脑上安装 graphviz</li><li>运行 <code>dot -Tpng tree.dot -o filename.png</code></li><li>在当前目录查看生成的决策树 filename.png</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier </span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier(min_samples_split=22)</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line">train_score = clf.score(X_train, y_train)</span><br><span class="line">test_score = clf.score(X_test, y_test)</span><br><span class="line">print(&apos;train score: &#123;0&#125;; test score: &#123;1&#125;&apos;.format(train_score, test_score))</span><br><span class="line"></span><br><span class="line"># 导出 titanic.dot 文件</span><br><span class="line">with open(&quot;tree.dot&quot;, &apos;w&apos;) as f:</span><br><span class="line">    f = tree.export_graphviz(clf, out_file=f)</span><br></pre></td></tr></table></figure><blockquote><p>train score: 0.8834269662921348; test score: 0.8268156424581006</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E7%BB%84%E5%8F%82%E6%95%B0%E4%B9%8B%E9%97%B4%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%8F%82%E6%95%B0%EF%BC%9A.jpg" alt=""></p><h1 id="模型调参注意事项："><a href="#模型调参注意事项：" class="headerlink" title="模型调参注意事项："></a>模型调参注意事项：</h1><ul><li>当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型</li><li>如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。</li><li>推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</li><li>在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。</li><li>决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。</li><li>如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/titanic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="http://frankblog.site/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>sklearn之数据预处理和创建模型</title>
    <link href="http://frankblog.site/2018/06/05/sklearn%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B/"/>
    <id>http://frankblog.site/2018/06/05/sklearn之数据预处理和创建模型/</id>
    <published>2018-06-05T07:43:32.158Z</published>
    <updated>2018-06-06T04:14:14.934Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"></span><br><span class="line">data = np.array([[ 3, -1.5, 2, -5.4],</span><br><span class="line">[ 0, 4, -0.3, 2.1],</span><br><span class="line">[ 1, 3.3, -1.9, -4.3]])</span><br></pre></td></tr></table></figure><h2 id="均值移除-mean-removal"><a href="#均值移除-mean-removal" class="headerlink" title="均值移除 mean removal"></a>均值移除 mean removal</h2><ul><li>“通常我们会把每个特征的平均值移除，以保证特征均值为0（即标准化处理）。这样做可以消除特征彼此间的偏差（bias）”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_standardized = preprocessing.scale(data)</span><br><span class="line">print (&quot;\nMean特征均值 =&quot;, data_standardized.mean(axis=0))</span><br><span class="line">print (&quot;Std deviation标准偏差 =&quot;, data_standardized.std(axis=0))</span><br></pre></td></tr></table></figure><blockquote><p>Mean特征均值 = [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]<br>Std deviation标准偏差 = [1. 1. 1. 1.]</p></blockquote><h2 id="范围缩放-min-max-scaling"><a href="#范围缩放-min-max-scaling" class="headerlink" title="范围缩放 min max scaling"></a>范围缩放 min max scaling</h2><ul><li>“数据点中每个特征的数值范围可能变化很大，因此，有时将特征的数值范围缩放到合理的大小是非常重要的。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data_scaled = data_scaler.fit_transform(data)</span><br><span class="line">print (&quot;\nMin max scaled data范围缩放数据:\n&quot;, data_scaled)</span><br></pre></td></tr></table></figure><blockquote><p>Min max scaled data范围缩放数据:<br> [[1.         0.         1.         0.        ]<br> [0.         1.         0.41025641 1.        ]<br> [0.33333333 0.87272727 0.         0.14666667]]</p></blockquote><h2 id="归一化-normalization"><a href="#归一化-normalization" class="headerlink" title="归一化 normalization"></a>归一化 normalization</h2><ul><li>“数据归一化用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放到相同的数值范围。机器学习中最常用的归一化形式就是将特征向量调整为L1范数，使特征向量的数值之和为1。”</li><li>“这个方法经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级，提高不同特征数据的可比性。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_normalized = preprocessing.normalize(data, norm=&apos;l1&apos;)</span><br><span class="line">print (&quot;\nL1 normalized data归一化后数据:\n&quot;, data_normalized)</span><br></pre></td></tr></table></figure><blockquote><p>L1 normalized data归一化后数据:<br> [[ 0.25210084 -0.12605042  0.16806723 -0.45378151]<br> [ 0.          0.625      -0.046875    0.328125  ]<br> [ 0.0952381   0.31428571 -0.18095238 -0.40952381]]</p></blockquote><h2 id="二值化-binarization"><a href="#二值化-binarization" class="headerlink" title="二值化 binarization"></a>二值化 binarization</h2><ul><li>“二值化用于将数值特征向量转换为布尔类型向量。”</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)</span><br><span class="line">print (&quot;\n二值化 data:\n&quot;, data_binarized)</span><br></pre></td></tr></table></figure><blockquote><p>二值化 data:<br> [[1. 0. 1. 0.]<br> [0. 1. 0. 1.]<br> [0. 1. 0. 0.]]</p></blockquote><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><ul><li><p>one hot encoding独热编码<br>“通常，需要处理的数值都是稀疏地、散乱地分布在空间中，然而，我们并不需要存储这些大数值，这时就需要使用独热编码（One-Hot Encoding）。可以把独热编码看作是一种收紧 （tighten）特征向量的工具。它把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k 的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，这样可以更加有效地表示空间。例如，我们需要处理4维向量空间，当给一个特性向量的第n 个特征进行编码时，编码器会遍历每个特征向量的第n 个特征，然后进行非重复计数。如果非重复计数的值是K ，那么就把这个特征转换为只有一个值是1其他值都是0的K 维向量。”</p></li><li><p>“在下面的示例中，观察一下每个特征向量的第三个特征，分别是1 、5 、2 、4 这4个不重复的值，也就是说独热编码向量的长度是4。如果你需要对5 进行编码，那么向量就是[0, 1, 0, 0] 。向量中只有一个值是1。第二个元素是1，对应的值是5 。”</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = preprocessing.OneHotEncoder()</span><br><span class="line">encoder.fit([[0, 2, 1, 12], [1, 3, 5, 3], [2, 3, 2, 12], [1, 2, 4, 3]])</span><br><span class="line">encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()</span><br><span class="line">print (&quot;\n编码矢量:\n&quot;, encoded_vector)</span><br></pre></td></tr></table></figure><blockquote><p>编码矢量:<br> [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]</p></blockquote><h2 id="标记编码方法"><a href="#标记编码方法" class="headerlink" title="标记编码方法"></a>标记编码方法</h2><p>在监督学习中，经常需要处理各种各样的标记。这些标记可能是数字，也可能是单词。如果标记是数字，那么算法可以直接使用它们，但是，许多情况下，标记都需要以人们可理解的形式存在，因此，人们通常会用单词标记训练数据集。标记编码就是要把单词标记转换成数值形式，让算法懂得如何操作标记。接下来看看如何标记编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line"># 定义一个标记编码器</span><br><span class="line">label_encoder = preprocessing.LabelEncoder()</span><br><span class="line"></span><br><span class="line"># label_encoder对象知道如何理解单词标记，接下来创建标记</span><br><span class="line">input_classes = [&apos;audi&apos;, &apos;ford&apos;, &apos;audi&apos;, &apos;toyota&apos;, &apos;ford&apos;, &apos;bmw&apos;]</span><br><span class="line"># 开始标记</span><br><span class="line">label_encoder.fit(input_classes)</span><br><span class="line">print(&quot;Classes mapping: 结果显示单词背转换成从0开始的索引值&quot;)</span><br><span class="line">for i, item in enumerate(lable_encoder.classes_):</span><br><span class="line">print(item, &apos;--&gt;&apos;, i)</span><br></pre></td></tr></table></figure><blockquote><p>Classes mapping: 结果显示单词背转换成从0开始的索引值<br>audi –&gt; 0<br>bmw –&gt; 1<br>ford –&gt; 2<br>toyota –&gt; 3</p></blockquote><p>这时，如果遇到一组数据就可以轻松的转换它们了。（如药品数据的药品名）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">labels = [&apos;toyota&apos;, &apos;f</span><br><span class="line">ord&apos;, &apos;audi&apos;]</span><br><span class="line">encoded_labels = label_encoder.transform(labels)</span><br><span class="line">print (&quot;\nLabels =&quot;, labels)</span><br><span class="line">print (&quot;Encoded labels =&quot;, list(encoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>Labels = [‘toyota’, ‘ford’, ‘audi’]<br>Encoded labels = [3, 2, 0]</p></blockquote><p>还可以数字反转回单词（或字符串）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoded_labels = [2,1,0,3,1]</span><br><span class="line">decoded_labels = label_encoder.inverse_transform(encoded_labels)</span><br><span class="line">print(encoded_labels)</span><br><span class="line">print(list(decoded_labels))</span><br></pre></td></tr></table></figure></p><blockquote><p>[2, 1, 0, 3, 1]<br>[‘ford’, ‘bmw’, ‘audi’, ‘toyota’, ‘bmw’]</p></blockquote><h1 id="创建线性回归"><a href="#创建线性回归" class="headerlink" title="创建线性回归"></a>创建线性回归</h1><p>回归是估计输入数据与连续值输出数据之间关系的过程。数据通常是实数形式的，我们的目标是<strong>估计满足输入到输出映射关系的基本函数。</strong></p><p>线性回归的目标是提取输入变量与输出变量的关联线性模型，这就要求实际输出与线性方程预测的输出的残差平方和（sum of squares of differences）最小化。这种方法被称为普通最小二乘法 （Ordinary Least Squares，OLS）。</p><p>你可能觉得用一条曲线对这些点进行拟合效果会更好，但是线性回归不允许这样做。线性回归的主要优点就是方程简单。如果你想用非线性回归，可能会得到更准确的模型，但是拟合速度会慢很多。线性回归模型就像前面那张图里显示的，<strong>用一条直线近似数据点的趋势</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line"># 加载数据</span><br><span class="line">filename = sys.argv[1]</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_singlevar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import linear_model</span><br><span class="line">linear_regr = linear_model.LinearRegression()</span><br><span class="line">linear_regr.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>我们利用训练数据集训练了线性回归器。向fit 方法提供输入数据即可训练模型。用下面的代码看看它如何拟合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(&apos;训练集拟合效果&apos;)</span><br><span class="line">y_train_pred = linear_regr.predict(X_train)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X_train, y_train)</span><br><span class="line">plt.plot(X_train, y_train_pred, color=&apos;green&apos;, linewidth=2)</span><br><span class="line">plt.title(&apos;Training Data&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%861.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = linear_regr.predict(X_test)</span><br><span class="line">print(&quot;测试集拟合效果&quot;)</span><br><span class="line">plt.scatter(X_test, y_test)</span><br><span class="line">plt.plot(X_test, y_test_pred, color=&apos;green&apos;)</span><br><span class="line">plt.title(&quot;Test Data&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%A2%84%E5%A4%84%E7%90%862.png" alt=""></p><h1 id="计算回归准确性"><a href="#计算回归准确性" class="headerlink" title="计算回归准确性"></a>计算回归准确性</h1><p>现在已经建立了回归器，接下来最重要的就是如何评价回归器的拟合效果。在模型评价的相关内容中，用误差 （error）表示实际值与模型预测值之间的差值。</p><p>下面快速了解几个衡量回归器拟合效果的重要指标（metric）。回归器可以用许多不同的指标进行衡量，部分指标如下所示。</p><ul><li><p><strong>平均绝对误差（mean absolute error）</strong> ：这是给定数据集的所有数据点的绝对误差平均值。</p></li><li><p><strong>均方误差（mean squared error）</strong> ：这是给定数据集的所有数据点的误差的平方的平均值。这是最流行的指标之一。</p></li><li><p><strong>中位数绝对误差（median absolute error）</strong> ：这是给定数据集的所有数据点的误差的中位数。这个指标的主要优点是可以消除异常值（outlier）的干扰。测试数据集中的单个坏点不会影响整个误差指标，均值误差指标会受到异常点的影响。</p></li><li><p><strong>解释方差分（explained variance score）</strong> ：这个分数用于衡量我们的模型对数据集波动的解释能力。如果得分1.0分，那么表明我们的模型是完美的。</p></li><li><p><strong>R方得分（R2 score）</strong> ：这个指标读作“R方”，是指确定性相关系数，用于衡量模型对未知样本预测的效果。最好的得分是1.0，值也可以是负数。</p></li></ul><p>“每个指标都描述得面面俱到是非常乏味的，因此只选择一两个指标来评估我们的模型。通常的做法是尽量保证均方误差最低，而且解释方差分最高”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 0.54<br>均方误差（mean squared error） ： 0.38<br>中位数绝对误差（median absolute error） ： 0.54<br>解释方差分（explained variance score） ： 0.68<br>R方得分（R2 score） ： 1.0</p></blockquote><h1 id="保存模型数据"><a href="#保存模型数据" class="headerlink" title="保存模型数据"></a>保存模型数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">regr = pickle.dumps(linear_regr) # 保存</span><br><span class="line">regr1 = pickle.loads(regr) # 加载</span><br><span class="line">regr1.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p></blockquote><p>在scikit的具体情况下，使用 joblib 替换 pickle（ joblib.dump &amp; joblib.load ）可能会更有趣，这对大数据更有效，但只能序列化 (pickle) 到磁盘而不是字符串变量:</p><p>之后，您可以加载已保存的模型（可能在另一个 Python 进程中）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.externals import joblib</span><br><span class="line">joblib.dump(linear_regr, &apos;regr.pkl&apos;) </span><br><span class="line">regr2 = joblib.load(&apos;regr.pkl&apos;) </span><br><span class="line">regr2.predict(X_test)</span><br></pre></td></tr></table></figure><blockquote><p>array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973])</p></blockquote><h1 id="创建岭回归"><a href="#创建岭回归" class="headerlink" title="创建岭回归"></a>创建岭回归</h1><p>线性回归的主要问题是对异常值敏感。在真实世界的数据收集过程中，经常会遇到错误的度量结果。而线性回归使用的普通最小二乘法，其目标是使平方误差最小化。这时，由于异常值误差的绝对值很大，因此会引起问题，从而破坏整个模型。</p><p>普通最小二乘法在建模时会考虑每个数据点的影响，因此，最终模型就会瘦异常值影响较大。显然，我们发现这个模型不是最优的。为了避免这个问题，我们引入正则化项 的系数作为阈值来消除异常值的影响。这个方法被称为岭回归 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">X = []</span><br><span class="line">y = []</span><br><span class="line">with open(&apos;data_multivar.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    for line in f.readlines():</span><br><span class="line">    data = [float(i) for i in line.split(&apos;,&apos;)]</span><br><span class="line">    xt, yt = data[:-1], data[-1]</span><br><span class="line">    X.append(xt)</span><br><span class="line">    y.append(yt)</span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,3)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 3)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p>alpha 参数控制回归器的复杂程度。当alpha 趋于0 时，岭回归器就是用普通最小二乘法的线性回归器。因此，如果你希望模型对异常值不那么敏感，就需要设置一个较大的alpha 值。这里把alpha 值设置为0.01 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">rid = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)</span><br><span class="line"></span><br><span class="line">rid.fit(X_train, y_train)</span><br><span class="line">y_test_pred = rid.predict(X_test)</span><br><span class="line"></span><br><span class="line">import sklearn.metrics as sm</span><br><span class="line"></span><br><span class="line">print(&quot;平均绝对误差（mean absolute error） ：&quot;</span><br><span class="line"> , round(sm.mean_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差（mean squared error） ：&quot;</span><br><span class="line"> , round(sm.mean_squared_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;中位数绝对误差（median absolute error） ：&quot;</span><br><span class="line"> , round(sm.median_absolute_error(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;解释方差分（explained variance score） ：&quot;</span><br><span class="line"> , round(sm.explained_variance_score(y_test, y_test_pred), 2))</span><br><span class="line"></span><br><span class="line">print(&quot;R方得分（R2 score） ：&quot;</span><br><span class="line"> , round(sm.r2_score(y_test, y_test_pred)))</span><br></pre></td></tr></table></figure></p><blockquote><p>平均绝对误差（mean absolute error） ： 3.95<br>均方误差（mean squared error） ： 23.15<br>中位数绝对误差（median absolute error） ： 3.69<br>解释方差分（explained variance score） ： 0.84<br>R方得分（R2 score） ： 1.0</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Line</span><br><span class="line">line = Line(&quot;期望值测试对比&quot;)</span><br><span class="line">line.add(&apos;测试目标值&apos;, np.linspace(-20,40,len(y_test)), y_test, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line.add(&apos;实际测试值&apos;, np.linspace(-20,40,len(y_test)),  y_test_pred, mark_line=[&quot;average&quot;], is_datazoom_show=True)</span><br><span class="line">line</span><br><span class="line"></span><br><span class="line"># 80%训练集和20%测试集</span><br><span class="line">num_train = int(0.8 * len(X))</span><br><span class="line">num_test = len(X) - num_train</span><br><span class="line">X_train = np.array(X[:num_train]).reshape(num_train,1)</span><br><span class="line">y_train = np.array(y[:num_train])</span><br><span class="line"></span><br><span class="line">X_test = np.array(X[num_train:]).reshape(num_test, 1)</span><br><span class="line">y_test = np.array(y[num_train:])</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%88%9B%E5%BB%BA%E5%B2%AD%E5%9B%9E%E5%BD%92.png" alt=""></p><h1 id="创建多项式回归器（重点）"><a href="#创建多项式回归器（重点）" class="headerlink" title="创建多项式回归器（重点）"></a>创建多项式回归器（重点）</h1><p>数据点本身的模式中带有自然的曲线，而线性模型是不能捕捉到这一点的。多项式回归模型的曲率是由多项式的次数决定的。随着模型曲率的增加，模型变得更准确。但是，增加曲率的同时也增加了模型的复杂性，因此拟合速度会变慢。当我们对模型的准确性的理想追求与计算能力限制的残酷现实发生冲突时，就需要综合考虑了。</p><p>下面使用岭回归的数据，注意和简单线性回归的区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">#将曲线的多项式次数初始值设置为3</span><br><span class="line">poly = PolynomialFeatures(degree = 20)</span><br><span class="line"># “其中，X_train_transformed 表示多项式形式的输入，与线性回归模型是一样的。”</span><br><span class="line">X_train_transformed = poly.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line">#测试一下</span><br><span class="line">dp = X_train[0].reshape(1,-1)</span><br><span class="line">poly_dp = poly.fit_transform(dp)</span><br><span class="line"></span><br><span class="line">poly_liner = linear_model.LinearRegression()</span><br><span class="line">poly_liner.fit(X_train_transformed, y_train) #这里注意输入转换后的X_train</span><br><span class="line"></span><br><span class="line">print (&quot;\nLinear regression:&quot;, rid.predict(dp)[0])</span><br><span class="line">print (&quot;\nPolynomial regression:&quot;, poly_liner.predict(poly_dp)[0]) ##这输入转换后的X_test</span><br></pre></td></tr></table></figure><blockquote><p>Linear regression: -11.058646635286552<br>Polynomial regression: -8.070076359128953</p></blockquote><ul><li>多项式次数为1时 返回预测结果为：-11.058729498335897，欠拟合</li><li>多项式次数为10时 返回预测结果为：-8.206005341193759，这里与真实值-8.07已经非常接近了</li><li>多项式次数为20时 返回预测结果为：-8.070076359128953，针对这个值的预测最完美</li><li>多项式次数为100时 返回预测结果为：10.01397529328105，说明出现过拟合</li></ul><h1 id="AdaBoost算法估算房屋价格"><a href="#AdaBoost算法估算房屋价格" class="headerlink" title="AdaBoost算法估算房屋价格"></a>AdaBoost算法估算房屋价格</h1><p><strong>利用AdaBoost算法的决策树回归器<code>（decision tree regreessor）</code>来估算房屋价格</strong></p><p>决策树是一个树状模型，每个节点都做出一个决策，从而影响最终结果。叶子节点表示输出数值，分支表示根据输入特征做出的中间决策。<code>AdaBoost</code>算法是指自适应增强（<code>adaptive boosting</code>）算法，这是一种利用其他系统增强模型准确性的技术。这种技术是将不同版本的算法结果进行组合，用加权汇总的方式获得最终结果，被称为弱学习器 （<code>weak learners</code>）。<code>AdaBoost</code>算法在每个阶段获取的信息都会反馈到模型中，这样学习器就可以在后一阶段重点训练难以分类的样本。这种学习方式可以增强系统的准确性。</p><p>首先使用<code>AdaBoost</code>算法对数据集进行回归拟合，再计算误差，然后根据误差评估结果，用同样的数据集重新拟合。可以把这些看作是回归器的调优过程，直到达到预期的准确性。假设你拥有一个包含影响房价的各种参数的数据集，我们的目标就是估计这些参数与房价的关系，这样就可以根据未知参数估计房价了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">from sklearn.ensemble import AdaBoostRegressor</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.metrics import mean_squared_error, explained_variance_score</span><br><span class="line">from sklearn.utils import shuffle</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">hous_data = datasets.load_boston()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 利用shuffle函数把数据的顺序打乱（参数random_state用来控制如何打乱数据）</span><br><span class="line">X, y = shuffle(hous_data.data, hous_data.target, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.8 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line"># 选择最大深度为5的决策树回归模型</span><br><span class="line">dtre = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 再用带AdaBoost算法的决策树回归模型进行拟合与上面进行比较</span><br><span class="line">abre = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=400, random_state=7)</span><br><span class="line">abre.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">#  看看AdaBoost算法对决策树回归器的训练效果有多大改善</span><br><span class="line">y_pred_dt = dtre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_dt)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_dt)</span><br><span class="line">print(&quot;决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;决策树-解释方差: &quot;, evs)</span><br><span class="line"></span><br><span class="line">y_pred_ab = abre.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred_ab)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred_ab)</span><br><span class="line">print(&quot;\nAbaBoost决策树-均方误差: &quot;, mse)</span><br><span class="line">print(&quot;AbaBoost决策树-解释方差: &quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>决策树-均方误差:  12.74782456548819<br>决策树-解释方差:  0.8454595720920495<br>AbaBoost决策树-均方误差:  7.015648111222207<br>AbaBoost决策树-解释方差:  0.9147414844474588</p></blockquote><h1 id="计算特征的相对重要性-（如交通案例计算各出口贡献率）"><a href="#计算特征的相对重要性-（如交通案例计算各出口贡献率）" class="headerlink" title="计算特征的相对重要性 （如交通案例计算各出口贡献率）"></a>计算特征的相对重要性 （如交通案例计算各出口贡献率）</h1><p><strong>(<em>modle.feature__importances</em>)</strong></p><p>在这个案例中，我们用了13个特征，它们对模型都有贡献。但是，有一个重要的问题出现了：如何判断哪个特征更加重要？显然，所有的特征对结果的贡献是不一样的。如果需要忽略一些特征，就需要知道哪些特征不太重要。scikit-learn里面有这样的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def plot_feature_importances(feature_importances, title, feature_names):</span><br><span class="line"> # 将重要性值标准化</span><br><span class="line">    feature_importances = 100.0 * (feature_importances / max(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 将得分从高到低排序</span><br><span class="line">    index_sorted = np.flipud(np.argsort(feature_importances))</span><br><span class="line"></span><br><span class="line"> # 让X坐标轴上的标签居中显示</span><br><span class="line">    pos = np.arange(index_sorted.shape[0]) + 0.5</span><br><span class="line"></span><br><span class="line"> # 画条形图</span><br><span class="line"> plt.figure()</span><br><span class="line"> plt.bar(pos, feature_importances[index_sorted], align=&apos;center&apos;)</span><br><span class="line"> plt.xticks(pos, feature_names[index_sorted])</span><br><span class="line"> plt.ylabel(&apos;Relative Importance&apos;)</span><br><span class="line"> plt.title(title)</span><br><span class="line"> plt.show()</span><br><span class="line"></span><br><span class="line"># 画出特征的相对重要性</span><br><span class="line">plot_feature_importances(dtre.feature_importances_, &apos;Decision Tree regressor&apos;, hous_data.feature_names)</span><br><span class="line">plot_feature_importances(abre.feature_importances_, &apos;AdaBoost regressor&apos;, hous_data.feature_names)</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A71.png" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A72.png" alt=""></p><p>上图可以看出不带AbaBoost的决策树回归器显示最重要的特征是RM，而带AbaBoost算法的决策回归器现实的最主要特征是LASTAT。现实生活中如果对这个数据集建立不同的回归器会发现最重要的特征就是LSTAT，这足以体现AbaBoost算法对决策树训练效果的改善。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;影响房价的因素分析&quot;)</span><br><span class="line">pie.add(&quot;决策树回归器&quot;, hous_data.feature_names, dtre.feature_importances_, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[35, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie.add(&quot;AbaBoost决策树&quot;, hous_data.feature_names, abre.feature_importances_,is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[75, 50],radius=[0, 50])</span><br><span class="line"></span><br><span class="line">pie</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E5%AF%B9%E9%87%8D%E8%A6%81%E6%80%A73.png" alt=""></p><h1 id="随机森林评估共享单车的需求分布"><a href="#随机森林评估共享单车的需求分布" class="headerlink" title="随机森林评估共享单车的需求分布"></a>随机森林评估共享单车的需求分布</h1><p><strong>采用随机森林回归器<code>(random forest regressor)</code>估计输出结果。</strong></p><p>随机森林死一个决策树合集，它基本上就是用一组由数据集的若干子集构建的决策树构成，再用决策树平均值改善整体学习效果</p><p>我们将使用bike_day.csv文件中的数据集，它可以在 <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a> 获取。这份数据集一共16列，前两列是序列号与日期，分析的时候可以不用；最后三列数据是不同类型的输出结果；最后一列是第十四列与第十五列的和，因此建立模型时可以不考虑第十四列与第十五列。</p><p>参数<code>n_estimators</code>是指评估器<code>（estimator）</code>的数量，表示随机森林需要使用的决策树数量；<br>参数<code>max_depth</code> 是指每个决策树的最大深度；参数<code>min_samples_split</code>是指决策树分裂一个节点需要用到的最小数据样本量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from housing import plot_feature_importances   #这个方法源码参考上例</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&apos;bike_day.csv&apos;,sep=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">X = data[data.columns[2:13]]</span><br><span class="line">y = data[data.columns[-1]]</span><br><span class="line">f_name = X.columns</span><br><span class="line"></span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9 * len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line"></span><br><span class="line">rf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=12)</span><br><span class="line">rf_regr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = rf_regr.predict(X_test)</span><br><span class="line"></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;随机森林回归器效果：&quot;)</span><br><span class="line">print(&quot;均方误差：&quot;, round(mse, 2))</span><br><span class="line">print(&quot;解释方差分：&quot;, round(evs, 2))</span><br></pre></td></tr></table></figure></p><blockquote><p>随机森林回归器效果：<br>均方误差： 368026.24<br>解释方差分： 0.89</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pyecharts import Pie</span><br><span class="line"></span><br><span class="line">attr = f_name</span><br><span class="line">v1 = rf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A6.png" alt=""></p><p><strong>利用按小时的数据计算相关性</strong></p><p>这里要用到3～14列<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(X_train)</span><br></pre></td></tr></table></figure></p><blockquote><p>15641</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hour_data = pd.read_csv(&apos;bike_hour.csv&apos;, sep=&apos;,&apos;)</span><br><span class="line">X = hour_data[hour_data.columns[2:14]]</span><br><span class="line">y = hour_data[hour_data.columns[-1]]</span><br><span class="line">X, y = shuffle(X, y, random_state=7)</span><br><span class="line"></span><br><span class="line">num = int(0.9*len(X))</span><br><span class="line">X_train, y_train = X[:num], y[:num]</span><br><span class="line">X_test, y_test = X[num:], y[num:]</span><br><span class="line">f_names = X.columns</span><br><span class="line"></span><br><span class="line">hrf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=10)</span><br><span class="line">hrf_regr.fit(X_train, y_train)</span><br><span class="line">y_pred = hrf_regr.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">evs = explained_variance_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(&quot;均方误差：&quot;, mse)</span><br><span class="line">print(&quot;解释方差分：&quot;, evs)</span><br></pre></td></tr></table></figure><blockquote><p>均方误差： 1884.1767363623571<br>解释方差分： 0.9414038595964176</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">attr = f_names</span><br><span class="line">v1 = hrf_regr.feature_importances_</span><br><span class="line">pie = Pie(&quot;共享单车因素分析&quot;)</span><br><span class="line">pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,</span><br><span class="line">label_emphasis_textsize=14, is_random=True, </span><br><span class="line">legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A62.png" alt=""></p><p>由图可见，其中最重要的特征是一天中的不同时间点（hr），其次重要的是温度，这完全符合人们的直觉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86_%E5%B0%81%E9%9D%A2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="sklearn" scheme="http://frankblog.site/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之逻辑回归</title>
    <link href="http://frankblog.site/2018/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://frankblog.site/2018/06/04/机器学习之逻辑回归/</id>
    <published>2018-06-04T06:31:16.158Z</published>
    <updated>2018-06-08T04:10:19.100Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="逻辑回归基础"><a href="#逻辑回归基础" class="headerlink" title="逻辑回归基础"></a>逻辑回归基础</h1><h2 id="Sigmoid预测函数"><a href="#Sigmoid预测函数" class="headerlink" title="Sigmoid预测函数"></a>Sigmoid预测函数</h2><p>在逻辑回归中，定义预测函数为：</p><p>$$<br>h_\theta (x) = g(z)<br>$$<br>其中，\(z=\theta^Tx\)是<strong>分类边界</strong>，且\(g(z)=\frac{1}{1+e^{-z}}\)<br>g(z)称之为 Sigmoid Function，亦称 Logic Function，其函数图像如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/sigmoid%E5%87%BD%E6%95%B0.jpg" alt=""></p><p>可以看到，预测函数 hθ(x)被很好地限制在了 0、1 之间，并且，sigmoid 是一个非常好的阈值函数：阈值为 0.5，大于 0.5为 1 类，反之为 0 类。函数曲线过渡光滑自然，关于 0.5中心对称也极具美感。</p><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><ul><li><strong>线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><ul><li><strong>非线性决策边界</strong><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.png" alt="link"></li></ul><h2 id="预测代价函数"><a href="#预测代价函数" class="headerlink" title="预测代价函数"></a>预测代价函数</h2><p>下面两幅图中，左图这样犬牙差互的代价曲线（非凸函数）显然会使我们在做梯度下降的时候陷入迷茫，任何一个极小值都有可能被错认为最小值，但无法获得最优预测精度。但在右图的代价曲线中，就像滑梯一样，我们就很容易达到最小值：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E4%B8%8E%E9%9D%9E%E5%87%B8%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>逻辑回归定义的代价函数为：</p><p>$$<br>J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})<br>$$</p><p>为保证代价函数呈凸形曲线，则定义 \(Cost(h_\theta(x^{(i)}),y^{(i)})\)：</p><p>$$<br>Cost(h_\theta(x),y)=<br>\begin{cases}<br>-log(h_\theta(x)),&amp;\mbox{if $y=1$}\<br>-log(1-h_\theta(x)),&amp;\mbox{if $y=0$}<br>\end{cases}<br>$$<br>该函数等价于：</p><p>$$<br>\begin{align<em>}<br>Cost(h_\theta(x),y) &amp;=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \<br>&amp;= (\,log\,(g(X\theta))^Ty+(\,log\,(1-g(X\theta))^T(1-y)<br>\end{align</em>}<br>$$</p><p>代价函数随预测值 hθ(x)hθ(x) 的变化如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.png" alt=""> </p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.png" alt=""></p><h2 id="手推LR"><a href="#手推LR" class="headerlink" title="手推LR"></a>手推LR</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR1.png" alt="link"></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%89%8B%E6%8E%A8LR2.png" alt="link"></p><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</p><p>如下例所示，我们将 θ3 及 θ4 减小（惩罚）到趋近于 00，原本过拟合的曲线就变得更加平滑，趋近于一条二次曲线（在本例中，二次曲线显然更能反映住房面积和房价的关系），也就能够更好的根据住房面积来预测房价。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98.png" alt="link"></p><p>直观来看，如果我们想解决这个例子中的过拟合问题，最好能将的影响消除，也就是让。假设我们对进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大惩罚项，例如：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%982.png" alt=""></p><p>这样在最小化Cost函数的时候，。正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%983.png" alt=""></p><p>lambda是正则项系数：</p><p>1.如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；</p><p>2.如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。</p><p>正则化后的梯度下降中θ的更新变为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%984.png" alt=""></p><p>正则化后的线性回归的Normal Equation的公式为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%985.png" alt=""></p><hr><h1 id="scikit-learn-逻辑回归类"><a href="#scikit-learn-逻辑回归类" class="headerlink" title="scikit-learn 逻辑回归类"></a>scikit-learn 逻辑回归类</h1><h2 id="带L1与L2正则的逻辑回归损失函数"><a href="#带L1与L2正则的逻辑回归损失函数" class="headerlink" title="带L1与L2正则的逻辑回归损失函数"></a>带L1与L2正则的逻辑回归损失函数</h2><p>scikit-learn在<code>LogisticRegression</code>的<code>sklearn.linear_model.LogisticRegression</code>类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p><p>作为优化问题，带 L2 正则的二分类 logistic 回归要最小化以下代价函数（cost function）：</p><p>$$\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) $$</p><p>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p><p>$$<br>\underset{w, c}{min\,} |w|<em>1 + C \sum</em>{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)$$</p><p> L1 范数作为正则项由以下几个用途：</p><ul><li>特征选择： 它会让模型参数向量里的元素为0的点尽量多。 因此可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以 L1 范数解决过拟合措施实际上是减少特征数量。</li><li>可解释性： 模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。 这样我们就容易解释模型的因果关系。 比如针对某个癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性成关键作用。 稀疏化后，只留下几个关键特征，就更容易看到因果关系</li></ul><p>由此可见， L1 范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。 大部分情况下，我们解决过拟合问题，还是选择 L2 单数作为正则项， 这也是 sklearn 里的默认值。</p><h2 id="优化方法参数"><a href="#优化方法参数" class="headerlink" title="优化方法参数"></a>优化方法参数</h2><p>solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</p><ul><li>liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li><li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li><li>sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li><li>saga：线性收敛的随机优化算法的的变重。</li></ul><table><thead><tr><th>Case</th><th>Solver</th></tr></thead><tbody><tr><td>L1正则</td><td>“liblinear” or “saga”</td></tr><tr><td>多项式损失（multinomial loss）</td><td>“lbfgs”, “sag”, “saga” or “newton-cg”</td></tr><tr><td>大数据集（n_samples）</td><td>“sag” or “saga”</td></tr></tbody></table><p>“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear”</p><h1 id="乳腺癌检测"><a href="#乳腺癌检测" class="headerlink" title="乳腺癌检测"></a>乳腺癌检测</h1><p>使用逻辑回归算法解决乳腺癌检测问题。 我们需要先采集肿瘤病灶造影图片， 然后对图片进行分析， 从图片中提取特征， 在根据特征来训练模型。 最终使用模型来检测新采集到的肿瘤病灶造影， 判断是良性还是恶性。 这个是典型的二元分类问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 加载数据</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape,&apos;\n&apos;, X[0], &apos;\n&apos;, y[0])</span><br></pre></td></tr></table></figure></p><blockquote><p>(569, 30) (569,)<br> [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01<br> 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02<br> 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01<br> 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01<br> 4.601e-01 1.189e-01]<br> 0</p></blockquote><p>实际上它只关注了 10 个特征，然后又构造出来每个特征的标准差及最大值，这样每个特征又衍生出了两个特征，所以共有30个特征。</p><p>疑问： 该方式是否直接会导致多重共线性的出现？</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">train_score = model.score(X_train, y_train)</span><br><span class="line">test_score = model.score(X_test, y_test)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">print(&quot;train_score:&quot;, train_score)</span><br><span class="line">print(&quot;test_score:&quot;, test_score)</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9626373626373627<br>test_score: 0.9473684210526315</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">from sklearn.metrics import precision_score, recall_score, f1_score</span><br><span class="line">print(&quot;查准率：&quot;, precision_score(y_test, y_pred))</span><br><span class="line">print(&quot;召回率：&quot;, recall_score(y_test, y_pred))</span><br><span class="line">print(&quot;F1Score：&quot;, f1_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">print(np.equal(y_pred, y_test).shape[0], y_test.shape[0]) # 输出预测匹配成功数量和测试样本的数量</span><br></pre></td></tr></table></figure><blockquote><p>查准率： 0.9358974358974359<br> 召回率： 0.9733333333333334<br>F1Score： 0.954248366013072<br>114 114</p></blockquote><p>这里数量上显示全部都预测正确，而test_score却不是1，是因为sklearn不是使用这个数据来计算得分，因为这个数据不能完全反映误差情况，而是使用预测概率来计算模型得分。</p><h2 id="查看预测自信度"><a href="#查看预测自信度" class="headerlink" title="查看预测自信度"></a>查看预测自信度</h2><p>二元分类模型会针对每个样本输出的两个概率，即0和1的概率，哪个概率高就预测器哪个类别。我们可以找出针对测试数据集，模型预测的“自信度”低于90%的样本。我们先计算出测试数据集里每个样本的预测概率数据，针对每个样本会有两个数据：一个预测为0，一个预测为1。结合找出预测为阴性和阳性的概率大于0.1的样本。我们可以看下概率数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 计算每个测试样本的预测概率：</span><br><span class="line"></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">print(&quot;自信度示例：&quot;,y_pred_proba[0])</span><br></pre></td></tr></table></figure><blockquote><p>自信度示例： [0.00452578 0.99547422]</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba_0 = y_pred_proba[:, 0] &gt; 0.1</span><br><span class="line">result = y_pred_proba[y_pred_proba_0]</span><br><span class="line"></span><br><span class="line">y_pred_proba_1 = result[:, 1] &gt; 0.1</span><br><span class="line">print(result[y_pred_proba_1])</span><br></pre></td></tr></table></figure><blockquote><p>[[0.11338788 0.88661212]<br> [0.18245824 0.81754176]<br> [0.13110396 0.86889604]<br> [0.35245276 0.64754724]<br> [0.30664405 0.69335595]<br> [0.24931118 0.75068882]<br> [0.8350464  0.1649536 ]<br> [0.44807883 0.55192117]<br> [0.74071324 0.25928676]<br> [0.43085792 0.56914208]<br> [0.13388416 0.86611584]<br> [0.33507985 0.66492015]<br> [0.53672412 0.46327588]<br> [0.11422612 0.88577388]<br> [0.42946531 0.57053469]<br> [0.69759146 0.30240854]<br> [0.25982004 0.74017996]<br> [0.12179042 0.87820958]<br> [0.88546887 0.11453113]]</p></blockquote><h2 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#这里使用Pipeline来增加多项式特征</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">def poly_model(degree=2, penalty=penalty):</span><br><span class="line">    poly_features = PolynomialFeatures(degree=degree, include_bias=False)</span><br><span class="line">    log_regr = LogisticRegression(penalty=penalty) # 注意这里是L1而不是11，指的是使用L1范式作为其正则项</span><br><span class="line">    pipeline = Pipeline([(&quot;poly_features&quot;,poly_features),(&quot;log_regr&quot;,log_regr)])</span><br><span class="line">    return pipeline</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 接着增加二阶多项式特征，创建并训练模型</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">model = poly_model(degree=2, penalty=&apos;l1&apos;)</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&quot;train_score:&quot;,model.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score:&quot;,model.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>train_score: 0.9934065934065934<br>test_score: 0.9649122807017544</p></blockquote><p>这里要注意的是使用L1范式作为其正则项，参数为<code>penalty=l1</code>。L1范数作为其正则项，可以实现参数的稀疏化，即自动帮我买选择出哪些对模型有关联的特征。我买可以观察下有多少个特征没有被丢弃即对应的模型参数θj非0：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_regr = model.named_steps[&apos;log_regr&apos;]</span><br><span class="line">print(&quot;特征总量：&quot;,log_regr.coef_.shape[1])</span><br><span class="line">print(&quot;特征保留量：&quot;, np.count_nonzero(log_regr.coef_))</span><br></pre></td></tr></table></figure><blockquote><p>特征总量： 495<br>特征保留量： 114</p></blockquote><p>逻辑回归模型的<code>coef_</code>属性里保存的就是模型参数。 从输出结果看，增加二阶多项式特征后，输入特征由原来的30个增加到了595个，在L1范数的“惩罚”下最终只保留了92个有效特征</p><h2 id="实验：利用决策树画出原始数据对预测相关性非0对特征"><a href="#实验：利用决策树画出原始数据对预测相关性非0对特征" class="headerlink" title="实验：利用决策树画出原始数据对预测相关性非0对特征"></a>实验：利用决策树画出原始数据对预测相关性非0对特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">dtmodel = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">dtmodel.fit(X_train, y_train)</span><br><span class="line">print(&quot;train_score&quot;, dtmodel.score(X_train, y_train))</span><br><span class="line">print(&quot;test_score&quot;, dtmodel.score(X_test, y_test))</span><br><span class="line">from pyecharts import Bar</span><br><span class="line">index = np.nonzero(dtmodel.feature_importances_)</span><br><span class="line">bar = Bar()</span><br><span class="line">bar.add(&quot;&quot;, cancer.feature_names[index],dtmodel.feature_importances_[index])</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><blockquote><p>train_score 0.9910875596851206<br>test_score 0.6296416546416548</p></blockquote><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%88%A9%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%BB%E5%87%BA%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%AF%B9%E9%A2%84%E6%B5%8B%E7%9B%B8%E5%85%B3%E6%80%A7%E9%9D%9E0%E5%AF%B9%E7%89%B9%E5%BE%81.png" alt=""></p><h2 id="评估模型：画出学习曲线"><a href="#评估模型：画出学习曲线" class="headerlink" title="评估模型：画出学习曲线"></a>评估模型：画出学习曲线</h2><p>首先画出L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line">def plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 5)):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;train exs&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    return plt</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L1&quot;,&quot;degree:2 penalty=L1&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l1&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i], penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" alt=""></p><p>L2范数作为正则项画出对应一阶和二阶多项式学习曲线<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span><br><span class="line">titles = [&quot;degree:1 penalty=L2&quot;,&quot;degree:2 penalty=L2&quot;]</span><br><span class="line">degrees = [1,2]</span><br><span class="line">penalty = &apos;l2&apos;</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(12,4), dpi=120)</span><br><span class="line">for i in range(len(degrees)):</span><br><span class="line">    plt.subplot(1, len(degrees), i + 1)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i],penalty=penalty), titles[i],</span><br><span class="line">    X, y, ylim = (0.8, 1.01), cv = cv)</span><br><span class="line"></span><br><span class="line">print(&apos;耗时：&apos;, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%94%BB%E5%87%BA%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF1.png" alt=""></p><p>从上面两个图可以看出，使用二阶多项式并使用L1范数作为正则项的模型最优，训练样本评分最高，交叉验证样本评分最高。<br>训练样本评分和交叉验证样本评分之间的间隙还比较大，这说明可以通过采集更多数据来训练模型，以便进一步优化模型.</p><p>通过时间消耗对比上可以看出利用L1范式作为正则项需要花费的时间更多，是因为<code>sklearn</code>的<code>learning_curve()</code>函数在画学习曲线的过程中要对模型进行多次训练，并计算交叉验证样本评分。同时为了让曲线更平滑，针对每个点还会进行多次计算球平均值。这个就是<code>ShufferSplit</code>类的作用。在这个实例里只有569个样本是很小的数据集。如果数据集增加100倍，拿出来画学习曲线将是场灾难。</p><p>问题是针对大数据集，怎么画学习曲线？</p><p>思路一：可以考虑从大数据集选取一小部分数据来画学习曲线，待选择好最优的模型之后，在使用全部的数据来训练模型。这时需要警惕的是，尽量保证选择出来的这部分数据的<strong>标签分布与大数据集的标签分布相同</strong>，如针对二元分类，<strong>阳性和阴性比例要一致！</strong></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1-LR中损失函数的意义是什么？"><a href="#1-LR中损失函数的意义是什么？" class="headerlink" title="1.LR中损失函数的意义是什么？"></a>1.LR中损失函数的意义是什么？</h2><p>在LR中，最大似然函数与最小化对数损失函数等价</p><h2 id="2-LR与线性回归的联系和区别"><a href="#2-LR与线性回归的联系和区别" class="headerlink" title="2. LR与线性回归的联系和区别"></a>2. LR与线性回归的联系和区别</h2><p>逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</p><h2 id="3-LR与最大熵模型"><a href="#3-LR与最大熵模型" class="headerlink" title="3.LR与最大熵模型"></a>3.LR与最大熵模型</h2><p>逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。</p><ul><li><p>指数簇分布的最大熵等价于其指数形式的最大似然。</p></li><li><p>二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；</p></li><li><p>多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。</p></li></ul><h2 id="4-LR与svm"><a href="#4-LR与svm" class="headerlink" title="4.LR与svm"></a>4.LR与svm</h2><p>不同点:</p><ol><li><p>损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss</p></li><li><p>逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。</p></li><li><p>逻辑回归对概率建模，svm对分类超平面建模</p></li><li><p>逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有</p></li><li><p>逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响</p></li><li><p>逻辑回归是统计方法，svm是几何方法</p></li></ol><h2 id="5-LR与朴素贝叶斯"><a href="#5-LR与朴素贝叶斯" class="headerlink" title="5.LR与朴素贝叶斯"></a>5.LR与朴素贝叶斯</h2><ul><li><p>相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)形式跟逻辑回归是一样的。</p></li><li><p>不同的地方在于，逻辑回归为判别模型求的是p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。</p></li></ul><h2 id="6-多分类-softmax"><a href="#6-多分类-softmax" class="headerlink" title="6. 多分类-softmax"></a>6. 多分类-softmax</h2><p>如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。</p><h2 id="7-LR模型在工业界的应用"><a href="#7-LR模型在工业界的应用" class="headerlink" title="7.LR模型在工业界的应用"></a><strong>7.LR模型在工业界的应用</strong></h2><p>常见应用场景</p><ul><li><p>预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等）</p></li><li><p>分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等）</p></li></ul><p>LR适用上述场景的原因</p><p>LR模型自身的特点具备了应用广泛性</p><ul><li><p>模型易用：LR模型建模思路清晰，容易理解与掌握；</p></li><li><p>概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上；</p></li><li><p>强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性；</p></li><li><p>简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline；</p></li></ul><p>参考文献：<br><a href="https://blog.csdn.net/joycewyj/article/details/51596797" target="_blank" rel="noopener">https://blog.csdn.net/joycewyj/article/details/51596797</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92_%E5%B0%81%E9%9D%A2_%E7%9C%8B%E5%9B%BE%E7%8E%8B.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://frankblog.site/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib基本操作</title>
    <link href="http://frankblog.site/2018/06/03/Matplotlib%20%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://frankblog.site/2018/06/03/Matplotlib 基本操作/</id>
    <published>2018-06-03T03:59:17.192Z</published>
    <updated>2018-06-04T01:42:15.913Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">一张图胜过一千句话</font></blockquote><hr><h1 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h1><h2 id="1、设置坐标轴"><a href="#1、设置坐标轴" class="headerlink" title="1、设置坐标轴"></a>1、设置坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line"></span><br><span class="line">#使用`plt.figure`定义一个图像窗口. 使用`plt.plot`画(`x` ,`y2`)曲线. 使用`plt.plot`画(`x` ,`y1`)曲线，曲线的颜色属性(`color`)为红色;曲线的宽度(`linewidth`)为1.0；曲线的类型(`linestyle`)为虚线。</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line"></span><br><span class="line">#使用`plt.xlim`设置x坐标轴范围：(-1, 2)； 使用`plt.ylim`设置y坐标轴范围：(-2, 3)； 使用`plt.xlabel`设置x坐标轴名称：’I am x’； 使用`plt.ylabel`设置y坐标轴名称：’I am y’；</span><br><span class="line"></span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">plt.xlabel(&apos;I am x&apos;)</span><br><span class="line">plt.ylabel(&apos;I am y&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`np.linspace`定义范围以及个数：范围是(-1,2);个数是5\. 使用`print`打印出新定义的范围. 使用`plt.xticks`设置x轴刻度：范围是(-1,2);个数是5.</span><br><span class="line"></span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line"></span><br><span class="line">#使用`plt.yticks`设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用`plt.show`显示图像.</span><br><span class="line"></span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[r&apos;$really\ bad$&apos;, r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;, r&apos;$really\ good$&apos;])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%AE%BE%E7%BD%AE%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="2、调整坐标轴"><a href="#2、调整坐标轴" class="headerlink" title="2、调整坐标轴"></a>2、调整坐标轴</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y1 = 2*x + 1</span><br><span class="line">y2 = x**2</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)</span><br><span class="line">plt.xlim((-1, 2))</span><br><span class="line">plt.ylim((-2, 3))</span><br><span class="line">new_ticks = np.linspace(-1, 2, 5)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line">plt.yticks([-2, -1.8, -1, 1.22, 3],[&apos;$really\ bad$&apos;, &apos;$bad$&apos;, &apos;$normal$&apos;, &apos;$good$&apos;, &apos;$really\ good$&apos;])</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.xaxis.set_ticks_position`设置x坐标刻度数字或名称的位置：`bottom`.（所有位置：`top`，`bottom`，`both`，`default`，`none`）</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：x轴；使用`.set_position`设置边框位置：y=0的位置；（位置所有属性：`outward`，`axes`，`data`）</span><br><span class="line"></span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用`.yaxis.set_ticks_position`设置y坐标刻度数字或名称的位置：`left`.（所有位置：`left`，`right`，`both`，`default`，`none`）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line"></span><br><span class="line">#使用`.spines`设置边框：y轴；使用`.set_position`设置边框位置：x=0的位置；（位置所有属性：`outward`，`axes`，`data`） 使用`plt.show`显示图像。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># set line syles</span><br><span class="line">l1, = plt.plot(x, y1, label=&apos;linear line&apos;)</span><br><span class="line">l2, = plt.plot(x, y2, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;, label=&apos;square line&apos;)</span><br><span class="line"></span><br><span class="line">#参数 `loc=&apos;upper right&apos;` 表示图例将添加在图中的右上角.</span><br><span class="line">plt.legend(loc=&apos;upper right&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%AE%E6%94%B9%E5%9D%90%E6%A0%87%E8%BD%B4.png" alt=""></p><h2 id="3、辅助线和标识"><a href="#3、辅助线和标识" class="headerlink" title="3、辅助线和标识"></a>3、辅助线和标识</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 50)</span><br><span class="line">y = 2*x + 1</span><br><span class="line"></span><br><span class="line">#挪动坐标系</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0))</span><br><span class="line"></span><br><span class="line">#辅助线</span><br><span class="line">plt.figure(num=1, figsize=(8, 5),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line">x0 = 1</span><br><span class="line">y0 = 2*x0 + 1</span><br><span class="line">plt.plot([x0, x0,], [0, y0,], &apos;k--&apos;, linewidth=2.5)</span><br><span class="line"># set dot styles</span><br><span class="line">plt.scatter([x0, ], [y0, ], s=50, color=&apos;b&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#标注，其中参数xycoords=&apos;data&apos; 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords=&apos;offset points&apos; 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置.</span><br><span class="line">plt.annotate(r&apos;$2x+1=%s$&apos; % y0, xy=(x0, y0), xycoords=&apos;data&apos;, xytext=(+30, -30),</span><br><span class="line">             textcoords=&apos;offset points&apos;, fontsize=16,</span><br><span class="line">             arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">#注释</span><br><span class="line">plt.text(-3.7, 3, r&apos;$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$&apos;,</span><br><span class="line">         fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%BE%85%E5%8A%A9%E7%BA%BF%E5%92%8C%E6%A0%87%E8%AF%86.png" alt=""></p><h2 id="4、3D图框"><a href="#4、3D图框" class="headerlink" title="4、3D图框"></a>4、3D图框</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line"># X, Y value</span><br><span class="line">X = np.arange(-4, 4, 0.25)</span><br><span class="line">Y = np.arange(-4, 4, 0.25)</span><br><span class="line">X, Y = np.meshgrid(X, Y)    # x-y 平面的网格</span><br><span class="line">R = np.sqrt(X ** 2 + Y ** 2)</span><br><span class="line"># height value</span><br><span class="line">Z = np.sin(R)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/3d%E5%9B%BE.png" alt=""></p><h1 id="绘制动态图"><a href="#绘制动态图" class="headerlink" title="绘制动态图"></a>绘制动态图</h1><p>使用matplotlib为Jupyter / IPython中的动画图创建一些选项：</p><ul><li><p><strong>在循环中使用<code>display</code></strong>使用<code>IPython.display.display(fig)</code>在输出中显示图形。 使用一个循环，你需要在显示一个新数字之前清除输出。 请注意，这种技术通常不会那么流畅。 因此我会建议使用下面的任何一个。</p></li><li><p><strong><code>%matplotlib notebook</code></strong>使用IPython magic <code>%matplotlib notebook</code>将后端设置为笔记本后端。 这样可以保持图形不会显示静态PNG文件，因此也可以显示动画。 </p></li></ul><ul><li><strong><code>%matplotlib tk</code></strong>使用IPython magic <code>%matplotlib tk</code>将后端设置为tk后端。 这将在一个新的绘图窗口中打开这个图形，这是一个互动的，因此也可以显示动画。 </li></ul><ul><li><p><strong>将动画转换为mp4视频</strong> （已提供@Perfi选项）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_html5_video())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;html5&quot;</code> 。 这需要将ffmpeg视频编解码器转换为HTML5视频。 视频然后显示在内。 因此，这与<code>%matplotlib inline</code>后端兼容。 完整的例子：</p><ul><li><p><strong>将动画转换为JavaScript</strong> ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import HTML</span><br><span class="line">HTML(ani.to_jshtml())</span><br></pre></td></tr></table></figure></li></ul><p>或者在笔记本的开头使用<code>plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot;</code> 。 这将使用JavaScript将动画显示为HTML。 这与大多数新浏览器以及<code>%matplotlib inline</code>后端都非常兼容。 它在matplotlib 2.1或更高版本中可用。</p><h2 id="1、sin动态点曲线"><a href="#1、sin动态点曲线" class="headerlink" title="1、sin动态点曲线"></a>1、sin动态点曲线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">animation example 2</span><br><span class="line">author: Kiterun</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">x = np.linspace(0, 2*np.pi, 200)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">l = ax.plot(x, y)</span><br><span class="line">dot, = ax.plot([], [], &apos;ro&apos;)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(0, 2*np.pi)</span><br><span class="line">    ax.set_ylim(-1, 1)</span><br><span class="line">    return l</span><br><span class="line"></span><br><span class="line">def gen_dot():</span><br><span class="line">    for i in np.linspace(0, 2*np.pi, 200):</span><br><span class="line">        newdot = [i, np.sin(i)]</span><br><span class="line">        yield newdot</span><br><span class="line"></span><br><span class="line">def update_dot(newd):</span><br><span class="line">    dot.set_data(newd[0], newd[1])</span><br><span class="line">    return dot,</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update_dot, frames = gen_dot, interval = 100, init_func=init)</span><br><span class="line">ani.save(&apos;sin_dot.gif&apos;, writer=&apos;imagemagick&apos;, fps=30)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/sin_dot%20%281%29.gif" alt=""></p><h2 id="2、动态雨点"><a href="#2、动态雨点" class="headerlink" title="2、动态雨点"></a>2、动态雨点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"></span><br><span class="line"># New figure with white background</span><br><span class="line">fig = plt.figure(figsize=(6,6), facecolor=&apos;white&apos;)</span><br><span class="line"></span><br><span class="line"># New axis over the whole figure, no frame and a 1:1 aspect ratio</span><br><span class="line">ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)</span><br><span class="line"></span><br><span class="line"># Number of ring</span><br><span class="line">n = 50</span><br><span class="line">size_min = 50</span><br><span class="line">size_max = 50 ** 2</span><br><span class="line"></span><br><span class="line"># Ring position</span><br><span class="line">pos = np.random.uniform(0, 1, (n,2))</span><br><span class="line"></span><br><span class="line"># Ring colors</span><br><span class="line">color = np.ones((n,4)) * (0,0,0,1)</span><br><span class="line"># Alpha color channel geos from 0(transparent) to 1(opaque)</span><br><span class="line">color[:,3] = np.linspace(0, 1, n)</span><br><span class="line"></span><br><span class="line"># Ring sizes</span><br><span class="line">size = np.linspace(size_min, size_max, n)</span><br><span class="line"></span><br><span class="line"># Scatter plot</span><br><span class="line">scat = ax.scatter(pos[:,0], pos[:,1], s=size, lw=0.5, edgecolors=color, facecolors=&apos;None&apos;)</span><br><span class="line"></span><br><span class="line"># Ensure limits are [0,1] and remove ticks</span><br><span class="line">ax.set_xlim(0, 1), ax.set_xticks([])</span><br><span class="line">ax.set_ylim(0, 1), ax.set_yticks([])</span><br><span class="line"></span><br><span class="line">def update(frame):</span><br><span class="line">    global pos, color, size</span><br><span class="line"></span><br><span class="line">    # Every ring is made more transparnt</span><br><span class="line">    color[:, 3] = np.maximum(0, color[:,3]-1.0/n)</span><br><span class="line"></span><br><span class="line">    # Each ring is made larger</span><br><span class="line">    size += (size_max - size_min) / n</span><br><span class="line"></span><br><span class="line">    # Reset specific ring</span><br><span class="line">    i = frame % 50</span><br><span class="line">    pos[i] = np.random.uniform(0, 1, 2)</span><br><span class="line">    size[i] = size_min</span><br><span class="line">    color[i, 3] = 1</span><br><span class="line"></span><br><span class="line">    # Update scatter object</span><br><span class="line">    scat.set_edgecolors(color)</span><br><span class="line">    scat.set_sizes(size)</span><br><span class="line">    scat.set_offsets(pos)</span><br><span class="line"></span><br><span class="line">    # Return the modified object</span><br><span class="line">    return scat,</span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, update, interval=10, blit=True, frames=200)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8A%A8%E6%80%81%E9%9B%A8%E7%82%B9.png" alt=""></p><h2 id="3、阻尼摆"><a href="#3、阻尼摆" class="headerlink" title="3、阻尼摆"></a>3、阻尼摆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%98%BB%E5%B0%BC%E6%91%86.png" alt=""></p><h2 id="4、内切滚动球"><a href="#4、内切滚动球" class="headerlink" title="4、内切滚动球"></a>4、内切滚动球</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">from math import sin, cos</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy.integrate import odeint</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.animation as animation</span><br><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line">g = 9.8</span><br><span class="line">leng = 1.0</span><br><span class="line">b_const = 0.2</span><br><span class="line"></span><br><span class="line"># no decay case:</span><br><span class="line">def pendulum_equations1(w, t, l):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv  = - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line"># the decay exist case:</span><br><span class="line">def pendulum_equations2(w, t, l, b):</span><br><span class="line">    th, v = w</span><br><span class="line">    dth = v</span><br><span class="line">    dv = -b/l * v - g/l * sin(th)</span><br><span class="line">    return dth, dv</span><br><span class="line"></span><br><span class="line">t = np.arange(0, 20, 0.1)</span><br><span class="line">track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))</span><br><span class="line">#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))</span><br><span class="line">xdata = [leng*sin(track[i, 0]) for i in range(len(track))]</span><br><span class="line">ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.grid()</span><br><span class="line">line, = ax.plot([], [], &apos;o-&apos;, lw=2)</span><br><span class="line">time_template = &apos;time = %.1fs&apos;</span><br><span class="line">time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)</span><br><span class="line"></span><br><span class="line">def init():</span><br><span class="line">    ax.set_xlim(-2, 2)</span><br><span class="line">    ax.set_ylim(-2, 2)</span><br><span class="line">    time_text.set_text(&apos;&apos;)</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">def update(i):</span><br><span class="line">    newx = [0, xdata[i]]</span><br><span class="line">    newy = [0, ydata[i]]</span><br><span class="line">    line.set_data(newx, newy)</span><br><span class="line">    time_text.set_text(time_template %(0.1*i))</span><br><span class="line">    return line, time_text</span><br><span class="line"></span><br><span class="line">ani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)</span><br><span class="line">#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/roll.gif" alt=""></p><h2 id="5、分类超平面可视化"><a href="#5、分类超平面可视化" class="headerlink" title="5、分类超平面可视化"></a>5、分类超平面可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"># 算法可视化</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">%matplotlib notebook</span><br><span class="line">import copy</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from matplotlib import animation</span><br><span class="line"> </span><br><span class="line">training_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]]</span><br><span class="line">w = [0, 0]</span><br><span class="line">b = 0</span><br><span class="line">history = []</span><br><span class="line"> </span><br><span class="line">def update(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    update parameters using stochastic gradient descent</span><br><span class="line">    :param item: an item which is classified into wrong class</span><br><span class="line">    :return: nothing</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    global w, b, history</span><br><span class="line">    w[0] += 1 * item[1] * item[0][0]</span><br><span class="line">    w[1] += 1 * item[1] * item[0][1]</span><br><span class="line">    b += 1 * item[1]</span><br><span class="line">    print(w, b)</span><br><span class="line">    history.append([copy.copy(w), b])</span><br><span class="line">    # you can uncomment this line to check the process of stochastic gradient descent</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def cal(item):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    calculate the functional distance between &apos;item&apos; an the dicision surface. output yi(w*xi+b).</span><br><span class="line">    :param item:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    res = 0</span><br><span class="line">    for i in range(len(item[0])):</span><br><span class="line">        res += item[0][i] * w[i]</span><br><span class="line">    res += b</span><br><span class="line">    res *= item[1]</span><br><span class="line">    return res</span><br><span class="line"> </span><br><span class="line">def check():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    check if the hyperplane can classify the examples correctly</span><br><span class="line">    :return: true if it can</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    flag = False</span><br><span class="line">    for item in training_set:</span><br><span class="line">        if cal(item) &lt;= 0:</span><br><span class="line">            flag = True</span><br><span class="line">            update(item)</span><br><span class="line">    # draw a graph to show the process</span><br><span class="line">    if not flag:</span><br><span class="line">        print (&quot;RESULT: w: &quot; + str(w) + &quot; b: &quot; + str(b))</span><br><span class="line">    return flag</span><br><span class="line"> </span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    for i in range(1000):</span><br><span class="line">        if not check(): break</span><br><span class="line"> </span><br><span class="line">    # first set up the figure, the axis, and the plot element we want to animate</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = plt.axes(xlim=(0, 2), ylim=(-2, 2))</span><br><span class="line">    line, = ax.plot([], [], &apos;g&apos;, lw=2)</span><br><span class="line">    label = ax.text([], [], &apos;&apos;)</span><br><span class="line"> </span><br><span class="line">    # initialization function: plot the background of each frame</span><br><span class="line">    def init():</span><br><span class="line">        line.set_data([], [])</span><br><span class="line">        x, y, x_, y_ = [], [], [], []</span><br><span class="line">        for p in training_set:</span><br><span class="line">            if p[1] &gt; 0:</span><br><span class="line">                x.append(p[0][0])</span><br><span class="line">                y.append(p[0][1])</span><br><span class="line">            else:</span><br><span class="line">                x_.append(p[0][0])</span><br><span class="line">                y_.append(p[0][1])</span><br><span class="line"> </span><br><span class="line">        plt.plot(x, y, &apos;bo&apos;, x_, y_, &apos;rx&apos;)</span><br><span class="line">        plt.axis([-6, 6, -6, 6])</span><br><span class="line">        plt.grid(True)</span><br><span class="line">        plt.xlabel(&apos;x&apos;)</span><br><span class="line">        plt.ylabel(&apos;y&apos;)</span><br><span class="line">        plt.title(&apos;Perceptron Algorithm&apos;)</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    # animation function.  this is called sequentially</span><br><span class="line">    def animate(i):</span><br><span class="line">        global history, ax, line, label</span><br><span class="line"> </span><br><span class="line">        w = history[i][0]</span><br><span class="line">        b = history[i][1]</span><br><span class="line">        if w[1] == 0: return line, label</span><br><span class="line">        x1 = -7</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        x2 = 7</span><br><span class="line">        y2 = -(b + w[0] * x2) / w[1]</span><br><span class="line">        line.set_data([x1, x2], [y1, y2])</span><br><span class="line">        x1 = 0</span><br><span class="line">        y1 = -(b + w[0] * x1) / w[1]</span><br><span class="line">        label.set_text(history[i])</span><br><span class="line">        label.set_position([x1, y1])</span><br><span class="line">        return line, label</span><br><span class="line"> </span><br><span class="line">    # call the animator.  blit=true means only re-draw the parts that have changed.</span><br><span class="line">    print (history)</span><br><span class="line">    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=1000, repeat=True,</span><br><span class="line">                                   blit=True)</span><br><span class="line">    plt.show()</span><br><span class="line">    anim.save(&apos;perceptron.gif&apos;, fps=2, writer=&apos;imagemagick&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/perceptron1.gif" alt=""></p><h1 id="python其他可视化模块"><a href="#python其他可视化模块" class="headerlink" title="python其他可视化模块"></a>python其他可视化模块</h1><ul><li>Traits-为Python添加类型定义</li><li>TraitsUI-制作用户界面</li><li>Chaco-交互式图表</li><li>TVTK-三维可视化数据</li><li>Visual-制作3D演示动画</li><li>Mayavi-更方便的可视化</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://frankblog.site/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="Matplotlib" scheme="http://frankblog.site/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>markdown公式编辑</title>
    <link href="http://frankblog.site/2018/06/03/markdown%E5%85%AC%E5%BC%8F%E7%BC%96%E8%BE%91/"/>
    <id>http://frankblog.site/2018/06/03/markdown公式编辑/</id>
    <published>2018-06-03T03:42:27.605Z</published>
    <updated>2018-06-04T01:33:58.285Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/mathjax1.png" alt=""></p><a id="more"></a><hr><h1 id="加载mathjax"><a href="#加载mathjax" class="headerlink" title="加载mathjax"></a>加载mathjax</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>引入脚本对网页进行渲染<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="插入方式"><a href="#插入方式" class="headerlink" title="插入方式"></a>插入方式</h2><blockquote><p>这里分两种，一种是行间插入，另一种是另取一行</p></blockquote><h3 id="行内插入"><a href="#行内插入" class="headerlink" title="行内插入"></a>行内插入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(a+b\\)</span><br></pre></td></tr></table></figure><p>\(a+b\)</p><h3 id="单独一行"><a href="#单独一行" class="headerlink" title="单独一行"></a>单独一行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$a + b$$</span><br></pre></td></tr></table></figure><p>$$a + b$$</p><h2 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h2><h3 id="上、下标"><a href="#上、下标" class="headerlink" title="上、下标"></a>上、下标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$x_1$$</span><br><span class="line"></span><br><span class="line">$$x_1^2$$</span><br><span class="line"></span><br><span class="line">$$x^2_1$$</span><br><span class="line"></span><br><span class="line">$$x_&#123;22&#125;^&#123;(n)&#125;$$ #多于一位是要加 `&#123;&#125;` 包裹的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$$x_&#123;balabala&#125;^&#123;bala&#125;$$</span><br></pre></td></tr></table></figure><p>$$x_1$$</p><p>$$x_1^2$$</p><p>$$x^2_1$$</p><p>$$x_{22}^{(n)}$$</p><p>$$x_{balabala}^{bala}$$</p><h3 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$\frac&#123;x+y&#125;&#123;2&#125;$$</span><br><span class="line"></span><br><span class="line">$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$</span><br></pre></td></tr></table></figure><p>$$\frac{x+y}{2}$$</p><p>$$\frac{1}{1+\frac{1}{2}}$$</p><h3 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$</span><br><span class="line"></span><br><span class="line">$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$</span><br></pre></td></tr></table></figure><p>$$\sqrt{2}&lt;\sqrt[3]{3}$$<br>$$\sqrt{1+\sqrt[p]{1+a^2}}$$<br>$$\sqrt{1+\sqrt[^p!]{1+a^2}}$$</p><h3 id="求和、积分"><a href="#求和、积分" class="headerlink" title="求和、积分"></a>求和、积分</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$\sum_&#123;k=1&#125;^&#123;n&#125;\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class="line"></span><br><span class="line">\\(\sum_&#123;k=1&#125;^n\frac&#123;1&#125;&#123;k&#125;\\)</span><br><span class="line"></span><br><span class="line">$$\int_a^b f(x)dx$$</span><br><span class="line"></span><br><span class="line">\\(\int_a^b f(x)dx\\)</span><br></pre></td></tr></table></figure><p>$$\sum_{k=1}^{n}\frac{1}{k}$$</p><p>\(\sum_{k=1}^n\frac{1}{k}\)</p><p>$$\int_a^b f(x)dx$$</p><p>\(\int_a^b f(x)dx\)</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">紧贴 $a\\!b$</span><br><span class="line"></span><br><span class="line">没有空格 $ab$</span><br><span class="line"></span><br><span class="line">小空格 a\,b</span><br><span class="line"></span><br><span class="line">中等空格 a\;b</span><br><span class="line"></span><br><span class="line">大空格 a\ b</span><br><span class="line"></span><br><span class="line">quad空格 $a\quad b$</span><br><span class="line"></span><br><span class="line">两个quad空格 $a\qquad b$</span><br></pre></td></tr></table></figure><p>$$a\!b$$</p><p>$${ab}$$</p><p>$$a\,b$$</p><p>$$a\;b$$</p><p>$$a\ b$$</p><p>$$a\quad b$$</p><p>$$a\qquad b$$</p><h3 id="公式界定符"><a href="#公式界定符" class="headerlink" title="公式界定符"></a>公式界定符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\left(\sum_&#123;k=\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\frac&#123;1&#125;&#123;k&#125;\right)$$</span><br></pre></td></tr></table></figure><p>通过 <code>\left</code> 和 <code>\right</code> 后面跟界定符来对同时进行界定。<br>$$\left(\sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k}\right)$$</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;pmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Bmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;vmatrix&#125;$$</span><br><span class="line"></span><br><span class="line">$$\left|\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;\right|$$</span><br><span class="line"></span><br><span class="line">$$\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Vmatrix&#125;$$</span><br></pre></td></tr></table></figure><p>$$\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}$$</p><p>$$\begin{pmatrix}1 &amp; 2\\3 &amp;4\end{pmatrix}$$</p><p>$$\begin{bmatrix}1 &amp; 2\\3 &amp;4\end{bmatrix}$$</p><p>$$\begin{Bmatrix}1 &amp; 2\\3 &amp;4\end{Bmatrix}$$</p><p>$$\begin{vmatrix}1 &amp; 2\\3 &amp;4\end{vmatrix}$$</p><p>$$\left|\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}\right|$$</p><p>$$\begin{Vmatrix}1 &amp; 2\\3 &amp;4\end{Vmatrix}$$<br>类似于 left right，这里是 begin 和 end。而且里面有具体的矩阵语法，<code>&amp;</code> 区分行间元素，<code>\\\\</code> 代表换行。可以理解为 HTML 的标签之类的。</p><h3 id="排版数组"><a href="#排版数组" class="headerlink" title="排版数组"></a>排版数组</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;X&#125; =</span><br><span class="line">\left( \begin&#123;array&#125;&#123;ccc&#125;</span><br><span class="line">x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\\</span><br><span class="line">x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots</span><br><span class="line">\end&#123;array&#125; \right)</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\mathbf{X} =<br>\left( \begin{array}{ccc}<br>x_{11} &amp; x_{12} &amp; \ldots \\<br>x_{21} &amp; x_{22} &amp; \ldots \\<br>\vdots &amp; \vdots &amp; \ddots<br>\end{array} \right)<br>$$</p><h1 id="常用公式举例"><a href="#常用公式举例" class="headerlink" title="常用公式举例"></a>常用公式举例</h1><h2 id="公式组"><a href="#公式组" class="headerlink" title="公式组"></a>公式组</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">a &amp;= b+c+d \\\\</span><br><span class="line"></span><br><span class="line">x &amp;= y+z</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{align}<br>a &amp;= b+c+d \\<br>x &amp;= y+z<br>\end{align}<br>$$</p><h2 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">y=\begin&#123;cases&#125;</span><br><span class="line">-x,\quad x\leq 0 \\\\</span><br><span class="line">x,\quad x&gt;0</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>y=\begin{cases}<br>-x,\quad x\leq 0 \\<br>x,\quad x&gt;0<br>\end{cases}<br>$$</p><h1 id="常用数学符号"><a href="#常用数学符号" class="headerlink" title="常用数学符号"></a>常用数学符号</h1><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line">&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{array}{|c|c|c|c|c|c|c|c|}<br>\hline<br>{\alpha} &amp; {\backslash alpha} &amp; {\theta} &amp; {\backslash theta} &amp; {o} &amp; {o} &amp; {\upsilon} &amp; {\backslash upsilon} \\<br>\hline<br>{\beta} &amp; {\backslash beta} &amp; {\vartheta} &amp; {\backslash vartheta} &amp; {\pi} &amp; {\backslash pi} &amp; {\phi} &amp; {\backslash phi} \\<br>\hline<br>{\gamma} &amp; {\backslash gamma} &amp; {\iota} &amp; {\backslash iota} &amp; {\varpi} &amp; {\backslash varpi} &amp; {\varphi} &amp; {\backslash varphi} \\<br>\hline<br>{\delta} &amp; {\backslash delta} &amp; {\kappa} &amp; {\backslash kappa} &amp; {\rho} &amp; {\backslash rho} &amp; {\chi} &amp; {\backslash chi} \\<br>\hline<br>{\epsilon} &amp; {\backslash epsilon} &amp; {\lambda} &amp; {\backslash lambda} &amp; {\varrho} &amp; {\backslash varrho} &amp; {\psi} &amp; {\backslash psi} \\<br>\hline<br>{\varepsilon} &amp; {\backslash varepsilon} &amp; {\mu} &amp; {\backslash mu} &amp; {\sigma} &amp; {\backslash sigma} &amp; {\omega} &amp; {\backslash omega} \\<br>\hline<br>{\zeta} &amp; {\backslash zeta} &amp; {\nu} &amp; {\backslash nu} &amp; {\varsigma} &amp; {\backslash varsigma} &amp; {} &amp; {} \\<br>\hline<br>{\eta} &amp; {\backslash eta} &amp; {\xi} &amp; {\backslash xi} &amp; {\tau} &amp; {\backslash tau} &amp; {} &amp; {} \\<br>\hline<br>{\Gamma} &amp; {\backslash Gamma} &amp; {\Lambda} &amp; {\backslash Lambda} &amp; {\Sigma} &amp; {\backslash Sigma} &amp; {\Psi} &amp; {\backslash Psi} \\<br>\hline<br>{\Delta} &amp; {\backslash Delta} &amp; {\Xi} &amp; {\backslash Xi} &amp; {\Upsilon} &amp; {\backslash Upsilon} &amp; {\Omega} &amp; {\backslash Omega} \\<br>\hline<br>{\Omega} &amp; {\backslash Omega} &amp; {\Pi} &amp; {\backslash Pi} &amp; {\Phi} &amp; {\backslash Phi} &amp; {} &amp; {} \\<br>\hline<br>\end{array}<br>$$</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://blog.csdn.net/xiahouzuoxin/article/details/26478179" target="_blank" rel="noopener">Markdown中插入数学公式的方法</a></li><li><a href="http://www.cnblogs.com/houkai/p/3399646.html" target="_blank" rel="noopener">LATEX数学公式基本语法</a></li><li><a href="https://liam0205.me/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/mathjax1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="写作" scheme="http://frankblog.site/categories/%E5%86%99%E4%BD%9C/"/>
    
    
      <category term="markdown" scheme="http://frankblog.site/tags/markdown/"/>
    
      <category term="LaTex" scheme="http://frankblog.site/tags/LaTex/"/>
    
  </entry>
  
  <entry>
    <title>SVM可视化</title>
    <link href="http://frankblog.site/2018/06/02/svm%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://frankblog.site/2018/06/02/svm可视化/</id>
    <published>2018-06-02T08:02:15.545Z</published>
    <updated>2018-06-06T01:54:17.664Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4]])</span><br><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Decision Boundary&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;r&apos;)</span><br><span class="line">plt.plot([3, 3], [0.5, 6], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, 0, shape=&apos;full&apos;, color=&apos;b&apos;, linestyle=&apos;--&apos;)</span><br><span class="line">plt.annotate(r&apos;margin 1&apos;,</span><br><span class="line">             xy=(3.5, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(3.1, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;margin 2&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4, 3.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;support vector&apos;,</span><br><span class="line">             xy=(2, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(0.5, 1.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Support Vector Machine&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 8)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 0$&apos;,</span><br><span class="line">             xy=(5, 1), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(6, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = 1$&apos;,</span><br><span class="line">             xy=(6, 2), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(7, 2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$w^T x + b = -1$&apos;,</span><br><span class="line">             xy=(3.5, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(4.5, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;d&apos;,</span><br><span class="line">             xy=(3.5, 3.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(2, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;A&apos;,</span><br><span class="line">             xy=(4, 4), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(5, 4.5), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/SVM%E7%94%BB%E5%9B%BE1.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=140)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=100, </span><br><span class="line"> n_features=2, </span><br><span class="line"> centers=[(1, 1), (2, 2)], </span><br><span class="line"> random_state=4, </span><br><span class="line"> shuffle=False,</span><br><span class="line"> cluster_std=0.4)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 3)</span><br><span class="line">plt.ylim(0, 3)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([0.5, 2.5], [2.5, 0.5], &apos;-r&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2], [1.5, 1.5], [1.2, 1.7]])</span><br><span class="line">class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4], [5.5, 3.5], [4.5, 4.5], [2, 1.5]])</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Slack Variable&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 7)</span><br><span class="line">plt.ylim(0, 7)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [5, 1], &apos;-r&apos;)</span><br><span class="line">plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)</span><br><span class="line">plt.arrow(2, 1.5, 2.25, 2.25, shape=&apos;full&apos;, color=&apos;b&apos;)</span><br><span class="line">plt.annotate(r&apos;violate margin rule.&apos;,</span><br><span class="line"> xy=(2, 1.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(0.2, 0.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;normal sample. $\epsilon = 0$&apos;,</span><br><span class="line"> xy=(4, 5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(4.5, 5.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$\epsilon &gt; 0$&apos;,</span><br><span class="line"> xy=(3, 2.5), xycoords=&apos;data&apos;,</span><br><span class="line"> xytext=(3, 1.5), fontsize=10,</span><br><span class="line"> arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE2.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(6, 4), dpi=120)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Cost&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 4)</span><br><span class="line">plt.ylim(0, 2)</span><br><span class="line">plt.xlabel(&apos;$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$&apos;)</span><br><span class="line">plt.ylabel(&apos;Cost&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [1.5, 0], &apos;-r&apos;)</span><br><span class="line">plt.plot([1, 3], [0.015, 0.015], &apos;-r&apos;)</span><br><span class="line">plt.annotate(r&apos;$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$&apos;,</span><br><span class="line">             xy=(0.7, 0.5), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1, 1), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br><span class="line">plt.annotate(r&apos;$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$&apos;,</span><br><span class="line">             xy=(1.5, 0), xycoords=&apos;data&apos;,</span><br><span class="line">             xytext=(1.8, 0.2), fontsize=10,</span><br><span class="line">             arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE3.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4), dpi=144)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [3, 2], [4, 1], [5, 1]])</span><br><span class="line">class2 = np.array([[2.2, 4], [1.5, 5], [1.8, 4.6], [2.4, 5], [3.2, 5], [3.7, 4], [4.5, 4.5], [5.4, 3]])</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Non-linear Separatable in Low Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;left&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], np.zeros(class1[:, 0].shape[0]) + 0.05, marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], np.zeros(class2[:, 0].shape[0]) + 0.05, marker=&apos;s&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;Linear Separatable in High Dimension&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 6)</span><br><span class="line">plt.ylim(0, 6)</span><br><span class="line">plt.xlabel(&apos;X1&apos;)</span><br><span class="line">plt.ylabel(&apos;X2&apos;)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)</span><br><span class="line">plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)</span><br><span class="line">plt.plot([1, 5], [3.8, 2], &apos;-r&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE4.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def gaussian_kernel(x, mean, sigma):</span><br><span class="line"> return np.exp(- (x - mean)**2 / (2 * sigma**2))</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 6, 500)</span><br><span class="line">mean = 1</span><br><span class="line">sigma1 = 0.1</span><br><span class="line">sigma2 = 0.3</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 3), dpi=144)</span><br><span class="line"></span><br><span class="line"># sub plot 1</span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma1))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma1), &apos;r-&apos;)</span><br><span class="line"></span><br><span class="line"># sub plot 2</span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma2))</span><br><span class="line"></span><br><span class="line">plt.xlim(0, 2)</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">ax = plt.gca()                                  # gca 代表当前坐标轴，即 &apos;get current axis&apos;</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)            # 隐藏坐标轴</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma2), &apos;r-&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://p4rlzrioq.bkt.clouddn.com/svm%E7%94%BB%E5%9B%BE5.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/SVM_%E5%B0%81%E9%9D%A2_1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="可视化" scheme="http://frankblog.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="SVM" scheme="http://frankblog.site/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？</title>
    <link href="http://frankblog.site/2018/06/01/%E5%A6%82%E4%BD%95%E7%94%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E6%AF%94%E5%96%BB%E6%8F%8F%E8%BF%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%EF%BC%9FHadoop%E3%80%81Hive%E3%80%81Spark%20%E4%B9%8B%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F/"/>
    <id>http://frankblog.site/2018/06/01/如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？/</id>
    <published>2018-06-01T12:06:43.304Z</published>
    <updated>2018-06-08T11:34:11.717Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><p>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？<br>这是知乎上某大神的解释：</p><p><strong>学习很重要的是能将纷繁复杂的信息进行归类和抽象。</strong><br>对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题：</p><ul><li>存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka；</li><li>计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等；</li><li>查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等；</li></ul><p><strong>大数据技术生态其实是一个江湖….</strong><br>在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…<br>这三本秘籍分别为：</p><ul><li>《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据；</li><li>《Google MapReduce》：论述了怎样快速计算海量的数据；</li><li>《Google BigTable》：论述了怎样实现海量数据的快速查询；</li></ul><p><strong>以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术；</strong></p><p>在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。<br>Hadoop包括三大部分，分别是hdfs、MapReduce和hbase：</p><ul><li>hdfs解决大数据的存储问题。</li><li>mapreduce解决大数据的计算问题。</li><li>hbase解决大数据量的查询问题。</li></ul><p>之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进：</p><ul><li>1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍；</li><li>2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。</li><li>3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。</li><li>4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼；</li></ul><p>原文地址：<a href="https://www.zhihu.com/question/27974418/answer/156227565" target="_blank" rel="noopener">知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/Bigdata.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://frankblog.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://frankblog.site/tags/hadoop/"/>
    
      <category term="spark" scheme="http://frankblog.site/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降法总结</title>
    <link href="http://frankblog.site/2018/06/01/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://frankblog.site/2018/06/01/梯度下降法总结/</id>
    <published>2018-06-01T08:20:06.419Z</published>
    <updated>2018-06-02T07:09:29.464Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度实际上就是多变量微分的一般化。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_0.png" alt="link"></p><p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p><ul><li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li><li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。</li></ul><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2.png" alt="link"></p><p>此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！<br>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A66.2_1.png" alt="link"></p><h1 id="多元函数的梯度下降"><a href="#多元函数的梯度下降" class="headerlink" title="多元函数的梯度下降"></a>多元函数的梯度下降</h1><p>我们假设有一个目标函数</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2.png" alt="多元函数的梯度下降"></p><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_1.png" alt="image.png"></p><p>初始的学习率为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_2.png" alt="image.png"></p><p>函数的梯度为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_3.png" alt="image.png"></p><p>进行多次迭代：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_4.png" alt="image.png"></p><p>我们发现，已经基本靠近函数的最小值点</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B06.2_5.png" alt="image.png"></p><h1 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h1><h2 id="选择合适的学习速率"><a href="#选择合适的学习速率" class="headerlink" title="选择合适的学习速率"></a>选择合适的学习速率</h2><p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg" alt=""></p><p>假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果 learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。</p><p>虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。</p><h2 id="自适应学习速率"><a href="#自适应学习速率" class="headerlink" title="自适应学习速率"></a>自适应学习速率</h2><p>举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate</p><ul><li>通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate</li><li>update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate</li><li>比如 \(\eta^{t} = \eta / \sqrt{t+1}\)，t是次数。随着次数的增加，\(η_t\)减小</li></ul><h1 id="Feature-Scaling（特征缩放）"><a href="#Feature-Scaling（特征缩放）" class="headerlink" title="Feature Scaling（特征缩放）"></a>Feature Scaling（特征缩放）</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.jpg" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE1.jpg" alt=""></p><p>图左边是\(x_{1}\)的scale比的scale比\(x_{2}\)要小很多，所以当要小很多，所以当\(w_{1}\)和和\(w_{2}\)做同样的变化时，做同样的变化时，\(w_{1}\)对y的变化影响是比较小的，对y的变化影响是比较小的，\(x_{2}\)对y的变化影响是比较大的。</p><p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为\(w_{1}\)对y的变化影响比较小，所以对y的变化影响比较小，所以\(w_{1}\)对损失函数的影响比较小，对损失函数的影响比较小，\(w_{1}\)对损失函数有比较小的微分，所以对损失函数有比较小的微分，所以vw_{1}\)方向上是比较平滑的。同理方向上是比较平滑的。同理\(x_{2}\)对y的影响比较大，所以对y的影响比较大，所以\(x_{2}\)对损失函数的影响比较大，所以在对损失函数的影响比较大，所以在\(x_{2}\)方向有比较尖的峡谷。</p><p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p><p>对于左边的情况，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p><h1 id="常见的算法"><a href="#常见的算法" class="headerlink" title="常见的算法"></a>常见的算法</h1><ul><li><strong>批量梯度下降</strong>：批量梯度下降每次更新使用了所有的训练数据。<strong>如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，</strong>但是缺点是如果样本值很大的话，更新速度会很慢。</li><li><strong>随机梯度下降</strong>：随机也就是说用一个样本的梯度来近似所有的样本，来调整θ，这样会大大加快训练数据，但是有可能由于训练数据的噪声点较多。<strong>每一次利用噪声点进行更新的过程中，不一定是朝着极小值方向更新，但是由于多次迭代，整体方向还是大致朝着极小值方向更新，提高了速度。</strong></li><li><strong>小批量梯度下降</strong>：小批量梯度下降法是<strong>为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的</strong>。</li></ul><h2 id="批量梯度下降代码"><a href="#批量梯度下降代码" class="headerlink" title="批量梯度下降代码"></a>批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line">#这里更新权重的时候所有的样本点都用上了</span><br><span class="line">    for i in range (3):</span><br><span class="line"> pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line"> theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"></span><br><span class="line">iters_count 219</span><br><span class="line">iters_count 220</span><br><span class="line">iters_count 221</span><br><span class="line">iters_count 222</span><br><span class="line">iters_count 223</span><br><span class="line">iters_count 224</span><br><span class="line">iters_count 225</span><br><span class="line">theta: [3.0027765778748003, 3.997918297015663]</span><br><span class="line">final loss: 9.68238055213e-05</span><br><span class="line">iters: 225</span><br><span class="line">[Finished in 0.2s]</span><br></pre></td></tr></table></figure><h2 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码"></a>随机梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 每次选取一个值,随机一个点更新 θ</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"> #每一次选取随机的一个点进行权重的更新</span><br><span class="line">    i = random.randint(0,3)</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#output:</span><br><span class="line">iters_count 1226</span><br><span class="line">iters_count 1227</span><br><span class="line">iters_count 1228</span><br><span class="line">iters_count 1229</span><br><span class="line">iters_count 1230</span><br><span class="line">iters_count 1231</span><br><span class="line">iters_count 1232</span><br><span class="line">theta: [3.002441488688225, 3.9975844154600226]</span><br><span class="line">final loss: 9.989420302e-05</span><br><span class="line">iters: 1232</span><br><span class="line">[Finished in 0.3s]</span><br></pre></td></tr></table></figure><h2 id="小批量梯度下降代码"><a href="#小批量梯度下降代码" class="headerlink" title="小批量梯度下降代码"></a>小批量梯度下降代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 这里用2个样本点</span><br><span class="line">import random</span><br><span class="line">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span><br><span class="line">input_x = [[1,4], [2,5], [5,1], [4,2]] </span><br><span class="line">y = [19,26,19,20] </span><br><span class="line">theta = [1,1]</span><br><span class="line">loss = 10</span><br><span class="line">step_size = 0.001</span><br><span class="line">eps =0.0001</span><br><span class="line">max_iters = 10000</span><br><span class="line">error =0</span><br><span class="line">iter_count = 0</span><br><span class="line">while( loss &gt; eps and iter_count &lt; max_iters):</span><br><span class="line">    loss = 0</span><br><span class="line"></span><br><span class="line">    i = random.randint(0,3) #注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span><br><span class="line">    j = (i+1)%4</span><br><span class="line">    pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[0]*input_x[j][0]+theta[1]*input_x[j][1]</span><br><span class="line">    theta[0] = theta[0] - step_size * (pred_y - y[j]) * input_x[j][0]</span><br><span class="line">    theta[1] = theta[1] - step_size * (pred_y - y[j]) * input_x[j][1]</span><br><span class="line">    for i in range (3):</span><br><span class="line">pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]</span><br><span class="line">error = 0.5*(pred_y - y[i])**2</span><br><span class="line">loss = loss + error</span><br><span class="line">    iter_count += 1</span><br><span class="line">    print &apos;iters_count&apos;, iter_count</span><br><span class="line"></span><br><span class="line">print &apos;theta: &apos;,theta </span><br><span class="line">print &apos;final loss: &apos;, loss</span><br><span class="line">print &apos;iters: &apos;, iter_count</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line">iters_count 543</span><br><span class="line">iters_count 544</span><br><span class="line">iters_count 545</span><br><span class="line">iters_count 546</span><br><span class="line">iters_count 547</span><br><span class="line">iters_count 548</span><br><span class="line">iters_count 549</span><br><span class="line">theta: [3.0023012574840764, 3.997553282857357]</span><br><span class="line">final loss: 9.81717138358e-05</span><br><span class="line">iters: 549</span><br></pre></td></tr></table></figure><h1 id="梯度下降的局限"><a href="#梯度下降的局限" class="headerlink" title="梯度下降的局限"></a>梯度下降的局限</h1><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%B1%80%E9%99%90.jpg" alt=""></p><ul><li>容易陷入局部极值</li><li>还有可能卡在不是极值，但微分值是0的地方</li><li>还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</li></ul><p>参考：<a href="https://zhuanlan.zhihu.com/qinlibo-ml" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/qinlibo-ml</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="优化算法" scheme="http://frankblog.site/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之Tensorflow(一)</title>
    <link href="http://frankblog.site/2018/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BTensorflow(%E4%B8%80)/"/>
    <id>http://frankblog.site/2018/06/01/深度学习之Tensorflow(一)/</id>
    <published>2018-06-01T07:35:47.712Z</published>
    <updated>2018-06-01T09:10:04.104Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/tensorflow.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="Tensorflow简单示例"><a href="#Tensorflow简单示例" class="headerlink" title="Tensorflow简单示例"></a>Tensorflow简单示例</h1><h2 id="1-基本运算"><a href="#1-基本运算" class="headerlink" title="1. 基本运算"></a>1. 基本运算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#定义一个常量</span><br><span class="line">a = tf.constant([3,3])</span><br><span class="line">#定义一个变量</span><br><span class="line">x = tf.Variable([1,2])</span><br><span class="line"></span><br><span class="line">#定义一个加法op</span><br><span class="line">add = tf.add(a,x)</span><br><span class="line">#定义一个减法</span><br><span class="line">sub = tf.subtract(a,x)</span><br><span class="line">#定义一个乘法op</span><br><span class="line">mul = tf.multiply(a,x)</span><br><span class="line">#定义初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">#定义多个操作</span><br><span class="line">add2 = tf.add(a,add)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">print(&quot;加法：&quot;,sess.run(add)) #执行加法</span><br><span class="line">print(&quot;减法：&quot;,sess.run(sub)) #执行减法</span><br><span class="line">print(&quot;乘法：&quot;,sess.run(mul)) #执行乘法</span><br><span class="line">#同时执行乘法op和加法op</span><br><span class="line">result = sess.run([add,add2,sub,mul])</span><br><span class="line">print(&quot;执行多个：&quot;,result)</span><br></pre></td></tr></table></figure><h2 id="2-使用占位符"><a href="#2-使用占位符" class="headerlink" title="2. 使用占位符"></a>2. 使用占位符</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#Feed：先定义占位符，等需要的时候再传入数据</span><br><span class="line">#创建占位符</span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">#定义乘法op</span><br><span class="line">output = tf.multiply(input1,input2)</span><br><span class="line">add = tf.add(input1,input2)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">#feed的数据以字典的形式传入</span><br><span class="line">print(sess.run(add, feed_dict=&#123;input1:[8.],input2:[2.]&#125;))</span><br></pre></td></tr></table></figure><h1 id="Tensorflow简单回归模型"><a href="#Tensorflow简单回归模型" class="headerlink" title="Tensorflow简单回归模型"></a>Tensorflow简单回归模型</h1><h2 id="1-最简单的线性回归模型"><a href="#1-最简单的线性回归模型" class="headerlink" title="1. 最简单的线性回归模型"></a>1. 最简单的线性回归模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#使用numpy生成100个随机点</span><br><span class="line">#样本点</span><br><span class="line">x_data = np.random.rand(100)</span><br><span class="line">y_data = x_data*0.1 + 0.2</span><br><span class="line"></span><br><span class="line">#构造一个线性模型</span><br><span class="line">d = tf.Variable(1.1)</span><br><span class="line">k = tf.Variable(0.5)</span><br><span class="line">y = k*x_data + d</span><br><span class="line"></span><br><span class="line">#二次代价函数&lt;均方差&gt;</span><br><span class="line">loss = tf.losses.mean_squared_error(y_data,y)</span><br><span class="line">#定义一个梯度下降法来进行训练的优化器</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.3)</span><br><span class="line">#最小化代价函数</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line"> for step in range(1000):</span><br><span class="line"> sess.run(train)</span><br><span class="line"> if step%100 ==0:</span><br><span class="line"> print(step,sess.run([k,d]))</span><br></pre></td></tr></table></figure><h2 id="2-非线性回归的问题"><a href="#2-非线性回归的问题" class="headerlink" title="2. 非线性回归的问题"></a>2. 非线性回归的问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data = np.linspace(-0.5,0.5,200).reshape(-1,1)</span><br><span class="line">noise = np.random.normal(0,0.015,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder，列数为1，行数未知</span><br><span class="line">x = tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 = tf.Variable(tf.random_normal([1,20])) # 初始化1行20列权值</span><br><span class="line">biases_L1 = tf.Variable(tf.zeros([1,20])) # 初始化1行20列偏置</span><br><span class="line">Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 # 计算神经元信号</span><br><span class="line">L1 = tf.nn.tanh(Wx_plus_b_L1) # 使用激活函数计算神经元输出信号</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 = tf.Variable(tf.random_normal([20,1]))</span><br><span class="line">biases_L2 = tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction = tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"># prediction = Wx_plus_b_L2</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line">#使用梯度下降法最小化代价函数训练</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">#变量初始化</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">for _ in range(1000):</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line"></span><br><span class="line">#获得预测值</span><br><span class="line">prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;)</span><br><span class="line">#画图</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.plot(x_data,prediction_value,&apos;r-&apos;,lw=5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这里思考下中间层和输出层的激活函数的选取问题。</span><br><span class="line">1\. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。</span><br><span class="line">2\. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。</span><br><span class="line">3\. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。</span><br><span class="line">4\. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。</span><br><span class="line">5\. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。</span><br></pre></td></tr></table></figure><h1 id="Tensorflow分类模型"><a href="#Tensorflow分类模型" class="headerlink" title="Tensorflow分类模型"></a>Tensorflow分类模型</h1><p>本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。</p><h2 id="1-简单的MNIST数据集分类"><a href="#1-简单的MNIST数据集分类" class="headerlink" title="1. 简单的MNIST数据集分类"></a>1. 简单的MNIST数据集分类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">W = tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b = tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction) </span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中。</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率。</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">#epoch：所有数据训练一次，就是一个epoch周期</span><br><span class="line">for epoch in range(21):</span><br><span class="line">#batch：一般为32，64个数据</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(acc))</span><br></pre></td></tr></table></figure><h2 id="2-过拟合解决及梯度下降优化器"><a href="#2-过拟合解决及梯度下降优化器" class="headerlink" title="2. 过拟合解决及梯度下降优化器"></a>2. 过拟合解决及梯度下降优化器</h2><p>Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。<br>在神经网络中它们之间也可以结合使用，dropout应用较多些。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义三个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 784-1000-500-10</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([784,1000],stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.zeros([1000])+0.1)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([1000,500],stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.zeros([500])+0.1)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([500,10],stddev=0.1))</span><br><span class="line">b3 = tf.Variable(tf.zeros([10])+0.1)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line">#同样这里也可以使用正则项</span><br><span class="line">#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"></span><br><span class="line">#正则后的交叉熵代价函数</span><br><span class="line">#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率</span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)</span><br><span class="line"></span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">for epoch in range(31):</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.5&#125;) #这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了</span><br><span class="line"></span><br><span class="line">test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(test_acc) +&quot;,Training Accuracy &quot; + str(train_acc))</span><br></pre></td></tr></table></figure></p><h2 id="3-神经网络优化"><a href="#3-神经网络优化" class="headerlink" title="3. 神经网络优化"></a>3. 神经网络优化</h2><p>这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line">#定义三个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line">lr = tf.Variable(0.001, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"># 784-500-300-10</span><br><span class="line">#创建一个神经网络</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.zeros([500])+0.1)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.zeros([300])+0.1)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob)</span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))</span><br><span class="line">b3 = tf.Variable(tf.zeros([10])+0.1)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line">#训练</span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(init)</span><br><span class="line">for epoch in range(21):</span><br><span class="line">sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:1.0&#125;)</span><br><span class="line"></span><br><span class="line">learning_rate = sess.run(lr)</span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc) + &quot;, Learning Rate= &quot; + str(learning_rate))</span><br></pre></td></tr></table></figure></p><h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a>CNN卷积神经网络</h1><p>以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。</p><ul><li>CNN通过<strong>局部感受野</strong>和<strong>权值共享</strong>减少了神经网络需要训练的参数（权值）的个数。</li><li><p>卷积核/滤波器<br><img src="https://ihoge.cn/2018/media/15273889821782.jpg" alt=""></p></li><li><p>卷积Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li><li>池化<ul><li>max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数</li><li>mean-pooling</li><li>随机-pooling<br><img src="https://ihoge.cn/2018/media/15273890234010.jpg" alt=""></li></ul></li><li><p>池化Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li></ul><p>下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size = 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line">#定义两个placeholder</span><br><span class="line">x = tf.placeholder(tf.float32,[None,784])#28*28</span><br><span class="line">y = tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#初始化权值</span><br><span class="line">def weight_variable(shape):</span><br><span class="line">initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布</span><br><span class="line">return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">#初始化偏置</span><br><span class="line">def bias_variable(shape):</span><br><span class="line">initial = tf.constant(0.1,shape=shape)</span><br><span class="line">return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">#卷积层</span><br><span class="line">def conv2d(x,W):</span><br><span class="line">#x input tensor of shape `[batch, in_height, in_width, in_channels]`</span><br><span class="line">#W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]</span><br><span class="line">#`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长</span><br><span class="line">#padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`</span><br><span class="line">return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=&apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">#池化层</span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">#ksize [1,x,y,1]</span><br><span class="line">return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`</span><br><span class="line">x_image = tf.reshape(x,[-1,28,28,1])</span><br><span class="line"></span><br><span class="line">#初始化第一个卷积层的权值和偏置</span><br><span class="line">W_conv1 = weight_variable([5,5,1,32])#5*5的采样窗口，32个卷积核从1个平面抽取特征</span><br><span class="line">b_conv1 = bias_variable([32])#每一个卷积核一个偏置值</span><br><span class="line"></span><br><span class="line">#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling</span><br><span class="line"></span><br><span class="line">#初始化第二个卷积层的权值和偏置</span><br><span class="line">W_conv2 = weight_variable([5,5,32,64])#5*5的采样窗口，64个卷积核从32个平面抽取特征</span><br><span class="line">b_conv2 = bias_variable([64])#每一个卷积核一个偏置值</span><br><span class="line"></span><br><span class="line">#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling</span><br><span class="line"></span><br><span class="line">#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14</span><br><span class="line">#第二次卷积后为14*14，第二次池化后变为了7*7</span><br><span class="line">#进过上面操作后得到64张7*7的平面</span><br><span class="line"></span><br><span class="line">#初始化第一个全连接层的权值</span><br><span class="line">W_fc1 = weight_variable([7*7*64,1024])#上一层有7*7*64个神经元，全连接层有1024个神经元</span><br><span class="line">b_fc1 = bias_variable([1024])#1024个节点</span><br><span class="line"></span><br><span class="line">#把池化层2的输出扁平化为1维</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])</span><br><span class="line">#求第一个全连接层的输出</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">#keep_prob用来表示神经元的输出概率</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line">#初始化第二个全连接层</span><br><span class="line">W_fc2 = weight_variable([1024,10])</span><br><span class="line">b_fc2 = bias_variable([10])</span><br><span class="line"></span><br><span class="line">#计算输出</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line">#交叉熵代价函数</span><br><span class="line">cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line">#使用AdamOptimizer进行优化</span><br><span class="line">train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">#结果存放在一个布尔列表中</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置</span><br><span class="line">#求准确率</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">for epoch in range(21):</span><br><span class="line">for batch in range(n_batch):</span><br><span class="line">batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line"></span><br><span class="line">acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/tensorflow.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://frankblog.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://frankblog.site/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础（二）</title>
    <link href="http://frankblog.site/2018/05/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://frankblog.site/2018/05/30/机器学习基础（二）/</id>
    <published>2018-05-30T06:05:51.455Z</published>
    <updated>2018-06-09T02:25:54.922Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/ai%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="1-数据类型比较"><a href="#1-数据类型比较" class="headerlink" title="1.数据类型比较"></a>1.数据类型比较</h2><p>Scalar: 标量，可以看成一个数<br>Vector: 向量，可以看成一个一维数组<br>Matrix: 矩阵，可以看成二维数组<br>Tensor: 张量，三维或三维以上的数组的统称，维度不定</p><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。<br>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%B8%89%E9%98%B6%E5%BC%A0%E9%87%8F.jpg" alt=""></p><ul><li><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p></li><li><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p></li></ul><h2 id="2-线性代数概念"><a href="#2-线性代数概念" class="headerlink" title="2.线性代数概念"></a>2.线性代数概念</h2><p><strong>  逆矩阵</strong>：对于n阶矩阵A，如果有一个n阶矩阵B，使 AB = BA = E， 则说矩阵A是可逆的，并把矩阵B称为A的逆矩阵。当 |A| =0 时，A称为奇异矩阵。可逆矩阵一定是非奇异矩阵，因为矩阵可逆的充分必要条件是 |A|不为0。</p><p><strong> 矩阵的秩</strong>：矩阵A的行阶梯形中非零行的行数，就是矩阵A的秩。 对于n阶矩阵A，由于A的n阶子式只有一个|A|，故当 |A|不为0时R(A)=n，当|A|=0时R(A)&lt;n。因此，可逆矩阵的秩 = 矩阵的阶数，不可逆矩阵的秩 &lt; 矩阵的阶数。那么秩有什么实际意义吗？答案是肯定的。在做矩阵SVD分解的时候用于降噪，如果矩阵秩远小于样本维数（即矩阵列数），那么这些样本相当于只生活在外围空间中的一个低维子空间，这样就能实施降维操作。再者，如果把矩阵看成线性映射，那么秩就是象空间的的维数。</p><p><strong> 线性方程组的解</strong>：矩阵在数学上的基本的应用就是解线性方程组，这也是贯穿整个课本的样例。一个复杂的线性方程组可以表示为Ax=b，其中x，b是向量。通过求A的秩可以判定方程组的解：无解的充分必要条件是R(A) &lt; R(A,b)；有唯一解的充分必要条件是R(A) = R(A,b) = n；有无限多解的充分必要条件是R(A) = R(A,b) &lt; n。</p><p><strong>   向量空间的基</strong>：设V是一个向量空间，V上有r个向量a1,a2…ar，并且满足 a1,a2…ar线性无关，并且V中的任一向量都可以由a1,a2…ar线性表示，那么向量组a1,a2…ar就称为向量空间V的一个基，r是向量空间的维数，并称V为r维向量空间。可以这么理解，把向量空间看做是向量组，那么基就是一个极大线性无关组，可以用来表示其他向量的最小组合，维数就是向量组的秩。</p><p><strong>  特征值与特征向量</strong>：对于n阶矩阵A，如果数 λ 和 n 维非零列向量 x 使关系式 Ax=λ x 成立，那么 λ 称为矩阵A的特征值，向量 x 就是A的对应于λ 的特征向量。那么如何理解特征值与特征向量呢？</p><p> 我们知道，矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。实际上，这段话既讲了矩阵变换特征值及特征向量的几何意义（图形变换）也讲了其物理含义。物理的含义就是运动的图景：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。</p><p> <strong>相似矩阵</strong>：设A，B都是n阶矩阵，若有可逆矩阵P，使P-1 AP=B,则B是A的相似矩阵。</p><p> <strong>对角阵</strong>：是一个主对角线之外的元素皆为 0 的矩阵。对角线上的元素可以为 0 或其他值。</p><p> <strong>二次型</strong>：含有n个变量的二次齐次函数叫做二次型。只含平方项叫做二次型的标准型。用矩阵表示二次型就是f = xT Ax，A为对称矩阵。</p><h2 id="3-新视角看待矩阵运算"><a href="#3-新视角看待矩阵运算" class="headerlink" title="3.新视角看待矩阵运算"></a>3.新视角看待矩阵运算</h2><h3 id="1-矩阵"><a href="#1-矩阵" class="headerlink" title="1 矩阵"></a>1 矩阵</h3><p>矩阵<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97.png" alt="">构成行视图的方程组为 2x-y=1;x+y=5，表示的是二维平面的两条直线，方程组的解就是两条直线的交点。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%971.png" alt=""> </p><p>如果用列视图表达，就是<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%972.png" alt="">，表示了两个向量之间的关系，如下图，有向量a（2,1）和向量b（-1,1）,2a+3b=（1,5）。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%973.png" alt=""></p><p>这里我们来探究一下“维度”。数学中的维度和物理研究的维度不可一概而论。数学的维度表示的是独立参数的数目，物理学的维度指的是独立的时空坐标的数目。根据爱因斯坦的理论，我们生活的空间是四维空间，包含了三维的空间+时间。</p><h3 id="2-线性相关和线性无关"><a href="#2-线性相关和线性无关" class="headerlink" title="2 线性相关和线性无关"></a>2 线性相关和线性无关</h3><p>我们先看线性相关和线性无关的定义。在向量空间V的一组向量A，如果存在不全为零的数 k1, k2, ···,km , 使<img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%974.png" alt="">则称向量组A是线性相关的，否则数 k1, k2, ···,km全为0时，称它是线性无关。线性无关可以这么理解，如果是二维空间，这组向量的任意两个向量不共线；如果是三维空间，则任意三个向量不共面，如果共面了就可以用另外两个表示第三个向量了不是么。</p><h3 id="3-向量空间的基"><a href="#3-向量空间的基" class="headerlink" title="3 向量空间的基"></a>3 向量空间的基</h3><p>基：向量空间V的一组向量若满足(1)线性无关，(2)V中任一向量可由此向量线性表出，则称该组向量V中的一个基（亦称基底）。</p><h3 id="4-四个基本的子空间"><a href="#4-四个基本的子空间" class="headerlink" title="4 四个基本的子空间"></a>4 四个基本的子空间</h3><p>列空间：是包含所有列的线性组合。列向量是m维的，所以C(A)在Rm里。维数上，A的主列就是列空间的一组基，dim(C(A))=Rank(A)=r，维数就是秩的大小。</p><p>   零空间：n维向量，是Ax=0的所有解的集合，所以N(A)在Rn里。零空间有可能不存在。维数上，一组基就是一组特殊解，r是主变量的个数，n-r是自由变量的个数，零空间的维数等于n-r。</p><p>   行空间：是包含所有行的线性组合。A的行的所有线性组合，即A转置的列的线性组合（因为我们不习惯处理行向量），C(AT)在Rn里。维数上，有一个重要的性质：行空间和列空间维数相同，都等于秩的大小。</p><p>   左零空间：和列空间垂直的空间，交于一个零点，维数为m-r。</p><p>   可以画出四个子空间如下，行空间和零空间在Rn里，他们的维数加起来等于n，列空间和左零空间在Rm里，他们的维数加起来等于m。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E8%A7%92%E7%9C%8B%E5%BE%85%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%975.png" alt=""></p><h2 id="4-矩阵导数"><a href="#4-矩阵导数" class="headerlink" title="4.矩阵导数"></a>4.矩阵导数</h2><ul><li>分子、分母布局：<br>分子布局：分子为列向量，或者分母为行向量；<br>分母布局：分母为列向量，或者分子为行向量；</li><li>运算规则：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E5%88%86%E5%AD%90%E5%88%86%E6%AF%8D%E5%B8%83%E5%B1%80%E8%BF%90%E7%AE%97.png" alt=""></li><li>需要注意的规则：<br>以下公式默认在分子布局下的结果<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%99.png" alt="">；<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%991.png" alt="">；<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E8%A7%84%E5%88%992.png" alt=""></li><li>常用公式<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E6%B1%82%E5%AF%BC%E6%96%B9%E5%BC%8F.png" alt=""></li></ul><h2 id="5-特征分解"><a href="#5-特征分解" class="headerlink" title="5.特征分解"></a>5.特征分解</h2><h3 id="1-特征分解"><a href="#1-特征分解" class="headerlink" title="1 特征分解"></a>1 特征分解</h3><ul><li>特征分解的性质：对于Ax=λ x,如果所有的特征值都不相同，则相应的所有的特征向量线性无关，此时X可以被对角化为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3.jpg" alt="">。</li></ul><ul><li>对称矩阵的特征分解：如果一个对称矩阵的特征值不同，则其相应的所有的特征向量正交。</li><li>二次型：用于判定一个矩阵是正定矩阵、半正定矩阵、负定矩阵还是不定矩阵。如果矩阵的特征值都大于0，就是正定矩阵。下图中是两个二次型图形的例子，左边是凸函数，右边是非凸函数。之后会详细讲凸函数的优化问题。</li><li><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A31.jpg" alt=""></li></ul><h3 id="2-PCA"><a href="#2-PCA" class="headerlink" title="2 PCA"></a>2 PCA</h3><p>PCA是特征分解的一个有效应用。在进行图像的特征提取的过程中，提取的特征维数太多经常会导致特征匹配时过于复杂，消耗系统资源，不得不采用特征降维的方法。所谓特征降维，即采用一个低纬度的特征来表示高纬度。特征降维一般有两类方法：特征选择和特征抽取。特征选择即从高纬度的特征中选择其中的一个子集来作为新的特征；而特征抽取是指将高纬度的特征经过某个函数映射至低纬度作为新的特征。常用的特征抽取方法就是PCA（Principal Component Analysis）。降维过程举例：比如有矩阵X，希望把它由两行降成一行。</p><p>矩阵X：<img src="http://p4rlzrioq.bkt.clouddn.com/PCA.jpg" alt=""></p><p>1.计算协方差矩阵<img src="http://p4rlzrioq.bkt.clouddn.com/PCA1.jpg" alt=""></p><p>2.计算Cx的特征值为：λ1=2，λ2=2/5。特征值对应的特征向量为<img src="http://p4rlzrioq.bkt.clouddn.com/PCA2.jpg" alt=""></p><p>3.降维：特征向量的转置*X。<img src="http://p4rlzrioq.bkt.clouddn.com/PCA3.jpg" alt="">得到一行的矩阵。</p><h2 id="6-奇异值分解（Singular-Value-Decomposition，SVD）"><a href="#6-奇异值分解（Singular-Value-Decomposition，SVD）" class="headerlink" title="6.奇异值分解（Singular Value Decomposition，SVD）"></a>6.<strong>奇异值分解（Singular Value Decomposition，SVD）</strong></h2><p>  PCA的实现一般有两种方式，分别是特征值分解和SVD分解。SVD奇异值分解可以将一个复杂的矩阵用几个更小的子矩阵表示，每个子矩阵代表了原矩阵的重要特性。 SVD奇异值分解：任何秩为r的矩阵，都可以特征分解为以下公式：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A30.jpg" alt=""> </p><p>其中U和V是正交矩阵。公式表示了SVD与子空间的关系。公式(13)便于分析，但并不计算有效；公式(14)计算有效，但有时候不方便分析；公式(15)方便展开，用于低秩矩阵的计算。另外SVD还提供了计算四个子空间正交基的一种快速方法。</p><p>SVD应用之一图像压缩：给定一幅图像，256<em>512像素，考虑用低秩矩阵近似的方式，存储奇异向量，若保留一个奇异向量k=1，压缩比大致是 (256</em>512) / (256+512) = 170。但是k太小图像的质量也损失较大，实际中k不会这么小，下面四个图分别是原图、k=1、k=10、k=80 时图片的表现。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3.jpg" alt=""></p><h2 id="7-常见距离"><a href="#7-常见距离" class="headerlink" title="7.常见距离"></a>7.常见距离</h2><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p><p>设有两个n维变量\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)和\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)，则一些常用的距离公式定义如下：</p><h3 id="1-闵可夫斯基距离"><a href="#1-闵可夫斯基距离" class="headerlink" title="1.闵可夫斯基距离"></a>1.<strong>闵可夫斯基距离</strong></h3><ul><li>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</li></ul><p>$$d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } $$</p><ul><li>实际上，当p=1时，就是<strong>曼哈顿距离</strong>；当p=2时，就是<strong>欧式距离</strong>。当p=无穷时，就是<strong>切比雪夫距离</strong></li></ul><p>$$d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)$$<br><img src="" alt=""></p><ul><li>切比雪夫距离Python实现如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure><h3 id="2-向量内积"><a href="#2-向量内积" class="headerlink" title="2. 向量内积"></a>2. 向量内积</h3><p>向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的<strong>余弦相似度</strong>（Cosine similarity）：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt=""></p><p>余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度（<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">TF-IDF</a>）和图片相似性（<a href="http://www.ruanyifeng.com/blog/2013/03/similar_image_search_part_ii.html" target="_blank" rel="noopener">histogram</a>）计算上都有它的身影。需要注意一点的是，余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。怎样才能实现平移不变性？这就是下面要说的<strong>皮尔逊相关系数</strong>（Pearson correlation），有时候也直接叫<strong>相关系数</strong>:</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0.png" alt=""></p><p>皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%B3%BB%E6%95%B0.png" alt=""></p><p>由于皮尔逊系数具有的良好性质，在各个领域都应用广泛，例如，在推荐系统根据为某一用户查找喜好相似的用户,进而提供推荐，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。</p><h3 id="3-分类数据点间的距离"><a href="#3-分类数据点间的距离" class="headerlink" title="3. 分类数据点间的距离"></a>3. 分类数据点间的距离</h3><p><strong>汉明距离</strong>（Hamming distance）是指，两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB.png" alt=""></p><p>还可以用简单的<strong><em>匹配系数</em></strong>来表示两点之间的相似度——匹配字符数/总字符数。</p><p>在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影（两个都是 0），并不能说明两者相似。反而言之，如果两个用户都看过某一部电影（序列中都是 1），则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的<strong>杰卡德相似系数</strong>（Jaccard similarity）。</p><p>在上面的例子中，用 M11 表示两个用户都看过的电影数目，M10 表示用户 A 看过，用户 B 没看过的电影数目，M01 表示用户 A 没看过，用户 B 看过的电影数目，M00 表示两个用户都没有看过的电影数目。Jaccard 相似性系数可以表示为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/Jaccard%20%E7%9B%B8%E4%BC%BC%E6%80%A7%E7%B3%BB%E6%95%B0.png" alt=""></p><hr><h1 id="概率论和统计"><a href="#概率论和统计" class="headerlink" title="概率论和统计"></a>概率论和统计</h1><h2 id="1-统计量"><a href="#1-统计量" class="headerlink" title="1.统计量"></a>1.统计量</h2><h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE1.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE2.png" alt=""></p><h3 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a><strong>Pearson相关系数</strong></h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/Pearson%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0.png" alt=""></p><h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a><strong>协方差矩阵</strong></h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt=""></p><h2 id="2-独立与不相关"><a href="#2-独立与不相关" class="headerlink" title="2.独立与不相关"></a>2.<strong>独立与不相关</strong></h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%8B%AC%E7%AB%8B%E4%B8%8E%E4%B8%8D%E7%9B%B8%E5%85%B3.png" alt=""> </p><h2 id="3-贝叶斯公式"><a href="#3-贝叶斯公式" class="headerlink" title="3.贝叶斯公式"></a>3.<strong>贝叶斯公式</strong></h2><p>先看看什么是“先验概率”和“后验概率”，以一个例子来说明：</p><p>假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： <strong>P(患病) = 0.1%</strong>；即：在没有做检验之前，我们预计的患病率为<strong>P(患病)=0.1%</strong>，这个就叫作<strong>“先验概率”</strong>。</p><p>再假设现在有一种该病的检测方法，其检测的准确率为<strong>95%</strong>；即：如果真的得了这种病，该检测法有<strong>95%</strong>的概率会检测出阳性，但也有<strong>5%</strong>的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有<strong>95%</strong>的概率检测出阴性，但也有<strong>5%</strong>的概率检测为阳性。用概率条件概率表示即为：<strong>P(显示阳性|患病)=95%</strong></p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率<strong>P(患病|显示阳性)</strong>，这个其实就称为<strong>“后验概率”。</strong></p><p>而这个叫贝叶斯的人其实就是为我们提供了一种可以<strong>利用先验概率计算后验概率</strong>的方法，我们将其称为<strong>“贝叶斯公式”。</strong></p><p>这里先了解<strong>条件概率公式</strong>：</p><p>$$P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}$$</p><p>由条件概率可以得到<strong>乘法公式</strong>：</p><p>$$P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)$$<br>将条件概率公式和乘法公式结合可以得到：</p><p>$$P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}$$</p><p>再由<strong>全概率公式</strong>：</p><p>$$P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} $$</p><p>代入可以得到<strong>贝叶斯公式</strong>：</p><p>$$P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }$$</p><h2 id="4-常见分布函数"><a href="#4-常见分布函数" class="headerlink" title="4.常见分布函数"></a>4.<strong>常见分布函数</strong></h2><h3 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a><strong>0-1分布</strong></h3><p>0-1分布是单个二值型离散随机变量的分布，其概率分布函数为：</p><p>$$P\left( X=1 \right) =p$$<br>$$P\left( X=0 \right) =1-p$$</p><h3 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a><strong>几何分布</strong></h3><p>几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为：</p><p>$$P\left( X=k \right) =\left( 1-p \right) ^{k-1} p$$</p><p>性质：<br>\(E\left( X \right) =\frac{1}{p}\)<br>\(Var\left( X \right) =\frac{1-p}{p^{2} }\)</p><h3 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a><strong>二项分布</strong></h3><p>二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：</p><p>$$P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}$$</p><p>性质：<br>\(E\left( X \right) =np\)<br>\(Var\left( X \right) =np\left( 1-p \right)\)</p><h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a><strong>高斯分布</strong></h3><p>高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：<br><img src="http://p4rlzrioq.bkt.clouddn.com/488px-Normal_Distribution_PDF.svg.png" alt=""></p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.png" alt=""></p><p>若随机变量X服从一个数学期望为\(\mu\)，方差为\(\sigma ^{2}\)的正态分布，则我们将其记为：\(N\left( \mu ,\sigma^{2} \right)\)决定了正态分布的位置，其标准差\(\sigma\)（方差的开方）决定了正态分布的幅度。</p><h3 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a><strong>指数分布</strong></h3><p>指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了t小时，它总共使用至少t+s小时的条件概率，与从开始使用时算起它使用至少s小时的概率相等。下面这些都属于指数分布：</p><ul><li>婴儿出生的时间间隔</li><li>网站访问的时间间隔</li><li>奶粉销售的时间间隔</li></ul><p>指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间t，就等同于t之内没有任何婴儿出生，即：</p><p>$$P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t} $$<br>则：</p><p>$$P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}$$<br>如：接下来15分钟，会有婴儿出生的概率为：</p><p>$$P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53$$</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83.jpg" alt=""></p><h3 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a><strong>泊松分布</strong></h3><p>日常生活中，大量事件是有固定频率的，比如：</p><ul><li>某医院平均每小时出生3个婴儿</li><li>某网站平均每分钟有2次访问</li><li>某超市平均每小时销售4包奶粉</li></ul><p>它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p><p><strong>泊松分布就是描述某段时间内，事件具体的发生概率。</strong>其概率函数为：</p><p>$$P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!} $$<br>其中：</p><p>P表示概率，N表示某种函数关系，t表示时间，n表示数量，1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) ；λ 表示事件的频率。</p><p>还是以上面医院平均每小时出生3个婴儿为例，则\(\lambda =3\)；</p><p>那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：</p><p>$$P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025$$<br>同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：</p><p>$$P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8$$</p><h2 id="5-常见的分布总结"><a href="#5-常见的分布总结" class="headerlink" title="5.常见的分布总结"></a>5.常见的分布总结</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83.jpg" alt="常见分布"></p><h2 id="6-极大似然估计（MLE）"><a href="#6-极大似然估计（MLE）" class="headerlink" title="6.极大似然估计（MLE）"></a>6.极大似然估计（MLE）</h2><p>极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p><p>求极大似然函数估计值的一般步骤：</p><p>（1） 写出似然函数，即每个随机实验出现概率相乘，为这个抽样出现的概率。<br>（2） 对似然函数取对数，为了方便求导；<br>（3） 对参数求导数。<br>（4） 令导数=0，即求解极值，由实际情况知，该极值为极大值。解似然方程。</p><hr><h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><h2 id="1-导数与梯度"><a href="#1-导数与梯度" class="headerlink" title="1.导数与梯度"></a>1.导数与梯度</h2><ul><li><strong>导数</strong>：一个一元函数函数在某一点的导数描述了这个函数在这一点附近的变化率。</li><li><strong>梯度</strong>:多元函数的导数就是梯度。<ul><li>一阶导数，即梯度（gradient）</li><li>二阶导数，Hessian矩阵</li></ul></li></ul><h2 id="2-泰勒展开"><a href="#2-泰勒展开" class="headerlink" title="2.泰勒展开"></a>2.<strong>泰勒展开</strong></h2><ul><li>一元函数的泰勒展开：<br><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d0070da6cb78cda75d9ff9521f85702c97862673" alt="link"></li><li>基尼指数的图像、熵、分类误差率三者之间的关系。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80.jpg" alt="link"></li></ul><h2 id="3-Lagrange乘子法"><a href="#3-Lagrange乘子法" class="headerlink" title="3.Lagrange乘子法"></a>3.<strong>Lagrange乘子法</strong></h2><p>对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是<strong>把约束项添加到原函数上，然后对构造的新函数求导</strong>。</p><p>对于一个要求极值的函数\(f\left( x,y \right)\)，图上的蓝圈就是这个函数的等高图，就是说 \(f\left( x,y \right) =c_{1} ,c_{2} ,…,c_{n}\)分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组\(\left( x,y \right)\)，使它的\(c_{i}\)值越大越好，但是这点必须满足约束条件\(g\left( x,y \right)\)（在黄线上）。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0.jpg" alt=""></p><p>也就是说\(f(x,y)\)相切，或者说它们的梯度▽ \({f}\)和▽\({g}\)平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为\(\lambda\)倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。</p><p>在<strong>支持向量机模型（SVM）</strong>的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。</p><hr><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.<strong>信息熵</strong></h2><p>熵的概念最早由统计热力学引入。</p><p>信息熵是由信息论之父香农提出来的，它用于随机变量的不确定性度量，先上信息熵的公式。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="link"><br>信息是用来减少随机不确定性的东西（即不确定性的减少）。</p><p>我们可以用log ( 1/P )来衡量不确定性。P是一件事情发生的概率，概率越大，不确定性越小。</p><p>可以看到信息熵的公式，其实就是log ( 1/P )的期望，就是不确定性的期望，它代表了一个系统的不确定性，信息熵越大，不确定性越大。</p><p>注意这个公式有个默认前提，就是X分布下的随机变量x彼此之间相互独立。还有log的底默认为2，实际上底是多少都可以，但是在信息论中我们经常讨论的是二进制和比特，所以用2。</p><p>信息熵在联合概率分布的自然推广，就得到了联合熵</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B51.png" alt=""></p><p>当X, Y相互独立时，H(X, Y) = H(X) + H(Y)</p><h3 id="信息熵的实例解释"><a href="#信息熵的实例解释" class="headerlink" title="信息熵的实例解释"></a><strong>信息熵的实例解释</strong></h3><p>举个例子说明信息熵的作用。</p><p>例子是知乎上看来的，我觉得讲的挺好的。</p><blockquote><p>比如赌马比赛，有4匹马{ A, B, C, D}，获胜概率分别为{ 1/2, 1/4, 1/8, 1/8 }，将哪一匹马获胜视为随机变量X属于 { A, B, C, D } 。</p><p>假定我们需要用尽可能少的二元问题来确定随机变量 X 的取值。</p><p>例如，问题1：A获胜了吗？　问题2：B获胜了吗？　问题3：C获胜了吗？</p><p>最后我们可以通过最多3个二元问题，来确定取值。</p><p>如果X = A，那么需要问1次（问题1：是不是A？），概率为1/2 </p><p>如果X = B，那么需要问2次（问题1：是不是A？问题2：是不是B？），概率为1/4 </p><p>如果X = C，那么需要问3次（问题1，问题2，问题3），概率为1/8 </p><p>如果X = D，那么需要问3次（问题1，问题2，问题3），概率为1/8 </p><p>那么为确定X取值的二元问题的数量为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B52.png" alt=""></p><p>回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E7%86%B52.png" alt=""> </p><p>在二进制计算机中，一个比特为0或1，其实就代表了一个二元问题的回答。也就是说，在计算机中，我们给哪一匹马夺冠这个事件进行编码，所需要的平均码长为1.75个比特。</p><p>很显然，为了尽可能减少码长，我们要给发生概率 p(x)较大的事件，分配较短的码长 l(x)。这个问题深入讨论，可以得出霍夫曼编码的概念。</p><p>霍夫曼编码就是利用了这种大概率事件分配短码的思想，而且可以证明这种编码方式是最优的。我们可以证明上述现象：</p><ul><li>为了获得信息熵为 H(X) 的随机变量 X 的一个样本，平均需要抛掷均匀硬币（或二元问题） H(X)次（参考猜赛马问题的案例）</li><li>信息熵是数据压缩的一个临界值（参考码长部分的案例）</li></ul></blockquote><p>所以，信息熵H(X)可以看做，对X中的样本进行编码所需要的编码长度的期望值。</p><h2 id="2-相对熵-交叉熵-K-L散度"><a href="#2-相对熵-交叉熵-K-L散度" class="headerlink" title="2.相对熵/交叉熵/K-L散度"></a>2.<strong>相对熵/交叉熵/K-L散度</strong></h2><p>这里可以引申出交叉熵的理解，现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p。</p><p>按照真实分布p来编码样本所需的编码长度的期望为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B51.png" alt="">，这就是上面说的<strong>信息熵H( p )</strong></p><p>按照不真实分布q来编码样本所需的编码长度的期望为<img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B52.png" alt="">，这就是所谓的<strong>交叉熵H( p,q )</strong><br>交叉熵和熵，相当于，协方差和方差</p><p>这里引申出<strong>KL散度D(p||q)</strong> = H(p,q) - H(p) = <img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B53.png" alt="">，也叫做<strong>相对熵</strong>，它表示两个分布的差异，差异越大，相对熵越大。</p><p>机器学习中，我们用非真实分布q去预测真实分布p，因为真实分布p是固定的，D(p||q) = H(p,q) - H(p) 中 H(p) 固定，也就是说交叉熵H(p,q)越大，相对熵D(p||q)越大，两个分布的差异越大。</p><p>所以交叉熵用来做损失函数就是这个道理，它衡量了真实分布和预测分布的差异性。</p><p>用图像形象化的表示二者之间的关系可以如下图：<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%9B%B8%E5%AF%B9%E7%86%B54.png" alt="这里写图片描述"><br>上面是q所含的信息量/平均编码长度H(p)<br>第二行是cross-entropy，即用q来编码p所含的信息量/平均编码长度|或者称之为q对p的cross-entropy<br>第三行是上面两者之间的差值即为q对p的KL距离</p><p>从编码的角度，可以这样简单总结，信息熵是最优编码（最短的平均码长），交叉熵是非最优编码（大于最短的平均码长），KL散度是两者的差异（距离最优编码的差距）。</p><h2 id="3-互信息（信息增益）"><a href="#3-互信息（信息增益）" class="headerlink" title="3.互信息（信息增益）"></a>3.<strong>互信息（信息增益）</strong></h2><p>互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量。<br><strong>决策树中的信息增益就是互信息</strong>，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%92%E4%BF%A1%E6%81%AF.png" alt=""></p><p>可以通过简单的计算得到：</p><p>H(X|Y) = H(X) - I(X, Y), </p><p>互信息为0，则随机变量X和Y是互相独立的。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%BA%92%E4%BF%A1%E6%81%AF1.png" alt=""></p><h2 id="信息论与机器学习的关系"><a href="#信息论与机器学习的关系" class="headerlink" title="信息论与机器学习的关系"></a>信息论与机器学习的关系</h2><table><thead><tr><th>信息论视角</th><th>机器学习视角</th></tr></thead><tbody><tr><td>接受信号</td><td>特征</td></tr><tr><td>信源</td><td>标签</td></tr><tr><td>平均互信息</td><td>特征有效性分析</td></tr><tr><td>最大熵模型</td><td>极大似然法</td></tr><tr><td>交叉熵</td><td>逻辑回归损失函数</td></tr></tbody></table><h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a><strong>最大熵模型</strong></h2><p>最大熵模型的原则：</p><ul><li>承认已知事物（知识）；</li><li>对未知事物不做任何假设，没有任何偏见。</li></ul><p>对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。</p><p>因为这时概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”（Maximum Entropy）。</p><p>Logistic回归是统计学习中的经典分类方法，可以用于二类分类也可以用于多类分类。</p><p>最大熵模型由最大熵原理推导出来，最大熵原理是概率模型学习或估计的一个准则，最大熵原理认为在所有可能的概率模型的集合中，熵最大的模型是最好的模型，最大熵模型也可以用于二类分类和多类分类。</p><p>Logistic回归模型与最大熵模型都属于对数线性模型。</p><p>逻辑回归跟最大熵模型<strong>没有本质区别</strong>。逻辑回归是最大熵对应类别为<strong>二类</strong>时的特殊情况</p><p><strong>指数簇分布的最大熵</strong>等价于其<strong>指数形式的最大似然</strong>。</p><p><strong>二项式</strong>分布的最大熵解等价于二项式指数形式(<strong>sigmoid</strong>)的最大似然；<br><strong>多项式</strong>分布的最大熵等价于多项式分布指数形式(<strong>softmax</strong>)的最大似然。</p><h2 id="熵总结"><a href="#熵总结" class="headerlink" title="熵总结"></a>熵总结</h2><ul><li><strong>熵：不确定性的度量；</strong></li><li><strong>似然：与知识的吻合程度；</strong></li><li><strong>最大熵模型：对不确定度的无偏分配；</strong></li><li><strong>最大似然估计：对知识的无偏理解。</strong></li></ul><h2 id="2-上溢和下溢"><a href="#2-上溢和下溢" class="headerlink" title="2.上溢和下溢"></a>2.<strong>上溢和下溢</strong></h2><p>在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。</p><p>一种特别毁灭性的舍入误差是<strong>下溢</strong>。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除<strong>。</strong></p><p>另一个极具破坏力的数值错误形式是<strong>上溢(overflow)</strong>。当大量级的数被近似为\(\varpi\)时发生上溢。进一步的运算通常将这些无限值变为非数字。</p><p>必须对上溢和下溢进行数值稳定的一个例子是<strong>softmax 函数</strong>。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E4%B8%8A%E6%BA%A2%E5%92%8C%E4%B8%8B%E6%BA%A2.jpg" alt=""></p><p>softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 \(x_{i}\) 的类别标签 \(y_{i}\)属于K个类别的概率，最后判别 \(y_{i}\)所属类别时就是将其归为对应概率最大的那一个。</p><p>当式中的\(w_{k} x_{i} +b\)都是很小的负数时，\(e^{w_{k} x_{i} +b }\)就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的\(x_{w_{k} x_{i} +b}\)是很大的正数时，\(e^{w_{k} x_{i} +b }\)就会发生上溢导致结果是未定的。</p><hr><h1 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a><strong>凸优化</strong></h1><h2 id="1-凸集-Convex-Sets"><a href="#1-凸集-Convex-Sets" class="headerlink" title="1.凸集(Convex Sets)"></a>1.凸集(Convex Sets)</h2><p>集合C是凸的，如果对于所有的\(x,y\in C\)和\(\theta\in\mathbb{R},0\leq\theta\leq 1\)有：<br>$$\theta x+(1-\theta)y\in C$$<br>可以这样理解：在集合C中任选两点，在这两点的连线上的所有点都属于集合C。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E9%9B%86.jpg" alt=""></p><h2 id="2-凸函数-Convex-Fuctions"><a href="#2-凸函数-Convex-Fuctions" class="headerlink" title="2.凸函数(Convex Fuctions)"></a>2.凸函数(Convex Fuctions)</h2><p>如果函数的定义域\({\cal D}(f)\)是一个凸集，并且对于所有的\(x,y\in {\cal D}(f)\)和\(\theta\in\mathbb{R},0\leq\theta\leq1\)，都有：<br>\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\)<br><img src="http://oddpnmpll.bkt.clouddn.com/2016-10-05-01%3A23%3A27.jpg" alt=""></p><h3 id="凸函数的一阶条件"><a href="#凸函数的一阶条件" class="headerlink" title="凸函数的一阶条件"></a>凸函数的一阶条件</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E6%9D%A1%E4%BB%B6.png" alt=""><br>直观上可以这样理解，在函数上随便挑一个点，该点的切线必然在函数的下方<br><img src="http://oddpnmpll.bkt.clouddn.com/2016-10-05-01%3A27%3A53.jpg" alt=""></p><h3 id="凸性质的二阶条件"><a href="#凸性质的二阶条件" class="headerlink" title="凸性质的二阶条件"></a>凸性质的二阶条件</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%BA%8C%E9%98%B6%E6%9D%A1%E4%BB%B6.png" alt=""></p><h3 id="琴生不等式-Jensen’s-Inequality"><a href="#琴生不等式-Jensen’s-Inequality" class="headerlink" title="琴生不等式(Jensen’s Inequality)"></a>琴生不等式(Jensen’s Inequality)</h3><p>假设凸函数的基本定义为:<br>\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\ \ \ \text{for} \ \ \ 0\leq\theta\leq1\)<br>上述等式可以扩展到多个点:<br>\(f\left(\sum_{i=1}^k\theta_ix_i\right)\leq\sum_{i=1}^k\theta_if(x_i)\ \ \ \text{for}\ \ \ \sum_{i=1}^k\theta_i=1,\theta_i\geq0 \ \ \forall i\)<br>再将上述等式扩展到积分形式:<br>\(f\left(\int p(x)xdx\right)\leq\int p(x)f(x)dx\ \ \ \text{for}\ \ \ \int p(x)dx=1,p(x)\leq0\ \ \forall x\)<br>由于\(p(x)\)积分为1，我们可以把\(p(x)\)看作是一个概率密度函数，所以尚属等式可以用以下形式表达：<br>\(f(\mathbb{E}[x])\leq\mathbb{E}[f(x)]\)<br>最后一条等式就是著名的<strong>琴生不等式</strong>。</p><h2 id="3-凸优化问题-Convex-Optimization-Problems"><a href="#3-凸优化问题-Convex-Optimization-Problems" class="headerlink" title="3.凸优化问题(Convex Optimization Problems)"></a>3.凸优化问题(Convex Optimization Problems)</h2><p>在凸优化问题中，一个最关键的点就是<strong>对于一个凸优化问题，所有的局部最优解(locally optimal)都是全局最优解(globally optimal)</strong>。<br>最优化的基本数学模型如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98.jpg" alt=""><br>它有三个基本要素，即：</p><ul><li>设计变量：x是一个实数域范围内的n维向量，被称为决策变量或问题的解；</li><li>目标函数：f(x)为目标函数；</li><li>约束条件：\(h_{i} \left( x \right) =0\)称为等式约束，\(g_{i} \left( x \right) \leq 0\)为不等式约束，\(i=0,1,2,……\)</li></ul><h2 id="4-牛顿法"><a href="#4-牛顿法" class="headerlink" title="4.牛顿法"></a>4.牛顿法</h2><h3 id="牛顿法介绍"><a href="#牛顿法介绍" class="headerlink" title="牛顿法介绍"></a><strong>牛顿法介绍</strong></h3><p><strong>牛顿法</strong>也是求解<strong>无约束最优化</strong>问题常用的方法，<strong>最大的优点是收敛速度快</strong>。</p><p>从本质上去看，<strong>牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快</strong>。<strong>通俗地说</strong>，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%95.jpg" alt=""></p><p>或者从几何上说，<strong>牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面</strong>，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><h3 id="牛顿法的推导"><a href="#牛顿法的推导" class="headerlink" title="牛顿法的推导"></a><strong>牛顿法的推导</strong></h3><p>将目标函数\(f\left( x \right)\) 在\(x_{k}\)处进行二阶泰勒展开，可得：</p><p>$$f\left( x \right) =f\left( x_{k} \right) +f^{‘} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{‘’}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}$$<br>因为目标函数\(f\left( x \right)\)有极值的必要条件是在极值点处一阶导数为0，即：\(f^{‘} \left( x \right) =0\)</p><p>所以对上面的展开式两边同时求导（注意\({x}\)才是变量，\(x_{k}\)是常量\(\Rightarrow f^{‘} \left( x_{k} \right) ,f^{‘’} \left( x_{k} \right)\)都是常量），并令\(f^{‘} \left( x \right) =0\)可得：</p><p>$$f^{‘} \left( x_{k} \right) +f^{‘’} \left( x_{k} \right) \left( x-x_{k} \right) =0$$<br>即：</p><p>$$x=x_{k} -\frac{f^{‘} \left( x_{k} \right) }{f^{‘’} \left( x_{k} \right) } $$</p><p>于是可以构造如下的迭代公式：</p><p>$$x_{k+1} =x_{k} -\frac{f^{‘} \left( x_{k} \right) }{f^{‘’} \left( x_{k} \right) }$$</p><p>这样，我们就可以利用该迭代式依次产生的序列逐渐逼近\(f\left( x \right)\)的极小值点了。</p><p>牛顿法的迭代示意图如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%95%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%A4%BA%E6%84%8F%E5%9B%BE%E5%A6%82%E4%B8%8B.jpg" alt=""></p><p>上面讨论的是2维情况，<strong>高维情况</strong>的牛顿迭代公式是：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E9%AB%98%E7%BB%B4%E6%83%85%E5%86%B5%E7%9A%84%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E5%85%AC%E5%BC%8F.jpg" alt=""></p><p>式中， ▽\({f}\)是\(f\left( x \right)\)的梯度，即：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%89%9B%E9%A1%BF%E6%B3%953.jpg" alt=""></p><p>H是Hessen矩阵，即：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/Hessen%E7%9F%A9%E9%98%B5.jpg" alt=""></p><h3 id="牛顿法的过程"><a href="#牛顿法的过程" class="headerlink" title="牛顿法的过程"></a><strong>牛顿法的过程</strong></h3><ul><li>1、给定初值\(x_{0} ]\)和精度阈值\(\varepsilon\)，并令\(k=0\)；</li><li>2、计算\(x_{k}\)和\(H_{k}\)；</li><li>3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)；</li><li>4、计算新的迭代点：\(x_{k+1} =x_{k} +d_{k}\)；</li><li>5、令\(k=k+1\)，转至2。</li></ul><h2 id="5-阻尼牛顿法"><a href="#5-阻尼牛顿法" class="headerlink" title="5.阻尼牛顿法"></a>5.<strong>阻尼牛顿法</strong></h2><h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a><strong>引入</strong></h3><p>注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现\(f\left( x_{k+1} \right) &gt;f\left( x_{k} \right)\)的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p><p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是\(x_{k}\)，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子\(\lambda _{k}\)，即：</p><p>\(\lambda <em>{k} = minf\left( x</em>{k} +\lambda d_{k} \right)\)</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a><strong>算法过程</strong></h3><ul><li>1、给定初值\(x_{0}\)和精度阈值\(\varepsilon\)，并令\(k=0\)；</li><li>2、计算\(g_{k}\)（\(f\left( x \right)\)在\(x_{k}\)处的梯度值）和\(H_{k}\)；</li><li>3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)；</li><li>4、利用\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)得到步长\(\lambda <em>{k}\)，并令\(x</em>{k+1} =x_{k} +\lambda <em>{k} d</em>{k}\)</li><li>5、令\(k=k+1\)，转至2。</li></ul><h2 id="6-拟牛顿法"><a href="#6-拟牛顿法" class="headerlink" title="6.拟牛顿法"></a>6.<strong>拟牛顿法</strong></h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>由于<strong>牛顿法</strong>每一步都要求解目标函数的<strong>Hessen矩阵的逆矩阵</strong>，<strong>计算量比较大</strong>（求矩阵的逆运算量比较大），因此提出一种<strong>改进方法</strong>，即<strong>通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程</strong>，改进后的方法称为<strong>拟牛顿法</strong>。</p><h3 id="拟牛顿法的推导"><a href="#拟牛顿法的推导" class="headerlink" title="拟牛顿法的推导"></a><strong>拟牛顿法的推导</strong></h3><p>先将目标函数在\(x_{k+1}\)处展开，得到：</p><p>$$f\left( x \right) =f\left( x_{k+1} \right) +f^{‘} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{‘’}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}$$<br>两边同时取梯度，得：</p><p>$$f^{‘}\left( x \right) = f^{‘} \left( x_{k+1} \right) +f^{‘’} \left( x_{k+1} \right) \left( x-x_{k+1} \right)$$</p><p>取上式中的\(x=x_{k}\)，得：</p><p>$$f^{‘}\left( x_{k} \right) = f^{‘} \left( x_{k+1} \right) +f^{‘’} \left( x_{k+1} \right) \left( x-x_{k+1} \right)$$</p><p>即：</p><p>$$g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)$$<br>可得：</p><p>$$H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}$$</p><p>上面这个式子称为<strong>“拟牛顿条件”</strong>，由它来对Hessen矩阵做约束。</p><hr><h1 id="计算复杂性与NP问题"><a href="#计算复杂性与NP问题" class="headerlink" title="计算复杂性与NP问题"></a><strong>计算复杂性与NP问题</strong></h1><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>表明问题规模扩大后，程序需要的时间长度增长得有多快。程序的时间复杂度一般可以分为两种级别：</p><ul><li>多项式级的复杂度，如O(1)，O(log(n))、O（n^a）等，</li><li>非多项式级的，如O(a^n)、O(n!)等。后者的复杂度计算机往往不能承受。</li></ul><h2 id="约化-Reducibility"><a href="#约化-Reducibility" class="headerlink" title="约化(Reducibility)"></a>约化(Reducibility)</h2><p>简单的说，一个问题A可以约化为问题B的含义是，可以用问题B的解法解决问题A。（个人感觉也就是说，问题A是B的一种特殊情况。）标准化的定义是，如果能找到一个变化法则，对任意一个A程序的输入，都能按照这个法则变换成B程序的输入，使两程序的输出相同，那么我们说，问题A可以约化为问题B。</p><p>例如求解一元一次方程这个问题可以约化为求解一元二次方程，即可以令对应项系数不变，二次项的系数为0，将A的问题的输入参数带入到B问题的求解程序去求解。</p><p>另外，约化还具有传递性，A可以化约为B，B可以约化为C，那么A也可以约化为C。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="P-Problem"><a href="#P-Problem" class="headerlink" title="P Problem"></a>P Problem</h3><p>假设有 n 个数要排序。一个初级的冒泡排序算法所需时间可能与 n2 成正比，快一点的算法所需时间与 nlog（n） 成正比。在某些条件下，桶排序算法所需时间甚至只和 n 成正比。最不实用的算法就是输入的数字随机排列，直到出现完全有序的情况为止……记前三个算法的时间复杂度分别记为 O(n2)、O(nlogn) 和 O(n)，最后的<a href="http://zh.wikipedia.org/wiki/Bogo%E6%8E%92%E5%BA%8F" target="_blank" rel="noopener">“猴子排序”(Bogosort)算法</a>平均时间复杂度则达到了 O(n*n!)。</p><p>在上面的例子中，前三种算法的复杂度是 n 的多项式函数；最后一种算法的复杂度是 n 的阶乘，根据斯特林公式，n! 相当于指数级别的增长。当 n 特别小时，多项式级的算法已经快过指数级的算法。当 n 非常大时，人类根本看不到指数级复杂度算法结束的那天。自然的，大家会对多项式级别的算法抱有好感，希望对每一个问题都能找到多项式级别的算法。问题是——每个问题都能找到想要的多项式级别的算法吗？</p><p>在一个由问题构成的集合中，如果每个问题都存在多项式级复杂度的算法，这个集合就是 P 类问题（Polynomial）。</p><h3 id="NP-Nondeterministic-Polynomial-问题"><a href="#NP-Nondeterministic-Polynomial-问题" class="headerlink" title="NP (Nondeterministic Polynomial)问题"></a>NP (Nondeterministic Polynomial)问题</h3><p>NP 类问题指的是，能在多项式时间内<strong>检验</strong>一个解是否正确的问题。比如我的机器上存有一个密码文件，于是就能在多项式时间内验证另一个字符串文件是否等于这个密码，所以“破译密码”是一个 NP 类问题。NP 类问题也等价为能在多项式时间内<strong>猜出</strong>一个解的问题。这里的“猜”指的是如果有解，那每次都能在很多种可能的选择中运气极佳地选择正确的一步。</p><p>不妨举个例子：给出 n 个城市和两两之间的距离，求找到一个行走方案，使得到达每个城市一次的总路程最短。我们可以这样来“猜测”它的解：先求一个总路程不超过 100 的方案，假设我们可以依靠极好的运气“猜出”一个行走路线，使得总长度确实不超过 100，那么我们只需要每次猜一条路一共猜 n 次。接下来我们再找总长度不超过 50 的方案，找不到就将阈值提高到75…… 假设最后找到了总长度为 90 的方案，而找不到总长度小于 90 的方案。我们最终便在多项式时间内“猜”到了这个旅行商问题的解是一个长度为 90 的路线。它是一个 NP 类的问题。</p><p>也就是说，NP 问题能在多项式时间内“解决”，只不过需要好运气。显然，P 类问题肯定属于 NP 类问题。所谓“P=NP”，就是问——是不是所有的 NP 问题，都能找到多项式时间的确定性算法？</p><h3 id="NPC-Problem"><a href="#NPC-Problem" class="headerlink" title="NPC Problem"></a>NPC Problem</h3><p>在与数不尽的问题搏斗的过程中，人们有时候会发现，解决问题 A 的算法可以同时用来解决问题 B。例如问题 A 是对学生的姓名与所属班级同时排序，问题 B 是对人们按照姓名做排序。这时候，我们只需要让班级全都相同，便能照搬问题 A 的算法来解决问题 B。这种情况下，数学家就说，问题 B 能归约为问题 A。</p><p>人们发现，不同的 NP 问题之间也会出现可归约的关系，甚至存在这么一类（不只是一个）问题，使得任何其它的 NP 问题都能归约到它们上。也就是说，能够解决它们的算法就能够解决所有其它的 NP 问题。这一类问题就是 NPC 问题。这样的问题人们已经找到了几千个，如果我们给其中任何一个找到了多项式级别的算法，就相当于证明了 P=NP。</p><p>但NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。</p><h3 id="P-NP？"><a href="#P-NP？" class="headerlink" title="P=NP？"></a>P=NP？</h3><p>证明 P=NP 的一个主要方法就是，给某一个 NPC 问题找到一个快速算法。但是，也不排除有人给出一个“存在性”而非“构造性”的证明，只是告诉大家存在符合要求的算法，但没法详细描述出来。如果 P=NP 被人以这种方式证明出来了，我们也没法依葫芦画瓢地把这个神奇的算法在电脑上写出来，所以对破解密码仍然没有帮助。</p><p>退一步说，假如有人构造出可以运用的多项式算法，以此证明了这个问题。这个算法恐怕也很复杂（毕竟这么难找），它的多项式级别的复杂度也可能会非常慢。假设这个算法的复杂度达到了 O(n10)，那我们依然面临着不小的麻烦。即使 n=100，运算时间也会增长到非常巨大的地步。</p><p>再退一步，假设人类的运气好到 P=NP 是真的，并且找到了复杂度不超过 O(n3) 的算法。如果到了这一步，我们就会有一个算法，能够很快算出某个帐号的密码。《基本演绎法》里面所想象的可能就要成真了，所有的加密系统都会失去效果——应该说，所有会把密码变成数字信息的系统都会失去效果，因为这个数字串很容易被“金钥匙”计算出来。</p><p>除此之外，我们需要担心或期许的事情还有很多：</p><ul><li>一大批耳熟能详的游戏，如扫雷、俄罗斯方块、超级玛丽等，人们将为它们编写出高效的AI，使得电脑玩游戏的水平无人能及。</li><li>整数规划、旅行商问题等许多运筹学中的难题会被高效地解决，这个方向的研究将提升到前所未有的高度。</li><li>蛋白质的折叠问题也是一个 NPC 问题，新的算法无疑是生物与医学界的一个福音。</li></ul><p>参考文献：<br><a href="http://www.junnanzhu.com/?p=141" target="_blank" rel="noopener">http://www.junnanzhu.com/?p=141</a><br><a href="https://www.zybuluo.com/frank-shaw/note/139175" target="_blank" rel="noopener">https://www.zybuluo.com/frank-shaw/note/139175</a><br><a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-09-Visual-Information/</a><br><a href="https://www.guokr.com/article/437662/" target="_blank" rel="noopener">https://www.guokr.com/article/437662/</a><br><a href="https://www.zhihu.com/question/22178202" target="_blank" rel="noopener">https://www.zhihu.com/question/22178202</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/ai%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础（一）</title>
    <link href="http://frankblog.site/2018/05/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <id>http://frankblog.site/2018/05/18/机器学习基础/</id>
    <published>2018-05-18T04:57:16.128Z</published>
    <updated>2018-06-09T02:02:34.253Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg" alt=""></p><a id="more"></a><blockquote class="blockquote-center"><font size="5">从IT时代走向DT时代</font></blockquote><hr><h1 id="机器学习应用"><a href="#机器学习应用" class="headerlink" title="机器学习应用"></a>机器学习应用</h1><h2 id="1、计算机视觉"><a href="#1、计算机视觉" class="headerlink" title="1、计算机视觉"></a>1、计算机视觉</h2><p>典型的应用包括：<strong>人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索</strong>等等。</p><h2 id="2、自然语言处理"><a href="#2、自然语言处理" class="headerlink" title="2、自然语言处理"></a>2、自然语言处理</h2><p>典型的应用包括：<strong>搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译</strong>等等。</p><h2 id="3、社会网络分析"><a href="#3、社会网络分析" class="headerlink" title="3、社会网络分析"></a>3、社会网络分析</h2><p>典型的应用包括：<strong>用户画像、网络关联分析、欺诈作弊发现、热点发现</strong>等等。</p><h2 id="4、推荐系统"><a href="#4、推荐系统" class="headerlink" title="4、推荐系统"></a>4、推荐系统</h2><p>典型的应用包括：<strong>虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”</strong>等等。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt=""></p><h2 id="数据挖掘流程"><a href="#数据挖掘流程" class="headerlink" title="数据挖掘流程"></a>数据挖掘流程</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%B5%81%E7%A8%8B.jpg" alt=""></p><h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><h2 id="数据分类"><a href="#数据分类" class="headerlink" title="数据分类"></a>数据分类</h2><p>正例(positive example)<br>反例(negative example)<br>训练集(training set)<br>验证集(validation set)：用作超参数验证<br>测试集(test set)<br>类别不平衡数据集（class-imbalanced data set）</p><h2 id="采样方式"><a href="#采样方式" class="headerlink" title="采样方式"></a>采样方式</h2><h3 id="1、分层采样-stratified-sampling"><a href="#1、分层采样-stratified-sampling" class="headerlink" title="1、分层采样(stratified sampling)"></a>1、分层采样(stratified sampling)</h3><p>保留类别比例的采样方式通常称为分层采样</p><h3 id="2、留出法（hold-out）"><a href="#2、留出法（hold-out）" class="headerlink" title="2、留出法（hold-out）"></a>2、留出法（hold-out）</h3><p>直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p><h3 id="3、k折交叉验证（k-fold-cross-validation）"><a href="#3、k折交叉验证（k-fold-cross-validation）" class="headerlink" title="3、k折交叉验证（k-fold cross validation）"></a>3、k折交叉验证（k-fold cross validation）</h3><p>交叉验证先将数据集D划分为k个大小相似的互斥子集，每个子集从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。</p><h3 id="4、自助法-bootstrapping"><a href="#4、自助法-bootstrapping" class="headerlink" title="4、自助法(bootstrapping)"></a>4、自助法(bootstrapping)</h3><p>对数据集D有放回的随机采样m次后，一个样本不在样本集D1出现的概率：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E8%87%AA%E5%8A%A9%E6%B3%95.png" alt=""></p><p>当n足够大时，大约有36.8%的样本不会被采到，用没采到的部分做测试集，也是包外估计（out-of-bag-estimate）。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。</p><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="1、缺失值处理"><a href="#1、缺失值处理" class="headerlink" title="1、缺失值处理"></a>1、缺失值处理</h2><p>1.直接删除—-适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况。<br>2.使用一个全局常量填充—譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用<br>3.使用均值或中位数代替—-优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好。<br>4.插补法<br>    1）随机插补法—-从总体中随机抽取某个样本代替缺失样本<br>    2）多重插补法—-通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理<br>    3）热平台插补—-指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。<br>优点：简单易行，准去率较高<br>缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补<br>    4)拉格朗日差值法和牛顿插值法<br>5.建模法<br>可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。</p><p><strong>使用sklearn进行插补：</strong><br>其实sklearn里也有一个工具<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer" target="_blank" rel="noopener">Imputer</a>可以对缺失值进行插补。Imputer类可以对缺失值进行均值插补、中位数插补或者某行/列出现的频率最高的值进行插补，也可以对不同的缺失值进行编码。<strong>并且支持稀疏矩阵。</strong></p><p>在稀疏矩阵中，缺失值被编码为0存储为矩阵中，这种格式是适合于缺失值比非缺失值多得多的情况。此外，Imputer类也可以用于Pipeline中。</p><blockquote><p>Imputor类的参数：_class _<code>sklearn.preprocessing.`</code>Imputer`(_missing_values=’NaN’_, <em>strategy=’mean’</em>, <em>axis=0</em>, <em>verbose=0</em>, <em>copy=True</em>)</p></blockquote><blockquote><p><strong>missing_values</strong> : int或”NaN”,默认NaN（String类型）<br><strong>strategy</strong> : string, 默认为mean，可选则mean、median、most_frequent<br><strong>axis</strong> :int, 默认为0（axis = 0，对列进行插值；axis= 1，对行进行插值）<br><strong>verbose</strong> : int, 默认为0<br><strong>copy</strong> : boolean, 默认为True<br>　　True：会创建一个X的副本<br>　　False：在任何合适的地方都会进行插值。<br>　　但是以下四种情况，计算设置的copy = Fasle，也会创建一个副本：<br>　　1.X不是浮点型数组<br>　　2.X是稀疏矩阵，而且miss_value = 0<br>　　3.axis= 0，X被编码为CSR矩阵<br>　　4.axis= 1，X被编码为CSC矩阵</p></blockquote><p>p.s.：LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。</p><h2 id="2、异常点处理"><a href="#2、异常点处理" class="headerlink" title="2、异常点处理"></a>2、异常点处理</h2><p>1.简单的统计分析<br>　　拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。<br>2.3∂原则<br>　　如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。</p><p>3.箱型图分析<br>　　箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。</p><p>4.基于模型检测<br>　　首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象</p><p>优缺点：1.有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；2.对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。</p><p>5.基于距离<br>　　通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象</p><p>优缺点：1.简单；2.缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；3.该方法对参数的选择也是敏感的；4.不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。</p><p>6.基于密度<br>　　当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。</p><p>优缺点：1.给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；2.与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；3.参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。</p><p>7.基于聚类：<br>　　基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。</p><p>优缺点：1.基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；2.簇的定义通常是离群点的补，因此可能同时发现簇和离群点；3.产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；4.聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。</p><p><strong>处理方法：</strong></p><p>1.删除异常值—-明显看出是异常且数量较少可以直接删除<br>2.不处理—如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。<br>3.平均值替代—-损失信息小，简单高效。<br>4.视为缺失值—-可以按照处理缺失值的方法来处理<br>5.标准化—-如果你的数据有离群点，对数据进行均差和方差的标准化效果并不好。这种情况你可以使用<code>sklearn</code>中的<code>robust_scale</code>和 <code>RobustScaler</code> 作为替代。</p><h2 id="3、去重处理"><a href="#3、去重处理" class="headerlink" title="3、去重处理"></a>3、去重处理</h2><h3 id="dataframe格式"><a href="#dataframe格式" class="headerlink" title="dataframe格式"></a>dataframe格式</h3><p>1、DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行<br>2、drop_duplicates方法用于返回一个移除了重复行的DataFrame<br>3、data.drop_duplicates([‘v1’]) #只判断某列</p><h3 id="list格式"><a href="#list格式" class="headerlink" title="list格式"></a>list格式</h3><p>1、使用set()<br>2、{}.fromkeys().keys()<br>3、set()+sort()<br>4、排序后比较相邻2个元素的数据，重复的删除</p><h2 id="4、噪音处理"><a href="#4、噪音处理" class="headerlink" title="4、噪音处理"></a>4、<strong>噪音处理</strong></h2><p> 噪音，是被测量变量的随机误差或方差。</p><h3 id="噪音与离群点"><a href="#噪音与离群点" class="headerlink" title="噪音与离群点"></a>噪音与离群点</h3><blockquote><p>离群点： 你正在从口袋的零钱包里面穷举里面的钱，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。这个100元就是个离群点，因为并不应该常出现在口袋里..</p></blockquote><blockquote><p>噪声： 你晚上去三里屯喝的酩酊大醉，很需要买点东西清醒清醒，这时候你开始翻口袋的零钱包，嘛，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。但是你突然眼晕，把那三个一角看成了三个1元…这样错误的判断使得数据集中出现了噪声。</p></blockquote><h3 id="噪音处理方法"><a href="#噪音处理方法" class="headerlink" title="噪音处理方法"></a>噪音处理方法</h3><p><strong>1.分箱法</strong><br>分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。</p><ul><li>用箱均值光滑：箱中每一个值被箱中的平均值替换。</li><li>用箱中位数平滑：箱中的每一个值被箱中的中位数替换。</li><li>用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。</li></ul><p>一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用.</p><p><strong>2.  回归法</strong><br>　　可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。</p><h2 id="5、其他实用小技巧"><a href="#5、其他实用小技巧" class="headerlink" title="5、其他实用小技巧"></a>5、其他实用小技巧</h2><p>1.<strong>去掉文件中多余的空行</strong><br>空行主要指的是（\n,\r,\r\n,\n\r等），在python中有个strip()的方法，该方法可以去掉字符串两端多余的“空白”，此处的空白主要包括空格，制表符(\t)，换行符。不过亲测以后发现，strip()可以匹配掉\n,\r\n,\n\r等，但是过滤不掉单独的\r。为了万无一失，我还是喜欢用麻烦的办法。</p><p>2.<strong>如何判断文件的编码格式</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import chardet</span><br><span class="line">if chardet.detect(data)[&apos;encoding&apos;] != &apos;utf-8&apos;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">批量处理编码格式转换的代码已上传到github上</span><br></pre></td></tr></table></figure><h1 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h1><h2 id="1、离散型"><a href="#1、离散型" class="headerlink" title="1、离散型"></a>1、离散型</h2><p>1、<strong>one-hot 编码</strong><br>编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。</p><blockquote><p>sklearn.preprocessing.OneHotEncoder(_n_values=’auto’_, _categorical_features=’all’_, <em>dtype=<class 'float'=""></class></em>, <em>sparse=True</em>, _handle_unknown=’error’_)</p></blockquote><blockquote><p><strong>n_values</strong> : ‘auto’, int or array of ints 每个特征的数量</p><blockquote><p>auto : 从训练数据的范围中得到<br>     int : 所有特征的最大值（number）<br>     array : 每个特征的最大值（number）</p></blockquote></blockquote><blockquote><p><strong>categorical_features: “all” or array of indices or mask</strong> :确定哪些特征是类别特征</p><blockquote><p>all (默认): 所有特征都是类别特征，意味着所有特征都要进行OneHot编码<br>    array of indices: 类别特征的数组索引<br>    mask: n_features 长度的数组，切dtype = bool<br>非类别型特征通常会放到矩阵的右边</p></blockquote></blockquote><blockquote><p><strong>dtype</strong> : number type, default=np.float<br>输出数据的类型<br><strong>sparse</strong> : boolean, default=True<br>设置True会返回稀疏矩阵，否则返回数组<br><strong>handle_unknown</strong> : str, ‘error’ or ‘ignore’<br>当一个不明类别特征出现在变换中时，报错还是忽略</p></blockquote><p>２、<strong>Hash编码成词向量</strong>：<br><img src="http://p4rlzrioq.bkt.clouddn.com/hash%E7%BC%96%E7%A0%81.jpg" alt=""></p><h2 id="2、文本型"><a href="#2、文本型" class="headerlink" title="2、文本型"></a>2、文本型</h2><p>１. <strong>词袋</strong>：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．<br>２. 把词袋中的词扩充到<strong>n-gram</strong>：n-gram代表n个词的组合。比如“我喜欢你”、“你喜欢我”这两句话如果用词袋表示的话，分词后包含相同的三个词，组成一样的向量：“我 喜欢 你”。显然两句话不是同一个意思，用n-gram可以解决这个问题。如果用2-gram，那么“我喜欢你”的向量中会加上“我喜欢”和“喜欢你”，“你喜欢我”的向量中会加上“你喜欢”和“喜欢我”。这样就区分开来了。<br>３. 使用<strong>TF-IDF</strong>特征：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)，IDF(t) = ln(总文档数/ 含t的文档数)，TF-IDF权重 = TF(t) * IDF(t)。自然语言处理中经常会用到。</p><h2 id="3、数值型"><a href="#3、数值型" class="headerlink" title="3、数值型"></a>3、数值型</h2><h3 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%BD%92%E4%B8%80%E5%8C%96.svg" alt="link"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])</span><br><span class="line">min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line">#将上述得到的scale参数应用至测试数据</span><br><span class="line">X_test = np.array([[ -3., -1., 4.]]) </span><br><span class="line">X_test_minmax = min_max_scaler.transform(X_test)</span><br></pre></td></tr></table></figure></p><h3 id="区间缩放（scaling）"><a href="#区间缩放（scaling）" class="headerlink" title="区间缩放（scaling）"></a>区间缩放（scaling）</h3><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E7%BC%A9%E6%94%BE.svg" alt="link"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">                    [ 2.,  0.,  0.],</span><br><span class="line">                    [ 0.,  1., -1.]])</span><br><span class="line">max_abs_scaler = preprocessing.MaxAbsScaler() </span><br><span class="line">X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line">X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line">X_test_maxabs = max_abs_scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h3 id="标准化（Standardization）"><a href="#标准化（Standardization）" class="headerlink" title="标准化（Standardization）"></a>标准化（Standardization）</h3><h4 id="适用情况"><a href="#适用情况" class="headerlink" title="适用情况"></a>适用情况</h4><p>看模型是否具有伸缩不变性。<br> 不是所有的模型都一定需要标准化，有些模型对量纲不同的数据比较敏感，譬如SVM等。当各个维度进行不均匀伸缩后，最优解与原来不等价，这样的模型，除非原始数据的分布范围本来就不叫接近，否则<strong>必须</strong>进行标准化，以免模型参数被分布范围较大或较小的数据主导。<br>但是如果模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression等，对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。<br>所以对于具有伸缩不变性的模型，<strong>最好</strong>也进行数据标准化。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A0%87%E5%87%86%E5%8C%96.svg" alt="link"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing  </span><br><span class="line">import numpy as np </span><br><span class="line">X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])  </span><br><span class="line">X_scaled = preprocessing.scale(X)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler().fit(X) </span><br><span class="line">#测试将该scaler用于输入数据，变换之后得到的结果同上</span><br><span class="line">scaler.transform(X)</span><br></pre></td></tr></table></figure><h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a><strong>二值化</strong></h3><p><strong>1.特征二值化</strong><br>特征二值化是把数值特征转化成布尔值的过程。这个方法对符合多变量伯努利分布的输入数据进行预测概率参数很有效。详细可以见这个例子<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM" target="_blank" rel="noopener">sklearn.neural_network.BernoulliRBM</a>.</p><p>对于 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer" target="_blank" rel="noopener">Normalizer</a>，<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer" title="sklearn.preprocessing.Binarizer" target="_blank" rel="noopener">Binarizer</a>工具类通常是在Pipeline阶段（<a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline" target="_blank" rel="noopener">sklearn.pipeline.Pipeline</a>）的前期过程会用到。</p><h2 id="4、比赛实际场景"><a href="#4、比赛实际场景" class="headerlink" title="4、比赛实际场景"></a>4、比赛实际场景</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AF%94%E8%B5%9B%E5%9C%BA%E6%99%AF1.jpg" alt="link"><br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%AF%94%E8%B5%9B%E5%9C%BA%E6%99%AF2.jpg" alt="link"><br>可见，选手需要进行制定规则、数据清洗、各个种类的特征处理等，对特征的研究是非常细化的。</p><h1 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h1><h2 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h2><p>1. 拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。 </p><ul><li>user_id&amp;&amp;category: 10001&amp;&amp;女裙 10002&amp;&amp;男士牛仔 </li><li>user_id&amp;&amp;style: 10001&amp;&amp;蕾丝 10002&amp;&amp;全棉　　<br>2. 模型特征组合： </li><li>用GBDT产出特征组合路径 </li><li>组合特征和原始特征一起放进LR训练</li></ul><h2 id="生成多项式特征"><a href="#生成多项式特征" class="headerlink" title="生成多项式特征"></a>生成多项式特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(9).reshape(3, 3)</span><br><span class="line">#只需要特征的交叉项，可以设置interaction_only=True</span><br><span class="line">poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class="line">poly.fit_transform(X)</span><br></pre></td></tr></table></figure><p>此方法经常用于核方法中</p><h2 id="自定义特征"><a href="#自定义特征" class="headerlink" title="自定义特征"></a>自定义特征</h2><p>1.想用对数据取对数，可以自己用 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer" target="_blank" rel="noopener">FunctionTransformer</a>自定义一个转化器,并且可以在Pipeline中使用`</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import FunctionTransformer </span><br><span class="line">transformer = FunctionTransformer(np.log1p)#括号内的就是自定义函数</span><br><span class="line">X = np.array([[0, 1], [2, 3]]) </span><br><span class="line">transformer.transform(X)</span><br></pre></td></tr></table></figure><p>2.如果你在做一个分类任务时，发现第一主成分与这个不相关，你可以用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer" target="_blank" rel="noopener">FunctionTransformer</a>把第一列除去，剩下的列用PCA</p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p><strong>特征选择</strong>，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。 </p><h3 id="1-过滤型"><a href="#1-过滤型" class="headerlink" title="1 过滤型"></a>1 过滤型</h3><ul><li>方法：  评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。 </li><li>评价方式：Pearson相关系数， 互信息， 距离相关度。 </li><li>缺点：只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。 </li><li>python包：SelectKBest指定过滤个数、SelectPercentile指定过滤百分比。</li></ul><h3 id="2-包裹型"><a href="#2-包裹型" class="headerlink" title="2 包裹型"></a>2 包裹型</h3><ul><li>方法：把特征选择看做一个特征子集搜索问题， 筛选各种特<br>征子集， 用模型评估效果。 </li><li>典型算法：“递归特征删除算法”。 </li><li>应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。 </li><li>python包：RFE </li></ul><h3 id="3-嵌入型"><a href="#3-嵌入型" class="headerlink" title="3 嵌入型"></a>3 嵌入型</h3><ul><li>方法：根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。 </li><li>举例：最早在电商用LR做CTR预估， 在3-5亿维的系数特征上用L1正则化的LR模型。上一篇介绍了L1正则化有截断作用，剩余2-3千万的feature， 意味着其他的feature重要度不够。 </li><li>python包：feature_selection.SelectFromModel选出权重不为0的特征。</li></ul><h1 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h1><pre><code>   在数据处理中，经常会遇到特征维度比样本数量多得多的情况，如果拿到实际工程中去跑，效果不一定好。一是因为冗余的特征会带来一些噪音，影响计算的结果；二是因为无关的特征会加大计算量，耗费时间和资源。所以我们通常会对数据重新变换一下，再跑模型。数据变换的目的不仅仅是降维，还可以消除特征之间的相关性，并发现一些潜在的特征变量。</code></pre><h2 id="PCA-过程"><a href="#PCA-过程" class="headerlink" title="PCA 过程"></a>PCA 过程</h2><p>1.去掉数据的类别特征（label），将去掉后的d维数据作为样本<br>2.计算d维的均值向量（即所有数据的每一维向量的均值）<br>3.计算所有数据的散布矩阵（或者协方差矩阵）<br>4.计算特征值（e1,e2,…,ed）以及相应的特征向量（lambda1,lambda2,…,lambda d）<br>5.按照特征值的大小对特征向量降序排序，选择前k个最大的特征向量，组成d<em>k维的矩阵W（其中每一列代表一个特征向量）<br>6.运用d</em>K的特征向量矩阵W将样本数据变换成新的子空间。（用数学式子表达就是<img src="http://p4rlzrioq.bkt.clouddn.com/pca%E9%99%8D%E7%BB%B4%E5%A4%84%E7%90%86.png" alt="">，其中x是d<em>1维的向量，代表一个样本，y是K</em>1维的在新的子空间里的向量）</p><p>python里有已经写好的模块，可以直接拿来用，但是我觉得不管什么模块，都要懂得它的原理是什么。matplotlib有<a href="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml" target="_blank" rel="noopener">matplotlib.mlab.PCA()</a>，sklearn也有专门一个模块<a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions" target="_blank" rel="noopener">Dimensionality reduction</a>专门讲PCA，包括传统的PCA，也就是我上文写的，以及增量PCA，核PCA等等，除了PCA以外，还有ZCA白化等等，在图像处理中也经常会用到。</p><p>　　推荐一个博客，动态展示了PCA的过程：<a href="http://setosa.io/ev/principal-component-analysis/" target="_blank" rel="noopener">http://setosa.io/ev/principal-component-analysis/</a>  写的也很清楚，可以看一下；再推荐一个维基百科的，讲的真的是详细啊<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><ul><li>线性回归的L1正则化通常称为Lasso回归，α来调节损失函数的均方差项和正则化项的权重。</li><li>Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0，故具有特征选择的功能，增强了模型的泛化能力。</li></ul><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><ul><li>线性回归的L2正则化通常称为Ridge回归。</li><li>Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。</li><li>Ridge回归的求解比较简单，一般用最小二乘法。</li></ul><p><strong>L1正则化产生稀疏的权值, 具有特征选择的作用；L2正则化产生平滑的权值</strong>。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/L1,L2%E6%AD%A3%E5%88%99%E5%8C%96.png" alt=""></p><h2 id="最小二乘法的局限性"><a href="#最小二乘法的局限性" class="headerlink" title="最小二乘法的局限性"></a>最小二乘法的局限性</h2><ol><li>最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了</li><li>当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。</li><li>如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用</li><li>当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。</li></ol><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="决策树构建中的分裂准则"><a href="#决策树构建中的分裂准则" class="headerlink" title="决策树构建中的分裂准则"></a>决策树构建中的分裂准则</h2><p>决策树可以通过一系列规则递归地分割特征空间</p><h3 id="信息增益（information-gain）"><a href="#信息增益（information-gain）" class="headerlink" title="信息增益（information gain）"></a><strong>信息增益（information gain）</strong></h3><p>属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合</p><h3 id="增益率（gain-ratio）"><a href="#增益率（gain-ratio）" class="headerlink" title="增益率（gain ratio）"></a><strong>增益率（gain ratio）</strong></h3><p>信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。</p><h3 id="基尼指数-Gini-index"><a href="#基尼指数-Gini-index" class="headerlink" title="基尼指数(Gini index)"></a><strong>基尼指数(Gini index)</strong></h3><p>基尼指数是另外一种数据的不纯度的度量方法，其定义如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/gini%E7%B3%BB%E6%95%B01.jpg" alt="">　　</p><p>其中的m仍然表示数据集D中类别C的个数，Pi表示D中任意一个记录属于Ci的概率，计算时Pi=(D中属于Ci类的集合的记录个数/|D|)。如果所有的记录都属于同一个类中，则P1=1，Gini(D)=0，此时不纯度最低。<br>在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树，对每个属性都会枚举其属性的非空真子集，以属性R分裂后的基尼系数为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B02.jpg" alt=""></p><p>D1为D的一个非空真子集，D2为D1在D的补集，即D1+D2=D，对于属性R来说，有多个真子集，即GiniR(D)有多个值，但我们选取最小的那么值作为R的基尼指数。最后：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B03.jpg" alt=""></p><p>对于二类分类，基尼系数和熵之半的曲线如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/40658-24f62052d3f57559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代</p><h2 id="常用决策树模型"><a href="#常用决策树模型" class="headerlink" title="常用决策树模型"></a>常用决策树模型</h2><h3 id="决策树模型总结"><a href="#决策树模型总结" class="headerlink" title="决策树模型总结"></a>决策树模型总结</h3><table><thead><tr><th>算法</th><th>支持模型</th><th>树结构</th><th>特征选择</th><th>连续值处理</th><th>缺失值处理</th><th>剪枝</th></tr></thead><tbody><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td><td>不支持</td><td>不支持</td><td>不支持</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>CART</td><td>分类，回归</td><td>二叉树</td><td>基尼系数，均方差</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table><h3 id="CART决策树属性分裂方法"><a href="#CART决策树属性分裂方法" class="headerlink" title="CART决策树属性分裂方法"></a>CART决策树属性分裂方法</h3><ol><li>m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。</li><li>对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。</li></ol><h2 id="决策树优化方法"><a href="#决策树优化方法" class="headerlink" title="决策树优化方法"></a>决策树优化方法</h2><h3 id="后剪枝（postpruning）"><a href="#后剪枝（postpruning）" class="headerlink" title="后剪枝（postpruning）"></a>后剪枝（postpruning）</h3><p>先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。</p><h3 id="预剪枝（prepruning）"><a href="#预剪枝（prepruning）" class="headerlink" title="预剪枝（prepruning）"></a>预剪枝（prepruning）</h3><p>在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。</p><h3 id="抑制单颗决策树的复杂度的方法"><a href="#抑制单颗决策树的复杂度的方法" class="headerlink" title="抑制单颗决策树的复杂度的方法"></a>抑制单颗决策树的复杂度的方法</h3><ol><li>限制树的最大深度</li><li>限制叶子节点的最少样本数量</li><li>限制节点分裂时的最少样本数量</li><li>吸收 bagging 的思想对训练样本采样，在学习单颗决策树时只使用一部分训练样本</li><li>借鉴随机森林的思路在学习单颗决策树时只采样一部分特征，在目标函数中添加正则项惩罚复杂的树结。</li></ol><h2 id="决策树算法的优点"><a href="#决策树算法的优点" class="headerlink" title="决策树算法的优点"></a>决策树算法的优点</h2><ol><li>基本不需要预处理，不需要提前归一化，处理缺失值。</li><li>使用决策树预测的代价是O(log2m)。 m为样本数。</li><li>既可以处理离散值也可以处理连续值。很多算法只是4.专注于离散值或者连续值。</li><li>可以处理多维度输出的分类问题。</li><li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li><li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li><li>对于异常点的容错能力好，健壮性高。</li></ol><h2 id="决策树算法的缺陷"><a href="#决策树算法的缺陷" class="headerlink" title="决策树算法的缺陷"></a>决策树算法的缺陷</h2><ol><li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li><li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li><li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li><li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li><li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li></ol><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。</p><h2 id="梯度下降法的超参数"><a href="#梯度下降法的超参数" class="headerlink" title="梯度下降法的超参数"></a>梯度下降法的超参数</h2><ul><li>步长（step size）<br>学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" alt=""></li></ul><h2 id="梯度下降的算法调优"><a href="#梯度下降的算法调优" class="headerlink" title="梯度下降的算法调优"></a>梯度下降的算法调优</h2><ol><li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p></li><li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p></li><li><p>标准化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据标准化，这样特征的新期望为0，新方差为1，迭代次数可以大大加快。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E4%BC%98%E5%8A%BF.png" alt=""></p></li></ol><h2 id="梯度下降方法总结"><a href="#梯度下降方法总结" class="headerlink" title="梯度下降方法总结"></a>梯度下降方法总结</h2><h3 id="批梯度下降-batch-gradient-descent-BGD"><a href="#批梯度下降-batch-gradient-descent-BGD" class="headerlink" title="批梯度下降(batch gradient descent/BGD)"></a>批梯度下降(batch gradient descent/BGD)</h3><p>求梯度的时候就用了所有m个样本的梯度数据。</p><h3 id="随机梯度下降（stochastic-gradient-descent-SGD）"><a href="#随机梯度下降（stochastic-gradient-descent-SGD）" class="headerlink" title="随机梯度下降（stochastic gradient descent/SGD）"></a>随机梯度下降（stochastic gradient descent/SGD）</h3><p>随机梯度下降法由于每次仅仅采用一个样本来迭代。优点是速度快以及可以跳出局部最优解，缺点是导致迭代方向变化很大，不能很快的收敛到局部最优解。</p><h3 id="小批量随机梯度下降（mini-batch-stochastic-gradient-descent）"><a href="#小批量随机梯度下降（mini-batch-stochastic-gradient-descent）" class="headerlink" title="小批量随机梯度下降（mini-batch stochastic gradient descent）"></a>小批量随机梯度下降（mini-batch stochastic gradient descent）</h3><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。</p><h2 id="梯度下降法与最小二乘法"><a href="#梯度下降法与最小二乘法" class="headerlink" title="梯度下降法与最小二乘法"></a>梯度下降法与最小二乘法</h2><ul><li>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。</li><li>梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</li></ul><h1 id="分类模型指标"><a href="#分类模型指标" class="headerlink" title="分类模型指标"></a>分类模型指标</h1><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p><img src="https://upload-images.jianshu.io/upload_images/145616-0a7a7fd1ff77dcd9.png" alt="link"></p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p><strong>准确率</strong>是预测和标签一致的样本在所有样本中所占的比例</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89.svg" alt="link"></p><h2 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h2><p><strong>精确率</strong>是你预测为正类的数据中，有多少确实是正类</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%88Precision%EF%BC%89.svg" alt="link"></p><h2 id="查全率（Recall）"><a href="#查全率（Recall）" class="headerlink" title="查全率（Recall）"></a>查全率（Recall）</h2><p><strong>查全率</strong>是所有正类的数据中，你预测为正类的数据占比</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%9F%A5%E5%85%A8%E7%8E%87%EF%BC%88Recall%EF%BC%89.svg" alt="link"></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg" alt="link"></p><p>不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F1%20Score.svg" alt="link"></p><h2 id="F-beta-Score"><a href="#F-beta-Score" class="headerlink" title="F-beta Score"></a>F-beta Score</h2><p>有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/F-beta%20Score.svg" alt="link"></p><h2 id="ROC-（receiver-operating-characteristic-curve）"><a href="#ROC-（receiver-operating-characteristic-curve）" class="headerlink" title="ROC （receiver operating characteristic curve）"></a>ROC （receiver operating characteristic curve）</h2><p>绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。<br>ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</p><p><img src="https://habrastorage.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif" alt=""></p><h2 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h2><p>ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。</p><p><img src="https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="link"></p><p>理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p><h1 id="模型选择与评估"><a href="#模型选择与评估" class="headerlink" title="模型选择与评估"></a>模型选择与评估</h1><h2 id="算法选择"><a href="#算法选择" class="headerlink" title="算法选择"></a>算法选择</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/sklearn%20%E4%B8%AD%E6%96%87.png" alt=""></p><h2 id="泛化能力、欠拟合和过拟合"><a href="#泛化能力、欠拟合和过拟合" class="headerlink" title="泛化能力、欠拟合和过拟合"></a>泛化能力、欠拟合和过拟合</h2><p><img src="http://p4rlzrioq.bkt.clouddn.com/overfitting.jpg" alt=""></p><h2 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h2><p>偏差方差分解解释了机器的泛化误差。偏差度量了算法的期望预测与真实结果之间的误差。方差度量了训练集的变动所导致的学习性能的变化。<br><img src="http://p4rlzrioq.bkt.clouddn.com/%E5%87%86%E4%B8%8E%E7%A1%AE.jpeg" alt="link"></p><ul><li>有一些算法天生是高方差的算法。如KNN、决策树。非参数学习通常是高方差算法，对数据较为敏感，因为不对数据进行任何假设。</li><li>有一些算法天生就是高偏差算法。如线性回归。参数学习通常是高偏差算法，因为对数据具有极强的假设。</li><li>机器学习的主要挑战来自于方差，解决高方差的通常手段有：<ul><li>1.降低模型复杂度</li><li>2.减少数据维度；降噪</li><li>3.增加样本数</li><li>4.使用验证集</li><li>5.模型正则化</li></ul></li></ul><p>此图献给奋战在一线的调参侠们！</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/machine%20learning%20funny.jpg" alt="link"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/machine-learning-algorithms.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://frankblog.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://frankblog.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://frankblog.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Git学习</title>
    <link href="http://frankblog.site/2018/04/08/git%E5%AD%A6%E4%B9%A0/"/>
    <id>http://frankblog.site/2018/04/08/git学习/</id>
    <published>2018-04-07T16:08:44.007Z</published>
    <updated>2018-06-04T02:25:11.516Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/git.jpg" alt=""><br><a id="more"></a></p><hr><h2 id="Git-简介"><a href="#Git-简介" class="headerlink" title="Git 简介"></a>Git 简介</h2><p>Git是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件，是目前世界上最先进的分布式版本控制系统。<br>主要优势有：公共服务器压力和数据量都不会太大，任意两个开发者之间可以很容易的解决冲突并且可以进行离线工作。</p><h2 id="Git-术语"><a href="#Git-术语" class="headerlink" title="Git 术语"></a>Git 术语</h2><ul><li>commit<br>提交持有的库的当前状态，每个提交的对象有父commit对象的指针。从给定的commit可以遍历寻找父指针，查看历史记录的提交。</li><li>branches<br>分支用来创建另一条线的发展，默认情况下，git的主分支，是master分支，和上线的版本是一样的，平时要工作的新功能创建一个分支，功能完成之后，它被合并回master分支，每当做出一个commit，HEAD更新为最新提交</li><li>tags<br>git中的tag指向一次commit的id。通常用来给开发做版本号。</li><li>clone<br>克隆操作不仅仅是检出的工作拷贝，也反映了完整的信息</li><li>pull<br>pull操作是用于两个存储库实例之间的同步</li><li>push<br>将本地仓库中的文件同步到远端库中</li><li>head<br>HEAD指针总是指向分支的最新提交，每当你做了一个提交。HEAD更新为最新提交,HEAD树枝存储在.git/refs/heads/中</li><li>工作区：就是你在电脑里能看到的目录。</li><li>暂存区：英文叫stage, 或index。一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</li><li>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</li></ul><h2 id="Bash-基本指令"><a href="#Bash-基本指令" class="headerlink" title="Bash 基本指令"></a>Bash 基本指令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pwd : 显示当前所在的目录路径。</span><br><span class="line">ls(ll): 都是列出当前目录中的所有文件，只不过ll(两个ll)列出的内容更为详细。</span><br><span class="line">touch : 新建一个文件 如 touch index.js 就会在当前目录下新建一个index.js文件。</span><br><span class="line">rm: 删除一个文件, rm index.js 就会把index.js文件删除。</span><br><span class="line">mkdir: 新建一个目录,就是新建一个文件夹。</span><br><span class="line">rm -r : 删除一个文件夹, rm -r src 删除src目录， 好像不能用通配符。</span><br><span class="line">mv 移动文件, mv index.html src index.html 是我们要移动的文件, src 是目标文件夹,当然, 这样写,必须保证文件和目标文件夹在同一目录下。</span><br><span class="line">reset 重新初始化终端/清屏。</span><br><span class="line">clear 清屏。</span><br><span class="line">history 查看命令历史。</span><br><span class="line">elp 帮助。</span><br><span class="line">exit 退出。</span><br></pre></td></tr></table></figure><h2 id="常用基础命令"><a href="#常用基础命令" class="headerlink" title="常用基础命令"></a>常用基础命令</h2><h3 id="安装-amp-配置"><a href="#安装-amp-配置" class="headerlink" title="安装&amp;配置"></a>安装&amp;配置</h3><ul><li>官网下载安装完，右键看到<strong>Git Bash</strong>代表安装完成</li><li>初始配置（–local 项目级；–global 当前用户级；–system 系统级）<br><code>git config --global user.name&quot;Your Name&quot;</code><br><code>git config --global user.email&quot;email@example.com&quot;</code></li><li>查看配置 <code>- git config -l</code></li></ul><h3 id="初始化-amp-克隆"><a href="#初始化-amp-克隆" class="headerlink" title="初始化&amp;克隆"></a>初始化&amp;克隆</h3><ul><li>本地初始化：<code>git init</code> 仓库目录下会多了一个.git隐藏文件夹。</li><li>克隆版本库：<code>git clone &quot;url&quot;</code></li></ul><p>p.s. 版本控制系统可以告诉你每次的改动，比如在第x行加了代码。而图片、视频这些二进制文件没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/git%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4.png" alt=""></p><h3 id="管理分支"><a href="#管理分支" class="headerlink" title="管理分支"></a>管理分支</h3><ul><li>查看分支：<code>git branch</code></li><li>创建分支：<code>git branch branch_name</code></li><li>切换分支：<code>git checkout branch_name</code></li><li>创建+切换分支：<code>git checkout -b branch_name</code></li><li>合并某分支到当前分支：<code>git merge branch_name</code></li><li>重命名分支：<code>git branch -m branch_name branch_new_name</code> //不会覆盖已经存在的分支</li><li>重命名分支：<code>git branch -M branch_name branch_new_name</code> //会覆盖已经存在的分支</li><li>删除分支：<code>git branch -d branch_name</code></li><li>强制删除分支： <code>git branch -D branch_name</code></li><li>删除远程分支： <code>git push origin : branch_name</code> //可以使用这种语法，推送一个空分支到远程分支，其实就相当于删除远程分支</li></ul><h3 id="查看-amp-修改"><a href="#查看-amp-修改" class="headerlink" title="查看&amp;修改"></a>查看&amp;修改</h3><ul><li>拉取代码：<code>git pull orgin branch_name</code></li><li>查看更改：<code>git status</code>;<code>git status -s</code>//以简短格式输出</li><li>查看更改细节：<code>git diff file_name</code>//尚未缓存的改动<code>git diff --cached</code>//查看已缓存的改动</li><li>查看谁修改过代码：<code>git blame filename</code></li><li>回到上次修改：<code>git reset --hard</code></li><li>查看历史记录：<code>git log</code>；<code>git log --pretty=oneline</code>//将每次<code>commit</code>的记录打印成一行</li><li>查看git远程地址：<code>git remote -v</code></li><li>删除：<code>git rm</code> //将文件从缓存区中移除</li></ul><h3 id="添加文件"><a href="#添加文件" class="headerlink" title="添加文件"></a>添加文件</h3><ul><li>添加单个文件：<code>git add filename.js</code>  //该文件添加到缓存</li><li>添加所有js文件：<code>git add *.js</code></li><li>添加所有文件：<code>git add</code></li></ul><h3 id="提交文件"><a href="#提交文件" class="headerlink" title="提交文件"></a>提交文件</h3><ul><li>提交添加的文件：<code>git commit -m &quot;your description about this branch&quot;</code>//记录缓存区的快照。</li><li>提交单个文件：<code>git commit -m &quot;your description about this branch&quot; filename.js</code></li><li>推送分支：<code>git push orgin your_branch_name</code></li><li>备份当前分支内容：<code>git stash</code></li></ul><h3 id="标签操作"><a href="#标签操作" class="headerlink" title="标签操作"></a>标签操作</h3><ul><li>创建标签：<code>git tag 1.0.0</code>  //标签无法重命名</li><li>显示标签列表：<code>git tag</code></li><li>切出标签：<code>git checkout 1.0.0</code></li><li>删除标签：<code>git tag -d 1.0.0</code></li></ul><h2 id="流程化管理"><a href="#流程化管理" class="headerlink" title="流程化管理"></a>流程化管理</h2><ul><li>从主分支分支拉一下代码<br><code>git pull origin master</code></li><li>创建开发分支develop<br><code>git co(checkout) -b develop</code></li><li>如果其他分支有需要处理的bug，先将当前状态保存一下<br><code>git stash</code></li><li>切换到别的分支修改代码<br><code>git checkout -b branch_name</code></li><li>修复bug后提交代码查看修改<br><code>git status</code></li><li>需要查看修改的细节<br><code>git diff file_name</code></li><li><p>没有问题就提交</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit &quot;your description&quot;</span><br><span class="line">git push orgin your_branch_name</span><br></pre></td></tr></table></figure></li><li><p>解决完bug切换到原来的分支<br><code>git checkout -b you_old_branch</code></p></li><li><p>恢复刚刚保存的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git stash   //备份当前的工作区的内容，保存到git栈</span><br><span class="line">git stash pop  //从git栈中读取最近一次保存的内容，恢复工作区的相关内容，由于会存在多个stash内容，所以用栈来保存，pop出最近一个stash中读取的内容并恢复</span><br><span class="line">git stash list //显示git栈内所有的备份，可以利用这个列表来决定从哪个地方恢复</span><br><span class="line">git stash clear //清空git栈，此时使用git等图形化工具会发现，原来stash的那些节点都消失了</span><br></pre></td></tr></table></figure></li><li><p>最后，提交三部曲</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit &quot;your description&quot;</span><br><span class="line">git push orgin your_branch_name</span><br></pre></td></tr></table></figure></li></ul><h2 id="Github-pages"><a href="#Github-pages" class="headerlink" title="Github pages"></a>Github pages</h2><h3 id="Git初始设置"><a href="#Git初始设置" class="headerlink" title="Git初始设置"></a>Git初始设置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;你的GitHub用户名&quot;</span><br><span class="line">git config --global user.email &quot;你的GitHub注册邮箱&quot;</span><br><span class="line">ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;</span><br></pre></td></tr></table></figure><h3 id="hexo初始化设置"><a href="#hexo初始化设置" class="headerlink" title="hexo初始化设置"></a>hexo初始化设置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd d:/hexo</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line">hexo init folder</span><br><span class="line">cd folder</span><br><span class="line">npm install</span><br><span class="line">hexo g  或者hexo generate</span><br><span class="line">hexo s  或者hexo s -p 5000 （ctrl+c退出）</span><br><span class="line">hexo d  #部署到远程</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/git.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Software" scheme="http://frankblog.site/categories/Software/"/>
    
    
      <category term="Git" scheme="http://frankblog.site/tags/Git/"/>
    
      <category term="Github" scheme="http://frankblog.site/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>数学之美学习笔记</title>
    <link href="http://frankblog.site/2018/03/06/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    <id>http://frankblog.site/2018/03/06/数学之美/</id>
    <published>2018-03-06T13:59:33.332Z</published>
    <updated>2018-06-02T03:14:11.099Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.jpg" alt=""><br><a id="more"></a></p><blockquote class="blockquote-center"><font size="4">技术分为术和道，具体的做事方法是术，做事的原理和原则是道。——吴军</font></blockquote><h4 id="自然语言处理，语音识别，机器翻译"><a href="#自然语言处理，语音识别，机器翻译" class="headerlink" title="自然语言处理，语音识别，机器翻译"></a>自然语言处理，语音识别，机器翻译</h4><h5 id="基于规则的语言处理"><a href="#基于规则的语言处理" class="headerlink" title="基于规则的语言处理"></a>基于规则的语言处理</h5><p>早期学术界认为，要让机器完成翻译和语音识别这种人类才能做的事情，就必须先让计算机理解自然语言，而做到这点就要让机器有类似人类的智能。这个方法论被称为“鸟飞派”（通过观察鸟的飞行方式，采用仿生的思路造出飞机）。</p><p>那么怎么让机器理解自然语言呢？受传统语言学的影响，他们觉得要让机器做好两件事：分析句子语法和获取语义。分析句子语法就是按照语法把句子拆分，分清它的主语、谓语、宾语是什么，每个部分的词性是什么，用什么标点符号。而语义分析，就是弄清句子要表达的具体意思。语法规则很容易用计算机算法描述，这让人们觉得基于规则的方法是对的。但是这种方法很快就陷入困境，因为基于语法的分析器处理不了复杂句子，同时，词的多义性无法用规则表述，例如下面的例子：</p><blockquote><p>The pen is in the box. 和 The box is in the pen.<br>第二句话让非英语母语的人很难理解，盒子怎么在钢笔里呢？其实在这里，pen是围栏的意思。这里pen是钢笔还是围栏，通过上下文已经不能解决，而需要常识，即钢笔可以放在盒子里，但是盒子比钢笔大，所以不能放在盒子里，于是pen在这里是围栏的意思，盒子可以放在围栏里。</p></blockquote><h5 id="基于统计的语言处理"><a href="#基于统计的语言处理" class="headerlink" title="基于统计的语言处理"></a>基于统计的语言处理</h5><p>贾里尼克（Jelinek）把语音识别问题当作通信问题，并用两个隐含马尔可夫模型（声学和语言模型）概括了语音识别，推动了基于统计的语言处理方法。</p><p>在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子。早期的做法是判断给出的句子是否合乎语法，由前文可知这条路走不通。贾里尼克从另外角度看这个问题：<strong>通过计算一个句子出现的概率大小来判断它的合理性</strong>，于是语音识别问题转换成计算概率问题，根据这个思路，贾里尼克建立了<strong>统计语言模型</strong>。</p><p>假定S表示某一个有意义的句子，由一连串特定顺序排列的词w1,w2,w3…组成。我们想知道S在文本中出现的可能性，计算S的概率P(S)，根据条件概率公式：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/1.png" alt=""></p><p>其中P(w1)为w1出现的概率，P(w2|w1)为已知第一个词出现的条件下，第二个词出现的概率，以此类推。前面几个概率容易计算，但是后面的概率随着变量增多，变得不可计算。在这里需要应用马尔可夫假设来简化计算。<strong>马尔可夫假设</strong>假定当前状态只与前一个状态有关，即Wi出现的概率只同它前面的词有关Wi-1，于是上面的公式可以简化为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/2.png" alt=""></p><p>接下来的问题是估算条件概率P(Wi|Wi-1)，由条件概率公式得：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/3.png" alt=""></p><p>而估计联合概率P(Wi-1, Wi)和P(Wi-1)可以统计语料库得到，通过计算(Wi-1, Wi)这对词在语料库中前后相邻出现的次数C，以及Wi-1单独出现的次数，就可得到这些词或者二元组的相对频度。根据大数定理，只要统计量足够，相对频度就等于概率，于是</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/4.png" alt=""></p><p>于是复杂的语序合理性问题，变成了简单的次数统计问题。</p><p>上式对应的统计语言模型是二元模型，实际应用中，google翻译用到四元模型。</p><h5 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h5><p>对于西方拼音语言来说，词之间有明确的分界符（空格），但是中、日、韩、泰等语言没有。因此，首先要对句子进行分词，才能做进一步自然语言处理。对一个句子正确的分词结果如下：</p><blockquote><p>分词前：中国航天官员应邀到美国与太空总署官员开会。<br>分词后：中国/航天/官员/应邀/到/美国/与/太空/总署/官员/开会/。</p></blockquote><p>最容易想到的分词方法是“查字典”，即<strong>把一个句子从左到右扫描一遍，遇到字典里有的词就标出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字</strong>。这个方法能解决七八成的问题，但是遇到有二义性的分割就无能为力了，例如“发展中国家”，正确的分割是“发展-中-国家”，但是按照查字典法就会分成“发展-中国-家”。另外，并不是最长匹配都一定正确，例如“上海大学城书店”，正确的分割是“上海-大学城-书店”，而不是“上海大学-城-书店”。</p><p>按照前文的成功思路，依靠语法规则无法解决分词的二义性问题，还是得靠统计语言模型。</p><p>假设一个句子S有n种分词方法，利用前文的统计语言模型，<strong>分别计算出每种分词方法的概率，概率最大的即为最好的分词方法</strong>。因为穷举所有的分词方法计算量太大，所以可以把它看成是一个<strong>动态规划</strong>问题，并利用<strong>维特比算法</strong>快速找到最佳分词。具体应用时还要考虑分词的颗粒度。</p><h4 id="拼音输入法"><a href="#拼音输入法" class="headerlink" title="拼音输入法"></a>拼音输入法</h4><h5 id="拼音输入法中的数学"><a href="#拼音输入法中的数学" class="headerlink" title="拼音输入法中的数学"></a>拼音输入法中的数学</h5><p>中文输入法经历了以自然音节编码输入，到偏旁笔画拆字输入，再回归自然音节输入的过程。输入法输入汉字的快慢取决于对汉字编码的平均长度，也就是击键次数乘以寻找这个键需要的时间。单纯地减少编码长度未必能提高输入速度，因为寻找一个键的时间会增长。</p><p>将汉字输入到计算机中，是将人能看懂的信息编码变成计算机约定的编码（Unicode或UTF-8）的过程。对汉字的编码分为两部分：对拼音的编码和消除（一音多字）歧义。键盘上可使用的是26个字母和10个数字键，最直接的方式是让26个字母对应拼音，用10个数字消除歧义性。只有当两个编码都缩短时，汉字的输入才能够变快。早期的输入法常常只注重第一部分而忽略第二部分，例如双拼输入法和五笔输入法。</p><p>每一个拼音对应多个汉字，把一个拼音串对应的汉字由左向右连起来，就是一张有向图，如下图所示，y1,y2,y3…是输入的拼音串，W11,W12,W13是第一个音的候选汉字（后面的文字描述用W1代替），以此类推。从第一个字到最后一个字可以组成很多句子，每个句子对应图中的一条路径。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/5.jpg" alt=""></p><p>拼音输入法就是要根据上下文在给定的拼音条件下找到最优的句子，即求</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/6.1.png" alt=""></p><p>（Arg是argument的缩写，Arg Max为获得最大值的信息串）<br>化简这个概率需要用到<strong>隐含马尔可夫模型</strong>（见2.2介绍），我们把拼音串看成能观察到的“显状态”，候选汉字看成“隐状态”，然后求在这个“显状态”下的“隐状态”概率。带入下文中的隐含马尔可夫模型公式（2.3），式（2.1）化简为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/7.png" alt=""></p><p>化简连乘， 需要将等式两边取对数得</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/8.png" alt=""></p><p>乘法变成了加法。我们定义两个词之间的距离</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/9.png" alt=""></p><p>这样，寻找最大概率问题变成了寻找最短路径问题。</p><h5 id="隐含马尔可夫模型"><a href="#隐含马尔可夫模型" class="headerlink" title="隐含马尔可夫模型"></a>隐含马尔可夫模型</h5><p>上文介绍过马尔可夫假设（研究随机过程中的一个假设），即在随机状态序列中，假设其中的一个状态只于前一个状态有关。如天气预报，假设今天的天气只与昨天有关，这样就能得到近似解：<br><img src="http://p4rlzrioq.bkt.clouddn.com/9-1.png" alt=""><br><img src="http://p4rlzrioq.bkt.clouddn.com/10.png" alt=""></p><p>马尔可夫链</p><p>符合这个假设的随机过程称为马尔可夫过程，也叫马尔可夫链。隐含马尔可夫模型是马尔可夫链的一个扩展：任意时刻t的状态St是不可见的，但在每个时刻会输出Ot， Ot仅和St相关，这叫独立输出假设，数学公式如下：<br><img src="http://p4rlzrioq.bkt.clouddn.com/11.png" alt=""></p><p>P(Ot|St)我们可以通过观察得到。<br><img src="http://p4rlzrioq.bkt.clouddn.com/12.png" alt=""></p><p>隐马尔可夫模型</p><p>解决问题通常是通过已知求未知，我们要通过观察到$o_t$求出$s_t$的概率，即求<br><img src="http://p4rlzrioq.bkt.clouddn.com/12-1.png" alt=""></p><p>由条件概率公式可得：<br><img src="http://p4rlzrioq.bkt.clouddn.com/13.png" alt=""></p><p>因为观察到的状态O一旦产生就不会变了，所以它是一个可忽略的常数，上式可以化简为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/14.png" alt=""></p><p>因为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/16.png" alt=""></p><p>式(2.2)可以化简为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/17.png" alt=""></p><h4 id="信息论：信息的度量和作用"><a href="#信息论：信息的度量和作用" class="headerlink" title="信息论：信息的度量和作用"></a>信息论：信息的度量和作用</h4><h5 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h5><p>香农在他的论文“通信的数学原理”[想到牛顿的“自然哲学与数学原理”]，提出了信息熵（shang），把信息和数字联系起来，解决了信息的度量，并量化出信息的作用。</p><p>一条信息的信息量和它的不确定性正相关，信息熵约等于不确定性的多少。香农给出的信息熵公式为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/20.png" alt=""></p><p>P(x)为x的概率分布。</p><p>信息熵的公式为什么取负数？因为概率小于1，小数求得的对数是负数，给整个公式加上负号，最终的结果为正。</p><p>下面举例说明信息熵公式为什么会用到log和概率。</p><blockquote><p>猜中世界杯冠军需要多少次？<br>足球世界杯共32个球队，给他们编号1-32号，第一次猜冠军是否在1-16号之中，如果对了就会接着猜是否在1-8号，如果错了就知道冠军在9-16号，第三次猜是否在9-12号，这样只需要5次就能猜中，log32 = 5。这里采用的是折半查找，所以取对数。</p></blockquote><blockquote><p>但实际情况不需要猜5次，因为球队有强弱，可以先把夺冠热门分一组，剩下的分一组，问冠军是否在热门组中，再继续这个过程，按照夺冠概率对剩下的球队分组。引入概率就会让查找数更少，也就是不确定性更小，信息熵更小。可以计算，当每支球队夺冠概率相等时（1/32），信息熵的结果为5。</p></blockquote><h5 id="条件墒："><a href="#条件墒：" class="headerlink" title="条件墒："></a>条件墒：</h5><p>假定X和Y是两个随机变量，X是我们要了解的，已知X的随机分布P(X)，于是X的熵为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/21.png" alt=""></p><p>假定我们还知道Y的一些情况，包括它和X一起出现的概率，即联合概率分布，以及在Y取不同值前提下X的概率分布，即条件概率分布，于是在Y条件下X的条件熵为：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/22.png" alt=""></p><p>可证明H(X|Y) &lt;H(X), 即引入相关信息后，不确定性下降了。</p><h5 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h5><p>信息之间的相关性如果度量呢？ 香农提出了用互信息度量两个随机事件的相关性。例如，“好闷热”和“要下雨了”的互信息很高。<br>X与Y的互信息公式如下：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/23.png" alt=""></p><p>经过演算，可得到</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/24.png" alt=""></p><p>只要有足够的语料库，P(x,y), P(x) 和P(y)是很容易计算的。</p><p>机器翻译中最难的两个问题之一是二义性，如Bush 既可以是总统布什，也可以是灌木丛，Kerry既可以是国务卿克里，也可以是小母牛。如何正确的翻译？一种思路是通过语法辨别，但效果不好； 另一种思路是用互信息，从大量文本中找出和总统布什一起出现的词语，如总统、美国、国会等，再用同样的方法找出和灌木丛一起出现的词，如土壤、植物等，有了这两组词，在翻译Bush时，看看上下文中哪类词更多就可以了。</p><h5 id="相对熵-交叉熵"><a href="#相对熵-交叉熵" class="headerlink" title="相对熵/交叉熵"></a>相对熵/交叉熵</h5><p>相对熵（KL Divergence），衡量两个取值为正的函数的相似性:</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/25.png" alt=""></p><p>结论：</p><ol><li>两个完全相等的函数，相对熵为零；</li><li>相对熵越大，两个函数差异越大。</li><li>对于概率分布函数，或者概率密度函数，相对熵可以度量两个随机分布的差异性。</li></ol><p>在自然语言处理中，常用相对熵计算两个常用词在不同文本中的概率分布，看他们是否同义；或者根据两篇文章中不同词的分布，衡量它们的内容是否相等。利用相对熵，可以得到信息检索中最重要的概念：词频率-逆向文档频率（TF-IDF），在后面的搜索章节会对它详细介绍。</p><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><h5 id="获取网页：网络爬虫"><a href="#获取网页：网络爬虫" class="headerlink" title="获取网页：网络爬虫"></a>获取网页：网络爬虫</h5><p>把整个互联网看作一张大图，每个网页就是图中的一个节点，超链接是连接节点的弧。通过网络爬虫，用图的遍历算法，就能自动地访问到每个网页并把它们存起来。</p><p>网络爬虫是这样工作：假定从一家门户网站的首页出发，先下载这个网页，再通过这个网页分析出里面包含的所有超链接，接下来访问并下载这些超链接指向的网页。让计算机不同地做下去，就能下载整个互联网。 还需要用一个记事本（哈希表）记录下载了哪些网页避免重复下载。</p><p>工程实现问题：</p><ol><li>遍历算法采用广度优先还是深度优先？<br>搜索引擎要做到在有限的时间内，最多地爬下最重要的网页。显然各个网站最重要的是它的首页，那么就应该先下载所有网站的首页。如果把爬虫再扩大一点，就要继续下载首页直接链接的网页，因为这些网页是网站设计者自己认为相当重要的网页。在这个前提下，似乎应该采用广度优先。</li></ol><p>但是还要考虑网络通信的“握手”问题。网络爬虫每次访问网站服务器时，都要通过“握手”建立连接（TCP协议），如果采用广度优先，每个网站先轮流下载所有首页，再回过头来下载第二级网页，这样就要频繁的访问网站，增加“握手”耗时。</p><p>实际的网络爬虫是由成百上千台服务器组成的分布式系统，由调度系统决定网页下载的顺序，对于某个网站，一般是由特定的一台或几台服务器专门下载，这些服务器先下载完一个网站再进入下一个网站，这样可以减少握手次数（深度优先）。具体到每个网站，采用广度优先，先下载首页，再下载首页直接链接的网页。</p><ol><li><p>页面分析和超链接（URL）提取<br>早期的网页都是直接用HTML书写，URL以文本的形式放在网页中，前后有明显标识，很容易提取出来。但现在很多网页都是用脚本语言（如JavaScript）生成，URL不是直接可见的文本，所以网络爬虫要模拟浏览器运行网页后才能得到隐含的URL，但很多网页的脚本写的不规范，很难解析，这就导致这样的网页无法被搜索引擎收录。</p></li><li><p>维护超链接哈希表<br>在一台服务器上建立和维护一张哈希表并不是难事，但如果同时有成千上万台服务器一起下载网页，维护一张统一的哈希表就会遇到很多问题：</p></li></ol><p>首先，这张哈希表会大到存不下来；其次，每台服务器下载前和下载后都要访问哈希表，于是哈希表服务器的通信就成了整个爬虫系统的瓶颈。解决办法是：明确分工，将某个区间的URL分给特定的几台服务器，避免所有服务器对同一个URL做判断；批量询问哈希表，减少通信次数，每次更新一大批哈希表的内容。</p><h5 id="网页检索：布尔代数"><a href="#网页检索：布尔代数" class="headerlink" title="网页检索：布尔代数"></a>网页检索：布尔代数</h5><p>最简单的索引结构是用一个很长的二进制数表示一个关键字是否在每个网页中，有多少个网页就有多少位数，每一位对应一个网页，1代表相应的网页有这个关键字，0代表没有。比如关键字“原子能”对应的二进制数是0100 1000 1100 0001…表示（从左到右）第二、第五、第九、第十、第十六个网页包含这个关键字。假定关键字“应用”对应的二进制数是0010 1001 1000 0001…，那么要找到同时包含“原子能”和“应用”的网页时，只需要将这两个二进制数进行布尔AND运算，结果是0000 1000 0000 0001…表示第五和第十六个网页满足要求。 这个二进制数非常长，但是计算机做布尔运算非常快，现在最便宜的微机，在一个指令周期进行32位布尔运算，一秒钟十亿次以上。</p><p>为了保证对任何搜索都能提供相关网页，主要的搜索引擎都是对所有词进行索引，假如互联网上有100亿个有意义的网页，词汇表大小是30万，那么这个索引至少是100亿x30万=3000万亿。考虑到大多数的词只出现在一部分文本中，压缩比是100：1，也是30万亿的量级。为了网页排名方便，索引中还要存其他附加信息，如每个词出现的位置，次数等等。因此整个索引就变得非常大，需要通过分布式存储到不同服务器上（根据网页编号划分为很多小块，根据网页重要性建立重要索引和非重要索引）。</p><h5 id="度量网页和查询的相关性：TF-IDF"><a href="#度量网页和查询的相关性：TF-IDF" class="headerlink" title="度量网页和查询的相关性：TF-IDF"></a>度量网页和查询的相关性：TF-IDF</h5><p>我们以查找包含“原子能的应用”网页举例，“原子能的应用”可以分成三个关键词：原子能、的、应用。凭直觉，我们认为包含这三个关键词较多的网页，比包含它们较少的网页相关。但这并不可取，因为这样的话，内容长的网页比内容短的网页占便宜，所以要根据网页长度对关键词的次数进行归一化，用关键词的次数，除以网页的总字数，这个商叫做“关键词的频率”或“单文本频率”（TF：Term Frequency）。比如，某个网页上有1000词，其中“原子能”“的”“应用”分别出现了2次、35次、5次，那么它们的词频就是0.002、0.035、0.005，将这三个数相加就是相应网页和查询“原子能的应用”的单文本频率。所以，度量网页和查询的相关性，一个简单的方法就是直接使用各个关键词在网页中出现的总频率。</p><p>但是这也有不准确的地方，例如上面的例子中，“的”占了总词频的80%以上，但是它对确定网页的主题几乎没什么用，我们叫这样的词为停止词（stop word），类似的还有“是”“和”等。 另外“应用”是很普通的词，而“原子能”是专业词，后者在相关性排名中比前者重要。因此需要给每个词给一个权重，权重的设定满足两个条件：</p><ol><li>一个词预测主题的能力越强，权重就越大；</li><li>停止词权重为零。</li></ol><p>在信息检索中，使用最多的是“逆文本频率指数”（IDF：Inverse Document Frequency），公式为</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/30.png" alt=""></p><p>（D是全部网页数，Dw为关键词w出现的网页个数）。最终确定查询相关性，是利用TF和IDF的加权求和。 （IDF其实是在特定条件下关键词概率分布的交叉熵）</p><h5 id="搜索结果页排序：Page-Rank算法"><a href="#搜索结果页排序：Page-Rank算法" class="headerlink" title="搜索结果页排序：Page Rank算法"></a>搜索结果页排序：Page Rank算法</h5><p>这是拉里·佩奇和谢尔盖·布林发明的计算网页自身质量的数学模型，google凭借该算法，使搜索的相关性有了质的飞跃，圆满解决了以往搜索页中排序不好的问题。该算法的核心思想为：<strong>如果一个网页被很多其他网页所链接，说明它收到普遍的承认和信赖，那么它的排名就高</strong>。当然，在具体应用中还要加上权重，给排名高的网页链接更高的权重。这里有一个怪圈，计算搜索结果网页排名过程中需要用到网页本身的排名，这不是“先有鸡还是先有蛋的问题”吗？ 谢尔盖·布林解决了这个问题，他把这个问题变成了一个二维矩阵问题，先假定所有网页排名相同（1/N），在根据这个初始值不断迭代排名，最后能收敛到真实排名。</p><h5 id="新闻分类：余弦定理"><a href="#新闻分类：余弦定理" class="headerlink" title="新闻分类：余弦定理"></a>新闻分类：余弦定理</h5><p>google有新闻频道，里面的内容是由计算机聚合、整理并分类各网站内容。以前门户网站的内容是由编辑在读懂之后，再根据主题分类。但是计算机根本读不懂新闻，它只会计算，所以要让计算机分类新闻，首先就要把文字变成可计算的数字，再设计一个算法来计算任意两篇新闻的相似性。</p><p>计算一篇新闻中所有实词的TF-IDF值，再把这些值按照对应的实词在词汇表的位置依次排列，就得到一个向量。例如词汇表中有64000个词，其编号和词如左下表所示，在某一篇新闻中，这64000个词的TF-IDF值如右下表所示，这64000个数就组成了一个64000维的向量，我们就用这个向量代表这篇新闻，成为这篇新闻的特征向量。每篇新闻都有一个特征向量，向量中的每个数代表对应的词对这篇新闻主题的贡献。</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/31.png" alt=""></p><p>同一类的新闻，一定某些主题词用的较多，两篇相似的新闻，它们的特征向量一定在某几个纬度的值比较大。如果两个向量的方向一致，就说明新闻的用词比例基本一致，我们采用余弦定理计算两个向量间的夹角：</p><p><img src="http://p4rlzrioq.bkt.clouddn.com/32.png" alt=""></p><p>新闻分类算法分为有目标和无目标：第一种是已知一些新闻类别的特征向量，拿它分别和所有待分类的新闻计算余弦相似性，并分到对应的类别中，这些已知的新闻类别特征向量既可以手工建立，也可以自动建立； 第二种是没有分好类的特征向量做参考，它采用自底向上的聚类方法，计算所有新闻两两之间的余弦相似性，把相似性大于一个阈值的新闻分作一个小类，再比较各小类之间的余弦相似性，就这样不断待在聚合，一直到某一类因为太大而导致里面的新闻相似性很小时停止。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="http://frankblog.site/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="统计算法" scheme="http://frankblog.site/tags/%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%B3%95/"/>
    
      <category term="NLP" scheme="http://frankblog.site/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>优雅高效地使用windows</title>
    <link href="http://frankblog.site/2018/02/26/%E4%BC%98%E9%9B%85%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8windows/"/>
    <id>http://frankblog.site/2018/02/26/优雅高效地使用windows/</id>
    <published>2018-02-26T13:36:19.428Z</published>
    <updated>2018-05-30T08:35:58.092Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="http://p4rlzrioq.bkt.clouddn.com/v2-bd237c6411061932e99836393728ec02_r.jpg" alt=""><br><a id="more"></a></p><blockquote class="blockquote-center"><font size="5">工欲善其事，必先利其器！</font></blockquote><hr><h1 id="【日常工具】"><a href="#【日常工具】" class="headerlink" title="【日常工具】"></a>【<strong>日常工具</strong>】</h1><h2 id="文件搜索"><a href="#文件搜索" class="headerlink" title="文件搜索"></a>文件搜索</h2><ul><li><a href="http://www.listary.com/" target="_blank" rel="noopener">Listary</a>：Windows文件浏览增强工具，double-ctrl调用，并且可以在(Xftp，clover 等等)窗口中使用，极其方便！</li><li><a href="https://github.com/Wox-launcher/Wox" target="_blank" rel="noopener">Wox</a>：免费开源的效率启动器,不仅可以搜索文件还可以浏览网页，Alt+Space调用，以及定制的插件(<a href="http://www.getwox.com/plugin" target="_blank" rel="noopener">wox-plugin</a>)，堪比MAC上的 <strong>Alfred</strong></li></ul><h2 id="视频播放"><a href="#视频播放" class="headerlink" title="视频播放"></a>视频播放</h2><ul><li><a href="https://potplayer.en.softonic.com/" target="_blank" rel="noopener">Potplayer</a>：拥有强大的内置解码器，不用额外针对某类视频去下载了</li></ul><h2 id="下载神器"><a href="#下载神器" class="headerlink" title="下载神器"></a>下载神器</h2><ul><li><a href="https://www.internetdownloadmanager.com/" target="_blank" rel="noopener">IDM</a>(cracked version)： 全宇宙最快的下载器!唯一缺陷是不支持P2P，想下载磁力或者BT可以先用百度云网盘的离线功能，再通过油猴脚本抓取链接进行下载。</li><li><a href="http://www.eagleget.com/cn/" target="_blank" rel="noopener">EagleGet</a>：下载后自动安装Chrome扩展探测视频，缺陷也是不支持BT/磁力链接，方法同上</li><li><a href="http://download.flvcd.com/" target="_blank" rel="noopener">硕鼠</a>：主要是下载网站的视频，不过现在不支持像腾讯视频之类的大网站了</li></ul><h2 id="PDF阅读"><a href="#PDF阅读" class="headerlink" title="PDF阅读"></a>PDF阅读</h2><ul><li><a href="https://www.foxitsoftware.cn/" target="_blank" rel="noopener">福昕阅读器</a>：功能算比较齐全了(会占用端口4000)</li><li><a href="http://www.abbyy.cn/finereader/" target="_blank" rel="noopener">ABBYY_FineReader</a>：PDF转WORD</li></ul><h2 id="写作工具"><a href="#写作工具" class="headerlink" title="写作工具"></a>写作工具</h2><ul><li><a href="https://zh.snipaste.com/" target="_blank" rel="noopener">snipaste</a>：<br>开源、免费的国产截图神器。比QQ截图工具清晰很多。</li><li><a href="https://ivarptr.github.io/yu-writer.site/" target="_blank" rel="noopener">Yu writer</a>：windows上好用的markdown工具！</li><li><a href="https://sourceforge.net/projects/ditto-cp/files/" target="_blank" rel="noopener">Ditto</a>：剪切板工具，保存所有复制过的文字和图片，用ctrl+`调用</li><li><a href="https://www.pasteasy.com/" target="_blank" rel="noopener">Pasteasy</a>：全平台跨设备复制粘贴</li></ul><h2 id="素材库"><a href="#素材库" class="headerlink" title="素材库"></a>素材库</h2><ul><li><a href="http://www.officeplus.cn/Template/Home.shtml" target="_blank" rel="noopener">OfficePLUS</a>：微软Office官方在线模板网站！</li><li><a href="http://www.iconfont.cn/" target="_blank" rel="noopener">Iconfont</a>：阿里巴巴矢量图标库</li><li><a href="https://pixabay.com/" target="_blank" rel="noopener">Free Images - Pixabay</a></li><li><a href="http://icons8.com/" target="_blank" rel="noopener">icons8</a>：icon素材库</li><li><a href="https://images.nasa.gov/" target="_blank" rel="noopener">NASA IMAGE</a>：NASA素材库</li><li><a href="https://pixabay.com/zh/" target="_blank" rel="noopener">pixabay</a>:高清免费图片素材库</li></ul><h2 id="PPT制作"><a href="#PPT制作" class="headerlink" title="PPT制作"></a>PPT制作</h2><ul><li><a href="http://www.nordritools.com/" target="_blank" rel="noopener">Nordri Tools</a>：超级好用的ppt插件</li><li><a href="http://www.cr173.com/soft/285461.html" target="_blank" rel="noopener">Photozoom pro</a>：利用插值算法提高图片分辨率</li><li><a href="http://ppt.baidu.com/" target="_blank" rel="noopener">PPT遥控器</a>：代替遥控笔</li><li><a href="http://www.pptminimizer.com/chn/index.php" target="_blank" rel="noopener">FILEminimizer</a>：ppt压缩神器</li><li><a href="www.screentogif.com">Screen to Gif</a>: Gif制作软件</li><li><a href="https://wordart.com/" target="_blank" rel="noopener">Tagul</a>：文字云生成器</li></ul><h2 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h2><ul><li><a href="https://www.xmind.cn/" target="_blank" rel="noopener">Xmind</a>：付费，全平台，模板多，支持鱼骨图、二维图、树形图等格式，可以与Evernote同步</li><li><a href="https://mubu.com/" target="_blank" rel="noopener">幕布</a>：笔记一键生成思维导图</li></ul><h2 id="视频录制"><a href="#视频录制" class="headerlink" title="视频录制"></a>视频录制</h2><ul><li><a href="https://obsproject.com/download" target="_blank" rel="noopener">OBS Studio</a>：功能齐全的视频录制工具，直播必备</li><li><a href="https://www.adobe.com/cn/products/premiere/free-trial-download.html" target="_blank" rel="noopener">Adobe Premiere Pro CC</a>：视频剪辑工具</li></ul><h2 id="文件整理"><a href="#文件整理" class="headerlink" title="文件整理"></a>文件整理</h2><ul><li><a href="https://www.softwareok.com/?seite=Freeware/Q-Dir" target="_blank" rel="noopener">Q-dir</a>：需要在文件夹之间移动文件的时候，这个整理神器就能派上用场了！</li><li><a href="http://cn.ejie.me/" target="_blank" rel="noopener">Clover 3</a>：为资源管理器添加多标签页功能，可以将常用文件夹添加为书签</li><li><a href="https://www.goodsync.com/" target="_blank" rel="noopener">Goodsyne</a>：强大的数据同步工具</li><li><a href="https://www.bandisoft.com/bandizip/" target="_blank" rel="noopener">bandzip</a>：win10下好用的压缩软件</li></ul><h2 id="快速启动"><a href="#快速启动" class="headerlink" title="快速启动"></a>快速启动</h2><ul><li><a href="http://www.truelaunchbar.com/" target="_blank" rel="noopener">TrueLaunchBar</a>：对快速启动项进行分组；允许你把任何文件夹组织成菜单的形式；实现剪切板管理、性能监视等功能</li><li><a href="http://www.yingdev.com/projects/wgestures" target="_blank" rel="noopener">Wgestures</a>：全局鼠标手势！</li></ul><h1 id="【系统开发与优化工具】"><a href="#【系统开发与优化工具】" class="headerlink" title="【系统开发与优化工具】"></a>【<strong>系统开发与优化工具</strong>】</h1><h2 id="桌面优化"><a href="#桌面优化" class="headerlink" title="桌面优化"></a>桌面优化</h2><ul><li><a href="https://www.stardock.com/products/fences/download" target="_blank" rel="noopener">Fences</a>：付费,桌面文件分类整理软件</li><li><a href="http://store.steampowered.com/app/431960/Wallpaper_Engine/" target="_blank" rel="noopener">Wallpapaer</a>：动态壁纸软件，装逼神器！</li></ul><h2 id="屏保"><a href="#屏保" class="headerlink" title="屏保"></a>屏保</h2><ul><li><a href="https://fliqlo.en.softonic.com/" target="_blank" rel="noopener">Fliqlo</a>：数字时钟的屏幕保护，逼格满满</li><li><a href="https://justgetflux.com/" target="_blank" rel="noopener">Flux</a>： 视力保护，通过根据时间调节屏幕颜色，减少蓝光对视力的影响</li></ul><h2 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h2><ul><li>PowerTool：查看系统进程等信息，安全修复！</li><li><a href="https://www.chuyu.me/zh-Hans/" target="_blank" rel="noopener">Dism++</a>：简洁的系统管理软件，集成了很多小工具，还可以系统备份</li><li><a href="https://www.ccleaner.com/ccleaner" target="_blank" rel="noopener">Ccleaner</a>：系统清理工具</li></ul><h2 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h2><ul><li><a href="http://download.cnet.com/Bulk-Rename-Utility-64-Bit/3000-2248_4-75211571.html" target="_blank" rel="noopener">Bulk Rename Utility</a>：批量重命名工具 </li><li><a href="https://sourceforge.net/projects/rem-empty-dir/files/" target="_blank" rel="noopener">remove empty directories</a>：删除空文件夹，强迫症的福音</li></ul><h2 id="系统安装"><a href="#系统安装" class="headerlink" title="系统安装"></a>系统安装</h2><ul><li><a href="https://mirrors.tuna.tsinghua.edu.cn/" target="_blank" rel="noopener">清华大学开源软件镜像站</a>：可以下载到Linux镜像文件以及python第三方库文件等，速度很快！</li><li><a href="http://cn.ultraiso.net/xiazai.html" target="_blank" rel="noopener">Ultraiso</a>：制作启动盘</li></ul><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><ul><li><a href="https://www.telerik.com/fiddler" target="_blank" rel="noopener">Fiddler</a>：抓包工具</li></ul><h2 id="文本编辑器"><a href="#文本编辑器" class="headerlink" title="文本编辑器"></a>文本编辑器</h2><ul><li><a href="https://www.sublimetext.com/3" target="_blank" rel="noopener">Sublime Text3</a>：文本神器</li><li><a href="https://atom.io/" target="_blank" rel="noopener">Atom</a>：中文友好，渲染插件多</li></ul><h2 id="IDE"><a href="#IDE" class="headerlink" title="IDE"></a>IDE</h2><ul><li><a href="https://www.jetbrains.com/pycharm/download/" target="_blank" rel="noopener">Pycharm</a>：社区版免费，Python开发必备</li><li><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda</a>：集成了python科学计算的第三方库，内置spyder和jupyter notebook</li><li><a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener">IntelliJ IDEA</a>：前端必备IDE</li><li><a href="http://cmder.net/" target="_blank" rel="noopener">Cmder</a>：monokai配色主题，完美代替原生cmd</li></ul><h1 id="【chrome插件】"><a href="#【chrome插件】" class="headerlink" title="【chrome插件】"></a>【<strong>chrome插件</strong>】</h1><h2 id="开发必备"><a href="#开发必备" class="headerlink" title="开发必备"></a>开发必备</h2><ul><li>Vimium(键盘浏览插件)</li><li>JSONView(json数据进行转码和格式化)</li><li>Proxy SwitchyOmega (代理)</li><li>Qiniu upload files (七牛图床插件)</li><li>Markdown Here (转化为markdown格式）</li><li>The QR Code Extension (二维码生成器)</li></ul><h2 id="日常管理"><a href="#日常管理" class="headerlink" title="日常管理"></a>日常管理</h2><ul><li>Extensity (扩展管理工具)</li><li>LastPass (密码管理器)</li></ul><h2 id="浏览优化"><a href="#浏览优化" class="headerlink" title="浏览优化"></a>浏览优化</h2><ul><li>书签侧边栏</li><li>Imtranslator（翻译）</li><li>Imagus (悬停放大图片)</li><li>OneTab (内存优化神器)</li><li>Better History (查看历史记录)</li><li>Sexy Undo Close Tab (恢复关闭网页)</li><li>Infinity (方便的新标签页定制)</li><li>CrxMouse Chrome Gestures (鼠标手势、超级拖拽)</li><li>Tampermonkey (油猴：脚本管理平台，神器！！)</li><li>IE Tab (打开用IE内核支持的网页，常用于银行支付环境)</li><li>Listen 1 (集成各大平台的音乐，再也不用为音乐版权的问题头疼了)</li></ul><h2 id="下载-amp-收藏"><a href="#下载-amp-收藏" class="headerlink" title="下载&amp;收藏"></a>下载&amp;收藏</h2><ul><li>印象笔记裁剪</li><li>网页截图：注释&amp;录屏</li><li>RSS Subscription Extension </li><li>Eagleget Free Download</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4rlzrioq.bkt.clouddn.com/v2-bd237c6411061932e99836393728ec02_r.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Software" scheme="http://frankblog.site/categories/Software/"/>
    
    
      <category term="Windows" scheme="http://frankblog.site/tags/Windows/"/>
    
      <category term="Chrome" scheme="http://frankblog.site/tags/Chrome/"/>
    
  </entry>
  
  <entry>
    <title>Sublime Text3 快捷键</title>
    <link href="http://frankblog.site/2018/02/22/sublime%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://frankblog.site/2018/02/22/sublime快捷键/</id>
    <published>2018-02-22T07:11:51.808Z</published>
    <updated>2018-02-27T04:41:18.415Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="https://i.loli.net/2018/02/22/5a8e9d17ab4cd.jpg" alt=""></p><a id="more"></a><h1 id="选择类"><a href="#选择类" class="headerlink" title="选择类"></a>选择类</h1><p>Ctrl + D  选择单文本</p><p>Alt  + F3 选中文件所有相同文本 </p><p>Ctrl + L  选中整行</p><p>Ctrl + shift + M 选中括号中文本（与搜狗有热键冲突） </p><p>Ctrl + M  光标移动结束或开始位置</p><p>Ctrl + Enter  在下一行插入新行</p><p>Ctrl + Shift + Enter 在上一行插入新行</p><p>Ctrl + Shift + [  选中，折叠代码</p><p>Ctrl + Shift + ]  选中，展开代码</p><p>Ctrl + K + 0  展开所有折叠代码</p><p>Ctrl + ←/→  快速移动光标</p><p>shift + ↑/↓  向上/向下选中多行</p><p>Shift + ←/→  向左/向右选中文本</p><p>Ctrl + Shift + ←/→  向左/向右快速选择文本</p><h1 id="编辑类"><a href="#编辑类" class="headerlink" title="编辑类"></a>编辑类</h1><p>Ctrl + J  合并多行代码为一行</p><p>Ctrl + Shift + D  复制整行，插入到下一行</p><p>Tab  向右缩进 &amp; Shift + Tab  向左缩进</p><p>Ctrl + K + K  从光标处开始删除代码至行尾。</p><p>Ctrl + Shift + K  删除整行。</p><p>Ctrl + /  注释单行。</p><p>Ctrl + Shift + /  注释多行。</p><p>Ctrl + K + U/L  转换大/小写。</p><p>Ctrl + Z  撤销</p><p>Ctrl + Y  恢复撤销</p><p>Ctrl + F2  设置书签</p><p>Ctrl + T  左右字母互换。</p><p>F6  单词检测拼写</p><h1 id="搜索类"><a href="#搜索类" class="headerlink" title="搜索类"></a>搜索类</h1><p>Ctrl + F  文件内搜索</p><p>Ctrl + shift + F 文件夹内搜索</p><p>Ctrl + P  按类别搜索。举个栗子：1、输入当前项目中的文件名；快速搜索文件，2、输入@和关键字，查找文件中函数名；3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。</p><p>Ctrl + G  数字定位搜索</p><p>Ctrl + R  函数定位搜索</p><p>Ctrl + ： 变量、属性名定位搜索</p><p>Ctrl + Shift + P  打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。</p><h1 id="显示类"><a href="#显示类" class="headerlink" title="显示类"></a>显示类</h1><p>Ctrl + Tab  按浏览顺序切换窗口</p><p>Ctrl + PageDown  向左切换当前窗口的标签页</p><p>Ctrl + PageUp  向右切换当前窗口的标签页。</p><p>Alt + Shift + “1/2/3”  分屏</p><p>Ctrl + K + B  开启/关闭侧边栏。</p><p>F11  全屏模式</p><p>Shift + F11  免打扰模式</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/02/22/5a8e9d17ab4cd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="编辑器" scheme="http://frankblog.site/categories/%E7%BC%96%E8%BE%91%E5%99%A8/"/>
    
    
      <category term="Sublime" scheme="http://frankblog.site/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://frankblog.site/2018/02/16/hello-world/"/>
    <id>http://frankblog.site/2018/02/16/hello-world/</id>
    <published>2018-02-16T15:20:02.641Z</published>
    <updated>2018-02-25T13:11:14.319Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><img src="https://i.loli.net/2018/02/22/5a8e9ef8c09a8.jpg" alt=""></p><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/02/22/5a8e9ef8c09a8.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="网页配置" scheme="http://frankblog.site/categories/%E7%BD%91%E9%A1%B5%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="HEXO" scheme="http://frankblog.site/tags/HEXO/"/>
    
  </entry>
  
</feed>
