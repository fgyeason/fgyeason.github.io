<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习之逻辑回归]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。在某些文献中，logistic 回归又被称作 logit 回归，maximum-entropy classification（MaxEnt，最大熵分类），或 log-linear classifier（对数线性分类器）。 决策边界 线性决策边界 非线性决策边界 手推LR 过拟合问题正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。 直观来看，如果我们想解决这个例子中的过拟合问题，最好能将的影响消除，也就是让。假设我们对进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大惩罚项，例如： 这样在最小化Cost函数的时候，。正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为： lambda是正则项系数： 1.如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象； 2.如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。 正则化后的梯度下降算法θ的更新变为： 正则化后的线性回归的Normal Equation的公式为： scikit-learn 逻辑回归类带L1与L2正则的逻辑回归损失函数scikit-learn 中 logistic 回归在 LogisticRegression中的”sklearn.linear_model.LogisticRegression”类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。 作为优化问题，带 L2 正则的二分类 logistic 回归要最小化以下代价函数（cost function）： $$\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) $$ 类似地，带 L1 正则的 logistic 回归解决的是如下优化问题： $$\underset{w, c}{min\,} |w|1 + C \sum{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)$$ 在 LogisticRegression 中的sklearn.linear_model.LogisticRegression类中实现了这些优化算法: “liblinear”， “newton-cg”， “lbfgs”， “sag” 和 “saga”。 优化方法参数solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。 saga：线性收敛的随机优化算法的的变重。 Case Solver L1正则 “liblinear” or “saga” 多项式损失（multinomial loss） “lbfgs”, “sag”, “saga” or “newton-cg” 大数据集（n_samples） “sag” or “saga” “saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear” 总结1.LR中损失函数的意义是什么？在LR中，最大似然函数与最小化对数损失函数等价 2. LR与线性回归的联系和区别逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 3.LR与最大熵模型逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然； 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 4.LR与svm不同点: 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。 逻辑回归对概率建模，svm对分类超平面建模 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响 逻辑回归是统计方法，svm是几何方法 5.LR与朴素贝叶斯 相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)形式跟逻辑回归是一样的。 不同的地方在于，逻辑回归为判别模型求的是p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。 6. 多分类-softmax如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。 7.LR模型在工业界的应用常见应用场景 预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等） 分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等） LR适用上述场景的原因 LR模型自身的特点具备了应用广泛性 模型易用：LR模型建模思路清晰，容易理解与掌握； 概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上； 强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性； 简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline；]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib基本操作]]></title>
    <url>%2F2018%2F06%2F03%2FMatplotlib%20%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一张图胜过一千句话 基础操作1、设置坐标轴12345678910111213141516171819202122232425262728293031import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2#使用`plt.figure`定义一个图像窗口. 使用`plt.plot`画(`x` ,`y2`)曲线. 使用`plt.plot`画(`x` ,`y1`)曲线，曲线的颜色属性(`color`)为红色;曲线的宽度(`linewidth`)为1.0；曲线的类型(`linestyle`)为虚线。plt.figure()plt.plot(x, y2)plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)#使用`plt.xlim`设置x坐标轴范围：(-1, 2)； 使用`plt.ylim`设置y坐标轴范围：(-2, 3)； 使用`plt.xlabel`设置x坐标轴名称：’I am x’； 使用`plt.ylabel`设置y坐标轴名称：’I am y’；plt.xlim((-1, 2))plt.ylim((-2, 3))plt.xlabel(&apos;I am x&apos;)plt.ylabel(&apos;I am y&apos;)#使用`np.linspace`定义范围以及个数：范围是(-1,2);个数是5\. 使用`print`打印出新定义的范围. 使用`plt.xticks`设置x轴刻度：范围是(-1,2);个数是5.new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)#使用`plt.yticks`设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用`plt.show`显示图像.plt.yticks([-2, -1.8, -1, 1.22, 3],[r&apos;$really\ bad$&apos;, r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;, r&apos;$really\ good$&apos;])plt.show() 2、调整坐标轴12345678910111213141516171819202122232425262728293031323334353637383940414243x = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()plt.plot(x, y2)plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)plt.xlim((-1, 2))plt.ylim((-2, 3))new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3],[&apos;$really\ bad$&apos;, &apos;$bad$&apos;, &apos;$normal$&apos;, &apos;$good$&apos;, &apos;$really\ good$&apos;])ax = plt.gca()ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)#使用`.xaxis.set_ticks_position`设置x坐标刻度数字或名称的位置：`bottom`.（所有位置：`top`，`bottom`，`both`，`default`，`none`）ax.xaxis.set_ticks_position(&apos;bottom&apos;)#使用`.spines`设置边框：x轴；使用`.set_position`设置边框位置：y=0的位置；（位置所有属性：`outward`，`axes`，`data`）ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))#使用`.yaxis.set_ticks_position`设置y坐标刻度数字或名称的位置：`left`.（所有位置：`left`，`right`，`both`，`default`，`none`）ax.yaxis.set_ticks_position(&apos;left&apos;)#使用`.spines`设置边框：y轴；使用`.set_position`设置边框位置：x=0的位置；（位置所有属性：`outward`，`axes`，`data`） 使用`plt.show`显示图像。ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))# set line sylesl1, = plt.plot(x, y1, label=&apos;linear line&apos;)l2, = plt.plot(x, y2, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;, label=&apos;square line&apos;)#参数 `loc=&apos;upper right&apos;` 表示图例将添加在图中的右上角.plt.legend(loc=&apos;upper right&apos;)plt.show() 3、辅助线和标识123456789101112131415161718192021222324252627282930313233import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1#挪动坐标系ax = plt.gca()ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.xaxis.set_ticks_position(&apos;bottom&apos;)ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))ax.yaxis.set_ticks_position(&apos;left&apos;)ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0))#辅助线plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], &apos;k--&apos;, linewidth=2.5)# set dot stylesplt.scatter([x0, ], [y0, ], s=50, color=&apos;b&apos;)#标注，其中参数xycoords=&apos;data&apos; 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords=&apos;offset points&apos; 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置.plt.annotate(r&apos;$2x+1=%s$&apos; % y0, xy=(x0, y0), xycoords=&apos;data&apos;, xytext=(+30, -30), textcoords=&apos;offset points&apos;, fontsize=16, arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.2&quot;))#注释plt.text(-3.7, 3, r&apos;$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$&apos;, fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;) 4、3D图框1234567891011121314import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)# X, Y valueX = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y) # x-y 平面的网格R = np.sqrt(X ** 2 + Y ** 2)# height valueZ = np.sin(R) 绘制动态图使用matplotlib为Jupyter / IPython中的动画图创建一些选项： 在循环中使用display使用IPython.display.display(fig)在输出中显示图形。 使用一个循环，你需要在显示一个新数字之前清除输出。 请注意，这种技术通常不会那么流畅。 因此我会建议使用下面的任何一个。 %matplotlib notebook使用IPython magic %matplotlib notebook将后端设置为笔记本后端。 这样可以保持图形不会显示静态PNG文件，因此也可以显示动画。 %matplotlib tk使用IPython magic %matplotlib tk将后端设置为tk后端。 这将在一个新的绘图窗口中打开这个图形，这是一个互动的，因此也可以显示动画。 将动画转换为mp4视频 （已提供@Perfi选项）： 12from IPython.display import HTMLHTML(ani.to_html5_video()) 或者在笔记本的开头使用plt.rcParams[&quot;animation.html&quot;] = &quot;html5&quot; 。 这需要将ffmpeg视频编解码器转换为HTML5视频。 视频然后显示在内。 因此，这与%matplotlib inline后端兼容。 完整的例子： 将动画转换为JavaScript ： 12from IPython.display import HTMLHTML(ani.to_jshtml()) 或者在笔记本的开头使用plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; 。 这将使用JavaScript将动画显示为HTML。 这与大多数新浏览器以及%matplotlib inline后端都非常兼容。 它在matplotlib 2.1或更高版本中可用。 1、sin动态点曲线12345678910111213141516171819202122232425262728293031323334%matplotlib notebookimport numpy as np import matplotlib.pyplot as pltfrom matplotlib import animation&quot;&quot;&quot;animation example 2author: Kiterun&quot;&quot;&quot;fig, ax = plt.subplots()x = np.linspace(0, 2*np.pi, 200)y = np.sin(x)l = ax.plot(x, y)dot, = ax.plot([], [], &apos;ro&apos;)def init(): ax.set_xlim(0, 2*np.pi) ax.set_ylim(-1, 1) return ldef gen_dot(): for i in np.linspace(0, 2*np.pi, 200): newdot = [i, np.sin(i)] yield newdotdef update_dot(newd): dot.set_data(newd[0], newd[1]) return dot,ani = animation.FuncAnimation(fig, update_dot, frames = gen_dot, interval = 100, init_func=init)ani.save(&apos;sin_dot.gif&apos;, writer=&apos;imagemagick&apos;, fps=30)plt.show() 2、动态雨点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859%matplotlib notebookimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib import animation# New figure with white backgroundfig = plt.figure(figsize=(6,6), facecolor=&apos;white&apos;)# New axis over the whole figure, no frame and a 1:1 aspect ratioax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)# Number of ringn = 50size_min = 50size_max = 50 ** 2# Ring positionpos = np.random.uniform(0, 1, (n,2))# Ring colorscolor = np.ones((n,4)) * (0,0,0,1)# Alpha color channel geos from 0(transparent) to 1(opaque)color[:,3] = np.linspace(0, 1, n)# Ring sizessize = np.linspace(size_min, size_max, n)# Scatter plotscat = ax.scatter(pos[:,0], pos[:,1], s=size, lw=0.5, edgecolors=color, facecolors=&apos;None&apos;)# Ensure limits are [0,1] and remove ticksax.set_xlim(0, 1), ax.set_xticks([])ax.set_ylim(0, 1), ax.set_yticks([])def update(frame): global pos, color, size # Every ring is made more transparnt color[:, 3] = np.maximum(0, color[:,3]-1.0/n) # Each ring is made larger size += (size_max - size_min) / n # Reset specific ring i = frame % 50 pos[i] = np.random.uniform(0, 1, 2) size[i] = size_min color[i, 3] = 1 # Update scatter object scat.set_edgecolors(color) scat.set_sizes(size) scat.set_offsets(pos) # Return the modified object return scat,anim = animation.FuncAnimation(fig, update, interval=10, blit=True, frames=200)plt.show() 3、阻尼摆123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from math import sin, cosimport numpy as npfrom scipy.integrate import odeintimport matplotlib.pyplot as pltimport matplotlib.animation as animation%matplotlib notebookg = 9.8leng = 1.0b_const = 0.2# no decay case:def pendulum_equations1(w, t, l): th, v = w dth = v dv = - g/l * sin(th) return dth, dv# the decay exist case:def pendulum_equations2(w, t, l, b): th, v = w dth = v dv = -b/l * v - g/l * sin(th) return dth, dvt = np.arange(0, 20, 0.1)track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))xdata = [leng*sin(track[i, 0]) for i in range(len(track))]ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]fig, ax = plt.subplots()ax.grid()line, = ax.plot([], [], &apos;o-&apos;, lw=2)time_template = &apos;time = %.1fs&apos;time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)def init(): ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) time_text.set_text(&apos;&apos;) return line, time_textdef update(i): newx = [0, xdata[i]] newy = [0, ydata[i]] line.set_data(newx, newy) time_text.set_text(time_template %(0.1*i)) return line, time_textani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)plt.show() 4、内切滚动球123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from math import sin, cosimport numpy as npfrom scipy.integrate import odeintimport matplotlib.pyplot as pltimport matplotlib.animation as animation%matplotlib notebookg = 9.8leng = 1.0b_const = 0.2# no decay case:def pendulum_equations1(w, t, l): th, v = w dth = v dv = - g/l * sin(th) return dth, dv# the decay exist case:def pendulum_equations2(w, t, l, b): th, v = w dth = v dv = -b/l * v - g/l * sin(th) return dth, dvt = np.arange(0, 20, 0.1)track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))xdata = [leng*sin(track[i, 0]) for i in range(len(track))]ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]fig, ax = plt.subplots()ax.grid()line, = ax.plot([], [], &apos;o-&apos;, lw=2)time_template = &apos;time = %.1fs&apos;time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)def init(): ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) time_text.set_text(&apos;&apos;) return line, time_textdef update(i): newx = [0, xdata[i]] newy = [0, ydata[i]] line.set_data(newx, newy) time_text.set_text(time_template %(0.1*i)) return line, time_textani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)plt.show() 5、分类超平面可视化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 算法可视化# -*- coding:utf-8 -*-%matplotlib notebookimport copyfrom matplotlib import pyplot as pltfrom matplotlib import animation training_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]]w = [0, 0]b = 0history = [] def update(item): &quot;&quot;&quot; update parameters using stochastic gradient descent :param item: an item which is classified into wrong class :return: nothing &quot;&quot;&quot; global w, b, history w[0] += 1 * item[1] * item[0][0] w[1] += 1 * item[1] * item[0][1] b += 1 * item[1] print(w, b) history.append([copy.copy(w), b]) # you can uncomment this line to check the process of stochastic gradient descent def cal(item): &quot;&quot;&quot; calculate the functional distance between &apos;item&apos; an the dicision surface. output yi(w*xi+b). :param item: :return: &quot;&quot;&quot; res = 0 for i in range(len(item[0])): res += item[0][i] * w[i] res += b res *= item[1] return res def check(): &quot;&quot;&quot; check if the hyperplane can classify the examples correctly :return: true if it can &quot;&quot;&quot; flag = False for item in training_set: if cal(item) &lt;= 0: flag = True update(item) # draw a graph to show the process if not flag: print (&quot;RESULT: w: &quot; + str(w) + &quot; b: &quot; + str(b)) return flag if __name__ == &quot;__main__&quot;: for i in range(1000): if not check(): break # first set up the figure, the axis, and the plot element we want to animate fig = plt.figure() ax = plt.axes(xlim=(0, 2), ylim=(-2, 2)) line, = ax.plot([], [], &apos;g&apos;, lw=2) label = ax.text([], [], &apos;&apos;) # initialization function: plot the background of each frame def init(): line.set_data([], []) x, y, x_, y_ = [], [], [], [] for p in training_set: if p[1] &gt; 0: x.append(p[0][0]) y.append(p[0][1]) else: x_.append(p[0][0]) y_.append(p[0][1]) plt.plot(x, y, &apos;bo&apos;, x_, y_, &apos;rx&apos;) plt.axis([-6, 6, -6, 6]) plt.grid(True) plt.xlabel(&apos;x&apos;) plt.ylabel(&apos;y&apos;) plt.title(&apos;Perceptron Algorithm&apos;) return line, label # animation function. this is called sequentially def animate(i): global history, ax, line, label w = history[i][0] b = history[i][1] if w[1] == 0: return line, label x1 = -7 y1 = -(b + w[0] * x1) / w[1] x2 = 7 y2 = -(b + w[0] * x2) / w[1] line.set_data([x1, x2], [y1, y2]) x1 = 0 y1 = -(b + w[0] * x1) / w[1] label.set_text(history[i]) label.set_position([x1, y1]) return line, label # call the animator. blit=true means only re-draw the parts that have changed. print (history) anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=1000, repeat=True, blit=True) plt.show() anim.save(&apos;perceptron.gif&apos;, fps=2, writer=&apos;imagemagick&apos;) python其他可视化模块 Traits-为Python添加类型定义 TraitsUI-制作用户界面 Chaco-交互式图表 TVTK-三维可视化数据 Visual-制作3D演示动画 Mayavi-更方便的可视化]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown公式编辑]]></title>
    <url>%2F2018%2F06%2F03%2Fmarkdown%E5%85%AC%E5%BC%8F%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[加载mathjax 引入脚本对网页进行渲染1&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 基础知识插入方式 这里分两种，一种是行间插入，另一种是另取一行 行内插入1\\(a+b\\) \(a+b\) 单独一行1$$a + b$$ $$a + b$$ 基本类型上、下标12345678910$$x_1$$$$x_1^2$$$$x^2_1$$$$x_&#123;22&#125;^&#123;(n)&#125;$$ #多于一位是要加 `&#123;&#125;` 包裹的。$$x_&#123;balabala&#125;^&#123;bala&#125;$$ $$x_1$$ $$x_1^2$$ $$x^2_1$$ $$x_{22}^{(n)}$$ $$x_{balabala}^{bala}$$ 分式123$$\frac&#123;x+y&#125;&#123;2&#125;$$$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$ $$\frac{x+y}{2}$$ $$\frac{1}{1+\frac{1}{2}}$$ 根式12345$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$ $$\sqrt{2}&lt;\sqrt[3]{3}$$$$\sqrt{1+\sqrt[p]{1+a^2}}$$$$\sqrt{1+\sqrt[^p!]{1+a^2}}$$ 求和、积分1234567$$\sum_&#123;k=1&#125;^&#123;n&#125;\frac&#123;1&#125;&#123;k&#125;$$\\(\sum_&#123;k=1&#125;^n\frac&#123;1&#125;&#123;k&#125;\\)$$\int_a^b f(x)dx$$\\(\int_a^b f(x)dx\\) $$\sum_{k=1}^{n}\frac{1}{k}$$ \(\sum_{k=1}^n\frac{1}{k}\) $$\int_a^b f(x)dx$$ \(\int_a^b f(x)dx\) 空格12345678910111213紧贴 $a\\!b$没有空格 $ab$小空格 a\,b中等空格 a\;b大空格 a\ bquad空格 $a\quad b$两个quad空格 $a\qquad b$ $$a\!b$$ $${ab}$$ $$a\,b$$ $$a\;b$$ $$a\ b$$ $$a\quad b$$ $$a\qquad b$$ 公式界定符1$$\left(\sum_&#123;k=\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\frac&#123;1&#125;&#123;k&#125;\right)$$ 通过 \left 和 \right 后面跟界定符来对同时进行界定。$$\left(\sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k}\right)$$ 矩阵12345678910111213$$\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;$$$$\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;pmatrix&#125;$$$$\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;bmatrix&#125;$$$$\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Bmatrix&#125;$$$$\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;vmatrix&#125;$$$$\left|\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;\right|$$$$\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Vmatrix&#125;$$ $$\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}$$ $$\begin{pmatrix}1 &amp; 2\\3 &amp;4\end{pmatrix}$$ $$\begin{bmatrix}1 &amp; 2\\3 &amp;4\end{bmatrix}$$ $$\begin{Bmatrix}1 &amp; 2\\3 &amp;4\end{Bmatrix}$$ $$\begin{vmatrix}1 &amp; 2\\3 &amp;4\end{vmatrix}$$ $$\left|\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}\right|$$ $$\begin{Vmatrix}1 &amp; 2\\3 &amp;4\end{Vmatrix}$$类似于 left right，这里是 begin 和 end。而且里面有具体的矩阵语法，&amp; 区分行间元素，\\\\ 代表换行。可以理解为 HTML 的标签之类的。 排版数组12345678$$\mathbf&#123;X&#125; =\left( \begin&#123;array&#125;&#123;ccc&#125;x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\\x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\\\vdots &amp; \vdots &amp; \ddots\end&#123;array&#125; \right)$$ $$\mathbf{X} =\left( \begin{array}{ccc}x_{11} &amp; x_{12} &amp; \ldots \\x_{21} &amp; x_{22} &amp; \ldots \\\vdots &amp; \vdots &amp; \ddots\end{array} \right)$$ 常用公式举例公式组1234567$$\begin&#123;align&#125;a &amp;= b+c+d \\\\x &amp;= y+z\end&#123;align&#125;$$ $$\begin{align}a &amp;= b+c+d \\x &amp;= y+z\end{align}$$ 分段函数123456$$y=\begin&#123;cases&#125;-x,\quad x\leq 0 \\\\x,\quad x&gt;0\end&#123;cases&#125;$$ $$y=\begin{cases}-x,\quad x\leq 0 \\x,\quad x&gt;0\end{cases}$$ 常用数学符号希腊字母123456789101112131415161718192021222324252627$$\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;\hline&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\\hline&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\\hline&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\\hline&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\\hline&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\\hline&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\\hline&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\\hline&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\\hline&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline\end&#123;array&#125;$$ $$\begin{array}{|c|c|c|c|c|c|c|c|}\hline{\alpha} &amp; {\backslash alpha} &amp; {\theta} &amp; {\backslash theta} &amp; {o} &amp; {o} &amp; {\upsilon} &amp; {\backslash upsilon} \\\hline{\beta} &amp; {\backslash beta} &amp; {\vartheta} &amp; {\backslash vartheta} &amp; {\pi} &amp; {\backslash pi} &amp; {\phi} &amp; {\backslash phi} \\\hline{\gamma} &amp; {\backslash gamma} &amp; {\iota} &amp; {\backslash iota} &amp; {\varpi} &amp; {\backslash varpi} &amp; {\varphi} &amp; {\backslash varphi} \\\hline{\delta} &amp; {\backslash delta} &amp; {\kappa} &amp; {\backslash kappa} &amp; {\rho} &amp; {\backslash rho} &amp; {\chi} &amp; {\backslash chi} \\\hline{\epsilon} &amp; {\backslash epsilon} &amp; {\lambda} &amp; {\backslash lambda} &amp; {\varrho} &amp; {\backslash varrho} &amp; {\psi} &amp; {\backslash psi} \\\hline{\varepsilon} &amp; {\backslash varepsilon} &amp; {\mu} &amp; {\backslash mu} &amp; {\sigma} &amp; {\backslash sigma} &amp; {\omega} &amp; {\backslash omega} \\\hline{\zeta} &amp; {\backslash zeta} &amp; {\nu} &amp; {\backslash nu} &amp; {\varsigma} &amp; {\backslash varsigma} &amp; {} &amp; {} \\\hline{\eta} &amp; {\backslash eta} &amp; {\xi} &amp; {\backslash xi} &amp; {\tau} &amp; {\backslash tau} &amp; {} &amp; {} \\\hline{\Gamma} &amp; {\backslash Gamma} &amp; {\Lambda} &amp; {\backslash Lambda} &amp; {\Sigma} &amp; {\backslash Sigma} &amp; {\Psi} &amp; {\backslash Psi} \\\hline{\Delta} &amp; {\backslash Delta} &amp; {\Xi} &amp; {\backslash Xi} &amp; {\Upsilon} &amp; {\backslash Upsilon} &amp; {\Omega} &amp; {\backslash Omega} \\\hline{\Omega} &amp; {\backslash Omega} &amp; {\Pi} &amp; {\backslash Pi} &amp; {\Phi} &amp; {\backslash Phi} &amp; {} &amp; {} \\\hline\end{array}$$ 参考资料 Markdown中插入数学公式的方法 LATEX数学公式基本语法 一份其实很短的 LaTeX 入门文档]]></content>
      <categories>
        <category>写作</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM可视化]]></title>
    <url>%2F2018%2F06%2F02%2Fsvm%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 12345678910111213141516171819202122232425262728293031323334353637%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as npclass1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2]])class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4]])plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Decision Boundary&apos;)plt.xlim(0, 8)plt.ylim(0, 6)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;r&apos;)plt.plot([3, 3], [0.5, 6], &apos;--b&apos;)plt.arrow(4, 4, -1, 0, shape=&apos;full&apos;, color=&apos;b&apos;, linestyle=&apos;--&apos;)plt.annotate(r&apos;margin 1&apos;, xy=(3.5, 4), xycoords=&apos;data&apos;, xytext=(3.1, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;margin 2&apos;, xy=(3.5, 3.5), xycoords=&apos;data&apos;, xytext=(4, 3.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;support vector&apos;, xy=(4, 4), xycoords=&apos;data&apos;, xytext=(5, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;support vector&apos;, xy=(2, 2), xycoords=&apos;data&apos;, xytext=(0.5, 1.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 1234567891011121314151617181920212223242526272829303132333435plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Support Vector Machine&apos;)plt.xlim(0, 8)plt.ylim(0, 6)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;b&apos;)plt.annotate(r&apos;$w^T x + b = 0$&apos;, xy=(5, 1), xycoords=&apos;data&apos;, xytext=(6, 1), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$w^T x + b = 1$&apos;, xy=(6, 2), xycoords=&apos;data&apos;, xytext=(7, 2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$w^T x + b = -1$&apos;, xy=(3.5, 0.5), xycoords=&apos;data&apos;, xytext=(4.5, 0.2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;d&apos;, xy=(3.5, 3.5), xycoords=&apos;data&apos;, xytext=(2, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;A&apos;, xy=(4, 4), xycoords=&apos;data&apos;, xytext=(5, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn.datasets import make_blobsplt.figure(figsize=(10, 4), dpi=140)# sub plot 1plt.subplot(1, 2, 1)X, y = make_blobs(n_samples=100, n_features=2, centers=[(1, 1), (2, 2)], random_state=4, shuffle=False, cluster_std=0.4)plt.title(&apos;Non-linear Separatable&apos;)plt.xlim(0, 3)plt.ylim(0, 3)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker=&apos;o&apos;)plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker=&apos;s&apos;)plt.plot([0.5, 2.5], [2.5, 0.5], &apos;-r&apos;)# sub plot 2plt.subplot(1, 2, 2)class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2], [1.5, 1.5], [1.2, 1.7]])class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4], [5.5, 3.5], [4.5, 4.5], [2, 1.5]])plt.title(&apos;Slack Variable&apos;)plt.xlim(0, 7)plt.ylim(0, 7)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)plt.arrow(2, 1.5, 2.25, 2.25, shape=&apos;full&apos;, color=&apos;b&apos;)plt.annotate(r&apos;violate margin rule.&apos;, xy=(2, 1.5), xycoords=&apos;data&apos;, xytext=(0.2, 0.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;normal sample. $\epsilon = 0$&apos;, xy=(4, 5), xycoords=&apos;data&apos;, xytext=(4.5, 5.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$\epsilon &gt; 0$&apos;, xy=(3, 2.5), xycoords=&apos;data&apos;, xytext=(3, 1.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 12345678910111213141516171819202122plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Cost&apos;)plt.xlim(0, 4)plt.ylim(0, 2)plt.xlabel(&apos;$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$&apos;)plt.ylabel(&apos;Cost&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot([0, 1], [1.5, 0], &apos;-r&apos;)plt.plot([1, 3], [0.015, 0.015], &apos;-r&apos;)plt.annotate(r&apos;$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$&apos;, xy=(0.7, 0.5), xycoords=&apos;data&apos;, xytext=(1, 1), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$&apos;, xy=(1.5, 0), xycoords=&apos;data&apos;, xytext=(1.8, 0.2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 123456789101112131415161718192021222324252627282930313233343536373839plt.figure(figsize=(10, 4), dpi=144)class1 = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [3, 2], [4, 1], [5, 1]])class2 = np.array([[2.2, 4], [1.5, 5], [1.8, 4.6], [2.4, 5], [3.2, 5], [3.7, 4], [4.5, 4.5], [5.4, 3]])# sub plot 1plt.subplot(1, 2, 1)plt.title(&apos;Non-linear Separatable in Low Dimension&apos;)plt.xlim(0, 6)plt.ylim(0, 6)plt.yticks(())plt.xlabel(&apos;X1&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.spines[&apos;left&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], np.zeros(class1[:, 0].shape[0]) + 0.05, marker=&apos;o&apos;)plt.scatter(class2[:, 0], np.zeros(class2[:, 0].shape[0]) + 0.05, marker=&apos;s&apos;)# sub plot 2plt.subplot(1, 2, 2)plt.title(&apos;Linear Separatable in High Dimension&apos;)plt.xlim(0, 6)plt.ylim(0, 6)plt.xlabel(&apos;X1&apos;)plt.ylabel(&apos;X2&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [3.8, 2], &apos;-r&apos;) 123456789101112131415161718192021222324252627282930313233def gaussian_kernel(x, mean, sigma): return np.exp(- (x - mean)**2 / (2 * sigma**2))x = np.linspace(0, 6, 500)mean = 1sigma1 = 0.1sigma2 = 0.3plt.figure(figsize=(10, 3), dpi=144)# sub plot 1plt.subplot(1, 2, 1)plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma1))plt.xlim(0, 2)plt.ylim(0, 1.1)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot(x, gaussian_kernel(x, mean, sigma1), &apos;r-&apos;)# sub plot 2plt.subplot(1, 2, 2)plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma2))plt.xlim(0, 2)plt.ylim(0, 1.1)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot(x, gaussian_kernel(x, mean, sigma2), &apos;r-&apos;)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？]]></title>
    <url>%2F2018%2F06%2F01%2F%E5%A6%82%E4%BD%95%E7%94%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E6%AF%94%E5%96%BB%E6%8F%8F%E8%BF%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%EF%BC%9FHadoop%E3%80%81Hive%E3%80%81Spark%20%E4%B9%8B%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？这是知乎上某大神的解释： 学习很重要的是能将纷繁复杂的信息进行归类和抽象。对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题： 存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka； 计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等； 查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等； 大数据技术生态其实是一个江湖….在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…这三本秘籍分别为： 《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据； 《Google MapReduce》：论述了怎样快速计算海量的数据； 《Google BigTable》：论述了怎样实现海量数据的快速查询； 以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术； 在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。Hadoop包括三大部分，分别是hdfs、MapReduce和hbase： hdfs解决大数据的存储问题。 mapreduce解决大数据的计算问题。 hbase解决大数据量的查询问题。 之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进： 1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍； 2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。 3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。 4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼； 原文地址：知乎]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法总结]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 梯度梯度实际上就是多变量微分的一般化。 梯度是微积分中一个很重要的概念，之前提到过梯度的意义 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。 此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离。 多元函数的梯度下降我们假设有一个目标函数 现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！我们假设初始的起点为： 初始的学习率为： 函数的梯度为： 进行多次迭代： 我们发现，已经基本靠近函数的最小值点 学习率选择合适的学习速率 假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果 learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。 虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。 自适应学习速率举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate 通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate 比如 \(\eta^{t} = \eta / \sqrt{t+1}\)，t是次数。随着次数的增加，\(η_t\)减小 Feature Scaling（特征缩放） 图左边是\(x_{1}\)的scale比的scale比\(x_{2}\)要小很多，所以当要小很多，所以当\(w_{1}\)和和\(w_{2}\)做同样的变化时，做同样的变化时，\(w_{1}\)对y的变化影响是比较小的，对y的变化影响是比较小的，\(x_{2}\)对y的变化影响是比较大的。 坐标系中是两个参数的error surface（现在考虑左边蓝色），因为\(w_{1}\)对y的变化影响比较小，所以对y的变化影响比较小，所以\(w_{1}\)对损失函数的影响比较小，对损失函数的影响比较小，\(w_{1}\)对损失函数有比较小的微分，所以对损失函数有比较小的微分，所以vw_{1}\)方向上是比较平滑的。同理方向上是比较平滑的。同理\(x_{2}\)对y的影响比较大，所以对y的影响比较大，所以\(x_{2}\)对损失函数的影响比较大，所以在对损失函数的影响比较大，所以在\(x_{2}\)方向有比较尖的峡谷。 上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。 对于左边的情况，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。 常见的算法 批量梯度下降：批量梯度下降每次更新使用了所有的训练数据。如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，但是缺点是如果样本值很大的话，更新速度会很慢。 随机梯度下降：随机也就是说用一个样本的梯度来近似所有的样本，来调整θ，这样会大大加快训练数据，但是有可能由于训练数据的噪声点较多。每一次利用噪声点进行更新的过程中，不一定是朝着极小值方向更新，但是由于多次迭代，整体方向还是大致朝着极小值方向更新，提高了速度。 小批量梯度下降：小批量梯度下降法是为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的。 批量梯度下降代码12345678910111213141516171819202122232425262728import random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 #这里更新权重的时候所有的样本点都用上了 for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 12345678910111213output:iters_count 219iters_count 220iters_count 221iters_count 222iters_count 223iters_count 224iters_count 225theta: [3.0027765778748003, 3.997918297015663]final loss: 9.68238055213e-05iters: 225[Finished in 0.2s] 随机梯度下降代码1234567891011121314151617181920212223242526272829# 每次选取一个值,随机一个点更新 θimport random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 #每一次选取随机的一个点进行权重的更新 i = random.randint(0,3) pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 123456789101112#output:iters_count 1226iters_count 1227iters_count 1228iters_count 1229iters_count 1230iters_count 1231iters_count 1232theta: [3.002441488688225, 3.9975844154600226]final loss: 9.989420302e-05iters: 1232[Finished in 0.3s] 小批量梯度下降代码12345678910111213141516171819202122232425262728293031323334# 这里用2个样本点import random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 i = random.randint(0,3) #注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点 j = (i+1)%4 pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] pred_y = theta[0]*input_x[j][0]+theta[1]*input_x[j][1] theta[0] = theta[0] - step_size * (pred_y - y[j]) * input_x[j][0] theta[1] = theta[1] - step_size * (pred_y - y[j]) * input_x[j][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 1234567891011output:iters_count 543iters_count 544iters_count 545iters_count 546iters_count 547iters_count 548iters_count 549theta: [3.0023012574840764, 3.997553282857357]final loss: 9.81717138358e-05iters: 549 梯度下降的局限 容易陷入局部极值 还有可能卡在不是极值，但微分值是0的地方 还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点 参考：https://zhuanlan.zhihu.com/qinlibo-ml]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之Tensorflow(一)]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BTensorflow(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 Tensorflow简单示例1. 基本运算123456789101112131415161718192021222324#定义一个常量a = tf.constant([3,3])#定义一个变量x = tf.Variable([1,2])#定义一个加法opadd = tf.add(a,x)#定义一个减法sub = tf.subtract(a,x)#定义一个乘法opmul = tf.multiply(a,x)#定义初始化init = tf.global_variables_initializer()#定义多个操作add2 = tf.add(a,add)with tf.Session() as sess: sess.run(init) print(&quot;加法：&quot;,sess.run(add)) #执行加法 print(&quot;减法：&quot;,sess.run(sub)) #执行减法 print(&quot;乘法：&quot;,sess.run(mul)) #执行乘法 #同时执行乘法op和加法op result = sess.run([add,add2,sub,mul]) print(&quot;执行多个：&quot;,result) 2. 使用占位符1234567891011#Feed：先定义占位符，等需要的时候再传入数据#创建占位符input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)#定义乘法opoutput = tf.multiply(input1,input2)add = tf.add(input1,input2)with tf.Session() as sess: #feed的数据以字典的形式传入 print(sess.run(add, feed_dict=&#123;input1:[8.],input2:[2.]&#125;)) Tensorflow简单回归模型1. 最简单的线性回归模型1234567891011121314151617181920212223242526272829import tensorflow as tfimport numpy as np#使用numpy生成100个随机点#样本点x_data = np.random.rand(100)y_data = x_data*0.1 + 0.2#构造一个线性模型d = tf.Variable(1.1)k = tf.Variable(0.5)y = k*x_data + d#二次代价函数&lt;均方差&gt;loss = tf.losses.mean_squared_error(y_data,y)#定义一个梯度下降法来进行训练的优化器optimizer = tf.train.GradientDescentOptimizer(0.3)#最小化代价函数train = optimizer.minimize(loss)#初始化变量init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(1000): sess.run(train) if step%100 ==0: print(step,sess.run([k,d])) 2. 非线性回归的问题12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt#使用numpy生成200个随机点x_data = np.linspace(-0.5,0.5,200).reshape(-1,1)noise = np.random.normal(0,0.015,x_data.shape)y_data = np.square(x_data) + noise#定义两个placeholder，列数为1，行数未知x = tf.placeholder(tf.float32,[None,1])y = tf.placeholder(tf.float32,[None,1])#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元#定义神经网络中间层Weights_L1 = tf.Variable(tf.random_normal([1,20])) # 初始化1行20列权值biases_L1 = tf.Variable(tf.zeros([1,20])) # 初始化1行20列偏置Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 # 计算神经元信号L1 = tf.nn.tanh(Wx_plus_b_L1) # 使用激活函数计算神经元输出信号#定义神经网络输出层Weights_L2 = tf.Variable(tf.random_normal([20,1]))biases_L2 = tf.Variable(tf.zeros([1,1]))Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2prediction = tf.nn.tanh(Wx_plus_b_L2)# prediction = Wx_plus_b_L2#二次代价函数loss = tf.losses.mean_squared_error(y,prediction)#使用梯度下降法最小化代价函数训练train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)with tf.Session() as sess: #变量初始化 sess.run(tf.global_variables_initializer()) for _ in range(1000): sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;) #获得预测值 prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;) #画图 plt.figure() plt.scatter(x_data,y_data) plt.plot(x_data,prediction_value,&apos;r-&apos;,lw=5) plt.show() 123456这里思考下中间层和输出层的激活函数的选取问题。1\. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。2\. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。3\. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。4\. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。5\. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。 Tensorflow分类模型本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。 1. 简单的MNIST数据集分类12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义两个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])#创建一个简单的神经网络W = tf.Variable(tf.zeros([784,10]))b = tf.Variable(tf.zeros([10]))prediction = tf.nn.softmax(tf.matmul(x,W)+b)#二次代价函数loss = tf.losses.mean_squared_error(y,prediction)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction) #使用梯度下降法train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中。correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率。accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) #epoch：所有数据训练一次，就是一个epoch周期 for epoch in range(21): #batch：一般为32，64个数据 for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;) print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(acc)) 2. 过拟合解决及梯度下降优化器Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。在神经网络中它们之间也可以结合使用，dropout应用较多些。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义三个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob=tf.placeholder(tf.float32)# 784-1000-500-10W1 = tf.Variable(tf.truncated_normal([784,1000],stddev=0.1))b1 = tf.Variable(tf.zeros([1000])+0.1)L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)L1_drop = tf.nn.dropout(L1,keep_prob) W2 = tf.Variable(tf.truncated_normal([1000,500],stddev=0.1))b2 = tf.Variable(tf.zeros([500])+0.1)L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)L2_drop = tf.nn.dropout(L2,keep_prob) W3 = tf.Variable(tf.truncated_normal([500,10],stddev=0.1))b3 = tf.Variable(tf.zeros([10])+0.1)prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)#同样这里也可以使用正则项#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction)#正则后的交叉熵代价函数#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率#使用梯度下降法train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) for epoch in range(31): for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.5&#125;) #这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了 test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;) print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(test_acc) +&quot;,Training Accuracy &quot; + str(train_acc)) 3. 神经网络优化这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义三个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob=tf.placeholder(tf.float32)lr = tf.Variable(0.001, dtype=tf.float32)# 784-500-300-10#创建一个神经网络W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))b1 = tf.Variable(tf.zeros([500])+0.1)L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)L1_drop = tf.nn.dropout(L1,keep_prob)W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))b2 = tf.Variable(tf.zeros([300])+0.1)L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)L2_drop = tf.nn.dropout(L2,keep_prob)W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))b3 = tf.Variable(tf.zeros([10])+0.1)prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction)#训练train_step = tf.train.AdamOptimizer(lr).minimize(loss)#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) for epoch in range(21): sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch))) for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:1.0&#125;) learning_rate = sess.run(lr) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc) + &quot;, Learning Rate= &quot; + str(learning_rate)) CNN卷积神经网络以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。 CNN通过局部感受野和权值共享减少了神经网络需要训练的参数（权值）的个数。 卷积核/滤波器 卷积Padding SAME PADDING VALID PADDING 池化 max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数 mean-pooling 随机-pooling 池化Padding SAME PADDING VALID PADDING 下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义两个placeholderx = tf.placeholder(tf.float32,[None,784])#28*28y = tf.placeholder(tf.float32,[None,10])#初始化权值def weight_variable(shape): initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布 return tf.Variable(initial)#初始化偏置def bias_variable(shape): initial = tf.constant(0.1,shape=shape) return tf.Variable(initial)#卷积层def conv2d(x,W): #x input tensor of shape `[batch, in_height, in_width, in_channels]` #W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels] #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长 #padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;` return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=&apos;SAME&apos;)#池化层def max_pool_2x2(x): #ksize [1,x,y,1] return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;)#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`x_image = tf.reshape(x,[-1,28,28,1])#初始化第一个卷积层的权值和偏置W_conv1 = weight_variable([5,5,1,32])#5*5的采样窗口，32个卷积核从1个平面抽取特征b_conv1 = bias_variable([32])#每一个卷积核一个偏置值#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling#初始化第二个卷积层的权值和偏置W_conv2 = weight_variable([5,5,32,64])#5*5的采样窗口，64个卷积核从32个平面抽取特征b_conv2 = bias_variable([64])#每一个卷积核一个偏置值#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14#第二次卷积后为14*14，第二次池化后变为了7*7#进过上面操作后得到64张7*7的平面#初始化第一个全连接层的权值W_fc1 = weight_variable([7*7*64,1024])#上一层有7*7*64个神经元，全连接层有1024个神经元b_fc1 = bias_variable([1024])#1024个节点#把池化层2的输出扁平化为1维h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])#求第一个全连接层的输出h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)#keep_prob用来表示神经元的输出概率keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)#初始化第二个全连接层W_fc2 = weight_variable([1024,10])b_fc2 = bias_variable([10])#计算输出prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)#交叉熵代价函数cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)#使用AdamOptimizer进行优化train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#结果存放在一个布尔列表中correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(21): for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础（二）]]></title>
    <url>%2F2018%2F05%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 线性代数1.数据类型比较Scalar: 标量，可以看成一个数Vector: 向量，可以看成一个一维数组Matrix: 矩阵，可以看成二维数组Tensor: 张量，三维或三维以上的数组的统称，维度不定 2.矩阵导数 分子、分母布局：分子布局：分子为列向量，或者分母为行向量；分母布局：分母为列向量，或者分子为行向量； 运算规则： 需要注意的规则：以下公式默认在分子布局下的结果；； 常用公式 3.张量（Tensor） 几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格： 当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。 张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。 4.范数（Norm）有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。Lp 范数如下： $$Lp=\sqrt[p]{\sum\limits_{1}^n x_i^p}，x=(x_1,x_2,\cdots,x_n)$$所以：L1范数\(\left| \left| x \right| \right|\)：为x向量各个元素绝对值之和；L2范数\(\left| \left| x \right| \right| _{2}\)： 为x向量各个元素平方和。 5.特殊矩阵对角矩阵实对称矩阵及其性质：（1）A的特征值为实数，且其特征向量为实向量（2）A的不同特征值对应的特征向量必定正交（3）A一定有n个线性无关的特征向量，从而A相似于对角矩阵正交矩阵的性质： \(A^{-1}=A^{\top}\) 6.奇异值分解（Singular Value Decomposition，SVD）那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称SVD。 $$A=UDV^{T} $$假设A是一个m\(\times \)n矩阵，那么U是一个m\(\times \)m矩阵，D是一个m\(\times \)n矩阵，V是一个n\(\times \)n矩阵。 这些矩阵每一个都拥有特殊的结构，其中U和V都是正交矩阵，D是对角矩阵（注意，D不一定是方阵）。对角矩阵D对角线上的元素被称为矩阵A的奇异值。矩阵U的列向量被称为左奇异向量，矩阵V 的列向量被称右奇异向量。 SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。 7.Moore-Penrose伪逆对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程： \(Ax=y\) 等式两边同时左乘左逆B后，得到： \(x=By\) 是否存在唯一的映射将A映射到B取决于问题的形式。 如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。 Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为： 但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式： 其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。 8.常见距离上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。 设有两个n维变量\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)和\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)，则一些常用的距离公式定义如下： 曼哈顿距离 曼哈顿距离也称为城市街区距离，数学定义如下： $$d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| }$$ 曼哈顿距离的Python实现： 1234from numpy import *vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sum(abs(vector1-vector2)) 欧氏距离 欧氏距离其实就是L2范数，数学定义如下： $$d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } }$$ 欧氏距离的Python实现： 1234from numpy import *vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sqrt((vector1-vector2)*(vector1-vector2).T) 闵可夫斯基距离 从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义： $$d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } $$ 实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。 切比雪夫距离 切比雪夫距离就是\(L_{\varpi}\)即无穷范数，数学表达式如下： $$d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)$$ 切比雪夫距离额Python实现如下： 1234from numpy import *vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sqrt(abs(vector1-vector2).max) 夹角余弦 夹角余弦的取值范围为[-1,1]，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。 机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下： $$cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } $$ 夹角余弦的Python实现： 1234from numpy import *vector1 = mat([1,2,3])vector2 = mat([4,5,6])print dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2)) 汉明距离 汉明距离定义的是两个字符串中不相同位数的数目。例如：字符串‘1111’与‘1001’之间的汉明距离为2。 信息编码中一般应使得编码间的汉明距离尽可能的小。 汉明距离的Python实现： 1234from numpy import *matV = mat([1,1,1,1],[1,0,0,1])smstr = nonzero(matV[0]-matV[1])print smstr 杰卡德相似系数 两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号J(A,B)表示，数学表达式为： $$J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } $$ 杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。 杰卡德距离 与杰卡德相似系数相反的概念是杰卡德距离，其定义式为： $$J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } $$ 杰卡德距离的Python实现： 1234from numpy import *import scipy.spatial.distance as distmatV = mat([1,1,1,1],[1,0,0,1])print dist.pdist(matV,'jaccard') 概率论和统计1.统计量协方差 Pearson相关系数 协方差矩阵 2.独立与不相关 3.贝叶斯公式先看看什么是“先验概率”和“后验概率”，以一个例子来说明： 假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： P(患病) = 0.1%；即：在没有做检验之前，我们预计的患病率为P(患病)=0.1%，这个就叫作“先验概率”。 再假设现在有一种该病的检测方法，其检测的准确率为95%；即：如果真的得了这种病，该检测法有95%的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有95%的概率检测出阴性，但也有5%的概率检测为阳性。用概率条件概率表示即为：P(显示阳性|患病)=95% 现在我们想知道的是：在做完检测显示为阳性后，某人的患病率P(患病|显示阳性)，这个其实就称为“后验概率”。 而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。 这里先了解条件概率公式： $$P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}$$ 由条件概率可以得到乘法公式： $$P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)$$将条件概率公式和乘法公式结合可以得到： $$P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}$$ 再由全概率公式： $$P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} $$ 代入可以得到贝叶斯公式： $$P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }$$ 4.常见分布函数0-1分布0-1分布是单个二值型离散随机变量的分布，其概率分布函数为： $$P\left( X=1 \right) =p$$$$P\left( X=0 \right) =1-p$$ 几何分布几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为： $$P\left( X=k \right) =\left( 1-p \right) ^{k-1} p$$ 性质：\(E\left( X \right) =\frac{1}{p}\)\(Var\left( X \right) =\frac{1-p}{p^{2} }\) 二项分布二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为： $$P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}$$ 性质：\(E\left( X \right) =np\)\(Var\left( X \right) =np\left( 1-p \right)\) 高斯分布高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示： 若随机变量X服从一个数学期望为\(\mu\)，方差为\(\sigma ^{2}\)的正态分布，则我们将其记为：\(N\left( \mu ,\sigma^{2} \right)\)决定了正态分布的位置，其标准差\(\sigma\)（方差的开方）决定了正态分布的幅度。 指数分布指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了t小时，它总共使用至少t+s小时的条件概率，与从开始使用时算起它使用至少s小时的概率相等。下面这些都属于指数分布： 婴儿出生的时间间隔 网站访问的时间间隔 奶粉销售的时间间隔 指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间t，就等同于t之内没有任何婴儿出生，即： $$P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t} $$则： $$P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}$$如：接下来15分钟，会有婴儿出生的概率为： $$P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53$$ 泊松分布日常生活中，大量事件是有固定频率的，比如： 某医院平均每小时出生3个婴儿 某网站平均每分钟有2次访问 某超市平均每小时销售4包奶粉 它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。 泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为： $$P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!} $$其中： P表示概率，N表示某种函数关系，t表示时间，n表示数量，1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) ；λ 表示事件的频率。 还是以上面医院平均每小时出生3个婴儿为例，则\(\lambda =3\)； 那么，接下来两个小时，一个婴儿都不出生的概率可以求得为： $$P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025$$同理，我们可以求接下来一个小时，至少出生两个婴儿的概率： $$P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8$$ 5.常见的分布总结 6.极大似然估计（MLE）极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 求极大似然函数估计值的一般步骤： （1） 写出似然函数，即每个随机实验出现概率相乘，为这个抽样出现的概率。（2） 对似然函数取对数，为了方便求导；（3） 对参数求导数。（4） 令导数=0，即求解极值，由实际情况知，该极值为极大值。解似然方程。 微积分1.导数与梯度 导数：一个一元函数函数在某一点的导数描述了这个函数在这一点附近的变化率。 梯度:多元函数的导数就是梯度。 一阶导数，即梯度（gradient） 二阶导数，Hessian矩阵 2.泰勒展开 一元函数的泰勒展开： 基尼指数的图像、熵、分类误差率三者之间的关系。 3.Lagrange乘子法对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。 对于一个要求极值的函数\(f\left( x,y \right)\)，图上的蓝圈就是这个函数的等高图，就是说 \(f\left( x,y \right) =c_{1} ,c_{2} ,…,c_{n}\)分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组\(\left( x,y \right)\)，使它的\(c_{i}\)值越大越好，但是这点必须满足约束条件\(g\left( x,y \right)\)（在黄线上）。 也就是说\(f(x,y)\)相切，或者说它们的梯度▽ \({f}\)和▽\({g}\)平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为\(\lambda\)倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。 在支持向量机模型（SVM）的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。 信息论1.信息熵熵是随机变量不确定性的度量，不确定性越大，熵值越大； 若随机变量退化为定值，则熵最小，为0； 若随机分布为均匀分布，熵最大。 信息熵可以作为概率分布集散程度的度量，使用熵的近似可以推导出gini系数，在统计问题、决策树等问题中有重要应用 2.相对熵/交叉熵/K-L散度 交叉熵交叉熵考察的是两个的信息（分布）的期望交叉熵和熵，相当于，协方差和方差 相对熵相对熵考察两个信息（分布）之间的不相似性所谓相对，自然在两个随机变量之间。又称互熵，Kullback–Leibler divergence（K-L 散度）等。设p(x)和q(x)是X取值的两个概率分布，则p对q的相对熵为 在一定程度上，熵可以度量两个随机变量的距离。KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度是用来度量使用基于 Q 的编码来编码来自 P 的样本平均所需的额外的位元数。 典型情况下，P 表示数据的真实分布，Q 表示数据的理论分布，模型分布，或 P 的近似分布。 用图像形象化的表示二者之间的关系可以如下图：上面是q所含的信息量/平均编码长度[Math Processing Error]H(p)第二行是cross-entropy，即用q来编码p所含的信息量/平均编码长度|或者称之为q对p的cross-entropy第三行是上面两者之间的差值即为q对p的KL距离 个人理解是KL距离是对于同一个随机事件的不同分布度量之间的距离，所以是1.同一随机事件*2.不同分布* 3.互信息（信息增益）互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量。决策树中的信息增益就是互信息，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。 可以通过简单的计算得到： H(X|Y) = H(X) - I(X, Y), 互信息为0，则随机变量X和Y是互相独立的。 信息论与机器学习的关系 信息论视角 机器学习视角 接受信号 特征 信源 标签 平均互信息 特征有效性分析 最大熵模型 极大似然法 交叉熵 逻辑回归损失函数 最大熵模型最大熵模型的原则： 承认已知事物（知识）； 对未知事物不做任何假设，没有任何偏见。 对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。 因为这时概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”（Maximum Entropy）。 Logistic回归是统计学习中的经典分类方法，可以用于二类分类也可以用于多类分类。 最大熵模型由最大熵原理推导出来，最大熵原理是概率模型学习或估计的一个准则，最大熵原理认为在所有可能的概率模型的集合中，熵最大的模型是最好的模型，最大熵模型也可以用于二类分类和多类分类。 Logistic回归模型与最大熵模型都属于对数线性模型。 逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 熵总结 熵：不确定性的度量； 似然：与知识的吻合程度； 最大熵模型：对不确定度的无偏分配； 最大似然估计：对知识的无偏理解。 2.上溢和下溢在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。 一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。 另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\(\varpi\)时发生上溢。进一步的运算通常将这些无限值变为非数字。 必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为： softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 \(x_{i}\) 的类别标签 \(y_{i}\)属于K个类别的概率，最后判别 \(y_{i}\)所属类别时就是将其归为对应概率最大的那一个。 当式中的\(w_{k} x_{i} +b\)都是很小的负数时，\(e^{w_{k} x_{i} +b }\)就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的\(x_{w_{k} x_{i} +b}\)是很大的正数时，\(e^{w_{k} x_{i} +b }\)就会发生上溢导致结果是未定的。 凸优化1.凸集(Convex Sets)集合C是凸的，如果对于所有的\(x,y\in C\)和\(\theta\in\mathbb{R},0\leq\theta\leq 1\)有：$$\theta x+(1-\theta)y\in C$$可以这样理解：在集合C中任选两点，在这两点的连线上的所有点都属于集合C。 2.凸函数(Convex Fuctions)如果函数的定义域\({\cal D}(f)\)是一个凸集，并且对于所有的\(x,y\in {\cal D}(f)\)和\(\theta\in\mathbb{R},0\leq\theta\leq1\)，都有：\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\) 凸函数的一阶条件直观上可以这样理解，在函数上随便挑一个点，该点的切线必然在函数的下方 凸性质的二阶条件 琴生不等式(Jensen’s Inequality)假设凸函数的基本定义为:\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\ \ \ \text{for} \ \ \ 0\leq\theta\leq1\)上述等式可以扩展到多个点:\(f\left(\sum_{i=1}^k\theta_ix_i\right)\leq\sum_{i=1}^k\theta_if(x_i)\ \ \ \text{for}\ \ \ \sum_{i=1}^k\theta_i=1,\theta_i\geq0 \ \ \forall i\)再将上述等式扩展到积分形式:\(f\left(\int p(x)xdx\right)\leq\int p(x)f(x)dx\ \ \ \text{for}\ \ \ \int p(x)dx=1,p(x)\leq0\ \ \forall x\)由于\(p(x)\)积分为1，我们可以把\(p(x)\)看作是一个概率密度函数，所以尚属等式可以用以下形式表达：\(f(\mathbb{E}[x])\leq\mathbb{E}[f(x)]\)最后一条等式就是著名的琴生不等式。 3.凸优化问题(Convex Optimization Problems)在凸优化问题中，一个最关键的点就是对于一个凸优化问题，所有的局部最优解(locally optimal)都是全局最优解(globally optimal)。最优化的基本数学模型如下： 它有三个基本要素，即： 设计变量：x是一个实数域范围内的n维向量，被称为决策变量或问题的解； 目标函数：f(x)为目标函数； 约束条件：\(h_{i} \left( x \right) =0\)称为等式约束，\(g_{i} \left( x \right) \leq 0\)为不等式约束，\(i=0,1,2,……\) 4.牛顿法牛顿法介绍牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。 从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。 或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法的推导将目标函数\(f\left( x \right)\) 在\(x_{k}\)处进行二阶泰勒展开，可得： $$f\left( x \right) =f\left( x_{k} \right) +f^{‘} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{‘’}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}$$因为目标函数\(f\left( x \right)\)有极值的必要条件是在极值点处一阶导数为0，即：\(f^{‘} \left( x \right) =0\) 所以对上面的展开式两边同时求导（注意\({x}\)才是变量，\(x_{k}\)是常量\(\Rightarrow f^{‘} \left( x_{k} \right) ,f^{‘’} \left( x_{k} \right)\)都是常量），并令\(f^{‘} \left( x \right) =0\)可得： $$f^{‘} \left( x_{k} \right) +f^{‘’} \left( x_{k} \right) \left( x-x_{k} \right) =0$$即： $$x=x_{k} -\frac{f^{‘} \left( x_{k} \right) }{f^{‘’} \left( x_{k} \right) } $$ 于是可以构造如下的迭代公式： $$x_{k+1} =x_{k} -\frac{f^{‘} \left( x_{k} \right) }{f^{‘’} \left( x_{k} \right) }$$ 这样，我们就可以利用该迭代式依次产生的序列逐渐逼近\(f\left( x \right)\)的极小值点了。 牛顿法的迭代示意图如下： 上面讨论的是2维情况，高维情况的牛顿迭代公式是： 式中， ▽\({f}\)是\(f\left( x \right)\)的梯度，即： H是Hessen矩阵，即： 牛顿法的过程 1、给定初值\(x_{0} ]\)和精度阈值\(\varepsilon\)，并令\(k=0\)； 2、计算\(x_{k}\)和\(H_{k}\)； 3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)； 4、计算新的迭代点：\(x_{k+1} =x_{k} +d_{k}\)； 5、令\(k=k+1\)，转至2。 5.阻尼牛顿法引入注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现\(f\left( x_{k+1} \right) &gt;f\left( x_{k} \right)\)的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。 为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是\(x_{k}\)，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子\(\lambda _{k}\)，即： \(\lambda {k} = minf\left( x{k} +\lambda d_{k} \right)\) 算法过程 1、给定初值\(x_{0}\)和精度阈值\(\varepsilon\)，并令\(k=0\)； 2、计算\(g_{k}\)（\(f\left( x \right)\)在\(x_{k}\)处的梯度值）和\(H_{k}\)； 3、若\(\left| \left| g_{k} \right| \right| &lt;\varepsilon\)则停止迭代；否则确定搜索方向：\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)； 4、利用\(d_{k} =-H_{k}^{-1} \cdot g_{k}\)得到步长\(\lambda {k}\)，并令\(x{k+1} =x_{k} +\lambda {k} d{k}\) 5、令\(k=k+1\)，转至2。 6.拟牛顿法概述由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。 拟牛顿法的推导先将目标函数在\(x_{k+1}\)处展开，得到： $$f\left( x \right) =f\left( x_{k+1} \right) +f^{‘} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{‘’}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}$$两边同时取梯度，得： $$f^{‘}\left( x \right) = f^{‘} \left( x_{k+1} \right) +f^{‘’} \left( x_{k+1} \right) \left( x-x_{k+1} \right)$$ 取上式中的\(x=x_{k}\)，得： $$f^{‘}\left( x_{k} \right) = f^{‘} \left( x_{k+1} \right) +f^{‘’} \left( x_{k+1} \right) \left( x-x_{k+1} \right)$$ 即： $$g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)$$可得： $$H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}$$ 上面这个式子称为“拟牛顿条件”，由它来对Hessen矩阵做约束。 计算复杂性与NP问题时间复杂度表明问题规模扩大后，程序需要的时间长度增长得有多快。程序的时间复杂度一般可以分为两种级别： 多项式级的复杂度，如O(1)，O(log(n))、O（n^a）等， 非多项式级的，如O(a^n)、O(n!)等。后者的复杂度计算机往往不能承受。 约化(Reducibility)简单的说，一个问题A可以约化为问题B的含义是，可以用问题B的解法解决问题A。（个人感觉也就是说，问题A是B的一种特殊情况。）标准化的定义是，如果能找到一个变化法则，对任意一个A程序的输入，都能按照这个法则变换成B程序的输入，使两程序的输出相同，那么我们说，问题A可以约化为问题B。 例如求解一元一次方程这个问题可以约化为求解一元二次方程，即可以令对应项系数不变，二次项的系数为0，将A的问题的输入参数带入到B问题的求解程序去求解。 另外，约化还具有传递性，A可以化约为B，B可以约化为C，那么A也可以约化为C。 基本概念P Problem假设有 n 个数要排序。一个初级的冒泡排序算法所需时间可能与 n2 成正比，快一点的算法所需时间与 nlog（n） 成正比。在某些条件下，桶排序算法所需时间甚至只和 n 成正比。最不实用的算法就是输入的数字随机排列，直到出现完全有序的情况为止……记前三个算法的时间复杂度分别记为 O(n2)、O(nlogn) 和 O(n)，最后的“猴子排序”(Bogosort)算法平均时间复杂度则达到了 O(n*n!)。 在上面的例子中，前三种算法的复杂度是 n 的多项式函数；最后一种算法的复杂度是 n 的阶乘，根据斯特林公式，n! 相当于指数级别的增长。当 n 特别小时，多项式级的算法已经快过指数级的算法。当 n 非常大时，人类根本看不到指数级复杂度算法结束的那天。自然的，大家会对多项式级别的算法抱有好感，希望对每一个问题都能找到多项式级别的算法。问题是——每个问题都能找到想要的多项式级别的算法吗？ 在一个由问题构成的集合中，如果每个问题都存在多项式级复杂度的算法，这个集合就是 P 类问题（Polynomial）。 NP (Nondeterministic Polynomial)问题NP 类问题指的是，能在多项式时间内检验一个解是否正确的问题。比如我的机器上存有一个密码文件，于是就能在多项式时间内验证另一个字符串文件是否等于这个密码，所以“破译密码”是一个 NP 类问题。NP 类问题也等价为能在多项式时间内猜出一个解的问题。这里的“猜”指的是如果有解，那每次都能在很多种可能的选择中运气极佳地选择正确的一步。 不妨举个例子：给出 n 个城市和两两之间的距离，求找到一个行走方案，使得到达每个城市一次的总路程最短。我们可以这样来“猜测”它的解：先求一个总路程不超过 100 的方案，假设我们可以依靠极好的运气“猜出”一个行走路线，使得总长度确实不超过 100，那么我们只需要每次猜一条路一共猜 n 次。接下来我们再找总长度不超过 50 的方案，找不到就将阈值提高到75…… 假设最后找到了总长度为 90 的方案，而找不到总长度小于 90 的方案。我们最终便在多项式时间内“猜”到了这个旅行商问题的解是一个长度为 90 的路线。它是一个 NP 类的问题。 也就是说，NP 问题能在多项式时间内“解决”，只不过需要好运气。显然，P 类问题肯定属于 NP 类问题。所谓“P=NP”，就是问——是不是所有的 NP 问题，都能找到多项式时间的确定性算法？ NPC Problem在与数不尽的问题搏斗的过程中，人们有时候会发现，解决问题 A 的算法可以同时用来解决问题 B。例如问题 A 是对学生的姓名与所属班级同时排序，问题 B 是对人们按照姓名做排序。这时候，我们只需要让班级全都相同，便能照搬问题 A 的算法来解决问题 B。这种情况下，数学家就说，问题 B 能归约为问题 A。 人们发现，不同的 NP 问题之间也会出现可归约的关系，甚至存在这么一类（不只是一个）问题，使得任何其它的 NP 问题都能归约到它们上。也就是说，能够解决它们的算法就能够解决所有其它的 NP 问题。这一类问题就是 NPC 问题。这样的问题人们已经找到了几千个，如果我们给其中任何一个找到了多项式级别的算法，就相当于证明了 P=NP。 但NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。 P=NP？证明 P=NP 的一个主要方法就是，给某一个 NPC 问题找到一个快速算法。但是，也不排除有人给出一个“存在性”而非“构造性”的证明，只是告诉大家存在符合要求的算法，但没法详细描述出来。如果 P=NP 被人以这种方式证明出来了，我们也没法依葫芦画瓢地把这个神奇的算法在电脑上写出来，所以对破解密码仍然没有帮助。 退一步说，假如有人构造出可以运用的多项式算法，以此证明了这个问题。这个算法恐怕也很复杂（毕竟这么难找），它的多项式级别的复杂度也可能会非常慢。假设这个算法的复杂度达到了 O(n10)，那我们依然面临着不小的麻烦。即使 n=100，运算时间也会增长到非常巨大的地步。 再退一步，假设人类的运气好到 P=NP 是真的，并且找到了复杂度不超过 O(n3) 的算法。如果到了这一步，我们就会有一个算法，能够很快算出某个帐号的密码。《基本演绎法》里面所想象的可能就要成真了，所有的加密系统都会失去效果——应该说，所有会把密码变成数字信息的系统都会失去效果，因为这个数字串很容易被“金钥匙”计算出来。 除此之外，我们需要担心或期许的事情还有很多： 一大批耳熟能详的游戏，如扫雷、俄罗斯方块、超级玛丽等，人们将为它们编写出高效的AI，使得电脑玩游戏的水平无人能及。 整数规划、旅行商问题等许多运筹学中的难题会被高效地解决，这个方向的研究将提升到前所未有的高度。 蛋白质的折叠问题也是一个 NPC 问题，新的算法无疑是生物与医学界的一个福音。 参考文献：http://www.junnanzhu.com/?p=141https://www.zybuluo.com/frank-shaw/note/139175http://colah.github.io/posts/2015-09-Visual-Information/https://www.guokr.com/article/437662/]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础（一）]]></title>
    <url>%2F2018%2F05%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 【机器学习应用】计算机视觉典型的应用包括：人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索等等。 自然语言处理典型的应用包括：搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译等等。 社会网络分析典型的应用包括：用户画像、网络关联分析、欺诈作弊发现、热点发现等等。 推荐系统典型的应用包括：虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”等等。 【数据处理】数据分类正例(positive example)反例(negative example)训练集(training set)验证集(validation set)用作超参数验证测试集(test set)用作模型测试类别不平衡数据集（class-imbalanced data set）两个类别的标签的分布频率有很大的差异。 采样方式分层采样(stratified sampling)保留类别比例的采样方式通常称为分层采样 留出法（hold-out）直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。 k折交叉验证（k-fold cross validation）交叉验证先将数据集D划分为k个大小相似的互斥子集，每个子集从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。 自助法(bootstrapping)对数据集D有放回的随机采样m次后，一个样本不在样本集D1出现的概率： 当n足够大时，大约有36.8%的样本不会被采到，用没采到的部分做测试集，也是包外估计（out-of-bag-estimate）。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。 数据清洗处理缺失值 使用一个全局常量填补 使用属性的中心度量（平均数、中位数、众数等） 先聚类，再使用同类的中心度量 使用回归计算填补LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。 处理噪声 在机器学习中，下列都是异常值： 高绝对值的权重。 与实际值差距过大的预测值。 比平均值多大约 3 个标准差的输入数据的值。 噪声数据处理： 分箱（考察近邻数据值，有箱均值、箱中位数、箱边界光滑方法） 回归 离群点分析 【特征工程】特征编码 独热编码（one-hot encoding） 独热编码的优点：能够处理非数值属性；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。 特征二值化（Binarization） 特征二元化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。 归一化（Normalization） 在处理自然图像时，我们获得的像素值在 [0,255] 区间中，常用的处理是将这些像素值除以 255，使它们缩放到 [0,1] 中。 区间缩放（scaling） 标准化（Standardization） 特征变换 特征交叉（feature cross） 将特征进行交叉（乘积或者笛卡尔乘积）运算后得到的合成特征。特征交叉有助于表示非线性关系。 特征降维（DimensionalityReduction） 使用数据编码或变换，以便得到原数据的归约或“压缩”表示。归约分为无损的和有损的。有效的有损维归约方法为：小波变换和主成分分析 【线性回归模型】线性回归的正则化Lasso回归 线性回归的L1正则化通常称为Lasso回归，α来调节损失函数的均方差项和正则化项的权重。 Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0，故具有特征选择的功能，增强了模型的泛化能力。 岭回归 线性回归的L2正则化通常称为Ridge回归。 Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。 Ridge回归的求解比较简单，一般用最小二乘法。 L1正则化产生稀疏的权值, 具有特征选择的作用；L2正则化产生平滑的权值。 最小二乘法的局限性 最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了 当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。 如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。 【逻辑回归模型】逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。用sigmoid函数对线性回归模型进行变换。 θ为分类模型的要求出的模型参数。对于模型输出hθ(x)，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果hθ(x)&gt;0.5 ，即xθ&gt;0, 则y为1。如果hθ(x)&lt;0.5，即xθ&lt;0。 损失函数 概率分布函数根据二元逻辑回归的定义，假设我们的样本输出是0或者1两类。 似然函数 损失函数这里我们用对数似然函数最大化，对数似然函数取反即为我们的损失函数J(θ) 简写成矩阵形式 损失函数优化方法用梯度下降法对损失函数进行优化，得到迭代公式: 【决策树】熵熵用来度量事物的不确定性 联合熵两个变量X和Y的联合熵 条件熵 条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？从上面的描述大家可以看出，它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。在决策树ID3算法中叫做信息增益。 用下面这个图很容易明白他们的关系。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。 决策树构建中的分裂准则决策树可以通过一系列规则递归地分割特征空间 信息增益（information gain）属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合 增益率（gain ratio）信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。 基尼指数(Gini index)基尼指数是另外一种数据的不纯度的度量方法，其定义如下： 其中的m仍然表示数据集D中类别C的个数，Pi表示D中任意一个记录属于Ci的概率，计算时Pi=(D中属于Ci类的集合的记录个数/|D|)。如果所有的记录都属于同一个类中，则P1=1，Gini(D)=0，此时不纯度最低。在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树，对每个属性都会枚举其属性的非空真子集，以属性R分裂后的基尼系数为： D1为D的一个非空真子集，D2为D1在D的补集，即D1+D2=D，对于属性R来说，有多个真子集，即GiniR(D)有多个值，但我们选取最小的那么值作为R的基尼指数。最后： 对于二类分类，基尼系数和熵之半的曲线如下： 从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代 常用决策树模型决策树模型总结 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 CART决策树属性分裂方法 m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。 对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。 决策树优化方法后剪枝（postpruning）先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。 预剪枝（prepruning）在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。 抑制单颗决策树的复杂度的方法 限制树的最大深度 限制叶子节点的最少样本数量 限制节点分裂时的最少样本数量 吸收 bagging 的思想对训练样本采样，在学习单颗决策树时只使用一部分训练样本 借鉴随机森林的思路在学习单颗决策树时只采样一部分特征，在目标函数中添加正则项惩罚复杂的树结。 决策树算法的优点 基本不需要预处理，不需要提前归一化，处理缺失值。 使用决策树预测的代价是O(log2m)O(log2m)。 m为样本数。 既可以处理离散值也可以处理连续值。很多算法只是4.专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 对于异常点的容错能力好，健壮性高。 决策树算法的缺陷 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 【梯度】在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。 梯度下降法的超参数 步长（step size）学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。 梯度下降的算法调优 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 标准化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据标准化，这样特征的新期望为0，新方差为1，迭代次数可以大大加快。 梯度下降方法总结批梯度下降(batch gradient descent/BGD)求梯度的时候就用了所有m个样本的梯度数据。 随机梯度下降（stochastic gradient descent/SGD）随机梯度下降法由于每次仅仅采用一个样本来迭代。优点是速度快以及可以跳出局部最优解，缺点是导致迭代方向变化很大，不能很快的收敛到局部最优解。 小批量随机梯度下降（mini-batch stochastic gradient descent）小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。 梯度下降法与最小二乘法 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。 梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。 【分类模型指标】混淆矩阵（confusion matrix） 准确率（Accuracy）准确率是预测和标签一致的样本在所有样本中所占的比例 精确率（Precision）精确率是你预测为正类的数据中，有多少确实是正类 查全率（Recall）查全率是所有正类的数据中，你预测为正类的数据占比 不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。 F1 Score有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。 F-beta Score有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。 ROC （receiver operating characteristic curve）绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。 AUC（Area Under the Curve）ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。 理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 【集成学习】 一般来说，集成学习指的同质基学习器的集成，而同质基学习器使用最多的模型是CART决策树和神经网络。 同质基学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是基学习器之间存在强依赖关系，基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。 boosting算法 用初始权重训练出一个弱学习器1，根据其误差率表现来更新训练样本的权重，使误差率高的训练样本权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。 重复进行第一步骤，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。 bagging算法 基学习器的训练集是通过随机采样得到的。通过T次的基于自助法的随机采样，我们就可以得到T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。 大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。 随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择。bagging&amp;boosting对于 Bagging 算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差（variance），因为采用了相互独立的基分类器多了以后，h 的值自然就会靠近。所以对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树。 对于 Boosting 来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias），所以对于每个基分类器来说，问题就在于如何选择 variance 更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。 GBDT 一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 xgboost相比GBDT的优势1、传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。2、传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。3、xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。4、Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）5、列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。6、xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 随机森林的优劣 优势：1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。3） 在训练后，可以给出各个特征对于输出的重要性4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。6） 对部分特征缺失不敏感。 缺点：1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。2) 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。 集合策略 对于数值类的回归预测问题，通常使用的结合策略是平均法。 对于分类问题的预测，我们通常使用的是投票法。 对于学习误差较大的情况，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果，这种方法称为学习法。简单来说，对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。 【聚类】K-means算法 算法思想：按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。 优点：1）原理比较简单，实现也是很容易，收敛速度快。2）聚类效果较优。3）算法的可解释度比较强。4）主要需要调参的参数仅仅是簇数k。 缺点：1）K值的选取不好把握2）对于不是凸的数据集比较难收敛3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。4） 采用迭代方法，得到的结果只是局部最优。5） 对噪音和异常点比较的敏感。 【关联规则】 项集（itemset） 事务（transaction）：为一个非空项集 频度（frequency） 关联规则（association rules），X=&gt;Y，X，Y是两个不相交的非空项集。 强关联规则：支持度和置信度都高于阈值 支持度（support）：包含X∪YX∪Y的事务的出现概率 置信度（confidence）：包含X的事务同时也包含Y的概率，P(Y|X) 【文本挖掘】 Word2Vec(词向量学习模型) TF(Term Frequency词频) TF-IDF(Term Frequency-Inverse DocumentFrequency 词频-逆向文档频率) IG(InformationGain 信息增益) IGR(Information Gain Ratio 信息增益率) 【模型选择与评估】算法选择 泛化能力、欠拟合和过拟合 偏差和方差偏差方差分解解释了机器的泛化误差。偏差度量了算法的期望预测与真实结果之间的误差。方差度量了训练集的变动所导致的学习性能的变化。 有一些算法天生是高方差的算法。如KNN、决策树。非参数学习通常是高方差算法，对数据较为敏感，因为不对数据进行任何假设。 有一些算法天生就是高偏差算法。如线性回归。参数学习通常是高偏差算法，因为对数据具有极强的假设。 机器学习的主要挑战来自于方差，解决高方差的通常手段有： 1.降低模型复杂度 2.减少数据维度；降噪 3.增加样本数 4.使用验证集 5.模型正则化 此图献给奋战在一线的调参侠们！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git学习]]></title>
    <url>%2F2018%2F04%2F08%2Fgit%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Git 简介Git是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件，是目前世界上最先进的分布式版本控制系统。主要优势有：公共服务器压力和数据量都不会太大，任意两个开发者之间可以很容易的解决冲突并且可以进行离线工作。 Git 术语 commit提交持有的库的当前状态，每个提交的对象有父commit对象的指针。从给定的commit可以遍历寻找父指针，查看历史记录的提交。 branches分支用来创建另一条线的发展，默认情况下，git的主分支，是master分支，和上线的版本是一样的，平时要工作的新功能创建一个分支，功能完成之后，它被合并回master分支，每当做出一个commit，HEAD更新为最新提交 tagsgit中的tag指向一次commit的id。通常用来给开发做版本号。 clone克隆操作不仅仅是检出的工作拷贝，也反映了完整的信息 pullpull操作是用于两个存储库实例之间的同步 push将本地仓库中的文件同步到远端库中 headHEAD指针总是指向分支的最新提交，每当你做了一个提交。HEAD更新为最新提交,HEAD树枝存储在.git/refs/heads/中 工作区：就是你在电脑里能看到的目录。 暂存区：英文叫stage, 或index。一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Bash 基本指令123456789101112pwd : 显示当前所在的目录路径。ls(ll): 都是列出当前目录中的所有文件，只不过ll(两个ll)列出的内容更为详细。touch : 新建一个文件 如 touch index.js 就会在当前目录下新建一个index.js文件。rm: 删除一个文件, rm index.js 就会把index.js文件删除。mkdir: 新建一个目录,就是新建一个文件夹。rm -r : 删除一个文件夹, rm -r src 删除src目录， 好像不能用通配符。mv 移动文件, mv index.html src index.html 是我们要移动的文件, src 是目标文件夹,当然, 这样写,必须保证文件和目标文件夹在同一目录下。reset 重新初始化终端/清屏。clear 清屏。history 查看命令历史。elp 帮助。exit 退出。 常用基础命令安装&amp;配置 官网下载安装完，右键看到Git Bash代表安装完成 初始配置（–local 项目级；–global 当前用户级；–system 系统级）git config --global user.name&quot;Your Name&quot;git config --global user.email&quot;email@example.com&quot; 查看配置 - git config -l 初始化&amp;克隆 本地初始化：git init 仓库目录下会多了一个.git隐藏文件夹。 克隆版本库：git clone &quot;url&quot; p.s. 版本控制系统可以告诉你每次的改动，比如在第x行加了代码。而图片、视频这些二进制文件没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。 管理分支 查看分支：git branch 创建分支：git branch branch_name 切换分支：git checkout branch_name 创建+切换分支：git checkout -b branch_name 合并某分支到当前分支：git merge branch_name 重命名分支：git branch -m branch_name branch_new_name //不会覆盖已经存在的分支 重命名分支：git branch -M branch_name branch_new_name //会覆盖已经存在的分支 删除分支：git branch -d branch_name 强制删除分支： git branch -D branch_name 删除远程分支： git push origin : branch_name //可以使用这种语法，推送一个空分支到远程分支，其实就相当于删除远程分支 查看&amp;修改 拉取代码：git pull orgin branch_name 查看更改：git status;git status -s//以简短格式输出 查看更改细节：git diff file_name//尚未缓存的改动git diff --cached//查看已缓存的改动 查看谁修改过代码：git blame filename 回到上次修改：git reset --hard 查看历史记录：git log；git log --pretty=oneline//将每次commit的记录打印成一行 查看git远程地址：git remote -v 删除：git rm //将文件从缓存区中移除 添加文件 添加单个文件：git add filename.js //该文件添加到缓存 添加所有js文件：git add *.js 添加所有文件：git add 提交文件 提交添加的文件：git commit -m &quot;your description about this branch&quot;//记录缓存区的快照。 提交单个文件：git commit -m &quot;your description about this branch&quot; filename.js 推送分支：git push orgin your_branch_name 备份当前分支内容：git stash 标签操作 创建标签：git tag 1.0.0 //标签无法重命名 显示标签列表：git tag 切出标签：git checkout 1.0.0 删除标签：git tag -d 1.0.0 流程化管理 从主分支分支拉一下代码git pull origin master 创建开发分支developgit co(checkout) -b develop 如果其他分支有需要处理的bug，先将当前状态保存一下git stash 切换到别的分支修改代码git checkout -b branch_name 修复bug后提交代码查看修改git status 需要查看修改的细节git diff file_name 没有问题就提交 123git add .git commit &quot;your description&quot;git push orgin your_branch_name 解决完bug切换到原来的分支git checkout -b you_old_branch 恢复刚刚保存的内容 1234git stash //备份当前的工作区的内容，保存到git栈git stash pop //从git栈中读取最近一次保存的内容，恢复工作区的相关内容，由于会存在多个stash内容，所以用栈来保存，pop出最近一个stash中读取的内容并恢复git stash list //显示git栈内所有的备份，可以利用这个列表来决定从哪个地方恢复git stash clear //清空git栈，此时使用git等图形化工具会发现，原来stash的那些节点都消失了 最后，提交三部曲 123git add .git commit &quot;your description&quot;git push orgin your_branch_name Github pagesGit初始设置123git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; hexo初始化设置12345678cd d:/hexonpm install hexo-cli -ghexo init foldercd foldernpm installhexo g 或者hexo generatehexo s 或者hexo s -p 5000 （ctrl+c退出）hexo d #部署到远程]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学之美学习笔记]]></title>
    <url>%2F2018%2F03%2F06%2F%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%2F</url>
    <content type="text"><![CDATA[技术分为术和道，具体的做事方法是术，做事的原理和原则是道。——吴军 自然语言处理，语音识别，机器翻译基于规则的语言处理早期学术界认为，要让机器完成翻译和语音识别这种人类才能做的事情，就必须先让计算机理解自然语言，而做到这点就要让机器有类似人类的智能。这个方法论被称为“鸟飞派”（通过观察鸟的飞行方式，采用仿生的思路造出飞机）。 那么怎么让机器理解自然语言呢？受传统语言学的影响，他们觉得要让机器做好两件事：分析句子语法和获取语义。分析句子语法就是按照语法把句子拆分，分清它的主语、谓语、宾语是什么，每个部分的词性是什么，用什么标点符号。而语义分析，就是弄清句子要表达的具体意思。语法规则很容易用计算机算法描述，这让人们觉得基于规则的方法是对的。但是这种方法很快就陷入困境，因为基于语法的分析器处理不了复杂句子，同时，词的多义性无法用规则表述，例如下面的例子： The pen is in the box. 和 The box is in the pen.第二句话让非英语母语的人很难理解，盒子怎么在钢笔里呢？其实在这里，pen是围栏的意思。这里pen是钢笔还是围栏，通过上下文已经不能解决，而需要常识，即钢笔可以放在盒子里，但是盒子比钢笔大，所以不能放在盒子里，于是pen在这里是围栏的意思，盒子可以放在围栏里。 基于统计的语言处理贾里尼克（Jelinek）把语音识别问题当作通信问题，并用两个隐含马尔可夫模型（声学和语言模型）概括了语音识别，推动了基于统计的语言处理方法。 在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子。早期的做法是判断给出的句子是否合乎语法，由前文可知这条路走不通。贾里尼克从另外角度看这个问题：通过计算一个句子出现的概率大小来判断它的合理性，于是语音识别问题转换成计算概率问题，根据这个思路，贾里尼克建立了统计语言模型。 假定S表示某一个有意义的句子，由一连串特定顺序排列的词w1,w2,w3…组成。我们想知道S在文本中出现的可能性，计算S的概率P(S)，根据条件概率公式： 其中P(w1)为w1出现的概率，P(w2|w1)为已知第一个词出现的条件下，第二个词出现的概率，以此类推。前面几个概率容易计算，但是后面的概率随着变量增多，变得不可计算。在这里需要应用马尔可夫假设来简化计算。马尔可夫假设假定当前状态只与前一个状态有关，即Wi出现的概率只同它前面的词有关Wi-1，于是上面的公式可以简化为： 接下来的问题是估算条件概率P(Wi|Wi-1)，由条件概率公式得： 而估计联合概率P(Wi-1, Wi)和P(Wi-1)可以统计语料库得到，通过计算(Wi-1, Wi)这对词在语料库中前后相邻出现的次数C，以及Wi-1单独出现的次数，就可得到这些词或者二元组的相对频度。根据大数定理，只要统计量足够，相对频度就等于概率，于是 于是复杂的语序合理性问题，变成了简单的次数统计问题。 上式对应的统计语言模型是二元模型，实际应用中，google翻译用到四元模型。 中文分词对于西方拼音语言来说，词之间有明确的分界符（空格），但是中、日、韩、泰等语言没有。因此，首先要对句子进行分词，才能做进一步自然语言处理。对一个句子正确的分词结果如下： 分词前：中国航天官员应邀到美国与太空总署官员开会。分词后：中国/航天/官员/应邀/到/美国/与/太空/总署/官员/开会/。 最容易想到的分词方法是“查字典”，即把一个句子从左到右扫描一遍，遇到字典里有的词就标出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字。这个方法能解决七八成的问题，但是遇到有二义性的分割就无能为力了，例如“发展中国家”，正确的分割是“发展-中-国家”，但是按照查字典法就会分成“发展-中国-家”。另外，并不是最长匹配都一定正确，例如“上海大学城书店”，正确的分割是“上海-大学城-书店”，而不是“上海大学-城-书店”。 按照前文的成功思路，依靠语法规则无法解决分词的二义性问题，还是得靠统计语言模型。 假设一个句子S有n种分词方法，利用前文的统计语言模型，分别计算出每种分词方法的概率，概率最大的即为最好的分词方法。因为穷举所有的分词方法计算量太大，所以可以把它看成是一个动态规划问题，并利用维特比算法快速找到最佳分词。具体应用时还要考虑分词的颗粒度。 拼音输入法拼音输入法中的数学中文输入法经历了以自然音节编码输入，到偏旁笔画拆字输入，再回归自然音节输入的过程。输入法输入汉字的快慢取决于对汉字编码的平均长度，也就是击键次数乘以寻找这个键需要的时间。单纯地减少编码长度未必能提高输入速度，因为寻找一个键的时间会增长。 将汉字输入到计算机中，是将人能看懂的信息编码变成计算机约定的编码（Unicode或UTF-8）的过程。对汉字的编码分为两部分：对拼音的编码和消除（一音多字）歧义。键盘上可使用的是26个字母和10个数字键，最直接的方式是让26个字母对应拼音，用10个数字消除歧义性。只有当两个编码都缩短时，汉字的输入才能够变快。早期的输入法常常只注重第一部分而忽略第二部分，例如双拼输入法和五笔输入法。 每一个拼音对应多个汉字，把一个拼音串对应的汉字由左向右连起来，就是一张有向图，如下图所示，y1,y2,y3…是输入的拼音串，W11,W12,W13是第一个音的候选汉字（后面的文字描述用W1代替），以此类推。从第一个字到最后一个字可以组成很多句子，每个句子对应图中的一条路径。 拼音输入法就是要根据上下文在给定的拼音条件下找到最优的句子，即求 （Arg是argument的缩写，Arg Max为获得最大值的信息串）化简这个概率需要用到隐含马尔可夫模型（见2.2介绍），我们把拼音串看成能观察到的“显状态”，候选汉字看成“隐状态”，然后求在这个“显状态”下的“隐状态”概率。带入下文中的隐含马尔可夫模型公式（2.3），式（2.1）化简为： 化简连乘， 需要将等式两边取对数得 乘法变成了加法。我们定义两个词之间的距离 这样，寻找最大概率问题变成了寻找最短路径问题。 隐含马尔可夫模型上文介绍过马尔可夫假设（研究随机过程中的一个假设），即在随机状态序列中，假设其中的一个状态只于前一个状态有关。如天气预报，假设今天的天气只与昨天有关，这样就能得到近似解： 马尔可夫链 符合这个假设的随机过程称为马尔可夫过程，也叫马尔可夫链。隐含马尔可夫模型是马尔可夫链的一个扩展：任意时刻t的状态St是不可见的，但在每个时刻会输出Ot， Ot仅和St相关，这叫独立输出假设，数学公式如下： P(Ot|St)我们可以通过观察得到。 隐马尔可夫模型 解决问题通常是通过已知求未知，我们要通过观察到$o_t$求出$s_t$的概率，即求 由条件概率公式可得： 因为观察到的状态O一旦产生就不会变了，所以它是一个可忽略的常数，上式可以化简为 因为 式(2.2)可以化简为 信息论：信息的度量和作用信息熵香农在他的论文“通信的数学原理”[想到牛顿的“自然哲学与数学原理”]，提出了信息熵（shang），把信息和数字联系起来，解决了信息的度量，并量化出信息的作用。 一条信息的信息量和它的不确定性正相关，信息熵约等于不确定性的多少。香农给出的信息熵公式为 P(x)为x的概率分布。 信息熵的公式为什么取负数？因为概率小于1，小数求得的对数是负数，给整个公式加上负号，最终的结果为正。 下面举例说明信息熵公式为什么会用到log和概率。 猜中世界杯冠军需要多少次？足球世界杯共32个球队，给他们编号1-32号，第一次猜冠军是否在1-16号之中，如果对了就会接着猜是否在1-8号，如果错了就知道冠军在9-16号，第三次猜是否在9-12号，这样只需要5次就能猜中，log32 = 5。这里采用的是折半查找，所以取对数。 但实际情况不需要猜5次，因为球队有强弱，可以先把夺冠热门分一组，剩下的分一组，问冠军是否在热门组中，再继续这个过程，按照夺冠概率对剩下的球队分组。引入概率就会让查找数更少，也就是不确定性更小，信息熵更小。可以计算，当每支球队夺冠概率相等时（1/32），信息熵的结果为5。 条件墒：假定X和Y是两个随机变量，X是我们要了解的，已知X的随机分布P(X)，于是X的熵为： 假定我们还知道Y的一些情况，包括它和X一起出现的概率，即联合概率分布，以及在Y取不同值前提下X的概率分布，即条件概率分布，于是在Y条件下X的条件熵为： 可证明H(X|Y) &lt;H(X), 即引入相关信息后，不确定性下降了。 互信息信息之间的相关性如果度量呢？ 香农提出了用互信息度量两个随机事件的相关性。例如，“好闷热”和“要下雨了”的互信息很高。X与Y的互信息公式如下： 经过演算，可得到 只要有足够的语料库，P(x,y), P(x) 和P(y)是很容易计算的。 机器翻译中最难的两个问题之一是二义性，如Bush 既可以是总统布什，也可以是灌木丛，Kerry既可以是国务卿克里，也可以是小母牛。如何正确的翻译？一种思路是通过语法辨别，但效果不好； 另一种思路是用互信息，从大量文本中找出和总统布什一起出现的词语，如总统、美国、国会等，再用同样的方法找出和灌木丛一起出现的词，如土壤、植物等，有了这两组词，在翻译Bush时，看看上下文中哪类词更多就可以了。 相对熵/交叉熵相对熵（KL Divergence），衡量两个取值为正的函数的相似性: 结论： 两个完全相等的函数，相对熵为零； 相对熵越大，两个函数差异越大。 对于概率分布函数，或者概率密度函数，相对熵可以度量两个随机分布的差异性。 在自然语言处理中，常用相对熵计算两个常用词在不同文本中的概率分布，看他们是否同义；或者根据两篇文章中不同词的分布，衡量它们的内容是否相等。利用相对熵，可以得到信息检索中最重要的概念：词频率-逆向文档频率（TF-IDF），在后面的搜索章节会对它详细介绍。 搜索获取网页：网络爬虫把整个互联网看作一张大图，每个网页就是图中的一个节点，超链接是连接节点的弧。通过网络爬虫，用图的遍历算法，就能自动地访问到每个网页并把它们存起来。 网络爬虫是这样工作：假定从一家门户网站的首页出发，先下载这个网页，再通过这个网页分析出里面包含的所有超链接，接下来访问并下载这些超链接指向的网页。让计算机不同地做下去，就能下载整个互联网。 还需要用一个记事本（哈希表）记录下载了哪些网页避免重复下载。 工程实现问题： 遍历算法采用广度优先还是深度优先？搜索引擎要做到在有限的时间内，最多地爬下最重要的网页。显然各个网站最重要的是它的首页，那么就应该先下载所有网站的首页。如果把爬虫再扩大一点，就要继续下载首页直接链接的网页，因为这些网页是网站设计者自己认为相当重要的网页。在这个前提下，似乎应该采用广度优先。 但是还要考虑网络通信的“握手”问题。网络爬虫每次访问网站服务器时，都要通过“握手”建立连接（TCP协议），如果采用广度优先，每个网站先轮流下载所有首页，再回过头来下载第二级网页，这样就要频繁的访问网站，增加“握手”耗时。 实际的网络爬虫是由成百上千台服务器组成的分布式系统，由调度系统决定网页下载的顺序，对于某个网站，一般是由特定的一台或几台服务器专门下载，这些服务器先下载完一个网站再进入下一个网站，这样可以减少握手次数（深度优先）。具体到每个网站，采用广度优先，先下载首页，再下载首页直接链接的网页。 页面分析和超链接（URL）提取早期的网页都是直接用HTML书写，URL以文本的形式放在网页中，前后有明显标识，很容易提取出来。但现在很多网页都是用脚本语言（如JavaScript）生成，URL不是直接可见的文本，所以网络爬虫要模拟浏览器运行网页后才能得到隐含的URL，但很多网页的脚本写的不规范，很难解析，这就导致这样的网页无法被搜索引擎收录。 维护超链接哈希表在一台服务器上建立和维护一张哈希表并不是难事，但如果同时有成千上万台服务器一起下载网页，维护一张统一的哈希表就会遇到很多问题： 首先，这张哈希表会大到存不下来；其次，每台服务器下载前和下载后都要访问哈希表，于是哈希表服务器的通信就成了整个爬虫系统的瓶颈。解决办法是：明确分工，将某个区间的URL分给特定的几台服务器，避免所有服务器对同一个URL做判断；批量询问哈希表，减少通信次数，每次更新一大批哈希表的内容。 网页检索：布尔代数最简单的索引结构是用一个很长的二进制数表示一个关键字是否在每个网页中，有多少个网页就有多少位数，每一位对应一个网页，1代表相应的网页有这个关键字，0代表没有。比如关键字“原子能”对应的二进制数是0100 1000 1100 0001…表示（从左到右）第二、第五、第九、第十、第十六个网页包含这个关键字。假定关键字“应用”对应的二进制数是0010 1001 1000 0001…，那么要找到同时包含“原子能”和“应用”的网页时，只需要将这两个二进制数进行布尔AND运算，结果是0000 1000 0000 0001…表示第五和第十六个网页满足要求。 这个二进制数非常长，但是计算机做布尔运算非常快，现在最便宜的微机，在一个指令周期进行32位布尔运算，一秒钟十亿次以上。 为了保证对任何搜索都能提供相关网页，主要的搜索引擎都是对所有词进行索引，假如互联网上有100亿个有意义的网页，词汇表大小是30万，那么这个索引至少是100亿x30万=3000万亿。考虑到大多数的词只出现在一部分文本中，压缩比是100：1，也是30万亿的量级。为了网页排名方便，索引中还要存其他附加信息，如每个词出现的位置，次数等等。因此整个索引就变得非常大，需要通过分布式存储到不同服务器上（根据网页编号划分为很多小块，根据网页重要性建立重要索引和非重要索引）。 度量网页和查询的相关性：TF-IDF我们以查找包含“原子能的应用”网页举例，“原子能的应用”可以分成三个关键词：原子能、的、应用。凭直觉，我们认为包含这三个关键词较多的网页，比包含它们较少的网页相关。但这并不可取，因为这样的话，内容长的网页比内容短的网页占便宜，所以要根据网页长度对关键词的次数进行归一化，用关键词的次数，除以网页的总字数，这个商叫做“关键词的频率”或“单文本频率”（TF：Term Frequency）。比如，某个网页上有1000词，其中“原子能”“的”“应用”分别出现了2次、35次、5次，那么它们的词频就是0.002、0.035、0.005，将这三个数相加就是相应网页和查询“原子能的应用”的单文本频率。所以，度量网页和查询的相关性，一个简单的方法就是直接使用各个关键词在网页中出现的总频率。 但是这也有不准确的地方，例如上面的例子中，“的”占了总词频的80%以上，但是它对确定网页的主题几乎没什么用，我们叫这样的词为停止词（stop word），类似的还有“是”“和”等。 另外“应用”是很普通的词，而“原子能”是专业词，后者在相关性排名中比前者重要。因此需要给每个词给一个权重，权重的设定满足两个条件： 一个词预测主题的能力越强，权重就越大； 停止词权重为零。 在信息检索中，使用最多的是“逆文本频率指数”（IDF：Inverse Document Frequency），公式为 （D是全部网页数，Dw为关键词w出现的网页个数）。最终确定查询相关性，是利用TF和IDF的加权求和。 （IDF其实是在特定条件下关键词概率分布的交叉熵） 搜索结果页排序：Page Rank算法这是拉里·佩奇和谢尔盖·布林发明的计算网页自身质量的数学模型，google凭借该算法，使搜索的相关性有了质的飞跃，圆满解决了以往搜索页中排序不好的问题。该算法的核心思想为：如果一个网页被很多其他网页所链接，说明它收到普遍的承认和信赖，那么它的排名就高。当然，在具体应用中还要加上权重，给排名高的网页链接更高的权重。这里有一个怪圈，计算搜索结果网页排名过程中需要用到网页本身的排名，这不是“先有鸡还是先有蛋的问题”吗？ 谢尔盖·布林解决了这个问题，他把这个问题变成了一个二维矩阵问题，先假定所有网页排名相同（1/N），在根据这个初始值不断迭代排名，最后能收敛到真实排名。 新闻分类：余弦定理google有新闻频道，里面的内容是由计算机聚合、整理并分类各网站内容。以前门户网站的内容是由编辑在读懂之后，再根据主题分类。但是计算机根本读不懂新闻，它只会计算，所以要让计算机分类新闻，首先就要把文字变成可计算的数字，再设计一个算法来计算任意两篇新闻的相似性。 计算一篇新闻中所有实词的TF-IDF值，再把这些值按照对应的实词在词汇表的位置依次排列，就得到一个向量。例如词汇表中有64000个词，其编号和词如左下表所示，在某一篇新闻中，这64000个词的TF-IDF值如右下表所示，这64000个数就组成了一个64000维的向量，我们就用这个向量代表这篇新闻，成为这篇新闻的特征向量。每篇新闻都有一个特征向量，向量中的每个数代表对应的词对这篇新闻主题的贡献。 同一类的新闻，一定某些主题词用的较多，两篇相似的新闻，它们的特征向量一定在某几个纬度的值比较大。如果两个向量的方向一致，就说明新闻的用词比例基本一致，我们采用余弦定理计算两个向量间的夹角： 新闻分类算法分为有目标和无目标：第一种是已知一些新闻类别的特征向量，拿它分别和所有待分类的新闻计算余弦相似性，并分到对应的类别中，这些已知的新闻类别特征向量既可以手工建立，也可以自动建立； 第二种是没有分好类的特征向量做参考，它采用自底向上的聚类方法，计算所有新闻两两之间的余弦相似性，把相似性大于一个阈值的新闻分作一个小类，再比较各小类之间的余弦相似性，就这样不断待在聚合，一直到某一类因为太大而导致里面的新闻相似性很小时停止。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>统计算法</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优雅高效地使用windows]]></title>
    <url>%2F2018%2F02%2F26%2F%E4%BC%98%E9%9B%85%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8windows%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器！ 【日常工具】文件搜索 Listary：Windows文件浏览增强工具，double-ctrl调用，并且可以在(Xftp，clover 等等)窗口中使用，极其方便！ Wox：免费开源的效率启动器,不仅可以搜索文件还可以浏览网页，Alt+Space调用，以及定制的插件(wox-plugin)，堪比MAC上的 Alfred 视频播放 Potplayer：拥有强大的内置解码器，不用额外针对某类视频去下载了 下载神器 IDM(cracked version)： 全宇宙最快的下载器!唯一缺陷是不支持P2P，想下载磁力或者BT可以先用百度云网盘的离线功能，再通过油猴脚本抓取链接进行下载。 EagleGet：下载后自动安装Chrome扩展探测视频，缺陷也是不支持BT/磁力链接，方法同上 硕鼠：主要是下载网站的视频，不过现在不支持像腾讯视频之类的大网站了 PDF阅读 福昕阅读器：功能算比较齐全了(会占用端口4000) ABBYY_FineReader：PDF转WORD 写作工具 snipaste：开源、免费的国产截图神器。比QQ截图工具清晰很多。 Yu writer：windows上好用的markdown工具！ Ditto：剪切板工具，保存所有复制过的文字和图片，用ctrl+`调用 Pasteasy：全平台跨设备复制粘贴 素材库 OfficePLUS：微软Office官方在线模板网站！ Iconfont：阿里巴巴矢量图标库 Free Images - Pixabay icons8：icon素材库 NASA IMAGE：NASA素材库 pixabay:高清免费图片素材库 PPT制作 Nordri Tools：超级好用的ppt插件 Photozoom pro：利用插值算法提高图片分辨率 PPT遥控器：代替遥控笔 FILEminimizer：ppt压缩神器 Screen to Gif: Gif制作软件 Tagul：文字云生成器 思维导图 Xmind：付费，全平台，模板多，支持鱼骨图、二维图、树形图等格式，可以与Evernote同步 幕布：笔记一键生成思维导图 视频录制 OBS Studio：功能齐全的视频录制工具，直播必备 Adobe Premiere Pro CC：视频剪辑工具 文件整理 Q-dir：需要在文件夹之间移动文件的时候，这个整理神器就能派上用场了！ Clover 3：为资源管理器添加多标签页功能，可以将常用文件夹添加为书签 Goodsyne：强大的数据同步工具 bandzip：win10下好用的压缩软件 快速启动 TrueLaunchBar：对快速启动项进行分组；允许你把任何文件夹组织成菜单的形式；实现剪切板管理、性能监视等功能 Wgestures：全局鼠标手势！ 【系统开发与优化工具】桌面优化 Fences：付费,桌面文件分类整理软件 Wallpapaer：动态壁纸软件，装逼神器！ 屏保 Fliqlo：数字时钟的屏幕保护，逼格满满 Flux： 视力保护，通过根据时间调节屏幕颜色，减少蓝光对视力的影响 系统管理 PowerTool：查看系统进程等信息，安全修复！ Dism++：简洁的系统管理软件，集成了很多小工具，还可以系统备份 Ccleaner：系统清理工具 文件修改 Bulk Rename Utility：批量重命名工具 remove empty directories：删除空文件夹，强迫症的福音 系统安装 清华大学开源软件镜像站：可以下载到Linux镜像文件以及python第三方库文件等，速度很快！ Ultraiso：制作启动盘 网络 Fiddler：抓包工具 文本编辑器 Sublime Text3：文本神器 Atom：中文友好，渲染插件多 IDE Pycharm：社区版免费，Python开发必备 Anaconda：集成了python科学计算的第三方库，内置spyder和jupyter notebook IntelliJ IDEA：前端必备IDE Cmder：monokai配色主题，完美代替原生cmd 【chrome插件】开发必备 Vimium(键盘浏览插件) JSONView(json数据进行转码和格式化) Proxy SwitchyOmega (代理) Qiniu upload files (七牛图床插件) Markdown Here (转化为markdown格式） The QR Code Extension (二维码生成器) 日常管理 Extensity (扩展管理工具) LastPass (密码管理器) 浏览优化 书签侧边栏 Imtranslator（翻译） Imagus (悬停放大图片) OneTab (内存优化神器) Better History (查看历史记录) Sexy Undo Close Tab (恢复关闭网页) Infinity (方便的新标签页定制) CrxMouse Chrome Gestures (鼠标手势、超级拖拽) Tampermonkey (油猴：脚本管理平台，神器！！) IE Tab (打开用IE内核支持的网页，常用于银行支付环境) Listen 1 (集成各大平台的音乐，再也不用为音乐版权的问题头疼了) 下载&amp;收藏 印象笔记裁剪 网页截图：注释&amp;录屏 RSS Subscription Extension Eagleget Free Download]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text3 快捷键]]></title>
    <url>%2F2018%2F02%2F22%2Fsublime%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[选择类Ctrl + D 选择单文本 Alt + F3 选中文件所有相同文本 Ctrl + L 选中整行 Ctrl + shift + M 选中括号中文本（与搜狗有热键冲突） Ctrl + M 光标移动结束或开始位置 Ctrl + Enter 在下一行插入新行 Ctrl + Shift + Enter 在上一行插入新行 Ctrl + Shift + [ 选中，折叠代码 Ctrl + Shift + ] 选中，展开代码 Ctrl + K + 0 展开所有折叠代码 Ctrl + ←/→ 快速移动光标 shift + ↑/↓ 向上/向下选中多行 Shift + ←/→ 向左/向右选中文本 Ctrl + Shift + ←/→ 向左/向右快速选择文本 编辑类Ctrl + J 合并多行代码为一行 Ctrl + Shift + D 复制整行，插入到下一行 Tab 向右缩进 &amp; Shift + Tab 向左缩进 Ctrl + K + K 从光标处开始删除代码至行尾。 Ctrl + Shift + K 删除整行。 Ctrl + / 注释单行。 Ctrl + Shift + / 注释多行。 Ctrl + K + U/L 转换大/小写。 Ctrl + Z 撤销 Ctrl + Y 恢复撤销 Ctrl + F2 设置书签 Ctrl + T 左右字母互换。 F6 单词检测拼写 搜索类Ctrl + F 文件内搜索 Ctrl + shift + F 文件夹内搜索 Ctrl + P 按类别搜索。举个栗子：1、输入当前项目中的文件名；快速搜索文件，2、输入@和关键字，查找文件中函数名；3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。 Ctrl + G 数字定位搜索 Ctrl + R 函数定位搜索 Ctrl + ： 变量、属性名定位搜索 Ctrl + Shift + P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 显示类Ctrl + Tab 按浏览顺序切换窗口 Ctrl + PageDown 向左切换当前窗口的标签页 Ctrl + PageUp 向右切换当前窗口的标签页。 Alt + Shift + “1/2/3” 分屏 Ctrl + K + B 开启/关闭侧边栏。 F11 全屏模式 Shift + F11 免打扰模式]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>网页配置</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
</search>
