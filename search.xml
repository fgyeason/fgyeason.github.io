<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习基础（一）]]></title>
    <url>%2F2018%2F05%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。 数据数据集 正例(positive example) 反例(negative example) 训练集(training set) 验证集(validation set)用作超参数验证 测试集(test set)用作模型测试 类别不平衡数据集（class-imbalanced data set）两个类别的标签的分布频率有很大的差异。 采样方式 分层采样(stratified sampling)保留类别比例的采样方式通常称为分层采样 留出法（hold-out）直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。 k折交叉验证（k-fold cross validation）交叉验证先将数据集D划分为k个大小相似的互斥子集，每个自己都尽可能保持数据分布的一致性，即从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。交叉验证评估结果的稳定性和保真性很大程度上取决于k的取值，通常称之为k折交叉验证，最常用的k是10。取k为1的时候为留一法，对模型的衡量的准确度比较好，但当数据量过大，时间复杂度过大， 自助法(bootstrapping)对数据集D有放回的采样m次后，得到包含m个样本的数据集D’做训练集。当m足够大时，根据极限有36.8%的样本不会被采到，用没采到的部分做测试集。 预处理 异常值（outlier）在机器学习中，下列都是异常值： 高绝对值的权重。 与实际值差距过大的预测值。 比平均值多大约 3 个标准差的输入数据的值。 特征工程 one-hot 编码（one-hot encoding） 一个稀疏向量，其中： 一个元素设置为 1。 所有其他的元素设置为 0。 缩放（scaling）特征工程中常用的操作，用于控制特征值区间，使之与数据集中其他特征的区间匹配。例如，假设你想使数据集中所有的浮点特征的区间为 0 到 1。给定一个特征区间是 0 到 500，那么你可以通过将每个值除以 500，缩放特征值区间。 特征交叉（feature cross）将特征进行交叉（乘积或者笛卡尔乘积）运算后得到的合成特征。特征交叉有助于表示非线性关系。 算法机器学习方法归类 监督学习（Supervised Learning）训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。 无监督学习（Unsupervised Learning）与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类。 半监督学习（Semi-Supervised Learning）介于二者之间。 具体算法原理分类构造间隔理论分布：聚类分析和模式识别 人工神经网络 决策树 感知器 支持向量机 集成学习AdaBoost 降维与度量学习 聚类 贝叶斯分类器 构造条件概率：回归分析和统计分类 高斯过程回归 线性判别分析 最近邻居法 径向基函数核 通过再生模型构造概率密度函数： 最大期望算法 概率图模型：包括贝叶斯网和Markov随机场 Generative Topographic Mapping 近似推断技术： 马尔可夫链 蒙特卡罗方法 变分法 损失函数&amp;优化方法 常见凸函数（concex function） L2 损失函数 Log 损失函数 L1 正则化函数 L2 正则化函数 步长（step size）学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。 批梯度下降(batch gradient descent/BGD) 随机梯度下降（stochastic gradient descent/SGD）批量大小为 1 的梯度下降算法。也就是说，SGD 依赖于从数据集中随机均匀选择出的一个样本，以评估每一步的梯度。 小批量（mini-batch）在训练或推断的一个迭代中运行的整批样本的一个小的随机选择的子集。小批量的大小通常在 10 到 1000 之间。在小批量数据上计算损失比在全部训练数据上计算损失要高效的多。 小批量随机梯度下降（mini-batch stochastic gradient descent）使用小批量的梯度下降算法。也就是，小批量随机梯度下降基于训练数据的子集对 梯度进行评估。Vanilla SGD 使用 size 为 1 的小批量。 正则化（regularization）对模型复杂度的惩罚。正则化帮助防止过拟合。正则化包括不同种类： L1 正则化 L2 正则化 dropout 正则化 early stopping（这不是正式的正则化方法，但可以高效限制过拟合） 分类模型 混淆矩阵（confusion matrix） 真阳性(True Positive，TP)；假阳性(False Positive，FP)；真阴性(True Negative，TN)；假阴性(False Negative，FN) 准确率（Accuracy） 精确率（Precision） 召回率（Recall）准确率是预测和标签一致的样本在所有样本中所占的比例；精确率是你预测为正类的数据中，有多少确实是正类；召回率是所有正类的数据中，你预测为正类的数据有多少。 F1 Score F-beta ScoreF1得分可以理解为对查准率和查全率取平均，但是这里不是取算数平均，而是取调和平均。为什么？因为调和平均值更接近较小值，这样查准率或查全率中哪个值较小，调和平均值就更接近这个值，这样的测量指标更严格！ ROC 接收者操作特征曲线（Receiver Operating Characteristic）将真正例比例和假正例比例作为x，y坐标轴，画出来一条曲线，这条曲线就叫做ROC Curve。 AUC（Area Under the Curve）被定义为ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。作为一个数值，对应AUC更大的分类器效果更好。 理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 softmax为多类别分类模型中每个可能的类提供概率的函数。概率加起来的总和是 1.0。 交叉熵（cross-entropy）多类别分类问题中对 Log 损失函数的推广。 回归模型 线性模型(linear model)f(x) = w1x1+w2x2+…+wdxd+b 最小二乘法(least square method)基于均方误差最小化来进行模型求解的方法称为最小二乘法 多元线性回归(multivariate linear regression)样本由多个属性描述的线性回归 对数线性回归(log-linear regression)线性回归的一种变化，lny=wx+b 对数几率回归（log-linear regression）对于分类问题，需要找一个单调可微函数将数据和线性模型关联起来，单位阶跃函数不连续，对数几率函数则连续且单调可微，并近似单位阶跃函数y = 1/(1+e^-z) 广义线性模型（generalized linear model）最小二乘回归模型的推广/泛化，基于高斯噪声，相对于其它类型的模型（基于其它类型的噪声，比如泊松噪声，或类别噪声）。 广义线性模型的例子包括： logistic 回归 多分类回归 最小二乘回归 广义线性模型的参数可以通过凸优化得到，它具有以下性质： 最理想的最小二乘回归模型的平均预测结果等于训练数据的平均标签。 最理想的 logistic 回归模型的平均概率的预测结果等于训练数据的平均标签。 正则化（regularization）引入正则化项使线性方程解出多组w解，且都能使均方误差最小化，由学习算法的归纳偏好决定使用哪组解。 决策树 信息增益（information gain）属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合 增益率（gain ratio）信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。 ID3决策树（Iterative Dichotomiser decision tree）以信息增益为准则来划分属性的迭代二分器决策树。 CART决策树(Classfication and Regression Tree)使用基尼指数划分属性的决策树。 后剪枝（postpruning）先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。 预剪枝（prepruning）在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。 神经网络模型（neural networks） 输入权值 每个输入会对应一个权值 w，同时还会有一个偏置值 b。也就是图中的 w0。训练神经网络的过程，其实就是确定权值 w 的过程。 激活函数 经过权值运算之后还会经历激活函数再输出。比如我们可以用阶跃函数 f 来表示激活函数。 当然还有其他的激活函数如ReLU、Sigmoid等 阈值（bias/threshold） 非线性可分（linearly unseparable）用线性超平面无法划分 修正线性单元（Rectified Linear Unit/ReLU）一种具备以下规则的激活函数： 如果输入为负或零，则输出为 0。 如果输入为正，则输出与输入相同。 Sigmoid 函数（sigmoid function）把 logistic 或多项式回归输出（对数几率）映射到概率的函数，返回的值在 0 到 1 之间。 BP算法(BackPropagation algorithm)神经网络学习过程就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值。 （误差）逆向传播，从最后一层开始，利用广义的感知机学习规则，基于梯度下降策略，以目标的负梯度方向对参数进行调整， 对每个训练样例，先将输入示例提供给输入层神经元， 然后逐层将信号前传 知道产生输出层结果 计算输出层误差 将误差逆向传播至隐层 根据隐层神经元的误差来对连接权和阈值进行调整 迭代循环进行，直到某些条件（如训练误差已经很小）停止。 多层前馈神经网络(multi-layer feedforward neural network)多个感知机相连，每层神经元与下一层神经元全互联，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构被称为“多层前馈神经网络”。 模拟退火（simulated annealing）模拟退火在每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小，在每部迭代过程中，接受次优解的概率随着时间的推移而逐渐降低，从而保证算法稳定。 dropout 正则化（dropout regularization）训练神经网络时一种有用的正则化方法。dropout 正则化的过程是在单次梯度计算中删去一层网络中随机选取的固定数量的单元。删去的单元越多，正则化越强。 收敛(converge)感知机的学习过程使得权重向量趋于稳定。 tensorflow 队列（queue）实现队列数据结构的 TensorFlow 操作。通常在输入／输出（I/O）中使用。 会话（session）保持 TensorFlow 程序的状态（如变量）。 张量（tensor）TensorFlow 项目的主要数据结构。张量是 N 维数据结构（N 的值很大），经常是标量、向量或矩阵。张量可以包括整数、浮点或字符串值。 张量形状（Tensor shape）张量的元素数量包含在不同维度中。比如，[5, 10] 张量在一个维度中形状为 5，在另一个维度中形状为 10。 张量大小（Tensor size）张量包含的标量总数。比如，[5, 10] 张量的大小就是 50。 TensorBoard展示一个或多个 TensorFlow 项目运行过程中保存的摘要数据的控制面板。 嵌入（embeddings）一类表示为连续值特征的明确的特征。嵌入通常指将高维向量转换到低维空间中。例如，将一个英语句子中的单词以以下任何一种方式表示： 拥有百万数量级（高维）的元素的稀疏向量，其中所有的元素都是整数。向量的每一个单元表示一个单独的英语单词，单元中的数字表示该单词在一个句子中出现的次数。由于一个句子中的单词通常不会超过 50 个，向量中几乎所有的单元都是 0。少量的非零的单元将取一个小的整数值（通常为 1）表示句子中一个单词的出现次数。 拥有数百个（低维）元素的密集向量，其中每一个元素取 0 到 1 之间的浮点数。在 TensorFlow 中，嵌入是通过反向传播损失训练的，正如神经网络的其它参量一样。 图（graph）在 TensorFlow 中的一种计算过程展示。图中的节点表示操作。节点的连线是有指向性的，表示传递一个操作（一个张量）的结果（作为一个操作数）给另一个操作。使用 TensorBoard 能可视化计算图。 Saver负责存储模型检查点文件的 TensorFlow 对象 算法选择 模型评估奥卡姆剃刀（Occam’s razor）在同等结果下，优先选择较简单的模型 泛化能力、欠拟合和过拟合 偏差、误差和方差偏差（Error）反映的是整个模型的准确度，误差（Bias）反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，方差（Variance）反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。 最后此图献给奋战在一线的调包侠们！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git学习]]></title>
    <url>%2F2018%2F04%2F08%2Fgit%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Git 简介Git是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件，是目前世界上最先进的分布式版本控制系统。主要优势有：公共服务器压力和数据量都不会太大，任意两个开发者之间可以很容易的解决冲突并且可以进行离线工作。 Git 术语 commit提交持有的库的当前状态，每个提交的对象有父commit对象的指针。从给定的commit可以遍历寻找父指针，查看历史记录的提交。 branches分支用来创建另一条线的发展，默认情况下，git的主分支，是master分支，和上线的版本是一样的，平时要工作的新功能创建一个分支，功能完成之后，它被合并回master分支，每当做出一个commit，HEAD更新为最新提交 tagsgit中的tag指向一次commit的id。通常用来给开发做版本号。 clone克隆操作不仅仅是检出的工作拷贝，也反映了完整的信息 pullpull操作是用于两个存储库实例之间的同步 push将本地仓库中的文件同步到远端库中 headHEAD指针总是指向分支的最新提交，每当你做了一个提交。HEAD更新为最新提交,HEAD树枝存储在.git/refs/heads/中 工作区：就是你在电脑里能看到的目录。 暂存区：英文叫stage, 或index。一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Bash 基本指令123456789101112pwd : 显示当前所在的目录路径。ls(ll): 都是列出当前目录中的所有文件，只不过ll(两个ll)列出的内容更为详细。touch : 新建一个文件 如 touch index.js 就会在当前目录下新建一个index.js文件。rm: 删除一个文件, rm index.js 就会把index.js文件删除。mkdir: 新建一个目录,就是新建一个文件夹。rm -r : 删除一个文件夹, rm -r src 删除src目录， 好像不能用通配符。mv 移动文件, mv index.html src index.html 是我们要移动的文件, src 是目标文件夹,当然, 这样写,必须保证文件和目标文件夹在同一目录下。reset 重新初始化终端/清屏。clear 清屏。history 查看命令历史。elp 帮助。exit 退出。 常用基础命令安装&amp;配置 官网下载安装完，右键看到Git Bash代表安装完成 初始配置（–local 项目级；–global 当前用户级；–system 系统级）git config --global user.name&quot;Your Name&quot;git config --global user.email&quot;email@example.com&quot; 查看配置 - git config -l 初始化&amp;克隆 本地初始化：git init 仓库目录下会多了一个.git隐藏文件夹。 克隆版本库：git clone &quot;url&quot; p.s. 版本控制系统可以告诉你每次的改动，比如在第x行加了代码。而图片、视频这些二进制文件没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。 管理分支 查看分支：git branch 创建分支：git branch branch_name 切换分支：git checkout branch_name 创建+切换分支：git checkout -b branch_name 合并某分支到当前分支：git merge branch_name 重命名分支：git branch -m branch_name branch_new_name //不会覆盖已经存在的分支 重命名分支：git branch -M branch_name branch_new_name //会覆盖已经存在的分支 删除分支：git branch -d branch_name 强制删除分支： git branch -D branch_name 删除远程分支： git push origin : branch_name //可以使用这种语法，推送一个空分支到远程分支，其实就相当于删除远程分支 查看&amp;修改 拉取代码：git pull orgin branch_name 查看更改：git status;git status -s//以简短格式输出 查看更改细节：git diff file_name//尚未缓存的改动git diff --cached//查看已缓存的改动 查看谁修改过代码：git blame filename 回到上次修改：git reset --hard 查看历史记录：git log；git log --pretty=oneline//将每次commit的记录打印成一行 查看git远程地址：git remote -v 删除：git rm //将文件从缓存区中移除 添加文件 添加单个文件：git add filename.js //该文件添加到缓存 添加所有js文件：git add *.js 添加所有文件：git add 提交文件 提交添加的文件：git commit -m &quot;your description about this branch&quot;//记录缓存区的快照。 提交单个文件：git commit -m &quot;your description about this branch&quot; filename.js 推送分支：git push orgin your_branch_name 备份当前分支内容：git stash 标签操作 创建标签：git tag 1.0.0 //标签无法重命名 显示标签列表：git tag 切出标签：git checkout 1.0.0 删除标签：git tag -d 1.0.0 流程化管理 从主分支分支拉一下代码git pull origin master 创建开发分支developgit co(checkout) -b develop 如果其他分支有需要处理的bug，先将当前状态保存一下git stash 切换到别的分支修改代码git checkout -b branch_name 修复bug后提交代码查看修改git status 需要查看修改的细节git diff file_name 没有问题就提交 123git add .git commit &quot;your description&quot;git push orgin your_branch_name 解决完bug切换到原来的分支git checkout -b you_old_branch 恢复刚刚保存的内容 1234git stash //备份当前的工作区的内容，保存到git栈git stash pop //从git栈中读取最近一次保存的内容，恢复工作区的相关内容，由于会存在多个stash内容，所以用栈来保存，pop出最近一个stash中读取的内容并恢复git stash list //显示git栈内所有的备份，可以利用这个列表来决定从哪个地方恢复git stash clear //清空git栈，此时使用git等图形化工具会发现，原来stash的那些节点都消失了 最后，提交三部曲 123git add .git commit &quot;your description&quot;git push orgin your_branch_name Github pagesGit初始设置123git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; hexo初始化设置12345678cd d:/hexonpm install hexo-cli -ghexo init foldercd foldernpm installhexo g 或者hexo generatehexo s 或者hexo s -p 5000 （ctrl+c退出）hexo d #部署到远程]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Git，Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学之美学习笔记]]></title>
    <url>%2F2018%2F03%2F06%2F%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%2F</url>
    <content type="text"><![CDATA[技术分为术和道，具体的做事方法是术，做事的原理和原则是道。——吴军 自然语言处理，语音识别，机器翻译基于规则的语言处理早期学术界认为，要让机器完成翻译和语音识别这种人类才能做的事情，就必须先让计算机理解自然语言，而做到这点就要让机器有类似人类的智能。这个方法论被称为“鸟飞派”（通过观察鸟的飞行方式，采用仿生的思路造出飞机）。 那么怎么让机器理解自然语言呢？受传统语言学的影响，他们觉得要让机器做好两件事：分析句子语法和获取语义。分析句子语法就是按照语法把句子拆分，分清它的主语、谓语、宾语是什么，每个部分的词性是什么，用什么标点符号。而语义分析，就是弄清句子要表达的具体意思。语法规则很容易用计算机算法描述，这让人们觉得基于规则的方法是对的。但是这种方法很快就陷入困境，因为基于语法的分析器处理不了复杂句子，同时，词的多义性无法用规则表述，例如下面的例子： The pen is in the box. 和 The box is in the pen.第二句话让非英语母语的人很难理解，盒子怎么在钢笔里呢？其实在这里，pen是围栏的意思。这里pen是钢笔还是围栏，通过上下文已经不能解决，而需要常识，即钢笔可以放在盒子里，但是盒子比钢笔大，所以不能放在盒子里，于是pen在这里是围栏的意思，盒子可以放在围栏里。 基于统计的语言处理贾里尼克（Jelinek）把语音识别问题当作通信问题，并用两个隐含马尔可夫模型（声学和语言模型）概括了语音识别，推动了基于统计的语言处理方法。 在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子。早期的做法是判断给出的句子是否合乎语法，由前文可知这条路走不通。贾里尼克从另外角度看这个问题：通过计算一个句子出现的概率大小来判断它的合理性，于是语音识别问题转换成计算概率问题，根据这个思路，贾里尼克建立了统计语言模型。 假定S表示某一个有意义的句子，由一连串特定顺序排列的词w1,w2,w3…组成。我们想知道S在文本中出现的可能性，计算S的概率P(S)，根据条件概率公式： 其中P(w1)为w1出现的概率，P(w2|w1)为已知第一个词出现的条件下，第二个词出现的概率，以此类推。前面几个概率容易计算，但是后面的概率随着变量增多，变得不可计算。在这里需要应用马尔可夫假设来简化计算。马尔可夫假设假定当前状态只与前一个状态有关，即Wi出现的概率只同它前面的词有关Wi-1，于是上面的公式可以简化为： 接下来的问题是估算条件概率P(Wi|Wi-1)，由条件概率公式得： 而估计联合概率P(Wi-1, Wi)和P(Wi-1)可以统计语料库得到，通过计算(Wi-1, Wi)这对词在语料库中前后相邻出现的次数C，以及Wi-1单独出现的次数，就可得到这些词或者二元组的相对频度。根据大数定理，只要统计量足够，相对频度就等于概率，于是 于是复杂的语序合理性问题，变成了简单的次数统计问题。 上式对应的统计语言模型是二元模型，实际应用中，google翻译用到四元模型。 中文分词对于西方拼音语言来说，词之间有明确的分界符（空格），但是中、日、韩、泰等语言没有。因此，首先要对句子进行分词，才能做进一步自然语言处理。对一个句子正确的分词结果如下： 分词前：中国航天官员应邀到美国与太空总署官员开会。分词后：中国/航天/官员/应邀/到/美国/与/太空/总署/官员/开会/。 最容易想到的分词方法是“查字典”，即把一个句子从左到右扫描一遍，遇到字典里有的词就标出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字。这个方法能解决七八成的问题，但是遇到有二义性的分割就无能为力了，例如“发展中国家”，正确的分割是“发展-中-国家”，但是按照查字典法就会分成“发展-中国-家”。另外，并不是最长匹配都一定正确，例如“上海大学城书店”，正确的分割是“上海-大学城-书店”，而不是“上海大学-城-书店”。 按照前文的成功思路，依靠语法规则无法解决分词的二义性问题，还是得靠统计语言模型。 假设一个句子S有n种分词方法，利用前文的统计语言模型，分别计算出每种分词方法的概率，概率最大的即为最好的分词方法。因为穷举所有的分词方法计算量太大，所以可以把它看成是一个动态规划问题，并利用维特比算法快速找到最佳分词。具体应用时还要考虑分词的颗粒度。 拼音输入法拼音输入法中的数学中文输入法经历了以自然音节编码输入，到偏旁笔画拆字输入，再回归自然音节输入的过程。输入法输入汉字的快慢取决于对汉字编码的平均长度，也就是击键次数乘以寻找这个键需要的时间。单纯地减少编码长度未必能提高输入速度，因为寻找一个键的时间会增长。 将汉字输入到计算机中，是将人能看懂的信息编码变成计算机约定的编码（Unicode或UTF-8）的过程。对汉字的编码分为两部分：对拼音的编码和消除（一音多字）歧义。键盘上可使用的是26个字母和10个数字键，最直接的方式是让26个字母对应拼音，用10个数字消除歧义性。只有当两个编码都缩短时，汉字的输入才能够变快。早期的输入法常常只注重第一部分而忽略第二部分，例如双拼输入法和五笔输入法。 每一个拼音对应多个汉字，把一个拼音串对应的汉字由左向右连起来，就是一张有向图，如下图所示，y1,y2,y3…是输入的拼音串，W11,W12,W13是第一个音的候选汉字（后面的文字描述用W1代替），以此类推。从第一个字到最后一个字可以组成很多句子，每个句子对应图中的一条路径。 拼音输入法就是要根据上下文在给定的拼音条件下找到最优的句子，即求 （Arg是argument的缩写，Arg Max为获得最大值的信息串）化简这个概率需要用到隐含马尔可夫模型（见2.2介绍），我们把拼音串看成能观察到的“显状态”，候选汉字看成“隐状态”，然后求在这个“显状态”下的“隐状态”概率。带入下文中的隐含马尔可夫模型公式（2.3），式（2.1）化简为： 化简连乘， 需要将等式两边取对数得 乘法变成了加法。我们定义两个词之间的距离 这样，寻找最大概率问题变成了寻找最短路径问题。 隐含马尔可夫模型上文介绍过马尔可夫假设（研究随机过程中的一个假设），即在随机状态序列中，假设其中的一个状态只于前一个状态有关。如天气预报，假设今天的天气只与昨天有关，这样就能得到近似解： 马尔可夫链 符合这个假设的随机过程称为马尔可夫过程，也叫马尔可夫链。隐含马尔可夫模型是马尔可夫链的一个扩展：任意时刻t的状态St是不可见的，但在每个时刻会输出Ot， Ot仅和St相关，这叫独立输出假设，数学公式如下： P(Ot|St)我们可以通过观察得到。 隐马尔可夫模型 解决问题通常是通过已知求未知，我们要通过观察到$o_t$求出$s_t$的概率，即求 由条件概率公式可得： 因为观察到的状态O一旦产生就不会变了，所以它是一个可忽略的常数，上式可以化简为 因为 式(2.2)可以化简为 信息论：信息的度量和作用信息熵香农在他的论文“通信的数学原理”[想到牛顿的“自然哲学与数学原理”]，提出了信息熵（shang），把信息和数字联系起来，解决了信息的度量，并量化出信息的作用。 一条信息的信息量和它的不确定性正相关，信息熵约等于不确定性的多少。香农给出的信息熵公式为 P(x)为x的概率分布。 信息熵的公式为什么取负数？因为概率小于1，小数求得的对数是负数，给整个公式加上负号，最终的结果为正。 下面举例说明信息熵公式为什么会用到log和概率。 猜中世界杯冠军需要多少次？足球世界杯共32个球队，给他们编号1-32号，第一次猜冠军是否在1-16号之中，如果对了就会接着猜是否在1-8号，如果错了就知道冠军在9-16号，第三次猜是否在9-12号，这样只需要5次就能猜中，log32 = 5。这里采用的是折半查找，所以取对数。 但实际情况不需要猜5次，因为球队有强弱，可以先把夺冠热门分一组，剩下的分一组，问冠军是否在热门组中，再继续这个过程，按照夺冠概率对剩下的球队分组。引入概率就会让查找数更少，也就是不确定性更小，信息熵更小。可以计算，当每支球队夺冠概率相等时（1/32），信息熵的结果为5。 条件墒：假定X和Y是两个随机变量，X是我们要了解的，已知X的随机分布P(X)，于是X的熵为： 假定我们还知道Y的一些情况，包括它和X一起出现的概率，即联合概率分布，以及在Y取不同值前提下X的概率分布，即条件概率分布，于是在Y条件下X的条件熵为： 可证明H(X|Y) &lt;H(X), 即引入相关信息后，不确定性下降了。 互信息信息之间的相关性如果度量呢？ 香农提出了用互信息度量两个随机事件的相关性。例如，“好闷热”和“要下雨了”的互信息很高。X与Y的互信息公式如下： 经过演算，可得到 只要有足够的语料库，P(x,y), P(x) 和P(y)是很容易计算的。 机器翻译中最难的两个问题之一是二义性，如Bush 既可以是总统布什，也可以是灌木丛，Kerry既可以是国务卿克里，也可以是小母牛。如何正确的翻译？一种思路是通过语法辨别，但效果不好； 另一种思路是用互信息，从大量文本中找出和总统布什一起出现的词语，如总统、美国、国会等，再用同样的方法找出和灌木丛一起出现的词，如土壤、植物等，有了这两组词，在翻译Bush时，看看上下文中哪类词更多就可以了。 相对熵/交叉熵相对熵（KL Divergence），衡量两个取值为正的函数的相似性: 结论： 两个完全相等的函数，相对熵为零； 相对熵越大，两个函数差异越大。 对于概率分布函数，或者概率密度函数，相对熵可以度量两个随机分布的差异性。 在自然语言处理中，常用相对熵计算两个常用词在不同文本中的概率分布，看他们是否同义；或者根据两篇文章中不同词的分布，衡量它们的内容是否相等。利用相对熵，可以得到信息检索中最重要的概念：词频率-逆向文档频率（TF-IDF），在后面的搜索章节会对它详细介绍。 搜索获取网页：网络爬虫把整个互联网看作一张大图，每个网页就是图中的一个节点，超链接是连接节点的弧。通过网络爬虫，用图的遍历算法，就能自动地访问到每个网页并把它们存起来。 网络爬虫是这样工作：假定从一家门户网站的首页出发，先下载这个网页，再通过这个网页分析出里面包含的所有超链接，接下来访问并下载这些超链接指向的网页。让计算机不同地做下去，就能下载整个互联网。 还需要用一个记事本（哈希表）记录下载了哪些网页避免重复下载。 工程实现问题： 遍历算法采用广度优先还是深度优先？搜索引擎要做到在有限的时间内，最多地爬下最重要的网页。显然各个网站最重要的是它的首页，那么就应该先下载所有网站的首页。如果把爬虫再扩大一点，就要继续下载首页直接链接的网页，因为这些网页是网站设计者自己认为相当重要的网页。在这个前提下，似乎应该采用广度优先。 但是还要考虑网络通信的“握手”问题。网络爬虫每次访问网站服务器时，都要通过“握手”建立连接（TCP协议），如果采用广度优先，每个网站先轮流下载所有首页，再回过头来下载第二级网页，这样就要频繁的访问网站，增加“握手”耗时。 实际的网络爬虫是由成百上千台服务器组成的分布式系统，由调度系统决定网页下载的顺序，对于某个网站，一般是由特定的一台或几台服务器专门下载，这些服务器先下载完一个网站再进入下一个网站，这样可以减少握手次数（深度优先）。具体到每个网站，采用广度优先，先下载首页，再下载首页直接链接的网页。 页面分析和超链接（URL）提取早期的网页都是直接用HTML书写，URL以文本的形式放在网页中，前后有明显标识，很容易提取出来。但现在很多网页都是用脚本语言（如JavaScript）生成，URL不是直接可见的文本，所以网络爬虫要模拟浏览器运行网页后才能得到隐含的URL，但很多网页的脚本写的不规范，很难解析，这就导致这样的网页无法被搜索引擎收录。 维护超链接哈希表在一台服务器上建立和维护一张哈希表并不是难事，但如果同时有成千上万台服务器一起下载网页，维护一张统一的哈希表就会遇到很多问题： 首先，这张哈希表会大到存不下来；其次，每台服务器下载前和下载后都要访问哈希表，于是哈希表服务器的通信就成了整个爬虫系统的瓶颈。解决办法是：明确分工，将某个区间的URL分给特定的几台服务器，避免所有服务器对同一个URL做判断；批量询问哈希表，减少通信次数，每次更新一大批哈希表的内容。 网页检索：布尔代数最简单的索引结构是用一个很长的二进制数表示一个关键字是否在每个网页中，有多少个网页就有多少位数，每一位对应一个网页，1代表相应的网页有这个关键字，0代表没有。比如关键字“原子能”对应的二进制数是0100 1000 1100 0001…表示（从左到右）第二、第五、第九、第十、第十六个网页包含这个关键字。假定关键字“应用”对应的二进制数是0010 1001 1000 0001…，那么要找到同时包含“原子能”和“应用”的网页时，只需要将这两个二进制数进行布尔AND运算，结果是0000 1000 0000 0001…表示第五和第十六个网页满足要求。 这个二进制数非常长，但是计算机做布尔运算非常快，现在最便宜的微机，在一个指令周期进行32位布尔运算，一秒钟十亿次以上。 为了保证对任何搜索都能提供相关网页，主要的搜索引擎都是对所有词进行索引，假如互联网上有100亿个有意义的网页，词汇表大小是30万，那么这个索引至少是100亿x30万=3000万亿。考虑到大多数的词只出现在一部分文本中，压缩比是100：1，也是30万亿的量级。为了网页排名方便，索引中还要存其他附加信息，如每个词出现的位置，次数等等。因此整个索引就变得非常大，需要通过分布式存储到不同服务器上（根据网页编号划分为很多小块，根据网页重要性建立重要索引和非重要索引）。 度量网页和查询的相关性：TF-IDF我们以查找包含“原子能的应用”网页举例，“原子能的应用”可以分成三个关键词：原子能、的、应用。凭直觉，我们认为包含这三个关键词较多的网页，比包含它们较少的网页相关。但这并不可取，因为这样的话，内容长的网页比内容短的网页占便宜，所以要根据网页长度对关键词的次数进行归一化，用关键词的次数，除以网页的总字数，这个商叫做“关键词的频率”或“单文本频率”（TF：Term Frequency）。比如，某个网页上有1000词，其中“原子能”“的”“应用”分别出现了2次、35次、5次，那么它们的词频就是0.002、0.035、0.005，将这三个数相加就是相应网页和查询“原子能的应用”的单文本频率。所以，度量网页和查询的相关性，一个简单的方法就是直接使用各个关键词在网页中出现的总频率。 但是这也有不准确的地方，例如上面的例子中，“的”占了总词频的80%以上，但是它对确定网页的主题几乎没什么用，我们叫这样的词为停止词（stop word），类似的还有“是”“和”等。 另外“应用”是很普通的词，而“原子能”是专业词，后者在相关性排名中比前者重要。因此需要给每个词给一个权重，权重的设定满足两个条件： 一个词预测主题的能力越强，权重就越大； 停止词权重为零。 在信息检索中，使用最多的是“逆文本频率指数”（IDF：Inverse Document Frequency），公式为 （D是全部网页数，Dw为关键词w出现的网页个数）。最终确定查询相关性，是利用TF和IDF的加权求和。 （IDF其实是在特定条件下关键词概率分布的交叉熵） 搜索结果页排序：Page Rank算法这是拉里·佩奇和谢尔盖·布林发明的计算网页自身质量的数学模型，google凭借该算法，使搜索的相关性有了质的飞跃，圆满解决了以往搜索页中排序不好的问题。该算法的核心思想为：如果一个网页被很多其他网页所链接，说明它收到普遍的承认和信赖，那么它的排名就高。当然，在具体应用中还要加上权重，给排名高的网页链接更高的权重。这里有一个怪圈，计算搜索结果网页排名过程中需要用到网页本身的排名，这不是“先有鸡还是先有蛋的问题”吗？ 谢尔盖·布林解决了这个问题，他把这个问题变成了一个二维矩阵问题，先假定所有网页排名相同（1/N），在根据这个初始值不断迭代排名，最后能收敛到真实排名。 新闻分类：余弦定理google有新闻频道，里面的内容是由计算机聚合、整理并分类各网站内容。以前门户网站的内容是由编辑在读懂之后，再根据主题分类。但是计算机根本读不懂新闻，它只会计算，所以要让计算机分类新闻，首先就要把文字变成可计算的数字，再设计一个算法来计算任意两篇新闻的相似性。 计算一篇新闻中所有实词的TF-IDF值，再把这些值按照对应的实词在词汇表的位置依次排列，就得到一个向量。例如词汇表中有64000个词，其编号和词如左下表所示，在某一篇新闻中，这64000个词的TF-IDF值如右下表所示，这64000个数就组成了一个64000维的向量，我们就用这个向量代表这篇新闻，成为这篇新闻的特征向量。每篇新闻都有一个特征向量，向量中的每个数代表对应的词对这篇新闻主题的贡献。 同一类的新闻，一定某些主题词用的较多，两篇相似的新闻，它们的特征向量一定在某几个纬度的值比较大。如果两个向量的方向一致，就说明新闻的用词比例基本一致，我们采用余弦定理计算两个向量间的夹角： 新闻分类算法分为有目标和无目标：第一种是已知一些新闻类别的特征向量，拿它分别和所有待分类的新闻计算余弦相似性，并分到对应的类别中，这些已知的新闻类别特征向量既可以手工建立，也可以自动建立； 第二种是没有分好类的特征向量做参考，它采用自底向上的聚类方法，计算所有新闻两两之间的余弦相似性，把相似性大于一个阈值的新闻分作一个小类，再比较各小类之间的余弦相似性，就这样不断待在聚合，一直到某一类因为太大而导致里面的新闻相似性很小时停止。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>统计算法</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优雅高效地使用windows]]></title>
    <url>%2F2018%2F02%2F26%2F%E4%BC%98%E9%9B%85%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8windows%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器！ 【日常工具】文件搜索-Listary：Windows文件浏览增强工具，double-ctrl调用，并且可以在(Xftp，clover 等等)窗口中使用，极其方便！-Wox：免费开源的效率启动器,不仅可以搜索文件还可以浏览网页，Alt+Space调用，以及定制的插件(wox-plugin)，堪比MAC上的 Alfred 视频播放-Potplayer：拥有强大的内置解码器，不用额外针对某类视频去下载了 下载神器-IDM(cracked version)： 全宇宙最快的下载器!唯一缺陷是不支持P2P，想下载磁力或者BT可以先用百度云网盘的离线功能，再通过油猴脚本抓取链接进行下载。-EagleGet：下载后自动安装Chrome扩展探测视频，缺陷也是不支持BT/磁力链接，方法同上-硕鼠：主要是下载网站的视频，不过现在不支持像腾讯视频之类的大网站了 PDF阅读-福昕阅读器：功能算比较齐全了(会占用端口4000)-ABBYY_FineReader：PDF转WORD 写作工具-Yu writer：windows上最好用的markdown工具！-Ditto：保存所有复制过的文字和图片，用ctrl+`调用-Pasteasy：全平台跨设备复制粘贴 素材库-OfficePLUS：微软Office官方在线模板网站！-Iconfont：阿里巴巴矢量图标库-Free Images - Pixabay-icons8：icon素材库-NASA IMAGE：NASA素材库-pixabay:高清免费图片素材库 PPT制作-Nordri Tools：超级好用的ppt插件-Photozoom pro：利用插值算法提高图片分辨率-PPT遥控器：代替遥控笔-FILEminimizer：ppt压缩神器-Screen to Gif: Gif制作软件-Tagul：文字云生成器 思维导图-Xmind：付费，全平台，模板多，支持鱼骨图、二维图、树形图等格式，可以与Evernote同步-Mindmaster：免费！全平台，素材丰富，与office兼容性好以及有免费的云空间-幕布：笔记一键生成思维导图 视频录制-OBS Studio：功能齐全的视频录制工具，直播必备-Adobe Premiere Pro CC：视频剪辑工具 文件整理-Q-dir：需要在文件夹之间移动文件的时候，这个整理神器就能派上用场了！-Clover 3：为资源管理器添加多标签页功能，可以将常用文件夹添加为书签-Goodsyne：强大的数据同步工具 快速启动-TrueLaunchBar：对快速启动项进行分组；允许你把任何文件夹组织成菜单的形式；实现剪切板管理、性能监视等功能-Wgestures：全局鼠标手势！ 【系统开发与优化工具】桌面优化-Fences：付费,桌面文件分类整理软件-Wallpapaer：动态壁纸软件，装逼神器！ 屏保-Fliqlo：数字时钟的屏幕保护，逼格满满-Flux： 视力保护，通过根据时间调节屏幕颜色，减少蓝光对视力的影响 系统管理-PowerTool：查看系统进程等信息，安全修复！-Dism++：简洁的系统管理软件，集成了很多小工具，还可以系统备份-Ccleaner：系统清理工具 文件修改-Bulk Rename Utility：批量重命名工具-remove empty directories：删除空文件夹，强迫症的福音 系统安装-清华大学开源软件镜像站：可以下载到很多Linux镜像以及python第三方库，速度很快！-Ultraiso：制作启动盘 网络-Fiddler：抓包工具 文本编辑器-Sublime Text3：文本神器-Atom：中文友好，渲染插件多，内存占用高 IDE-Pycharm：付费，python最优秀的IDE之一-Anaconda：集成了python科学计算的第三方库，版本管理较方便，内置Jupyter Notebook-IntelliJ IDEA：前端必备IDE-Cmder：monokai配色主题，完美代替原生cmd 【chrome插件】开发必备-Vimium(键盘浏览插件)-JSONView(json数据进行转码和格式化)-Proxy SwitchyOmega (代理)-Qiniu upload files (七牛图床插件)-Markdown Here (转化为markdown格式）-The QR Code Extension (二维码生成器) 日常管理-Extensity (扩展管理工具)-LastPass (密码管理器) 浏览优化-书签侧边栏-Google翻译-Imagus (悬停放大图片)-OneTab (内存优化神器)-Better History (查看历史记录)-UndocloseTab (恢复关闭网页)-Infinity (方便的新标签页定制)-CrxMouse Chrome Gestures (鼠标手势、超级拖拽)-Tampermonkey (油猴：脚本管理平台，神器！！)-IE Tab (打开用IE内核支持的网页，常用于银行支付环境)-Listen 1 (集成各大平台的音乐，再也不用为音乐版权的问题头疼了) 下载&amp;收藏-印象笔记裁剪-网页截图：注释&amp;录屏-RSS Subscription Extension-Eagleget Free Download]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text3 快捷键]]></title>
    <url>%2F2018%2F02%2F22%2Fsublime%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[选择类Ctrl + D 选择单文本 Alt + F3 选中文件所有相同文本 Ctrl + L 选中整行 Ctrl + shift + M 选中括号中文本（与搜狗有热键冲突） Ctrl + M 光标移动结束或开始位置 Ctrl + Enter 在下一行插入新行 Ctrl + Shift + Enter 在上一行插入新行 Ctrl + Shift + [ 选中，折叠代码 Ctrl + Shift + ] 选中，展开代码 Ctrl + K + 0 展开所有折叠代码 Ctrl + ←/→ 快速移动光标 shift + ↑/↓ 向上/向下选中多行 Shift + ←/→ 向左/向右选中文本 Ctrl + Shift + ←/→ 向左/向右快速选择文本 编辑类Ctrl + J 合并多行代码为一行 Ctrl + Shift + D 复制整行，插入到下一行 Tab 向右缩进 &amp; Shift + Tab 向左缩进 Ctrl + K + K 从光标处开始删除代码至行尾。 Ctrl + Shift + K 删除整行。 Ctrl + / 注释单行。 Ctrl + Shift + / 注释多行。 Ctrl + K + U/L 转换大/小写。 Ctrl + Z 撤销 Ctrl + Y 恢复撤销 Ctrl + F2 设置书签 Ctrl + T 左右字母互换。 F6 单词检测拼写 搜索类Ctrl + F 文件内搜索 Ctrl + shift + F 文件夹内搜索 Ctrl + P 按类别搜索。举个栗子：1、输入当前项目中的文件名；快速搜索文件，2、输入@和关键字，查找文件中函数名；3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。 Ctrl + G 数字定位搜索 Ctrl + R 函数定位搜索 Ctrl + ： 变量、属性名定位搜索 Ctrl + Shift + P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 显示类Ctrl + Tab 按浏览顺序切换窗口 Ctrl + PageDown 向左切换当前窗口的标签页 Ctrl + PageUp 向右切换当前窗口的标签页。 Alt + Shift + “1/2/3” 分屏 Ctrl + K + B 开启/关闭侧边栏。 F11 全屏模式 Shift + F11 免打扰模式]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>网页配置</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
</search>
