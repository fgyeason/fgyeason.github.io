<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据挖掘面试总结(一)]]></title>
    <url>%2F2018%2F06%2F25%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 数据结构栈栈是一种特殊的列表，栈内的元素只能通过列表的一端访问，这一端称为栈顶。咖啡厅内的一摞盘子是现实世界中常见的栈的例子。只能从最上面取盘子，盘子洗净后，也只能摞在这一摞盘子的最上面。栈被称为一种后入先出（LIFO，last-in-first-out）的数据结构。 由于栈具有后入先出的特点，所以任何不在栈顶的元素都无法访问。为了得到栈底的元素，必须先拿掉上面的元素。 对栈的两种主要操作是将一个元素压入栈和将一个元素弹出栈。入栈使用push()方法，出栈使用pop()方法。下图演示了入栈和出栈的过程。 另一个常用的操作是预览栈顶的元素。pop()方法虽然可以访问栈顶的元素，但是调用该方法后，栈顶元素也从栈中被永久性地删除了。peek()方法则只返回栈顶元素，而不删除它。 为了记录栈顶元素的位置，同时也为了标记哪里可以加入新元素，我们使用变量top，当向栈内压入元素时，该变量增大；从栈内弹出元素时，该变量减小。 队列队列是一种列表，不同的是队列只能在队尾插入元素，在队首删除元素。队列用于存储按顺序排列的数据，先进先出，这点和栈不一样，在栈中，最后入栈的元素反而被优先处理。可以将队列想象成在银行前排队的人群，排在最前面的人第一个办理业务，新来的人只能在后面排队，直到轮到他们为止。 队列是一种先进先出（First-In-First-Out，FIFO）的数据结构。队列被用在很多地方，比如提交操作系统执行的一系列进程、打印任务池等，一些仿真系统用队列来模拟银行或杂货店里排队的顾客。 队列的两种主要操作是：向队列中插入新元素和删除队列中的元素。插入操作也叫做入队，删除操作也叫做出队。入队操作在队尾插入新元素，出队操作删除队头的元素。 队列的另外一项重要操作是读取队头的元素。这个操作叫做peek()。该操作返回队头元素，但不把它从队列中删除。除了读取队头元素，我们还想知道队列中存储了多少元素，可以使用size()满足该需求。 链表链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。 使用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大。 算法查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找表（Search Table）：由同一类型的数据元素（或记录）构成的集合关键字（Key）：数据元素中某个数据项的值，又称为键值。主键（Primary Key）：可唯一地标识某个数据元素或记录的关键字。 查找表按照操作方式可分为： 静态查找表（Static Search Table）：只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找中同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 无序表查找算法分析：最好情况是在第一个位置就找到了，此为O(1)；最坏情况在最后一个位置才找到，此为O(n)；所以平均查找次数为(n+1)/2。最终时间复杂度为O(n)12345678910111213# 最基础的遍历无序列表的查找算法# 时间复杂度O(n)def sequential_search(lis, key): length = len(lis) for i in range(length): if lis[i] == key: return i else: return Falseif __name__ == &apos;__main__&apos;: LIST = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] result = sequential_search(LIST, 123) print(result) 有序表查找1. 二分查找(Binary Search)算法核心：在查找表中不断取中间元素与查找值进行比较，以二分之一的倍率进行表范围的缩小。1234567891011121314151617181920212223# 针对有序查找表的二分查找算法# 时间复杂度O(log(n))def binary_search(lis, key): low = 0 high = len(lis) - 1 time = 0 while low &lt; high: time += 1 mid = int((low + high) / 2) if key &lt; lis[mid]: high = mid - 1 elif key &gt; lis[mid]: low = mid + 1 else: # 打印折半的次数 print(&quot;times: %s&quot; % time) return mid print(&quot;times: %s&quot; % time) return Falseif __name__ == &apos;__main__&apos;: LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 99) print(result) 2. 插值查找二分查找法虽然已经很不错了，但还有可以优化的地方。有的时候，对半过滤还不够狠，要是每次都排除十分之九的数据岂不是更好？选择这个值就是关键问题，插值的意义就是：以更快的速度进行缩减。 插值的核心就是使用公式：value = (key – list[low])/(list[high] – list[low]) 用这个value来代替二分查找中的1/2。上面的代码可以直接使用，只需要改一句。12345678910111213141516171819202122232425# 插值查找算法# 时间复杂度O(log(n))def binary_search(lis, key): low = 0 high = len(lis) - 1 time = 0 while low &lt; high: time += 1 # 计算mid值是插值算法的核心代码 mid = low + int((high - low) * (key - lis[low])/(lis[high] - lis[low])) print(&quot;mid=%s, low=%s, high=%s&quot; % (mid, low, high)) if key &lt; lis[mid]: high = mid - 1 elif key &gt; lis[mid]: low = mid + 1 else: # 打印查找的次数 print(&quot;times: %s&quot; % time) return mid print(&quot;times: %s&quot; % time) return Falseif __name__ == &apos;__main__&apos;: LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 444) print(result) 插值算法的总体时间复杂度仍然属于O(log(n))级别的。其优点是，对于表内数据量较大，且关键字分布比较均匀的查找表，使用插值算法的平均性能比二分查找要好得多。反之，对于分布极端不均匀的数据，则不适合使用插值算法。 线性索引查找对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。索引就是把一个关键字与它相对应的记录进行关联的过程。一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。索引按照结构可以分为：线性索引、树形索引和多级索引。线性索引：将索引项的集合通过线性结构来组织，也叫索引表。线性索引可分为：稠密索引、分块索引和倒排索引 稠密索引稠密索引指的是在线性索引中，为数据集合中的每个记录都建立一个索引项。这其实就相当于给无序的集合，建立了一张有序的线性表。其索引项一定是按照关键码进行有序的排列。这也相当于把查找过程中需要的排序工作给提前做了。 分块索引给大量的无序数据集合进行分块处理，使得块内无序，块与块之间有序。这其实是有序查找和无序查找的一种中间状态或者说妥协状态。因为数据量过大，建立完整的稠密索引耗时耗力，占用资源过多；但如果不做任何排序或者索引，那么遍历的查找也无法接受，只能折中，做一定程度的排序或索引。分块索引的效率比遍历查找的O(n)要高一些，但与二分查找的O(logn)还是要差不少。 倒排索引不是由记录来确定属性值，而是由属性值来确定记录的位置，这种被称为倒排索引。其中记录号表存储具有相同次关键字的所有记录的地址或引用（可以是指向记录的指针或该记录的主关键字）。倒排索引是最基础的搜索引擎索引技术。 二叉树遍历 二叉树是有限个元素的集合，该集合或者为空、或者有一个称为根节点（root）的元素及两个互不相交的、分别被称为左子树和右子树的二叉树组成。 二叉树的每个结点至多只有二棵子树(不存在度大于2的结点)，二叉树的子树有左右之分，次序不能颠倒。 二叉树的第i层至多有2^{i-1}个结点 深度为k的二叉树至多有2^k-1个结点； 对任何一棵二叉树T，如果其终端结点数为N0，度为2的结点数为N2，则N0=N2+1 首先构建二叉树：12345class Node: def __init__(self,value=None,left=None,right=None) self.value=value self.left=left #左子树 self.right=right #右子树 下面给出二叉树的前序遍历／中序遍历／后序遍历1234567891011121314151617181920212223242526272829def preTraverse(root): &apos;&apos;&apos; 前序遍历 &apos;&apos;&apos; if root==None: return print(root.value) preTraverse(root.left) preTraverse(root.right) def midTraverse(root): &apos;&apos;&apos; 中序遍历 &apos;&apos;&apos; if root==None: return midTraverse(root.left) print(root.value) midTraverse(root.right) def afterTraverse(root): &apos;&apos;&apos; 后序遍历 &apos;&apos;&apos; if root==None: return afterTraverse(root.left) afterTraverse(root.right) print(root.value) 下面给出一个例子，验证一下程序 1234567891011if __name__==&apos;__main__&apos;: root=Node(&apos;D&apos;,Node(&apos;B&apos;,Node(&apos;A&apos;),Node(&apos;C&apos;)),Node(&apos;E&apos;,right=Node(&apos;G&apos;,Node(&apos;F&apos;)))) print(&apos;前序遍历：&apos;) preTraverse(root) print(&apos;\n&apos;) print(&apos;中序遍历：&apos;) midTraverse(root) print(&apos;\n&apos;) print(&apos;后序遍历：&apos;) afterTraverse(root) print(&apos;\n&apos;) 散列表（哈希表）散列函数的构造方法好的散列函数：计算简单、散列地址分布均匀 直接定址法例如取关键字的某个线性函数为散列函数：f(key) = a*key + b (a,b为常数） 数字分析法抽取关键字里的数字，根据数字的特点进行地址分配 平方取中法将关键字的数字求平方，再截取部分 折叠法将关键字的数字分割后分别计算，再合并计算，一种玩弄数字的手段。 除留余数法最为常见的方法之一。对于表长为m的数据集合，散列公式为：f(key) = key mod p (p&lt;=m)mod：取模（求余数）该方法最关键的是p的选择，而且数据量较大的时候，冲突是必然的。一般会选择接近m的质数。 随机数法选择一个随机数，取关键字的随机函数值为它的散列地址。f(key) = random(key) 总结，实际情况下根据不同的数据特性采用不同的散列方法，考虑下面一些主要问题： 计算散列地址所需的时间 关键字的长度 散列表的大小 关键字的分布情况 记录查找的频率 处理散列冲突 开放定址法 就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 公式是：这种简单的冲突解决办法被称为线性探测，无非就是自家的坑被占了，就逐个拜访后面的坑，有空的就进，也不管这个坑是不是后面有人预定了的。线性探测带来的最大问题就是冲突的堆积，你把别人预定的坑占了，别人也就要像你一样去找坑。 改进的办法有二次方探测法和随机数探测法。 再散列函数法发生冲突时就换一个散列函数计算，总会有一个可以把冲突解决掉，它能够使得关键字不产生聚集，但相应地增加了计算的时间。 链接地址法碰到冲突时，不更换地址，而是将所有关键字为同义词的记录存储在一个链表里，在散列表中只存储同义词子表的头指针，如下图： 这样的好处是，不怕冲突多；缺点是降低了散列结构的随机存储性能。本质是用单链表结构辅助散列结构的不足。 公共溢出区法其实就是为所有的冲突，额外开辟一块存储空间。如果相对基本表而言，冲突的数据很少的时候，使用这种方法比较合适。 python 数据结构刷题 手推LR和SVMLR SVM 类别不平衡问题SMOTE算法： 对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集Smin中所有样本的距离，得到其k近邻。 根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn。 对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本 x_{new}=x+rand(0,1)*|x-xn|xgboost调参XGBoost的参数XGBoost的作者把所有的参数分成了三类： 通用参数：宏观函数控制。 Booster参数：控制每一步的booster(tree/regression)。 学习目标参数：控制训练目标的表现。 在这里我会类比GBM来讲解，所以作为一种基础知识，强烈推荐先阅读这篇文章。 通用参数这些参数用来控制XGBoost的宏观功能。 booster[默认gbtree] 选择每次迭代的模型，有两种选择：gbtree：基于树的模型gbliner：线性模型 silent[默认0] 当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。 nthread[默认值为最大可能的线程数] 这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。 booster参数尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。 eta[默认0.3] 和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。 min_child_weight[默认1] 决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小_样本权重的和_，而GBM参数是最小_样本总数_。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 max_depth[默认6] 和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10 max_leaf_nodes 树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。 gamma[默认0] 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 max_delta_step[默认0] 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample[默认1] 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 colsample_bytree[默认1] 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1 colsample_bylevel[默认1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1] 权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight[默认1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 学习目标参数这个参数用来控制理想的优化目标和每一步结果的度量方法。 objective[默认reg:linear] 这个参数定义需要被最小化的损失函数。最常用的值有： binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。 eval_metric[默认值取决于objective参数的取值] 对于有效数据的度量方法。 对于回归问题，默认值是rmse，对于分类问题，默认值是error。 典型值有： rmse 均方根误差(\(\sqrt \frac{\sum_{i=1}^N \epsilon^2}{N}\) mae 平均绝对误差(\(\frac{\sum_{i=1}^N |\epsilon|}{N}\) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) merror 多分类错误率 mlogloss 多分类logloss损失函数 auc 曲线下面积 seed(默认0) 随机数的种子 设置它可以复现随机数据的结果，也可以用于调整参数 如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是： 1、eta -&gt; learning_rate2、lambda -&gt; reg_lambda3、alpha -&gt; reg_alpha 参数调优的一般方法我们会使用和GBM中相似的方法。需要进行如下步骤： 选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。 xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。 降低学习速率，确定理想参数。 模型的评价指标分类任务 查准率 查全率 ROC 曲线：比较 AUC 的大小，AUC 是研究机器学习模型泛化性能的工具，横轴{假正例率，FP/(FP+TN)}，纵轴{真正例率，TP/(TP+FN)} 混淆矩阵 回归模型的评估指标均方差回归模型中用得最多的度量指标是均方差（mean square error） \frac{1}{n}\sum{(y_i - \hat{y}_i)}^2灵敏度和特异度灵敏度（Sensitivity），跟真正例率的定义一样，指真正例中被模型发现的比率，所以灵敏度又可以称为召回率。 特异度（specificity），指真负例中被模型发现的比率，也可以看做是负样本的召回率。 R-Squared在线性回归以及广义线性回归中，R-squared 误差的大小意味着模型的拟合度的好坏。R-squared 误差取值范围为 0 到 1，这个值越接近 1 说明模型的拟合度越好。 TSS：Total Square Sum / 总离差平方和 RSS：Residual Square Sum / 残差平方和 ESS：Explain Square Sum / 解释平方和 R-Squared = 1 - RSS / TSS R-Squared = 1 - \frac{\sum(y_i-\hat{y_i})^2}{\sum{(y_i-\bar{y_i})}^2} Bagging 和 Boosting 的区别 Bagging：处理过拟合（方差）；分类器之间相互独立；关注方差（注意数据扰动带来的影响） Boosting：分类器序列相关；降低方差和偏差，关注降低偏差 比较 LR 与 SVM 的区别 LR 是一种概率模型的手段，SVM 试图找到一个超平面 参数估计的方法：LR（最大似然估计法）；SVM（拉格朗日乘子法） SVM 的泛化性能更好，受异常点的影响比较小 LR 在不平衡数据集上的表现优于 SVM 防止过拟合的手段 早停止，如果模型的性能没有提高则停止训练 增大数据量 正则化 交叉验证：留一、K 折 特征选择、降维 dropout pandas总结导入数据12345678910111213141516171819202122* pd.read_table(filename)：从限定分隔符的文本文件导入数据* pd.read_excel(filename)：从Excel文件导入数据* pd.read_sql(query, connection_object)：从SQL表/库导入数据* pd.read_json(json_string)：从JSON格式的字符串导入数据* pd.read_html(url)：解析URL、字符串或者HTML文件，抽取其中的tables表格* pd.read_clipboard()：从你的粘贴板获取内容，并传给read_table()* pd.DataFrame(dict)：从字典对象导入数据，Key是列名，Value是数据df = pd.read_csv(path=&apos;file.csv&apos;)参数：header=None 用默认列名，0，1，2，3... names=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;...] 自定义列名 index_col=&apos;A&apos;|[&apos;A&apos;, &apos;B&apos;...] 给索引列指定名称，如果是多重索引，可以传list skiprows=[0,1,2] 需要跳过的行号，从文件头0开始，skip_footer从文件尾开始 nrows=N 需要读取的行数，前N行 chunksize=M 返回迭代类型TextFileReader，每M条迭代一次，数据占用较大内存时使用 sep=&apos;:&apos;数据分隔默认是&apos;,&apos;，根据文件选择合适的分隔符，如果不指定参数，会自动解析 skip_blank_lines=False 默认为True，跳过空行，如果选择不跳过，会填充NaN converters=&#123;&apos;col1&apos;, func&#125; 对选定列使用函数func转换，通常表示编号的列会使用（避免转换成int）dfjs = pd.read_json(&apos;file.json&apos;) 可以传入json格式字符串dfex = pd.read_excel(&apos;file.xls&apos;, sheetname=[0,1..]) 读取多个sheet页，返回多个df的字典 导出数据1234* df.to_csv(filename)：导出数据到CSV文件* df.to_excel(filename)：导出数据到Excel文件* df.to_sql(table_name, connection_object)：导出数据到SQL表* df.to_json(filename)：以Json格式导出数据到文本文件 创建测试对象123* pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象* pd.Series(my_list)：从可迭代对象my_list创建一个Series对象* df.index = pd.date_range(&apos;1900/1/30&apos;, periods=df.shape[0])：增加一个日期索引 查看、检查数据1234567* df.head(n)：查看DataFrame对象的前n行* df.tail(n)：查看DataFrame对象的最后n行* df.shape()：查看行数和列数* df.info()：查看索引、数据类型和内存信息* df.describe()：查看数值型列的汇总统计* s.value_counts(dropna=False)：查看Series对象的唯一值和计数* df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数 数据选取1234567891011121314151617181920df.columns 列名，返回Index类型的列的集合df.index 索引名，返回Index类型的索引的集合df.values 值的二维数组，以numpy.ndarray对象返回df.index DataFrame的索引，索引不可以直接赋值修改df.reindex(index=[&apos;row1&apos;, &apos;row2&apos;,...] columns=[&apos;col1&apos;, &apos;col2&apos;,...]) 根据新索引重新排序df[m:n] 切片，选取m~n-1行df[df[&apos;col1&apos;] &gt; 1] 选取满足条件的行df.query(&apos;col1 &gt; 1&apos;) 选取满足条件的行df.query(&apos;col1==[v1,v2,...]&apos;) df.ix[:,&apos;col1&apos;] 选取某一列df.ix[&apos;row1&apos;, &apos;col2&apos;] 选取某一元素df.ix[:,:&apos;col2&apos;] 切片选取某一列之前（包括col2）的所有列df.loc[m:n] 获取从m~n行（推荐）df.iloc[m:n] 获取从m~n-1行df.loc[m:n-1,&apos;col1&apos;:&apos;coln&apos;] 获取从m~n行的col1~coln列sr=df[&apos;col&apos;] 取某一列，返回Seriessr.values Series的值，以numpy.ndarray对象返回sr.index Series的索引，以index对象返回 数据清理1234567891011121314151617181920212223242526272829303132333435363738394041* df.columns = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;]：重命名列名* pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组* pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组* s.astype(float)：将Series中的数据类型更改为float类型* s.replace(1,&apos;one&apos;)：用‘one’代替所有等于1的值* s.replace([1,3],[&apos;one&apos;,&apos;three&apos;])：用&apos;one&apos;代替1，用&apos;three&apos;代替3* df.rename(columns=lambda x: x + 1)：批量更改列名* df.rename(columns=&#123;&apos;old_name&apos;: &apos;new_ name&apos;&#125;)：选择性更改列名* df.rename(index=&#123;&apos;row1&apos;:&apos;A&apos;&#125;, 重命名索引名和列名 columns=&#123;&apos;col1&apos;:&apos;A1&apos;&#125;) * df.set_index(&apos;column_one&apos;)：更改索引列* df.rename(index=lambda x: x + 1)：批量重命名索引df.duplicated() 返回各行是否是上一行的重复行df.drop_duplicates() 删除重复行，如果需要按照列过滤，参数选填[&apos;col1&apos;, &apos;col2&apos;,...]df.fillna(0) 用实数0填充nadf.dropna() axis=0|1 0-index 1-column how=&apos;all&apos;|&apos;any&apos; all-全部是NA才删 any-只要有NA就全删del df[&apos;col1&apos;] 直接删除某一列 df.drop([&apos;col1&apos;,...], aixs=1) 删除指定列，也可以删除行 def get_digits(str): m = re.match(r&apos;(\d+(\.\d+)?)&apos;, str.decode(&apos;utf-8&apos;)) if m is not None: return float(m.groups()[0]) else: return 0df.apply(get_digits) DataFrame.apply，只获取小数部分，可以选定某一列或行df[&apos;col1&apos;].map(func) Series.map，只对列进行函数转换df1.combine_first(df2) 用df2的数据补充df1的缺省值NaN，如果df2有更多行，也一并补上df.stack() 列旋转成行，也就是列名变为索引名，原索引变成多层索引，结果是具有多层索引的Series，实际上是把数据集拉长df.unstack() 将含有多层索引的Series转换为DataFrame，实际上是把数据集压扁，如果某一列具有较少类别，那么把这些类别拉出来作为列df.pivot() 实际上是unstack的应用，把数据集压扁pd.get_dummies(df[&apos;col1&apos;], prefix=&apos;key&apos;) 某列含有有限个值，且这些值一般是字符串，例如国家，借鉴位图的思想，可以把k个国家这一列量化成k列，每列用0、1表示 数据处理：Filter、Sort和GroupBy1234567891011* df[df[col] &gt; 0.5]：选择col列的值大于0.5的行* df.sort_values(col1)：按照列col1排序数据，默认升序排列* df.sort_values(col2, ascending=False)：按照列col1降序排列数据* df.sort_values([col1,col2], ascending=[True,False])：先按列col1升序排列，后按col2降序排列数据* df.groupby(col)：返回一个按列col进行分组的Groupby对象* df.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象* df.groupby(col1)[col2]：返回按列col1进行分组后，列col2的均值* df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表* df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值* data.apply(np.mean)：对DataFrame中的每一列应用函数np.mean* data.apply(np.max,axis=1)：对DataFrame中的每一行应用函数np.max 数据运算和排序123456789101112df.T DataFrame转置df1 + df2 按照索引和列相加，得到并集，NaN填充df1.add(df2, fill_value=0) 用其他值填充df1.add/sub//mul/div 四则运算的方法df - sr DataFrame的所有行同时减去Seriesdf * N 所有元素乘以Ndf.add(sr, axis=0) DataFrame的所有列同时减去Seriessr.order() Series升序排列df.sort_index(aixs=0, ascending=True) 按行索引升序df.sort_index(by=[&apos;col1&apos;, &apos;col2&apos;...]) 按指定列优先排序df.rank() 计算排名rank值 数据合并123456789101112* df1.append(df2)：将df2中的行添加到df1的尾部* df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部* df1.join(df2,on=col1,how=&apos;inner&apos;)：对df1的列和df2的列执行SQL形式的joinpd.merge(df1, df2, on=&apos;col1&apos;, how=&apos;inner&apos;，sort=True) 合并两个DataFrame，按照共有的某列做内连接（交集），outter为外连接（并集），结果排序pd.merge(df1, df2, left_on=&apos;col1&apos;, right_on=&apos;col2&apos;) df1 df2没有公共列名，所以合并需指定两边的参考列pd.concat([sr1, sr2, sr3,...], axis=0) 多个Series堆叠成多行，结果仍然是一个Seriespd.concat([sr1, sr2, sr3,...], axis=1) 多个Series组合成多行多列，结果是一个DataFrame，索引取并集，没有交集的位置填入缺省值NaN 数据统计123456789101112131415161718192021222324252627282930313233343536sr.unique Series去重sr.value_counts() Series统计频率，并从大到小排序，DataFrame没有这个方法sr.describe() 返回基本统计量和分位数df.describe() 按各列返回基本统计量和分位数df.count() 求非NA值得数量df.max() 求最大值df.min() 求最大值df.sum(axis=0) 按各列求和df.mean() 按各列求平均值df.median() 求中位数df.var() 求方差df.std() 求标准差df.mad() 根据平均值计算平均绝对利差df.cumsum() 求累计和sr1.corr(sr2) 求相关系数df.cov() 求协方差矩阵df1.corrwith(df2) 求相关系数pd.cut(array1, bins) 求一维数据的区间分布pd.qcut(array1, 4) 按指定分位数进行区间划分，4可以替换成自定义的分位数列表 df[&apos;col1&apos;].groupby(df[&apos;col2&apos;]) 列1按照列2分组，即列2作为keydf.groupby(&apos;col1&apos;) DataFrame按照列1分组grouped.aggreagte(func) 分组后根据传入函数来聚合grouped.aggregate([f1, f2,...]) 根据多个函数聚合，表现成多列，函数名为列名grouped.aggregate([(&apos;f1_name&apos;, f1), (&apos;f2_name&apos;, f2)]) 重命名聚合后的列名grouped.aggregate(&#123;&apos;col1&apos;:f1, &apos;col2&apos;:f2,...&#125;) 对不同的列应用不同函数的聚合，函数也可以是多个df.pivot_table([&apos;col1&apos;, &apos;col2&apos;], rows=[&apos;row1&apos;, &apos;row2&apos;], aggfunc=[np.mean, np.sum] fill_value=0, margins=True) 根据row1, row2对col1， col2做分组聚合，聚合方法可以指定多种，并用指定值替换缺省值pd.crosstab(df[&apos;col1&apos;], df[&apos;col2&apos;]) 交叉表，计算分组的频率 常用的机器学习工具tensorflow https://github.com/tensorflow/tensorflow pytorch https://github.com/pytorch/pytorch scikit-learn https://github.com/scikit-learn/scikit-learn xgboost https://github.com/dmlc/xgboost lightgbm https://github.com/Microsoft/LightGBM catboost https://github.com/catboost/catboost gcForest https://github.com/kingfengji/gcForest libfm https://github.com/srendle/libfm libffm https://github.com/guestwalk/libffm xlearn https://github.com/aksnzhy/xlearn libsvm https://github.com/cjlin1/libsvm liblinear https://github.com/cjlin1/liblinear h2o https://github.com/h2oai/h2o-3]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Hbase]]></title>
    <url>%2F2018%2F06%2F25%2F%E6%B5%85%E8%B0%88Hbase%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 目录 概述 HBase访问借口 HBase数据模型 HBase实现原理 HBase运行机制 HBase应用方案 概述简介HBase是一个高可靠、高性能、面向咧、可伸缩的分布式数据库。是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。 为什么要HBase？ 受限于Hadoop MR编程框架的高延迟数据处理机制，无法满足大规模数据实时处理的需求。 HDFS不能随机访问 传统关系型数据无法应对剧增的海量数据 传统关系型数据库在数据结构变化时需要停机维护；孔裂浪费存储空间 因此，业界出现了一类面相半结构化数据存储和处理的高可扩展、低写入、查询延迟的系统。如键值数据库、文档数据库和列族数据库（如BitTable和HBase）。如今HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中。 HBase与传统关系数据库的对比 特性 传统关系数据库 HBase 数据类型 关系模式，具有丰富的数据类型后和存储方式 采用更简洁的数据模型，吧数据存储为未经解释的字符串 数据操作 包含丰富的操作，涉及复杂的夺标链接 不存在复杂的表于表之间的关系，只有简单的增、删、查、清空等，避免表于表间复杂关系 存储模式 基于行存储 基于列存储，每个列族有几个文件保存，不同列族的文件是分离的 数据索引 通过针对不同列构建复杂多个索引以提高访问性能 只有一个索引（行键），访问方法为或行键访问或行键扫面，通过巧妙的设计，速度不会慢下来 数据维护 更新操作用最新的数据覆盖旧的 更新操作生成一个新的版本，久的版本仍然保留 可伸缩性 很难实现横向拓展，纵向拓展空间也有限 可以轻易通过在集群中增加或减少硬件数量实现性能伸缩 HBase访问接口 HBase数据模型 数据模型概述 数据模型相关概念 概念视图 物理视图 面向列的存储 数据模型概述 HBase是一个稀疏、多维度、排序的映射表，这张表的索引时行键、列族、列限定符、、时间戳。 每一个值是一个未经解释的字符串，没有数据类型。 每一行都有一个可排序的行键和任意多的列 表在水平方向由一个或者多个列族组成，一个列族可以包含任意多个列，同一个列族里面的数据存储在一起 列族支持动态扩展，轻松的添加列族或列，无预先定义列的数量和类型。所有列均以字符串形式存储，用户需自行进行数据类型转换。 HBase中执行更新操作时，生成新版本，保留旧版本，查询时默认返回最新版本。创建时可以设置最多保留版本数量。 数据模型相关概念 表：HBase采用表来组织数据，表由行和列组成，列换分为若干个列族，避免夺标链接操作，追求分析效率。 行：每个HBAse表由若干行组成，每个行由行键（Row Key）来标示。不给行键所在的列进行命名，让其拥有纵向可拓展性。 列族：一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元，也是基本存储单元。 列限定符：相当于列名 单元格：在HBaae表中，通过行、列族和列限定符确定一个“单元格”，（时间戳定义其版本）单元格中存储的数据没有数据类型，总被视为字节数组byte[] HBase中需要根据行键、列族、列限定符和时间戳确定一个单元格，因此是一个“四维”坐标。💡：上文中提到HBase数据只有一个索引（行键）， 概念视图 物理视图 面向列的存储 💡不同存储模型优劣对比 面向行存储的数据库主要采用NSM（N-ary Storage Model）存储模型，即一个元组（行）会被连续存储在磁盘页中，数据是一行行进行存储，读取也是一行行进行读取。当要选取某属性进行分析时，也需要首先扫面完整元组内容。 优点：适用于联机事务性数据处理，即将分布于不同地理位置的数据利用网络进行连接，进而进行统一的存储和管理。 缺点：鄙视和分析性操作 面向列存储的数据库主要采用DSM（Decompostion Storage Model）存储模型，该模型会对关系进行垂直分解，并为每个属性分配一个子关系，每个子关系单独存储。 优点：在批处理和即兴查询等分析操作中能够直接定位目标列，能够有有效I/O开销；同一列数据类型相同，存储过程能够拥有很高的数据压缩旅，从而节省存储空间。 缺点：执行连续操作时要付出昂贵的元组重构代价。 💡 总结：NSM存储模型更加适合事务型应用，DSM存储模型更加适合分析性应用 HBase实现原理HBase功能组件 库函数 一个Master主服务器 许多个Region服务器 Master负责管理和维护Hbase表的分区信息，维护Region服务器列表，分配Region，负责均衡，和Namenode功能类似。 Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求，和Datanode功能类似。 客户端并不是直接从Master主服务器读取数据，而是在获得Region的存储位置后，直接从Region服务器上读取数据。 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计是的Master负载很小。 表和Region 一个表包含多个Region 开始只有一个Region，后来不断分裂 Regin拆分操作非常快（开始只是修改文件指向），接近瞬间。因为拆分之后的Region读取的仍然是原存储文件，知道“合并”过程吧存储文件异步写到独立的文件之后，才会读取新文件。 每个Region默认大小是100MB到200MB（2006之前） 每个Region的最佳大小取决于丹台服曲奇的有效处理能力 目前每个Region的最佳大小建议1G～2G（2013以后） 每个Region不会被分拆到多个Region服务器（Region最小不可分） 每个Region服务器存储10～1000个Region Region的定位 元数据表，又名META表，存储了Region和Region服务器的映射关系。 当HBase表很大时，META表也会被分裂成多个Region 根数据表，有明ROOT表，记录所有元数据的具体位置 ROOT表只有唯一一个Region，名字是在程序中被写死的 Zookeeper文件记录了ROOT表的位置 为了加快访问速度，META表的全部Region都会被保存在内存中 假设META表的每行（一个映射条目）在内存中大约占用1KB，每个Region限制为128MB，那么上面三层结构可以曹村的用户数据表的Region数目的计算方法是： ROOT表能够寻址META表的Region个数 X 每个META表能寻址的个数 一个ROOT表最多只能有一个Region大小，也就是最多只能有128MB，按照每行占用1KB内存计算，128MB空间可以容纳128MB÷1kb=217128MB÷1kb=217。也就是一个ROOT可以寻址217217个.META表的Region。 同理每个META表的Region可以存之的用户数据表的Region个数也是217217个 客户访问数据时的“三级寻址”： 为了加速寻址，客户端会缓存位置信息，同时需要解决缓存失效的问题 寻址过程客户端需要询问Zookeeper服务器，不需要链接Master服务器。 HBase运行机制 HBase系统架构 Region服务器工作原理 Store工作原理 HLog工作原理 HBase系统架构 客户端客户端包含访问HBase的接口,同时在缓存中维护者已经好访问过的Region位置信息,用来加快后续数据访问的过程 Zookeeper服务器Zookeeper可以帮助选出一个Master作为集群的总管,并保证在任何时刻总有唯一一个Master在运行,这就避免了Master“单点失效”问题. 💡Zookeeper是一个很好的集群管理工具,被大量用于分布式计算,提供配置维护、域名服务、分布式同步、组服务等 Master服务器: 主服务器主要负责和Region的管理工作 管理用户对表的增删改查 实现不同Region服务器之间的负载均衡 在Region分裂或合并后,负责重新调整Region的分布 对发生故障时晓得Region服务器上的Region进行迁移 Region服务器: Region服务器时HBase最核心的模块,负责维护分配给自己的Region,并响应用户的读写请求 Region服务器的工作原理1.用户读写数据过程 用户写入数据时，被分配到相应Region服务器去执行 用户数据首先被写入到MemStore和Hlog中 只有当操作写入Hlog之后，commit()调用才会将其返回给客户端 当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找 2.缓存的刷新 系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记 每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件 每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务 3.StoreFile的合并 每次刷写都生成一个新的StoreFile，数量太多，影响查找速度 调用Store.compact()把多个合并成一个 合并操作比较耗费资源，只有数量达到一个阈值才启动合并 💡这么做是为了尽量一次性刷到磁盘,以此提高速度.但是如果StoreFile数量太多影响查找速度 Store工作原理 Store是Region服务器的核心 多个StoreFile合并成一个 单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region HLog工作原理 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复 HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log） 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘 Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器 Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志 应用方案 HBase实际应用中的性能优化方法 HBase性能监视 在HBase之上构建SQL引擎 构建HBase二级索引 HBase实际应用中的性能优化方法行键行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。InMemory创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。Max Version创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。Time to Live创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 _24 _60 * 60)。 HBase性能监视 Master-status(自带) Ganglia OpenTSDB Ambari Master-statusHBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面可以查看HBase集群的当前状态GangliaGanglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能OpenTSDBOpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等AmbariAmbari 的作用就是创建、管理、监视 Hadoop 的集群 (推荐HDP CDH) 在HBase之上构建SQL引擎NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因： 1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。2.减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。 方案：1.Hive整合HBase2.Phoenix HIVE和HBASE区别 两者分别是什么？Apache Hive是一个构建在Hadoop基础设施之上的数据仓库。通过Hive可以使用HQL语言查询存放在HDFS上的数据。HQL是一种类SQL语言，这种语言最终被转化为Map/Reduce. 虽然Hive提供了SQL查询功能，但是Hive不能够进行交互查询—因为它只能够在Haoop上批量的执行Hadoop。Apache HBase是一种Key/Value系统，它运行在HDFS之上。和Hive不一样，Hbase的能够在它的数据库上实时运行，而不是运行MapReduce任务。Hive被分区为表格，表格又被进一步分割为列簇。列簇必须使用schema定义，列簇将某一类型列集合起来（列不要求schema定义）。例如，“message”列簇可能包含：“to”, ”from” “date”, “subject”, 和”body”. 每一个 key/value对在Hbase中被定义为一个cell，每一个key由row-key，列簇、列和时间戳。在Hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。Hbase利用Hadoop的基础设施，可以利用通用的设备进行水平的扩展。 两者的特点Hive帮助熟悉SQL的人运行MapReduce任务。因为它是JDBC兼容的，同时，它也能够和现存的SQL工具整合在一起。运行Hive查询会花费很长时间，因为它会默认遍历表中所有的数据。虽然有这样的缺点，一次遍历的数据量可以通过Hive的分区机制来控制。分区允许在数据集上运行过滤查询，这些数据集存储在不同的文件夹内，查询的时候只遍历指定文件夹（分区）中的数据。这种机制可以用来，例如，只处理在某一个时间范围内的文件，只要这些文件名中包括了时间格式。HBase通过存储key/value来工作。它支持四种主要的操作：增加或者更新行，查看一个范围内的cell，获取指定的行，删除指定的行、列或者是列的版本。版本信息用来获取历史数据（每一行的历史数据可以被删除，然后通过Hbase compactions就可以释放出空间）。虽然HBase包括表格，但是schema仅仅被表格和列簇所要求，列不需要schema。Hbase的表格包括增加/计数功能。 限制Hive目前不支持更新操作。另外，由于hive在hadoop上运行批量操作，它需要花费很长的时间，通常是几分钟到几个小时才可以获取到查询的结果。Hive必须提供预先定义好的schema将文件和目录映射到列，并且Hive与ACID不兼容。HBase查询是通过特定的语言来编写的，这种语言需要重新学习。类SQL的功能可以通过Apache Phonenix实现，但这是以必须提供schema为代价的。另外，Hbase也并不是兼容所有的ACID特性，虽然它支持某些特性。最后但不是最重要的—为了运行Hbase，Zookeeper是必须的，zookeeper是一个用来进行分布式协调的服务，这些服务包括配置服务，维护元信息和命名空间服务。 应用场景Hive适合用来对一段时间内的数据进行分析查询，例如，用来计算趋势或者网站的日志。Hive不应该用来进行实时的查询。因为它需要很长时间才可以返回结果。Hbase非常适合用来进行大数据的实时查询。Facebook用Hbase进行消息和实时的分析。它也可以用来统计Facebook的连接数。 总结Hive和Hbase是两种基于Hadoop的不同技术—Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hbase</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式]]></title>
    <url>%2F2018%2F06%2F24%2F%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则在线检测: http://www.regexpal.com/ 正则表达式.png 一、校验数字的表达式1.数字：^[0-9]*$2.n位的数字：^\d{n}$3.至少n位的数字：^\d{n,}$4.m-n位的数字：^\d{m,n}$ 12345671. \d 数字:[0-9]2. &#123;m,n&#125; 匹配前一个字符m至n次 &#123;，n&#125; 匹配前一个字符0至n次 &#123;m,&#125; 匹配前一个字符m至无穷次3. * 匹配前一个字符0或无限次4. ^ 匹配字符串开头 $ 匹配字符串结尾 5.零和非零开头的数字：^(0|[1-9][0-9]*)$ 11\. 左右表达式任意一个 6.非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]{1,2})?$ 121\. ()被括起来的表达式将作为分组2\. ？匹配前一个字符0次或1次 7 带1-2位小数的正数或负数：^(-)?\d+(.\d{1,2})?$ 121\. \转义字符2\. ？重复0次或1次 8 正数、负数、和小数：^(-|+)?\d+(.\d+)?$9 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$10 有1~3位小数的正实数：^[0-9]+(.[0-9]{1,3})?$11 非零的正整数：^[1-9]\d_$12 非零的负整数： ^-[1-9]\d_$13 非负整数：^\d+$ 11\. 匹配前一个字符1次或无限次 14 非正整数：^((-\d+)|(0+))$15 非负浮点数：^\d+(.\d+)?$16 非正浮点数：^((-\d+(.\d+)?)|(0+(.0+)?))$17 正浮点数：^[0-9]\d_.\d_$ 二、校验字符的表达式1 汉字：^[\u4e00-\u9fa5]{0,}$, 对应的是中文的unicode编码 2 英文和数字：^[A-Za-z0-9]+$3 长度为3-20的所有字符：^.{3,20}$4 由26个英文字母组成的字符串：^[A-Za-z]+$5 由26个大写英文字母组成的字符串：^[A-Z]+$6 由26个小写英文字母组成的字符串：^[a-z]+$7 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$8 由数字、26个英文字母或者下划线组成的字符串：^\w+$9 中文、英文、数字包括下划线：^[\u4E00-\u9FA5A-Za-z0-9_]+$10 中文、英文、数字但不包括下划线等符号：^[\u4E00-\u9FA5A-Za-z0-9]+$11 可以输入含有^%&amp;&#39;,;=?$&quot; :[^%&amp;&#39;,;=?$]+ 12 禁止输入含有的字符：[^\x22]+ 三、特殊需求表达式123456789101112131415161718191 Email地址：^\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*$2 域名：[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;(/.[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;)+/.?3 InternetURL：[a-zA-z]+://[^\s]* 4 手机号码:^(13[0-9]|14[5|7]|15[0-9])\d&#123;8&#125;$5 电话号码 ^(\(\d&#123;3,4&#125;-)|\d&#123;3.4&#125;-)?\d&#123;7,8&#125;$ 6 国内电话号码(0511-4405222、021-87888822)：\d&#123;3&#125;-\d&#123;8&#125;|\d&#123;4&#125;-\d&#123;7&#125;7 身份证号(15位、18位数字)：^\d&#123;15&#125;|\d&#123;18&#125;$8 短身份证号码(数字、字母x结尾)：^([0-9])&#123;7,18&#125;(x|X)?$ 或 ^\d&#123;8,18&#125;|[0-9x]&#123;8,18&#125;|[0-9X]&#123;8,18&#125;?$9 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]&#123;4,15&#125;$10 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\w&#123;5,17&#125;$11 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$ 12 日期格式：^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;13 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$14 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ 15 空白行的正则表达式：\n\s*\r (可以用来删除空白行)16 首尾空白字符的正则表达式：^\s*|\s*$17腾讯QQ号：[1-9][0-9]&#123;4,&#125; (腾讯QQ号从10000开始)18 中国邮政编码：[1-9]\d&#123;5&#125;(?!\d) (中国邮政编码为6位数字)19 IP地址：\d+\.\d+\.\d+\.\d+ (提取IP地址时有用) 参考：http://www.cnblogs.com/zxin/archive/2013/01/26/2877765.html]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之风控评分卡模型]]></title>
    <url>%2F2018%2F06%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%A3%8E%E6%8E%A7%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 一般来说风控领域在意的是前两个模型种类，排序类以及决策类。其中：巴塞尔协议定义了金融风险类型：市场风险、作业风险、信用风险。信用风险ABC模型有进件申请评分、行为评分、催收评分。 模型 解释 应用场景 Logistics回归 影响程度大小与显著性，解释力度强，但只是线性，没有顾及到非线性，预测精度较低 申请评分、流失预测 决策树 1、描述性，重建用户场景，可做变量提取与用户画像 2、树的结构不稳定，可以得出变量重要性，可以作为变量筛选 流失模式识别 随机森林 随机森林比决策树在变量筛选中，变量排序比较优秀 神经网络 1、不可解释，内部使用，预测精度较高。可以作为初始模型的金模型（用以评估在给定数据条件下，逻辑回归可达到的最精确程度）2、线性（逻辑回归）+非线性关系，可用于行为评分的预测模型（行为评分对模型可解释性不强），可用于申请评分的金模型3、使用场景：先做一个神经网络，让预测精度（AUC）达到最大时，再用逻辑回归 申请评分的金模型；行为评分的预测模型 监控模型指标决策类：准确率/误分率、利润/成本排序类：ROC指标（一致性）、Gini指数、KS统计量、提升度 混淆矩阵（confusion matrix） 准确率（Accuracy）准确率是预测和标签一致的样本在所有样本中所占的比例 精确率（Precision）精确率是你预测为正类的数据中，有多少确实是正类 查全率（Recall）查全率是所有正类的数据中，你预测为正类的数据占比 不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。 F1 Score有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。 F-beta Score有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。 ROC （receiver operating characteristic curve）绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。 AUC（Area Under the Curve）ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。 理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 Lift提升图Lift =[TP/(TP+FP)] / [(TP+FN)/(TP+FP+FN+TN)] = PV_plus / pi1，它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。 不利用模型，我们只能利用“正例的比例是(TP+FN)/(TP+FP+FN+TN)”这个样本信息来估计正例的比例（baseline model），而利用模型之后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集TP+FP中挑选正例，这时预测的准确率PV_plus(Precision)为TP/(TP+FP)。 上图的纵坐标是lift，横坐标是正例集百分比。随着阈值的减小，更多的客户就会被归为正例，也就是预测成正例的比例变大。当阈值设得够大，只有一小部分观测值会归为正例，但这一小部分一定是最具有正例特征的观测值集合（用前面银行向客户推荐信用卡的例子来看，这一部分人群对推荐的反应最为活跃），所以在这个设置下，对应的lift值最大。同样，当阈值设定得足够的小，那么几乎所有的观测值都会被归为正例（占比几乎为100%）——这时分类的效果就跟baseline model差不多了，相对应的lift值就接近于1。 ROC曲线和lift曲线都能够评价逻辑回归模型的效果：类似信用评分的场景，希望能够尽可能完全地识别出有违约风险的客户，选择ROC曲线及相应的AUC作为指标； 类似数据库精确营销的场景，希望能够通过对全体消费者的分类而得到具有较高响应率的客户群从而提高投入产出比，选择lift曲线作为指标； Gain增益图Gains(增益) 与 Lift （提升）类似：Lift 曲线是不同阈值下Lift和Depth的轨迹，Gain曲线则是不同阈值下PV_plus和Depth的轨迹，而PV_plus=Lift*pi1= TP/TP+FP，所以它们显而易见的区别就在于纵轴刻度的不同。 增益图是描述整体精准率的指标。按照模型预测出的概率从高到低排列，将每一个百分位数内的精准率指标标注在图形区域内，就形成了非累积的增益图。如果对每一个百分位及其之前的精准率求和，并将值标注在图形区域内，则形成累积的增益图。 K-S图正样本洛伦兹曲线记为f(x)，负样本洛伦兹曲线记为g(x)，K-S曲线实际上是f(x)与g(x)的差值曲线。K-S曲线的最高点（最大值）定义为KS值，KS值越大，模型分值的区分度越好，KS值为0代表是最没有区分度的随机模型。准确的来说，K-S是用来度量阳性与阴性分类区分程度的。 PSI 群体稳定性指标(population stability index)psi = sum(（实际占比-预期占比）* ln(实际占比/预期占比)) 一般认为psi小于0.1时候模型稳定性很高，0.1-0.25一般，大于0.25模型稳定性差，建议重做。 常用特征 个人信息：学历 性别 收入 负债能力：在申请的金融机构或者其他金融机构的负债情况（例如月还债金额超过月收入的60%，说明负债较高），例如多头借贷信息等 消费能力：商品购买记录，出境游，奢侈品消费 历史信用记录：历史逾期行为 其他数据：个人交际、网络足迹、个人财务等 备注：客户还款能力*还款意愿 = 还款等级 非平衡样本的处理方法 过采样：优点方法简单，缺点容易造成模型过拟合。 欠采样：优点和过采样类似，缺点是容易造成模型的欠拟合； SMOTE：优点是不易过拟合，能够保留大量的信息，缺点是不能对缺失值和类别变量做处理。 SMOTE算法 采样最近邻算法，计算出每个少数类样本的K个同类近邻； 从K个同类近邻中随机挑选N个样本进行随机线性插值； 构造新的少数类样本：New=Xi+rand(0,1)∗(yj−xi),j=1,2,3,4…..N其中Xi为少类中的一个观测点，Yj为K个近邻中随机抽取的样本 将新样本与原数据合成，产生新的训练集 例子：选取了一个X1为年龄为22岁，月收入为8000元，则X1=（22，8000），选取了一个近邻点为X2，X2=(28,5000)，随机系数为0.5，计算逻辑为22+(28−22)∗0.5=25,8000+(5000−8000)∗0.5=6500，这样得到的一个新的X3点为(25,6500)。 构建申请评分卡缺失值处理对于类别型变量 删除缺失率超过50%的变量 缺失值作为一种新的状态 对于连续型变量 删除缺失率超过80%的变量 均值，众数，预测填充等 特征的分箱1. 分箱的重要性 稳定性：避免特征中无意义的波动对评分带来的波动 健壮性：避免了极端值的影响 2. 分箱的优势 可以将缺失作为独立的一个箱带入模型中 将所有变量变换到相似的尺度上 3. 分箱的限制 分箱后需要编码 计算量大 4. 分箱的方法 有监督的分箱 Best—KS Chimerge 无监督的分箱 等频分箱 等距分箱 聚类分箱 5. 有监督的分箱—最小熵法分箱(1) 假设因变量为分类变量，可取值1，… ，J。令pij表示第i个分箱内因变量取值为j的观测的比例，i=1，…，k，j=1，…，J；那么第i个分箱的熵值为∑Jj=0−pij×logpij。如果第i个分箱内因变量各类别的比例相等，即p11=p12=p1J=1/J，那么第i个分箱的熵值达到最大值；如果第i个分箱内因变量只有一种取值，即某个pij等于1而其他类别的比例等于0，那么第i个分箱的熵值达到最小值。 (2) 令ri表示第i个分箱的观测数占所有观测数的比例；那么总熵值为∑ki=0∑Jj=0(−pij×logpij)。需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。 6. 卡方分箱法(ChiMerge)自底向上的(即基于合并的)数据离散化方法。 它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。 基本思想:对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 根据显著性水平和自由度得到卡方值,自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度（10%显著性水平)下，卡方的值为4.6。 类别和属性独立时，有90%的可能性，计算得到的卡方值会小于4.6，这样，大于阈值的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大，区间合并就会进行很多次，离散后的区间数量少、区间大。【注】： ChiMerge算法推荐使用0.90、0.95、0.99置信度，最大区间数取10到15之间. 也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限，最多几个区间，最少几个区间。 对于类别型变量，需要分箱时需要按照某种方式进行排序 无监督分箱法 等距分箱从最小值到最大值之间，均分为 N 等份， 这样， 如果 A,B 为最小最大值， 则每个区间的 长度为 W=(B−A)/N , 则区间边界值为 A+W,A+2W,….A+(N−1)W . 等频分箱区间的边界值要经过选择，使得每个区间包含大致相等的实例数量。比如说 N=10 ，每个区间应该包含大约10%的实例。 两种算法的弊端比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。 分箱的注意点 对于连续型变量， 使用ChiMerge进行分箱(默认分成5个箱) 检查分箱后的bad rate单调性；倘若不满足，需要进行相邻两箱的合并，直到bad rate为止 上述过程是收敛的，因为当箱数为2时，bad rate自然单调 分箱必须覆盖所有训练样本外可能存在的值！ 对于类别型变量 当类别数较少时，原则上不需要分箱 当某个或者几个类别的bad rate为0时，需要和最小的非0bad rate的箱进行合并 当该变量可以完全区分目标变量时，需要认真检查该变量的合理性 例如：“该申请者在本机构历史信用行为”把客群的好坏样本完全区分时，需要检查该变量的合理性(有可能是事后变量) woe编码&amp;IV值1.woe编码的定义 其中，pyi是这个组中响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）占所有样本中所有响应客户的比例，pni是这个组中未响应客户占样本中所有未响应客户的比例，#yi是这个组中响应客户的数量，#ni是这个组中未响应客户的数量，#yT是样本中所有响应客户的数量，#nT是样本中所有未响应客户的数量。 从这个公式中我们可以体会到，WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。 2.woe 编码的优劣一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值优势：将特征的值规范到相近的尺度上(经验上讲，WOE的绝对值波动范围在0.1～3之间)缺点：需要每箱中同时包含好、坏两个类别 3.WOE编码的意义 符号与好样本比例相关 要求回归模型的系数为负 4. IV(Information Value)IV去衡量变量的预测能力：我们假设在一个分类问题中，目标变量的类别有两类：Y1，Y2。对于一个待预测的个体A，要判断A属于Y1还是Y2，我们是需要一定的信息的，假设这个信息总量是I，而这些所需要的信息，就蕴含在所有的自变量C1，C2，C3，……，Cn中，那么，对于其中的一个变量Ci来说，其蕴含的信息越多，那么它对于判断A属于Y1还是Y2的贡献就越大，Ci的信息价值就越大，Ci的IV就越大，它就越应该进入到入模变量列表中。 5. 特征信息度的计算和意义挑选变量 非负指标 高IV表示该特征和目标变量的关联度高 目标变量只能是二分类 过高的IV，可能有潜在的风险 特征分箱越细，IV越高 特征工程1.特征衍生特征衍生是指利用现有的特征进行某种组合生成新的特征。 1.时间切片2.比例数据（负责比例，收入支出比例等）3.平均数据（月均收入） 2.特征抽象特征抽象是指将数据转换成算法可以理解的数据。 分类型变量转换数值型变量 123loans[&apos;delinq_2yrs&apos;] = loans[&apos;delinq_2yrs&apos;].apply(lambda x: float(x))loans[&apos;total_acc&apos;] = loans[&apos;total_acc&apos;].apply(lambda x: float(x))loans[&apos;revol_bal&apos;] = loans [&apos;revol_bal&apos;].apply(lambda x: float(x)) 有序特征的映射A &lt;B &lt;C &lt; D &lt; E &lt; F &lt; G ; 信用风险从低到高排序 123456789&quot;grade&quot;:&#123;&quot;A&quot;: 1,&quot;B&quot;: 2,&quot;C&quot;: 3,&quot;D&quot;: 4,&quot;E&quot;: 5,&quot;F&quot;: 6,&quot;G&quot;: 7&#125; 独热编码（one-hot encoding）pandas的get_dummies( )方法创建虚拟特征，虚拟特征的每一列各代表变量属性的一个分类 特征缩放特征缩放本质是一个去量纲的过程，同时可以加快算法收敛的速度。目前，将不同变量缩放到相同的区间有两个常用的方法：归一化（normalization）和标准化（standardization） 建模后续我们将客户违约的概率表示为p，则正常的概率为1-p 评分卡设定的分值刻度可以通过将分值表示为比率对数的线性表达式来定义A和B是常数。式中的负号可以使得违约概率越低，得分越高。通常情况下，这是分值的理想变动方向，即高分值代表低风险，低分值代表高风险。式中的常数A、B的值可以通过将两个已知或假设的分值带入计算得到。通常情况下，需要设定两个假设： （1）给某个特定的比率设定特定的预期分值；（2）确定比率翻番的分数（PDO） 最后，进行分数分级 更多有关机器学习的总结可以阅读：机器学习基础(一)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>风控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark生态体系]]></title>
    <url>%2F2018%2F06%2F13%2FSpark%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 Spark的基本架构当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。 1. Spark ApplicationSpark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。 群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。 Spark有一些集群管理器，负责调度可用资源。 驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。 2. Spark’s Languge APIs尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。 Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。 3. SparkSession我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。 4. DataframeDataFrame是最常见的Structured API（结构化API），只是表示有类型的包含行和列的数据表。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。 DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的Spark’s Language APIs，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。 注意Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。以下概念适用于所有的核心抽象。 5. Partitions为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。 6. Transformations在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为转换。转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。 宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。 7. Lazy EvaluationLazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。 8. Actions转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI，UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。 9. Dataframe &amp; SQLSpark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。 现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。 With SQlWith DataFrame 现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。 第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation） 第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前指定聚合。 因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。 第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！ 第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。 第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。 最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT&XGBOOST（一）]]></title>
    <url>%2F2018%2F06%2F12%2FGBDT%26XGBOOST%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 GBDT（梯度提升决策树）GBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终结果. 要理解 GBDT，就要先理解 GB ，然后才是 DT，而且这里的 DT 是回归树，而不是分类树. DT（决策回归树）这里先补充一下什么是回归树，因为之前所讲的决策树都是属于分类树. 先回顾下分类树: 我们知道 分类树在每次分枝时，是穷举每一个 feature 的每一个阈值，找到使得按照 feature &lt;= 阈值和feature &gt; 阈值分成两枝的熵最大的 feature 和阈值，按照该标准分枝得到的两个新节点，按同样的方法递归分裂下去，直到所有样本都被分入唯一的叶子节点，或达到预设的终止条件（如果叶子节点的样本不唯一，则以多数类作为叶子节点的分类结果）. 回归树大致流程类似: 不过在每个节点（不一定是叶子节点）都会得到一个预测值，以人的年龄为例，该预测值等于属于这个节点的所有人的年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差—即（每个人的年龄-预测年龄）平方的总和除以 N（总人数），或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。直到每个叶子节点上人的年龄都唯一，或者达到预设终止条件，若最终叶子节点上的人的年龄不唯一，就以该节点上所有人的平均年龄作为该叶子节点的预测值. 就是分类树是按多数投票，以多数类的类别标号作为叶子节点的分类类别；回归树是按叶子节点的样本平均值作为该节点的预测值. GBDT 实例GBDT的核心就在于，_每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量_。 比如以年龄预测的例子来说明，A 的真实年龄是 18 岁，但第一棵树的预测年龄是 12 岁，差了 6 岁，即残差为 6 岁。那么在第二棵树里我们把 A 的年龄设为 6 岁去学习，如果第二棵树真的能把 A 分到 6 岁的叶子节点，那累加两棵树的结论就是 A 的真实年龄；如果第二棵树的结论是 5 岁，则 A 仍然存在 1 岁的残差，第三棵树里 A 的年龄就变成 1 岁，继续学。 我们来详细的说一下这个例子，为简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图1所示结果： 现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果： 在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。 此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是 16-15=1（注意，A 的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是 15，如果还有树则需要都累加起来作为 A 的预测值）。进而得到 A,B,C,D 的残差分别为 (-1,1,-1,1)。 然后我们拿残差替代 A,B,C,D 的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值 1 和 -1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 换句话说，现在 A,B,C,D 的预测值都和真实年龄一致了。Perfect!： A: 14 岁高一学生，购物较少，经常问学长问题；预测年龄 A = 15 + (– 1) = 14 B: 16 岁高三学生，购物较少，经常被学弟问问题；预测年龄 B= 15 + (1) = 16 C: 24 岁应届毕业生，购物较多，经常问师兄问题；预测年龄 C = 25 + (-1) = 24 D: 26 岁工作两年员工，购物较多，经常被师弟问问题；预测年龄 D = 25 + (– 1) = 26 那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量 (-1, 1, -1, 1) 都是它的全局最优方向，这就是Gradient。 其实 GBDT 大致的过程就是这例子中所讲的，下面我们再做一些深入的理解 GBDT 深入理解前面说了，GBDT 是先有 GB（梯度提升），再有 DT（决策树），所以我们先从 GB 讲起. _Boosting_boosting 在前面讲 AdaBoost提升算法的时候讲了，就是通过训练多个弱分类器来组合成一个强分类器，形式如下: F_m(x) = f_0 + \alpha_1 f_1(x) + \alpha_2 f_2(x) + \cdots + \alpha_m f_m(x)其中，\(f_i(x),i = 1,2,\cdots,m\),是弱分类器，比如在 AdaBoost提升中是 C4.5决策树；\(F_m(x)\)是最终得到的强分类器。 _Gradient Boosting Modeling_给定一个问题，我们如何构造这些弱分类器呢？ Gradient Boosting Modeling （梯度提升模型） 就是构造这些弱分类器的一种方法。它指的不是某个具体的算法，而是一种思想. 我们先从普通的梯度优化问题入手来理解: find \ \hat{x} = arg \min_{x} f(x)针对这种问题，有个经典的算法叫 _Steepest Gradient Descent_，也就是最深梯度下降法。算法的大致过程是: 给定一个起始点 \(x_0\) 对 \(i = 1,2,\cdots,K\)分别做如下迭代:\(\qquad x_i = x_{i-1} + \gamma_{i-1} \times g_{i-1}\)其中 \(g_{i-1} = -\frac{\partial f}{\partial x} |_{x = x_{i-1}}\)表示 f在 \(x_{i-1}\)点的梯度 直到 \(|g_{i-1}\)足够小，或者是 \(|x_i - x_{i-1}|\)足够小 以上迭代过程可以理解为: _整个寻优的过程就是小步快跑的过程，每跑一小步，都往函数当前下降最快的那个方向走一点，直到达到可接受的点_. 我们将这个迭代过程展开得到寻优的结果: x_k = x_0 + \gamma_1 g_1 + \gamma_2 g_2 + \cdots + \gamma_k g_k这个形式是不是与最开始我们要求的\(F_m(x)\)类似；构造\(F_m(x)\) 本身也是一个寻优的过程，只不过我们寻找的不是一个最优点，而是一个最优的函数。 寻找最优函数这个目标，也是定义一个损失函数来做: find \ F_m = arg \ \min_{F} L(F) = arg \ \min_{F} \sum_{i=0}^N Loss(F(x_i),y_i)其中，\(Loss(F(x_i),y_i)\)表示损失函数\(Loss()\) 在第i个样本上的损失值， \(x_i,y_i\)分别表示第i个样本的特征和目标值。类似最速梯度下降法，我们可以通过梯度下降法来构造弱分类器 \(f_1,f_2,\cdots,f_m\)，只不过每次迭代时，令: g_i = -\frac{\partial L}{\partial F}|_{F=F_{i-1}}即损失函数L()对F求取梯度。 但是函数对函数求导不好理解，而且通常都无法通过上述公式直接求解。于是就采取一个近似的方法，把函数 理解成在所有样本上的离散的函数值，即:\(F_{i-1}\)理解成在所有样本上的离散的函数值，即: \left[ F_{i-1}(x_1),F_{i-1}(x_2),\cdots,F_{i-1}(x_N) \right]这是一个N 维向量，然后计算: \hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N这是一个函数对向量的求导，得到的也是一个梯度向量。注意，这里求导时的变量还是函数F，不是样本Xk ，只不过对F(Xk)求导时，其他的 Xi 都可以看成常数。 _Gradient Boosting Decision Tree_在上述算法过程中，如何通过 \(\hat{g}_{i-1}(x_j),j = 1,2,\cdots,N\)构造拟合函数\(g_{i-1}\)呢，这里我们用的就是 Decision Tree（决策树）了. 所以理解 GBDT，重点是先理解 Gradient Boosting，其次才是 Decision Tree；也就是说 GBDT 是 Gradient Boosting 的一种具体实现，这个拟合函数也可以改用其他的方法，只不过决策树好用一点。 _损失函数_谈到 GBDT 常听到的一种描述是 先构造一个(决策)树，然后不断在已有模型和实际样本输出的残差上再构造一棵树，依次迭代；其实这个说法不全面，尽管在本篇开头的那个 GBDT 的例子中是这样描述的，但拟合残差只是 GDBT 的一种特殊情况，下面对损失函数进行解释就清楚了. 从对 GBM 的描述里可以看到 Gradient Boosting 过程和具体用什么样的弱分类器是完全独立的，可以任意组合，因此这里不再刻意强调用决策树来构造弱分类器，转而我们来仔细看看弱分类器拟合的目标值，即梯度| \(\hat{g}_{i-1}(x_j)\)，之前我们已经提到过 \hat{g}_i(x_k) = -\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}},k = 1,2,\cdots,N因此 \(\frac{\partial L}{\partial F(x_k)}\)很重要，以平方差损失函数为例，得: \frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = 2(F_{i-1}(x_k) - y_k)忽略 2 倍，后面括号中正是当前已经构造好的函数\(F_{i-1}\)在样本上和目标值Yk之间的差值.如果我们换一个损失函数，比如绝对差: Loss(F(x_i),y_i) = |F(x_i) - y_i|这个损失函数的梯度是个符号函数: {\frac{\partial L}{\partial F(x_k)}|_{F = F_{i-1}} = sign(F_{i-1}(x_k) - y_k)}由此可以看到，只有当损失函数为平方差函数时，才能说 GBDT 是通过拟合残差来构造弱分类器的，比如上面说的对残差不断的用决策回归树来拟合。 知乎上关于xgboost/gbdt讨论的经典问答【问】xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？ 用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？【答】 这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。 一句话的解释，来自周志华老师的机器学习教科书（ 机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。 随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。 Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。 其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X^2)-[E(X)]^2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。 如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。 当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。 也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。 对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。 对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。 【问】机器学习算法中GBDT和XGBOOST的区别有哪些？【答】传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）即特征抽样。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。 多种语言封装支持。 【问】为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？【答】作者：马超链接：https://www.zhihu.com/question/51818176/answer/127637712来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost）通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 （1）站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。 模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit.当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。 先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit. （2）站在数据的角度 除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 （3）站在系统实现的角度 除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。 在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。 在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。 最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
        <tag>boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyecharts链接mysql进行数据可视化]]></title>
    <url>%2F2018%2F06%2F12%2FPyecharts%E8%BF%9E%E6%8E%A5Mysql%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Pandas常用读取方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 读取Mysql整个表为DataFrame：import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)query = &quot;select * from table&quot;df = pd.read_sql(query,conn) # conn对象创建参考下文conn.close()# 读取json文件json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号js = pd.read_json(json_str)# 读取htmlurl = &quot;http://quote.stockstar.com&quot;dfs = pd.read_html(url)dfs[0]dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定tabledfs1[0]# 读取粘贴板文件称DataFramedf = pd.read_clipboard(sep=&apos;,&apos;, header=None)# 读取Mysql整个表为DataFrame：import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)query = &quot;select * from table&quot;df = pd.read_sql(query,conn) # conn对象创建参考下文conn.close()# 读取json文件json_str = &apos;&#123;&quot;name&quot;:[&quot;Alice&quot;,&quot;Tom&quot;],&quot;age&quot;:[20,22]&#125;&apos; # 外面单引号，里面双引号js = pd.read_json(json_str)# 读取htmlurl = &quot;http://quote.stockstar.com&quot;dfs = pd.read_html(url)dfs[0]dfs1 = pd.read_html(url, attrs=&#123;&quot;id&quot;:&quot;table1&quot;&#125;) # 使用sttrs属性读取网页里特定tabledfs1[0]# 读取粘贴板文件称DataFramedf = pd.read_clipboard(sep=&apos;,&apos;, header=None) 每日交易额汇总12345678910111213141516171819import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()cursor.execute(&quot;select day_date from day_sum order by day_date&quot;)tran_data = cursor.fetchall()cursor.execute(&quot;select day_sum from day_sum order by day_date&quot;)transum = cursor.fetchall()tran_sum = [*map(lambda x :x[0]/100,list(transum))]cursor.close()conn.colse()tran_add = [0]for i in range (len(tran_sum)): if i &gt; 0: tran_add.append((tran_sum[i] - tran_sum[i-1]) / tran_sum[i-1] * 100) 12345678910111213141516171819202122from pyecharts import Bar, Line, Overlapattr1 = tran_datav1 = tran_sumv2 = tran_addbar1 = Bar(&quot;每日交易信息汇总&quot;)bar1.add(&quot;日期 /金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)line1 = Line()line1.add(&apos;环比增长率(%)&apos;,attr1, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True )line2 = Line()line2.add(&quot;日期/ 金额&quot;, attr1, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show=True)overlap = Overlap()overlap.add(bar1)# overlap.add(bar2)overlap.add(line1)overlap.render()overlap#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])#bar.render() 各出口交易金额123456789101112131415161718192021import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()#2016-04-29每个出口id交易额信息cursor.execute(&quot;select in_id from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)in_plazaid = cursor.fetchall()#2016-04-29每个出口交易额cursor.execute(&quot;select in_sum from in_sum where in_date=&apos;429&apos; order by in_sum desc&quot;)in_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]cursor.close()from pyecharts import Barattr2 = in_plazaidv2 = in_transumbar2 = Bar(&quot;2016-4-29日各出口交易额汇总&quot;,&quot;&quot;)bar2.add(&quot;出口ID/金额（元）&quot;, attr2, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])bar2.render()bar2 入出口交易金额123456789101112131415161718192021import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()cursor.execute(&quot;select ent_plazaid from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)ent_plazaid = cursor.fetchall()cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where trans_date=&apos;2016-04-29&apos; order by trans_sum desc&quot;)ent_transum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]cursor.close()from pyecharts import Barattr3 = ent_plazaidv3 = ent_transumbar3 = Bar(&quot;2016-4-29日各入口交易额汇总&quot;,&quot;&quot;)bar3.add(&quot;入口ID/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])bar3.render()bar3 某入口交易额汇总12345678910111213141516171819import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()cursor.execute(&quot;select trans_date from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)ent_date = cursor.fetchall()cursor.execute(&quot;select trans_sum from test.tran_ent_plaza_sum where ent_plazaid=100859&quot;)transum_ = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]cursor.close()from pyecharts import Barattr3 = ent_datev3 = transum_bar3 = Bar(&quot;某出口每日交易额&quot;,&quot;&quot;)bar3.add(&quot;出口/金额（元）&quot;, attr3, v3, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])bar3.render()bar3 金额突增原因分析1234567891011121314from pyecharts import Funnel#根据2016-4-29出入口及各车型的交易额信息，每个组选取前4个贡献率最高的样本。#利用除均操作对这三种因素的影响率进行标准化。#下图可以看出2016-4-29日，这三种影响因素的前四个样本分别占改组贡献率的比例#100108红门主站出京入口、100861榆垡南出京出口、车型-1对该日的收费的贡献率最大attr = [&apos;100861榆垡南出京出&apos;,&apos;100158璃河南出京出&apos;,&apos;100109红门主站进京出&apos;,&apos;100135六里桥站进京出&apos;,&apos;100108红门主站出京入&apos;,&apos;100134六里桥站出京入&apos;,&apos;100862榆垡南进京入&apos;,&apos;100175西红门南桥出京入&apos;,&apos;车型-1&apos;,&apos;车型-4&apos;,&apos;车型-3&apos;,&apos;车型-2&apos;]value = [6.24, 6.01, 4.72, 4.71, 16.75, 15.52, 10.85, 8.08, 4.46, 0.29, 0.13, 0.1]funnel = Funnel(&quot;&quot;)funnel.add(&quot;因素&quot;, attr, value, is_label_show=True, label_pos=&quot;inside&quot;, label_text_color=&quot;#fff&quot;)funnel.render()funnel 各车型交易汇总123456789101112131415161718192021import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()cursor.execute(&quot;select distinct vehtype from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)type_id = cursor.fetchall()cursor.execute(&quot;select distinct trans_sum from test.tran_vehtype_sum where trans_date = &apos;2016-04-29&apos; order by vehtype&quot;)type_sum = [*map(lambda x :x[0]/100,list(cursor.fetchall()))]cursor.close()#========================from pyecharts import Barattr4 = type_idv4 = type_sumbar4 = Bar(&quot;2016-4-29日各车型交易额汇总&quot;,&quot;&quot;)bar4.add(&quot;车型/金额（元）&quot;, attr4, v4, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)#bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;])bar4.render()bar4 全日期各车型交易额汇总123456789101112131415161718192021222324252627282930313233343536import pandas as pdimport numpy as npimport pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)cursor = conn.cursor()cursor.execute(&quot;select * from tran_vehtype_sum&quot;)typedata = cursor.fetchall()cursor.close()typedata = pd.DataFrame(list(typedata))typedata.columns = [&apos;type&apos;,&apos;date&apos;,&apos;value&apos;]data=pd.pivot_table(typedata,index=&quot;date&quot;,columns=&quot;type&quot;,values=&quot;value&quot;,aggfunc=np.sum,fill_value=0)# data.to_csv(&quot;data.csv&quot;)from pyecharts import Bary, x1, x2, x3, x4, x5 = [], [], [], [], [], []for i in range(len(data)): y.append(data.index[i]) x1.append(data[1][i]) x2.append(data[2][i]) x3.append(data[3][i]) x4.append(data[4][i]) x5.append(data[5][i])bar = Bar(&quot;各车型每日交易额&quot;)bar.add(&quot;车型-1&quot;, y, x1, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)bar.add(&quot;车型-2&quot;, y, x2, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)bar.add(&quot;车型-3&quot;, y, x3, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)bar.add(&quot;车型-4&quot;, y, x4, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)bar.add(&quot;车型-5&quot;, y, x5, is_stack=True, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;], is_datazoom_show = True)bar.render()bar PyMysql使用12345678910111213141516171819202122import pymysql#创建连接conn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)# 创建游标cursor = conn.cursor()# 执行SQL，并返回收影响行数effect_row = cursor.execute(&quot;select * from tran_day_sum&quot;)# 执行SQL，并返回受影响行数#effect_row = cursor.execute(&quot;update tb7 set pass = &apos;123&apos; where nid = %s&quot;, (11,))# 执行SQL，并返回受影响行数,执行多次#effect_row = cursor.executemany(&quot;insert into tb7(user,pass,licnese)values(%s,%s,%s)&quot;, [(&quot;u1&quot;,&quot;u1pass&quot;,&quot;11111&quot;),(&quot;u2&quot;,&quot;u2pass&quot;,&quot;22222&quot;)])# 提交，不然无法保存新建或者修改的数据conn.commit()# 关闭游标cursor.close()# 关闭连接conn.close() 123456789101112131415#分析每个入口的交易笔数和交易总额import pymysqlconn = pymysql.connect(host=&apos;192.168.56.111&apos;, port=3306, user=&apos;hive&apos;, passwd=&apos;hive&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;)cursor = conn.cursor()cursor.execute(&quot;select ent_plazaid from tran_ent_plaza_sum group by ent_plazaid&quot;)tran_id = cursor.fetchall()cursor.execute(&quot;select count(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)tran_count = [*map(lambda x :x[0],list(cursor.fetchall()))]cursor.execute(&quot;select sum(trans_sum) from tran_ent_plaza_sum group by ent_plazaid&quot;)tran_sum = [*map(lambda x :int(x[0]),list(cursor.fetchall()))]cursor.close()]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>pyecharts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F06%2F11%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[日常Linux操作记录，用于查询不考虑编写顺序 [root@localhost ~]# root：当前登录用户localhost：主机名~：当前所在目录（家目录）#：超级用户提示符 ==&gt; 家目录(/root)$：普通用户提示符 ==&gt; 家目录(/home/user01)文件类型（- 文件；d 目录；l 快捷方式） 命令格式：命令 [选项] [参数]例：查询目录内容ls [选项] [文件或目录]ls -a 显示所有文件，包括隐藏文件(.开头)ls -l 显示详细信息(也可用ll) 下载器：更新系统（不建议使用apt-get dist-upgrade，会导致文件不匹配卸载的情况）apt-get update &amp;&amp; apt-get upgrade下载安装文件apt-get install [文件名]安装过程中存在依赖关系apt-get install -f 知道网址下载方法： wget [粘贴网址] 不同类型文件解压：tar -xvf file.tar//解压 tar包tar -xzvf file.tar.gz//解压tar.gztar -xjvf file.tar.bz2 //解压 tar.bz2tar -xZvf file.tar.Z//解压tar.Zunrar e file.rar //解压rarunzip file.zip//解压zip 未完待续。。。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈推荐系统]]></title>
    <url>%2F2018%2F06%2F10%2F%E6%B5%85%E8%B0%88%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 什么是推荐系统1. 为什么需要推荐系统结论是，为了解决互联网时代下的信息超载问题。 正如《大数据时代》中作者所言，这仅仅是一个开始，人们与世界的交流方式，从原来对因果关系的渴求，转变为现在对相关关系的发现和使用上。 2. 搜索引擎与推荐系统众所周知，解决信息过载问题，最有代表性的解决方案是【分类目录】和【搜索引擎】，这两种解决方案分别催生了互联网领域的两家著名公司—雅虎和谷歌。 推荐系统，和搜索引擎一样，是一种帮助用户快速发展有用信息的工具。通过分析用户的历史行为，给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。 并且，推荐系统能够很好的发掘物品的长尾，挑战传统的2/8原则（80%的销售额来自20%的热门品牌）。 从技术角度来看，搜索引擎和推荐系统的区别在于：1）搜索引擎，注重搜索结果之间的关系和排序；2）推荐系统，需要研究用户的兴趣模型，利用社交网络的信息进行个性化的计算；3）搜索引擎，由用户主导，需要输入关键词，自行选择结果。如果结果不满意，需要修改关键词，再次搜索；4）推荐系统，由系统主导，根据用户的浏览顺序，引导用户发现自己感兴趣的信息； 推荐算法详述 1 基于内容的推荐原理是基于用户感兴趣的物品A，找到和A内容信息相近的物品B。利用用户和物品本身的内容特征，如用户的地理位置、性别、年龄，电影物品的导演、演员、发布时间等。所以提取推荐对象的特征，是内容推荐算法的关键。但是对于多媒体内容，如视频、音乐，很难找到它们之间的特性关联性。 基于内容的推荐的优点如下：(1) 简单、有效，推荐结果直观，容易理解，不需要领域知识。(2) 不需要用户的历史数据，如对对象的评价等。(3) 没有关于新推荐对象出现的冷启动问题。(4) 没有稀疏问题。(5) 算法成熟，如数据挖掘、聚类分析等。 基于内容的推荐的缺点如下：(1) 受到了推荐对象特征提取能力的限制。比如图像、视频，没有有效的特征提取方法。即便是文本资源，特征提取也只能反应一部分内容，难以提取内容质量，会影响用户满意度。(2) 很难出现新的推荐结果。根据用户兴趣的喜好进行推荐，很难出现惊喜。对于时间敏感的内容，如新闻，推荐内容基本相同，体验度较差。(3)存在新用户出现时的冷启动问题。当新用户出现时， 系统较难获得该用户的兴趣偏好，无法进行有效推荐。(4) 推荐对象内容分类方法需要的数据量较大。 2 协同过滤算法仅仅基于用户行为数据设计的推荐算法，称为协同过滤算法。此方法主要根据用户对物品的历史行为，寻找用户或物品的近邻集合，以此计算用户对物品的偏好。 基于用户的协同过滤算法（UserCF）算法的关键是计算两个用户的兴趣相似度。协同过滤计算用户兴趣相似度是利用用户行为的相似度。 关键在于计算用户与用户之间的兴趣相似度。 计算用户相似度的方法有3种： 余弦相似性 相关相似性（皮尔森系数相关） 修正的余弦相似性这里主要使用余弦相似度来计算： w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|| N(v)|}}\(w_{uv}\)代表用户 u 与 v 之间的兴趣相似度，N(u)表示用户 u 曾经喜欢过的物品集合, N(v) 表示用户 v 曾经喜欢过的物品集合。 根据上述核心思想，可以有如下算法步骤： 建立物品-用户的倒排表 用户与用户之间的共现矩阵 C[u][v]，表示用户u与v喜欢相同物品的个数 用户与用户之间的相似度矩阵 W[u][v]，根据上述相似度计算公式计算。 用上面的相似度矩阵来给用户推荐和他兴趣相似的用户喜欢的物品。用户 u 对物品 i 的兴趣程度可以估计为 S(u,K) 为和用户 u 兴趣最接近的 K 个用户， N(i) 为对物品 i 有正反馈的用户集合， W[u][v] 为用户 u 和用户 v 的兴趣相似度，rvi 为用户 v 对物品 i 的兴趣。 基于物品的协同过滤算法（ItemCF）这种算法给用户推荐和他之前喜欢的物品相似的物品。该算法是目前业界应用最多的算法，如亚马逊、Netflix、YouTube，都是以该算法为基础。 物品之间的相似度可以使用如下公式计算： w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)|| N(j)|}}从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。 用户活跃度对物品相似度的影响IUF（Inverse User Frequence），用户活跃度对数的倒数的参数。论文提出的观点是，活跃用户对物品相似度的贡献应该小于不活跃的用户。用IUF修正物品相似度的计算。 物品相似度的归一化研究表明，将ItemCF的相似度矩阵按最大值归一，可以提高推荐的准确率。 根据上述核心思想，可以有如下算法步骤： 建立用户-物品的倒排表 物品与物品之间的共现矩阵 C[i][j]，表示物品 i 与 j 共同被多少用户所喜欢。 用户与用户之间的相似度矩阵 W[i][j] ， 根据上述相似度计算公式计算。 用上面的相似度矩阵来给用户推荐与他所喜欢的物品相似的其他物品。用户 u 对物品 j 的兴趣程度可以估计为 S(j,K) 为和物品 j 最相似的前 K 个物品， N(u) 为对用户 u 所喜欢的物品集合， W[j][i] 为物品 j 和物品 i 之间的相似度， rui 为用户 u 对物品 i 的兴趣。 ItemCF 与 基于内容的推荐算法的区别 基于内容的推荐算法，计算的是物品内容属性之间的相似度。如，电影的导演是不是同一个人； ItemCF是通过用户的行为计算物品之间的相似度。如，物品A、B具有很大相似度，是因为喜欢物品A的用户也大都喜欢物品B。 UserCF 与 ItemCF 的优缺点 3 基于关联规则的推荐关联规则分析中的关键概念包括：支持度(Support)、置信度(Confidence)与提升度(Lift)。首先，我们简单温故下这3个关键指标~ 1、支持度 (Support)：支持度是两件商品（A∩B）在总销售笔数(N)中出现的概率，即A与B同时被购买的概率。类似于中学学的交集，需要原始同时满足条件。 公式： 例子说明： 比如某超市2016年有100w笔销售，顾客购买可乐又购买薯片有20w笔，顾客购买可乐又购买面包有10w笔，那可乐和薯片的关联规则的支持度是20%，可乐和面包的支持度是10%。 2、置信度 (Confidence)：置信度是购买A后再购买B的条件概率。简单来说就是交集部分C在A中比例，如果比例大说明购买A的客户很大期望会购买B商品。 公式： 例子说明： 某超市2016年可乐购买次数40w笔，购买可乐又购买了薯片是30w笔，顾客购买可乐又购买面包有10w笔，则购买可乐又会购买薯片的置信度是75%，购买可乐又购买面包的置信度是25%，这说明买可乐也会买薯片的关联性比面包强，营销上可以做一些组合策略销售。 3、提升度 (Lift)：提升度表示先购买A对购买B的概率的提升作用，用来判断规则是否有实际价值，即使用规则后商品在购物车中出现的次数是否高于商品单独出现在购物车中的频率。如果大于1说明规则有效，小于1则无效。 公式： 例子说明： 可乐和薯片的关联规则的支持度是20%，购买可乐的支持度是3%，购买薯片的支持度是5%，则提升度是1.33&gt;1, A-B规则对于商品B有提升效果。 4 隐语义模型待学习 推荐系统评测 1. 实验方法获得评测指标的实验方法，通常分3种： 离线实验（offline experiment） 用户调查（user study） 在线实验（online experiment） 我们分别介绍3种实验方法的优缺点。 1）离线实验离线实验的方法的步骤如下：a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；b）将数据集按照一定的规则分成训练集和测试集；c）在训练集上训练用户兴趣模型，在测试集上进行预测；d）通过事先定义的离线指标，评测算法在测试集上的预测结果。 从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。 离线实验的优点是： 不需要有对实际系统的控制权； 不需要用户参与实践； 速度快，可以测试大量算法； 缺点是： 数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果； 评价结果的客观性，无法得到用户主观性的评价； 难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系； 2）用户调查用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。 最后，我们通过分析他们的行为和答案，了解测试系统的性能。 用户调查的优点是： 可以获得用户主观感受的指标，出错后容易弥补； 缺点是： 招募测试用户代价较大； 无法组织大规模的测试用户，统计意义不足； 3）在线实验在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。 在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。 它的核心思想是:a) 多个方案并行测试;b) 每个方案只有一个变量不同;c) 以某种规则优胜劣汰。 其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。对于推荐系统的评价中，唯一变量就是—推荐算法。 有个很棒的网站，http://www.abtests.com，里面有很多通过实际AB测试提高网站用户满意度的例子。 AB测试的优点是： 可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标； 缺点是： 周期较长，必须进行长期的实验才能得到可靠的结果； 大型网站做AB测试，可能会因为不同团队同时进行各种测试对结果造成干扰，所以切分流量是AB测试中的关键。 4）总结一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。 首先，通过离线实验证明它在很多离线指标上优于现有的算法； 其次，通过用户调查确定用户满意度不低于现有的算法； 最后，通过在线AB测试确定它在我们关心的指标上优于现有的算法； 2. 评测指标评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。 1）用户满意度用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。 调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。 在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。 一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。 2）预测准确度预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。 大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。 由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。 a）预测评分准确度预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。这针对于一些需要用户给物品评分的网站。 预测评分的准确度指标，一般通过以下指标计算： 平均绝对误差（MAE） MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。 所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。 均方根误差（RMSE） Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。 b）TopN推荐网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。 TopN推荐的预测准确率，一般通过2个指标度量： 准确率（precision） 召回率（recall） R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。 TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。 3）覆盖率覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。最简单的定义是，推荐系统推荐出来的物品占总物品的比例。 假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为： 覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。 除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。 如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。 信息论和经济学中有两个著名指标，可以定义覆盖率： 信息熵 p(i)是物品i的流行度除以所有物品流行度之和。 基尼系数（Gini Index） p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。 评测马太效应 马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。 但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。 如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。 4）多样性为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。 多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下： 推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值： 5）新颖性新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。 评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。 此计算比较粗糙，需要配合用户调查准确统计新颖度。 6）惊喜度推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。 目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。 7）信任度如果用户信任推荐系统，就会增加用户和推荐系统的交互。 提高信任度的方式有两种： 增加系统透明度提供推荐解释，让用户了解推荐系统的运行机制。 利用社交网络，通过好友信息给用户做推荐通过好友进行推荐解释 度量信任度的方式，只能通过问卷调查。 8）实时性推荐系统的实时性，包括两方面： 实时更新推荐列表满足用户新的行为变化； 将新加入系统的物品推荐给用户； 9）健壮性任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。 健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。 2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。 评测算法的健壮性，主要利用模拟攻击： a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；b）用常用的攻击方法向数据集中注入噪声数据；c）利用算法在有噪声的数据集上再次生成推荐列表；d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。 提高系统健壮性的方法： 选择健壮性高的算法； 选择代价较高的用户行为，如购买行为比浏览行为代价高； 在使用数据前，进行攻击检测，从而对数据进行清理。 3. 评测维度增加评测维度的目的，就是知道一个算法在什么情况下性能最好。 一般评测维度分3种： 用户维度主要包括用户的人口统计学信息、活跃度以及是不是新用户等； 物品维度包括物品的属性信息、流行度、平均分以及是不是新加入的物品等； 时间维度包括季节，是工作日还是周末，白天还是晚上等； 如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。 冷启动问题1 解决方案： 提供非个性化推荐，如热门排行。等有了数据之后再推荐。 利用用户注册信息，做粗粒度的个性化。 利用用户的社交网络账号，导入用户的好友，推荐好友喜欢的物品。 用户初次登录时，对一些物品进行反馈，根据这些信息做个性化。 对于新上线的物品，利用内容信息，推荐给喜欢类似物品的用户。 系统冷启动，可以引入外部资源，如专家知识，建立起物品的相关度。 2 冷启动，启动用户兴趣的物品需要具有以下特点： 比较热门 具有代表性和区分性 启动物品集合需要有多样性 3 选择启动物品集合的系统 如何设计一个选择启动物品集合的系统？Nadav Golbandi在论文中提出用一个决策树解决。 首先，给定一群用户，用这群用户对物品评分的方差度量这群用户兴趣的一致程度。如果方差很小，说明这一群用户的兴趣不太一致，也就是物品具有比较大的区分度，反之则说明这群用户的兴趣比较一致。 再根据用户的评分方差计算物品的区分度。 也就是说，对于物品i，将用户分为3类—喜欢物品i的用户，不喜欢物品i的用户和不知道物品i的用户。如果这3类用户集合内的用户对其他的物品兴趣很不一致，说明物品i具有较高的区分度。 算法首先从所有用户中找到具有最高区分度的物品i，然后将用户分成3类。然后在每类用户中再找到最具区分度的物品，然后将每一类用户又各自分为3类，也就是将总用户分为9类，然后继续这样下去，最终可以通过对一系列物品的看法将用户进行分类。 在冷启动时，从根节点开始询问用户对该节点物品的看法，然后根据用户的选择将用户放到不同的分枝，直到进入最后的叶子节点，此时对用户的兴趣有了比较清楚的了解，从而可以开始对用户进行比较准确地个性化推荐。 4 利用物品的内容信息 就是基于内容的推荐，很适合解决物品冷启动问题。 物品冷启动对诸如新闻网站等时效性很强的网站的推荐非常重要，因为那些网站中时时刻刻都有新加入的物品，而且每个物品必须能够在第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。 一般来说，物品的内容可以通过向量空间模型表示，该模型会将物品表示成一个关键词向量。 如果物品的内容是诸如导演、演员等实体，可以直接将实体作为关键词。如果内容是文本，需要引入自然语言的技术抽取关键词。如何建立文章、话题和关键词的关系是话题模型研究的重点，代表性的话题模型有LDA。 最后推荐几篇博文 基于内容和用户画像的推荐：此种算法，可见之前的一篇文章：http://www.rowkey.me/blog/2016/04/07/up-recommend/。 基于矩阵分解的推荐: 基于SVD/ALS算法对用户进行内容推荐。相比起SVD，ALS更加适合解决稀疏矩阵的问题。Spark mlib中已经集成了对als算法的实现，需要做的就是在etl-1中把数据转换为als需要的数据格式以及调整als算法的各种参数。这里有一篇文章比较具体地描述了如何使用spark来做基于ALS的推荐：http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/。 用户&amp;物品协同过滤推荐：包括UserBased CF和ItemBased CF。对于这两者，需要根据业务的不同来选择不同的算法。当用户非常多的时候，考虑到维护用户矩阵的成本，一般是不推荐选择用户协同过滤的，而对于候选item很多的时候，则不推荐使用物品协同过滤 参考文献：项亮《推荐系统实践》参考论文：http://t.cn/RjXktmChttp://t.cn/RjXkiFPhttp://blog.csdn.net/qingqingpiaoguo/article/details/60882309 https://www.zhihu.com/question/27141495/answer/161027882]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈分布式]]></title>
    <url>%2F2018%2F06%2F07%2F%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 分布式基础分布式，按字面意思就是并行计算，对于一些串行的计算可以运用。一些本来就并行的就不可以。 之前聊起这个，就会说mapreduce，就是把任务分拆开来，最后计算的时候再合并。但后来发现这样的回答太水了，看了几篇介绍分布式计算的PPT，特此写下来分布式到底是怎么一回事。 我们用一个矩阵运算作为例子： 对于两个矩阵，A和B： A = \(\begin{pmatrix}1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp;0 \\\\ 7 &amp; 8&amp;9\\\\ 10 &amp; 11&amp;12\end{pmatrix}\) B = \(\begin{pmatrix}10 &amp; 15\\\\ 0 &amp;2 \\\\ 11 &amp;9\end{pmatrix}\) 有 C = AB = \(\begin{pmatrix}43 &amp; 46\\\\ 40 &amp;70 \\\\ 169 &amp;202\\\\ 232 &amp;280\end{pmatrix}\) 我们要改变矩阵存储方式，只存储那些非零的数值。 所以矩阵A可以表示成 其中，每一行第一个字段为行标签i，第二个字段为列标签j，第三个字段是A[i,j],即矩阵对应的值。 同理，对于矩阵B，可以变换为 下图中，对于矩阵A，因为矩阵B一共两列，所以p=2，所以key会有（1，1）和（1，2）。而对于每个key，每一行中每一个位置的数据都得变成value，即value=”a:j,aij” 同理，对于矩阵B，k就在key的前面，因为是每一列拿出来做相乘。Value中也变成按行遍历。 在shuffle阶段，就把相同的key放在一起在reduce阶段，在同一个key中，value第一维相同的匹配一起，其第二维相乘，最后把所有相加（如key（1，1）中，\(110+311\)，因为a:2没有匹配的，所以去掉a:2） 最后把每一个key相加就可以得到结果了。 整体的流程实现如下图： Python代码实现对于wordcount，如果是串行的话，等于是对于每个word进行一次循环。而并行的话可以多个word一起遍历。 map代码如下： 12345import sys for line in sys.stdin: word_list = line.strip().split(&apos; &apos;) for word in word_list: print &apos;\t&apos;.join([word.strip(), str(1)]) reduce代码如下： 123456789101112131415161718import syscur_word = Nonesum = 0 for line in sys.stdin: ss = line.strip().split(&apos;\t&apos;) if len(ss) &lt; 2: continue word = ss[0].strip() count = ss[1].strip() if cur_word == None: cur_word = word if cur_word != word: print &apos;\t&apos;.join([cur_word, str(sum)]) cur_word = word sum = 0 sum += int(count) print &apos;\t&apos;.join([cur_word, str(sum)])sum = 0 然后在linux上运行以下代码： src.txt为自己随便找的文本文件txt格式 1cat src.txt | python map.py | sort -k 1 | python reduce.py 还算比较简单的，就不过多解释了。本意就是map中算出每一行中的word，然后传到reduce中处理。 实战中怎么运用并行化处理Python中有几个package可以做并行，如joblib，pp，multiprocessing等， 此处仅介绍joblib 123from joblib import Parallel, delayedfrom math import sqrtParallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10)) 得到： [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] 本质就是你要定义好一个子函数，这个子函数是并行任务中每个任务的输出。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之聚类]]></title>
    <url>%2F2018%2F06%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 无监督学习方法在现实场景中还是有很多应用的，例如在金融和计算广告等反欺诈场景中，我们是不可能能够获取大量的标签数据的，因为欺诈用户不会告诉你他是欺诈用户的。这时候，如果想要利用机器学习方法检测出他们来，只能使用无监督方法。 K-Means算法算法描述下面具体看到该算法的步骤： （1）根据设定的聚类数 K，随机地选择 K 个聚类中心（Cluster Centroid） （2）评估各个样本到聚类中心的距离，如果样本距离第 i 个聚类中心更近，则认为其属于第 ii 簇 （3）计算每个簇中样本的平均（Mean）位置，将聚类中心移动至该位置 重复以上步骤直至各个聚类中心的位置不再发生改变。 .变量标准化在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。 常用的方法有： 正态标准化：\(x_i=\frac {x_i-mean(X)}{std(X}\)归一化：\(x_i=\frac {x_i-min(X)}{max(X)-min(X)}\) 如何确定聚类数实际上，一开始是很难确定聚类数的，下图的两种聚类数似乎都是可行的： 但是，也存在一种称之为肘部法则（Elbow Method）的方法来选定适当的K值： 上图曲线类似于人的手肘，“肘关节”部分对应的 K 值就是最恰当的 K值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线： 一般来说，K-Means 得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择 K 值。在下面这个例子中，假定我们的衣服想要是分为 S,M,L 三个尺码，就设定 K=3，如果我们想要 XS、S、M、L、XL 5 个衣服的尺码，就设定 K=5： 质心的目标函数聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性 常见的邻近度、质心和目标函数组合 邻近度函数 质心 目标函数 曼哈顿距离 中位数 最小化对象与质心的绝对误差和SAE 平方欧几里得距离 均值 最小化对象与质心的误差平方和SSE 余弦 均值 最大化对象与质心的余弦相似度和 Bregman散度 均值 最小化对象到质心的Bregman散度和 Bregman散度实际上是一类紧邻性度量，包括平方欧几里得距离。Bregman散度函数的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。 K-means优缺点优点 简单并且可以用于各种数据类型； 具备适合的空间复杂度和计算负责度，适用于大样本数据； K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。 缺点1. 属于“硬聚类” ， 每个样本只能有一个类别。 其他的一些聚类方法(GMM或者模糊K-means允许“软聚类”)。2. K-means对异常点的“免疫力” 很差， 我们可以通过一些调整，比如中心不直接取均值， 而是找均值最近的样本点代替——k-medoids算法。3. 对于团状的数据点集区分度好， 对于带状(环绕)等“非凸”形状不太好。 (用谱聚类或者做特征映射) 密度聚类方法基本名词 核心点： 如果该点满足给定的邻域内（半径为EpsEps的范围内）的点的个数超过给定的阈值MinptsMinpts，则该点为满足该条件下的核心点。 边界点： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。 噪声点： 噪声点既非核心点，也不是边界点 ϵ 邻域 对于样本集 D 中样本点 xi，它的 ϵ 邻域定义为与 xi 距离不大于 ϵ 的样本的集合，即 Nϵ(xi)={x∈D|dist(x,xi)≤ϵ}； 核心对象 如果样本 x 的 ϵ 邻域内至少包含 mps 个样本，即 |Nϵ(xi)|≥mps，则称 x 为核心对象； 密度直达 如果 xi 是一个核心对象，并且 xj 位于它的 ϵ邻域内，那么我们称 xj 由 xi 密度直达； 密度可达 对于任意的两个不同的样本点 xi 与 xj，如果存在这样的样本序列 p1,p2,⋯,pn，其中 p1=xi,pn=xj且\(p_{i+1}\)由pi密度直达，则称 xi 与 xj 密度可达； 密度相连 对于任意的两个不同的样本点 xi 与 xj，如果存在第三个样本点 xk 使得 xi与 xj均由 xk密度可达，则称 xi与 xj密度相连。 DBSCAN算法步骤 将所有点标记为核心点、边界点或噪音点 删除噪音点 为距离在 EpsEps 之内的所有核心点之间赋予一条边 每组联通的核心点形成一个簇 将每个边界点指派到一个与之关联的核心点的簇中 选择 DBSCAN 参数 k−距离：如何选择 Eps 和 MinPts 参数，基本的方法是观察点到它的第 k 个最近邻的距离。考虑下，如何 k不大于簇个数的话， k−距离相对较小，反之对于不在簇中的点（噪音点或异常值）则k距离较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 k (一般为4)，计算所有点的k−距离，并递增排序，画出曲线图，则我们会看到k−距离的变化，并依照此图选择出合适的 MinPts参数，即对应拐点的位置。 优点和缺点 对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。 当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。 层次聚类方法有两种产生层次聚类的基本方法： 凝聚型： 从点作为个体簇开始，每一步合并两个最接近的簇 分裂型： 从包含所有的点某个簇开始，每一步分裂一个簇，直到成为单点簇到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。 簇之间的临近性 MIN：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做单链（sigle link） MAX：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做全链（complete link） 组平均：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。 层次聚类的主要问题处理不同大小簇的能力对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法： 非加权：平等的对待所有簇，赋予不同大小的簇中点不同的权值 加权： 赋予不同大小簇中点相同的权值 合并不可逆对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。 有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。 评价指标众所周知，对于有监督学习方法的分类预测结果，我们有很多种不同的评价指标来度量分类效果的好坏。例如，召回率、精准率、准确率、F1-Score 以及 AUC 值等等。但是，由于无监督学习方法与有监督学习不同，绝大多数情况下，我们根本不知道它的真实类别标签，所以我们不可能完全依据有监督学习方法的评价指标来度量聚类算法。 无监督聚类算法的评价指标大致可以分为两大类：一类是聚类的结果具有某个参考模型作为人为基准进行比较，称之为外部指标；第二种是直接考察聚类结果而不参考任何模型，称之为内部指标。 外部指标对数据集 \(D=\{x_1, x_2,\cdots,x_m\}\)，假定通过聚类算法将样本聚为 \(C=\{C_1,C_2,\cdots,C_k\}\)，参考模型给出的簇划分为 \(C^=\{C_1^,C_2^,\cdots,C_s^\}\)。相应的，令 λ 与 \(λ^∗\) 分别表示与 C 与 \(C^∗\)对应的簇标记向量。我们将样本两两配对考虑，定义如下： 其中集合 S1包含了在 C 中属于相同的簇并且在 C∗中也属于相同的簇的样本； S2 包含了在 C 中属于相同的簇并且在 C∗中不属于相同的簇的样本……以此类推。对每个样本对 (xi,xj) (i &lt; j) 仅能出现在一个集合中，因此有 a+b+c+d=m(m−1)/2。 基于以上定义，对无监督聚类算法的聚类结果，我们有如下的性能度量指标： Jaccard 系数（简记为 JCI） \begin{align} JCI = \frac{a}{a+b+c} \end{align} FM 指数（简记为FMI） Rand 指数（简记为 RI） \begin{align} RI=\frac{2(a+d)}{m(m-1)} \end{align}很显然，上述指数值都在 [0,1] 之间，并且值越大越好。 内部指标对于聚类结果\(C=\{C_1,C_2,\cdots,C_k\}\)，作如下定义： 其中，\(dist(x_i,x_j)\)用于计算两个样本间的距离 \(μ_i\)代表类 \(C_j\) 的样本中心。基于以上定义如下内部指标： DB 指数（简称 DBI） \begin{align} DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\bigl(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\bigl) \end{align} Dunn 指数（简称 DI） 显然，DBI 的值越小越好，而 DI 值越大越好。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之KNN]]></title>
    <url>%2F2018%2F06%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 KNN 算法核心：KDTreeKNN采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？ 也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。 因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。 KD 树简介 KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。 KD 树的构造KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。 构造跟节点 首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。 递归构造子节点 递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。 通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。 KNN算法原理KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别： 假设X_test为未标记的数据样本，X_train为已标记类别的样本，算法原理伪代码如下： 遍历X_train中所有样本，计算每个样本与X_test的距离，并保存在Distance数组中 对Distance数组进行排序，取距离最近的k个点，记为X_knn 在X_knn中统计每个类别的个数 代表记得样本的类别，就是在X_knn中样本最多的类别 算法优缺点优点：准确性高，对异常值和噪声有较高的容忍度 缺点：计算量大，对内存的需求也较大 算法参数（k）k越大：模型偏差越大，对噪声越不敏感。过大是造成欠拟合 k越小：模型的方差就会越大。太小是会造成过拟合 算法的变种增加邻居的权重：默认情况下X_knn的权重相等，我们可以指定算法的weights参数调整成距离越近权重越大 使用一定半径内的点取代距离最近的kk个点，RadiusNeighborsClassifier类实现了这个算法 使用KNN作分类 sklearn.neighbors.KNeighborsClassifier 123456789101112131415from sklearn.datasets.samples_generator import make_blobs# 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度centers = [[-2,2], [2,2], [0,4]]X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)# 画出数据%matplotlib inlinefrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdplt.figure(figsize=(8,5), dpi=100)c = np.array(centers)plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;)plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;) 12345from sklearn.neighbors import KNeighborsClassifierk = 5clf = KNeighborsClassifier(n_neighbors=k)clf.fit(X, y) 1234# X_test = np.array([0, 2]).reshape(1,-1)X_test = [[0,2]]y_test = clf.predict(X_test)neighbors = clf.kneighbors(X_test, return_distance=False) 12345678910111213141516171819from sklearn.datasets.samples_generator import make_blobs# 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度centers = [[-2,2], [2,2], [0,4]]X , y = make_blobs(n_samples=100, centers=centers, random_state=0, cluster_std=0.60)# 画出数据%matplotlib inlinefrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdplt.figure(figsize=(8,5), dpi=100)c = np.array(centers)plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=&apos;cool&apos;) # 样本plt.scatter(c[:,0], c[:,1], s=50, marker=&apos;^&apos;, c=&apos;red&apos;) # 中心点plt.scatter(X_test[0][0], X_test[0][1], marker=&apos;x&apos;, s=50, c=&apos;blue&apos;) # 中心点for i in neighbors[0]: plt.plot([X[i][0], X_test[0][0]], [X[i][1], X_test[0][1]],&apos;k--&apos;, linewidth=0.5) KNN回归拟合12345678910from sklearn.neighbors import KNeighborsRegressorimport numpy as np%matplotlib inlinefrom matplotlib import pyplot as pltn = 50X = 5 * np.random.rand(n ,1)y = np.cos(X).ravel()# 添加一些噪声y += 0.2 * np.random.rand(n) - 0.1 123k = 5knn = KNeighborsRegressor(k)knn.fit(X, y) KNeighborsRegressor(algorithm=’auto’, leaf_size=30,metric=’minkowski’,metric_params=None, n_jobs=1,n_neighbors=5, p=2,weights=’uniform’) 123T = np.linspace(0,5, 500)[:, np.newaxis]y_pred = knn.predict(T)knn.score(X,y) 0.9909058023770559 123456plt.figure(figsize=(8,5), dpi=100)plt.scatter(X, y, label=&apos;data&apos;, s=10)plt.scatter(T, y_pred, label=&apos;prediction&apos;, lw=4, s=0.1)plt.axis(&apos;tight&apos;)plt.title(&quot;KNeighborsRegressor (k=%i)&quot; % k)plt.show() 糖尿病预测总共有768个数据、8个特征，其中Outcome为标记值（1表示有糖尿病）12345678import numpy as npimport pandas as pddata = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)X = data.iloc[:,0:8]y = data.iloc[:,8]from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) 模型比较 分别使用普通KNN，加权重KNN，和指定权重的KNN分别对数据拟合计算评分 1234567from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifiermodels = []models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=10)))models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(n_neighbors=10, weights=&quot;distance&quot;)))models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=10, radius=500.0))) 123456results = []for name, model in models: model.fit(X_train, y_train) results.append((name, model.score(X_test, y_test)))for i in range(len(results)): print(&quot;name:&#123;&#125;; score:&#123;&#125;&quot;.format(results[i][0], results[i][1])) name:KNN; score:0.7207792207792207name:KNN + weights; score:0.6818181818181818name:Radius Neighbors; score:0.6558441558441559 此时单从得分上看，普通的KNN性能是最好的，但是我们的训练样本和测试样本是随机分配的，不同的训练集、测试集会造成不同得分。 为了消除随机样本集对得分结果可能的影响，scikit-learn提供了KFold和cross_val_score()函数来处理这个问题 12345678910from sklearn.model_selection import KFoldfrom sklearn.model_selection import cross_val_scoreresults = []for name , model in models: kfold = KFold(n_splits=10) cv_result = cross_val_score(model, X, y, cv=kfold) # 这里要给模型全部的样本集 results.append((name, cv_result))for i in range(len(results)): print(&quot;name:&#123;&#125;; cross_val_score:&#123;&#125;&quot;.format(results[i][0], results[i][1].mean())) name:KNN; cross_val_score:0.74865003417635name:KNN + weights; cross_val_score:0.7330485304169514name:Radius Neighbors; cross_val_score:0.6497265892002735 用查准率和召回率以及F1对该模型进行评估：123456789from sklearn.metrics import f1_score, precision_score, recall_scoreknn = KNeighborsClassifier(10)knn.fit(X_train, y_train)y_pred = knn.predict(X_test)print(&quot;该模型查准率为：&quot;, precision_score(y_test, y_pred))print(&quot;该模型召回率为：&quot;, recall_score(y_test, y_pred))print(&quot;该模型F1_score为：&quot;, f1_score(y_test, y_pred)) 该模型查准率为： 0.6086956521739131该模型召回率为： 0.5283018867924528该模型F1_score为： 0.5656565656565657 模型的训练及分析 – 学习曲线下面就选择用普通KNN算法模型对数据集进行训练，并查看训练样本的拟合情况及对策测试样本的预测准确性： 输入参数： estimator : 你用的分类器。title : 表格的标题。X : 输入的feature，numpy类型y : 输入的target vectorylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)n_jobs : 并行的的任务数(默认1)) 输出参数： train_sizes_abs :训练样本数train_scores:训练集上准确率test_scores:交叉验证集上的准确率) 123456knn = KNeighborsClassifier(n_neighbors=2)knn.fit(X_train, y_train)train_score = knn.score(X_train, y_train)test_score = knn.score(X_test, y_test)print(&apos;训练集得分：&apos;,train_score)print(&apos;测试集得分：&apos;,test_score) 训练集得分： 0.8517915309446255测试集得分： 0.6948051948051948 123456789101112131415161718192021222324from sklearn.model_selection import learning_curvefrom sklearn.model_selection import ShuffleSplit# from common.utils import plot_learning_curvedef plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 10)): plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel(&quot;train exs&quot;) plt.ylabel(&quot;Score&quot;) train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) train_score_mean = np.mean(train_scores, axis=1) train_score_std = np.std(train_scores, axis=1) test_score_mean = np.mean(test_scores, axis=1) test_score_std = np.std(test_scores, axis=1) plt.grid() plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;) plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;) plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;) plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;) plt.legend(loc=&apos;best&apos;) return plt 123456789data = pd.read_csv(&apos;code/datasets/pima-indians-diabetes/diabetes.csv&apos;)X = data.iloc[:,0:8]y = data.iloc[:,8]cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)plt.figure(figsize=(8,6), dpi=100)plot_learn_curve(KNeighborsClassifier(2),&quot;KNN score&quot;,X, y, ylim=(0.5, 1), cv=cv)plt.show() 当训练集和测试集的误差收敛但却很高时，为高偏差。左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。我们可以增加模型参数，比如，构建更多的特征，减小正则项。此时通过增加数据量是不起作用的。 当训练集和测试集的误差之间有大的差距时，为高方差。当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。 理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。 特征选择及数据可视化使用sklearn.feature_selection.SelectKBest选择相关性最大的两个特征 123456from sklearn.feature_selection import SelectKBestfrom sklearn.neighborse import KNselector = SelectKBest(k=2)X_new = selector.fit_transform(X,y)X_new[0:5] #把相关性最大的两个特征放到X_new里并查看前5个数据样本 array([[148. , 33.6],[ 85. , 26.6],[183. , 23.3],[ 89. , 28.1],[137. , 43.1]]) 使用相关性最大的两个特征，对3种不同的KNN算法进行检验 123456789101112131415from sklearn.model_selection import KFoldfrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifiermodels = []models.append((&quot;KNN&quot;, KNeighborsClassifier(n_neighbors=5)))models.append((&quot;KNN + weights&quot;, KNeighborsClassifier(n_neighbors=5, weights=&quot;distance&quot;)))models.append((&quot;Radius Neighbors&quot;, RadiusNeighborsClassifier(n_neighbors=5, radius=500.0)))results = []for name, model in models: kfold = KFold(n_splits=10) cv_result = cross_val_score(model, X_new, y, cv=kfold) results.append((name, cv_result))for i in range(len(results)): print(&quot;name: &#123;&#125;; cross_val_score: &#123;&#125;&quot;.format(results[i][0], results[i][1].mean())) name: KNN; cross_val_score: 0.7369104579630894name: KNN + weights; cross_val_score: 0.7199419002050581name: Radius Neighbors; cross_val_score: 0.6510252904989747 从输出结果来看，还是普通KNN的准确性更高，与所有特征放到一起训练的准确性差不多，这也侧面证明了SelectKNBest特征选取的准确性。 回到目标上来，我们是想看看为什么KNN不能很好的拟合训练样本。现在我们至于2个特征可以很方便的在二维坐标上画出所有的训练样本，观察这些数据分布情况 123456789%matplotlib inlinefrom matplotlib import pyplot as pltplt.figure(figsize=(8,6), dpi=100)plt.ylabel(&quot;BMI&quot;)plt.xlabel(&quot;Glucose&quot;)plt.scatter(X_new[y==0][:,0], X_new[y==0][:,1], marker=&apos;o&apos;, s=10)plt.scatter(X_new[y==1][:,0], X_new[y==1][:,1], marker=&apos;^&apos;, s=10) 横坐标是血糖值，纵坐标是BMI值反应身体肥胖情况。在数据密集的区域，代表糖尿病的阴性和阳性的样本几乎重叠到了一起。这样就很直观的看到，KNN在糖尿病预测的这个问题上无法达到很高的预测准确性。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树之泰坦之灾]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E6%B3%B0%E5%9D%A6%E4%B9%8B%E7%81%BE%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 数据预处理 筛选特征值，丢掉不需要的特征数据 对性别进行二值化处理（转换为0和1） 港口转换成数值型数据 处理缺失值（如年龄，有很多缺失值） 1、首先读取数据123456789101112131415161718import pandas as pdimport numpy as npdef read_dataset(fname):# 指定第一列作为行索引 data = pd.read_csv(fname, index_col=0)# 丢弃无用数据 data.drop([&apos;Name&apos;, &apos;Ticket&apos;, &apos;Cabin&apos;], axis=1, inplace=True)# 处理性别数据 lables = data[&apos;Sex&apos;].unique().tolist() data[&apos;Sex&apos;] = [*map(lambda x: lables.index(x) , data[&apos;Sex&apos;])]# 处理登船港口数据 lables = data[&apos;Embarked&apos;].unique().tolist() data[&apos;Embarked&apos;] = data[&apos;Embarked&apos;].apply(lambda n: lables.index(n))# 处理缺失数据填充0 data = data.fillna(0) return datatrain = read_dataset(&apos;code/datasets/titanic/train.csv&apos;) 2、拆分数据集把Survived列提取出来作为标签，然后在元数据集中将其丢弃。同时拆分数据集和交叉验证数据集12345678from sklearn.model_selection import train_test_splity = train[&apos;Survived&apos;].valuesX = train.drop([&apos;Survived&apos;], axis=1).valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)print(&quot;X_train_shape:&quot;, X_train.shape, &quot; y_train_shape:&quot;, y_train.shape)print(&quot;X_test_shape:&quot;, X_test.shape,&quot; y_test_shape:&quot;, y_test.shape) X_train_shape: (712, 7) y_train_shape: (712,)X_test_shape: (179, 7) y_test_shape: (179,) 3、拟合数据集123456from sklearn.tree import DecisionTreeClassifierclf = DecisionTreeClassifier()clf.fit(X_train, y_train)print(&quot;train score:&quot;, clf.score(X_train, y_train))print(&quot;test score:&quot;, clf.score(X_test, y_test)) train score: 0.9845505617977528test score: 0.7597765363128491 优化模型参数1、通过max_depth参数来优化模型从以上输出数据可以看出，针对训练样本评分很高，但针对测试数据集评分较低。很明显这是过拟合的特征。解决决策树过拟合的方法是剪枝，包括前剪枝和后剪枝。但是sklearn不支持后剪枝，这里通过max_depth参数限定决策树深度，在一定程度上避免过拟合。 这里先创建一个函数使用不同的模型深度训练模型，并计算评分数据。1234def cv_score(d): clf = DecisionTreeClassifier(max_depth=d) clf.fit(X_train, y_train) return(clf.score(X_train, y_train), clf.score(X_test, y_test)) 12345678910111213from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)depths = np.arange(1,10)scores = [cv_score(d) for d in depths]tr_scores = [s[0] for s in scores]te_scores = [s[1] for s in scores]# 找出交叉验证数据集评分最高的索引tr_best_index = np.argmax(tr_scores)te_best_index = np.argmax(te_scores)print(&quot;bestdepth:&quot;, te_best_index+1, &quot; bestdepth_score:&quot;, te_scores[te_best_index], &apos;\n&apos;) bestdepth: 5 bestdepth_score: 0.8603351955307262 这里由于以上train_test_split方法对数据切分是随机打散的，造成每次用不同的数据集训练模型总得到不同的最佳深度。这里写个循环反复测试，最终验证这里看到最佳的分支深度为5出现的频率最高，初步确定5为深度模型最佳。 把模型参数和对应的评分画出来：12345678910%matplotlib inlinefrom matplotlib import pyplot as pltdepths = np.arange(1,10)plt.figure(figsize=(6,4), dpi=120)plt.grid()plt.xlabel(&apos;max depth of decison tree&apos;)plt.ylabel(&apos;Scores&apos;)plt.plot(depths, te_scores, label=&apos;test_scores&apos;)plt.plot(depths, tr_scores, label=&apos;train_scores&apos;)plt.legend() 2、通过min_impurity_decrease来优化模型这个参数用来指定信息墒或者基尼不纯度的阈值，当决策树分裂后，其信息增益低于这个阈值时则不再分裂。123456789101112131415161718192021222324252627X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)def minsplit_score(val): clf = DecisionTreeClassifier(criterion=&apos;gini&apos;, min_impurity_decrease=val) clf.fit(X_train, y_train) return (clf.score(X_train, y_train), clf.score(X_test, y_test), )# 指定参数范围，分别训练模型并计算得分vals = np.linspace(0, 0.2, 100)scores = [minsplit_score(v) for v in vals]tr_scores = [s[0] for s in scores]te_scores = [s[1] for s in scores]bestmin_index = np.argmax(te_scores)bestscore = te_scores[bestmin_index]print(&quot;bestmin:&quot;, vals[bestmin_index])print(&quot;bestscore:&quot;, bestscore)plt.figure(figsize=(6,4), dpi=120)plt.grid()plt.xlabel(&quot;min_impurity_decrease&quot;)plt.ylabel(&quot;Scores&quot;)plt.plot(vals, te_scores, label=&apos;test_scores&apos;)plt.plot(vals, tr_scores, label=&apos;train_scores&apos;)plt.legend() bestmin: 0.00202020202020202bestscore: 0.7988826815642458 问题：每次使用不同随机切割的数据集得出最佳参数为0.002很接近0，该怎么解读？123456789101112131415至此为我们找到了两个参数,最佳深度depth=5 和最佳min_impurity_decrease=0.002，下面我来用两个参数简历模型进行测试：X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)from sklearn import metrics model = DecisionTreeClassifier(max_depth=5, min_impurity_decrease=0.002)model.fit(X_train, y_train)print(&quot;tees_score:&quot;, model.score(X_test, y_test))y_pred = model.predict(X_test)print(&quot;查准率:&quot;,metrics.precision_score(y_test, y_pred))print(&quot;召回率:&quot;,metrics.recall_score(y_test, y_pred))print(&quot;F1_score:&quot;,metrics.f1_score(y_test, y_pred)) tees_score: 0.7821229050279329查准率: 0.8461538461538461召回率: 0.5866666666666667F1_score: 0.6929133858267718 模型参数选择工具包至此发现以上两种模型优化方法有两问题： 1、数据不稳定：–&gt; 每次重新分配训练集测试集，原参数就不是最优了。 解决办法是多次计算求平均值。 2、不能一次选择多个参数：–&gt; 想考察max_depth和min_impurity_decrease两者结合起来的最优参数就没法实现。 所幸scikit-learn在sklearn.model_selection包提供了大量的模型选择和评估的工具供我们使用。针对该问题可以使用GridSearchCV类来解决。 利用GridSearchCV求最优参数123456789from sklearn.model_selection import GridSearchCVthresholds = np.linspace(0, 0.2, 50)param_grid = &#123;&apos;min_impurity_decrease&apos;:thresholds&#125;clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)clf.fit(X,y)print(&quot;best_parms:&#123;0&#125;\nbest_score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_)) best_parms:{‘min_impurity_decrease’: 0.00816326530612245}best_score:0.8114478114478114 模型解读：1、关键字参数param_grid是一个字典，字典的关键字对应的值是一个列表。GridSearchCV会枚举列表里所有值来构建模型多次计算训练模型，并计算模型评分，最终得出指定参数值的平均评分及标准差。 2、关键参数sv，用来指定交叉验证数据集的生成规则。这里sv=5表示每次计算都把数据集分成5份，拿其中一份作为交叉验证数据集，其他作为训练集。最终得出最优参数及最优评分保存在clf.best_params_和clf.best_score_里。 3、此外clf.cv_results_里保存了计算过程的所有中间结果。 画出学习曲线：1234567891011121314151617def plot_curve(train_sizes, cv_results, xlabel): train_scores_mean = cv_results[&apos;mean_train_score&apos;] train_scores_std = cv_results[&apos;std_train_score&apos;] test_scores_mean = cv_results[&apos;mean_test_score&apos;] test_scores_std = cv_results[&apos;std_test_score&apos;] plt.figure(figsize=(6, 4), dpi=120) plt.title(&apos;parameters turning&apos;) plt.grid() plt.xlabel(xlabel) plt.ylabel(&apos;score&apos;) plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;) plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;) plt.plot(train_sizes, train_scores_mean, &apos;.--&apos;, color=&quot;r&quot;,label=&quot;Training score&quot;) plt.plot(train_sizes, test_scores_mean, &apos;.-&apos;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;) plt.legend(loc=&quot;best&quot;) 123456789101112from sklearn.model_selection import GridSearchCVthresholds = np.linspace(0, 0.2, 50)# Set the parameters by cross-validationparam_grid = &#123;&apos;min_impurity_decrease&apos;: thresholds&#125;clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)clf.fit(X, y)print(&quot;best param: &#123;0&#125;\nbest score: &#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_))# plot_curve(thresholds, clf.cv_results_, xlabel=&apos;gini thresholds&apos;) best param: {‘min_impurity_decrease’: 0.00816326530612245}best score: 0.8114478114478114 多组参数之间选择最优参数：123456789from sklearn.model_selection import GridSearchCVentropy_thresholds = np.linspace(0, 1, 100)gini_thresholds = np.linspace(0, 0.2, 100)#设置参数矩阵：param_grid = [&#123;&apos;criterion&apos;: [&apos;entropy&apos;], &apos;min_impurity_decrease&apos;:entropy_thresholds&#125;,&#123;&apos;criterion&apos;: [&apos;gini&apos;], &apos;min_impurity_decrease&apos;: gini_thresholds&#125;,&#123;&apos;max_depth&apos;: np.arange(2,10)&#125;,&#123;&apos;min_samples_split&apos;: np.arange(2,30,2)&#125;]clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)clf.fit(X, y)print(&quot;best param:&#123;0&#125;\nbest score:&#123;1&#125;&quot;.format(clf.best_params_, clf.best_score_)) best param:{‘min_impurity_decrease’: 0.00816326530612245}best score:0.8114478114478114 结果1、{‘criterion’: ‘gini’, ‘min_impurity_decrease’: 0.00816326530612245} -&gt;6 结果2、{‘min_samples_split’: 22} -&gt;10 结果3、{‘min_samples_split’: 20} -&gt;4 结果波动很大，这里做了20次测试，对应结果1出现6次，结果2出现10次，结果3出现4次。 代码解读：关键部分还是param_grid参数，他是一个列表。很对列表的第一个字典，选择信息墒（entropy）作为判断标准，取值0～1范围50等分； 第二个字典选择基尼系数，min_impurity_decrease取值0～0.2范围50等分。 GridSearchCV会针对列表中的每个字典进行迭代，最终比较列表中每个字典所对应的参数组合，选择出最优的参数。 生成决策树图形下面代码可以生成.dot文件，需要电脑上安装graphviz才能把文件转换成图片格式。 Mac上可以使用brew install graphviz命令来安装，它会同时安装8个依赖包。这里一定注意Mac环境下的权限问题：由于Homebrew默认是安装在/usr/local下，而Mac有强制保护不支持sudo chown -R uname local对local文件夹进行权限修改。 这里的解决方式是把local下bin,lib,Cellar等所需单个文件夹下进行赋权，即可成功安装。 在电脑上安装 graphviz 运行 dot -Tpng tree.dot -o filename.png 在当前目录查看生成的决策树 filename.png 123456789101112from sklearn.tree import DecisionTreeClassifier from sklearn import treeclf = DecisionTreeClassifier(min_samples_split=22)clf = clf.fit(X_train, y_train)train_score = clf.score(X_train, y_train)test_score = clf.score(X_test, y_test)print(&apos;train score: &#123;0&#125;; test score: &#123;1&#125;&apos;.format(train_score, test_score))# 导出 titanic.dot 文件with open(&quot;tree.dot&quot;, &apos;w&apos;) as f: f = tree.export_graphviz(clf, out_file=f) train score: 0.8834269662921348; test score: 0.8268156424581006 模型调参注意事项： 当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型 如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。 推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。 在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。 决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。 如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn之数据预处理和创建模型]]></title>
    <url>%2F2018%2F06%2F05%2Fsklearn%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 数据预处理123456import numpy as npfrom sklearn import preprocessingdata = np.array([[ 3, -1.5, 2, -5.4],[ 0, 4, -0.3, 2.1],[ 1, 3.3, -1.9, -4.3]]) 均值移除 mean removal “通常我们会把每个特征的平均值移除，以保证特征均值为0（即标准化处理）。这样做可以消除特征彼此间的偏差（bias）” 123data_standardized = preprocessing.scale(data)print (&quot;\nMean特征均值 =&quot;, data_standardized.mean(axis=0))print (&quot;Std deviation标准偏差 =&quot;, data_standardized.std(axis=0)) Mean特征均值 = [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]Std deviation标准偏差 = [1. 1. 1. 1.] 范围缩放 min max scaling “数据点中每个特征的数值范围可能变化很大，因此，有时将特征的数值范围缩放到合理的大小是非常重要的。” 123data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))data_scaled = data_scaler.fit_transform(data)print (&quot;\nMin max scaled data范围缩放数据:\n&quot;, data_scaled) Min max scaled data范围缩放数据: [[1. 0. 1. 0. ] [0. 1. 0.41025641 1. ] [0.33333333 0.87272727 0. 0.14666667]] 归一化 normalization “数据归一化用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放到相同的数值范围。机器学习中最常用的归一化形式就是将特征向量调整为L1范数，使特征向量的数值之和为1。” “这个方法经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级，提高不同特征数据的可比性。” 12data_normalized = preprocessing.normalize(data, norm=&apos;l1&apos;)print (&quot;\nL1 normalized data归一化后数据:\n&quot;, data_normalized) L1 normalized data归一化后数据: [[ 0.25210084 -0.12605042 0.16806723 -0.45378151] [ 0. 0.625 -0.046875 0.328125 ] [ 0.0952381 0.31428571 -0.18095238 -0.40952381]] 二值化 binarization “二值化用于将数值特征向量转换为布尔类型向量。” 12data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)print (&quot;\n二值化 data:\n&quot;, data_binarized) 二值化 data: [[1. 0. 1. 0.] [0. 1. 0. 1.] [0. 1. 0. 0.]] 独热编码 one hot encoding独热编码“通常，需要处理的数值都是稀疏地、散乱地分布在空间中，然而，我们并不需要存储这些大数值，这时就需要使用独热编码（One-Hot Encoding）。可以把独热编码看作是一种收紧 （tighten）特征向量的工具。它把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k 的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，这样可以更加有效地表示空间。例如，我们需要处理4维向量空间，当给一个特性向量的第n 个特征进行编码时，编码器会遍历每个特征向量的第n 个特征，然后进行非重复计数。如果非重复计数的值是K ，那么就把这个特征转换为只有一个值是1其他值都是0的K 维向量。” “在下面的示例中，观察一下每个特征向量的第三个特征，分别是1 、5 、2 、4 这4个不重复的值，也就是说独热编码向量的长度是4。如果你需要对5 进行编码，那么向量就是[0, 1, 0, 0] 。向量中只有一个值是1。第二个元素是1，对应的值是5 。” 1234encoder = preprocessing.OneHotEncoder()encoder.fit([[0, 2, 1, 12], [1, 3, 5, 3], [2, 3, 2, 12], [1, 2, 4, 3]])encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()print (&quot;\n编码矢量:\n&quot;, encoded_vector) 编码矢量: [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]] 标记编码方法在监督学习中，经常需要处理各种各样的标记。这些标记可能是数字，也可能是单词。如果标记是数字，那么算法可以直接使用它们，但是，许多情况下，标记都需要以人们可理解的形式存在，因此，人们通常会用单词标记训练数据集。标记编码就是要把单词标记转换成数值形式，让算法懂得如何操作标记。接下来看看如何标记编码。 1234567891011from sklearn import preprocessing# 定义一个标记编码器label_encoder = preprocessing.LabelEncoder()# label_encoder对象知道如何理解单词标记，接下来创建标记input_classes = [&apos;audi&apos;, &apos;ford&apos;, &apos;audi&apos;, &apos;toyota&apos;, &apos;ford&apos;, &apos;bmw&apos;]# 开始标记label_encoder.fit(input_classes)print(&quot;Classes mapping: 结果显示单词背转换成从0开始的索引值&quot;)for i, item in enumerate(lable_encoder.classes_):print(item, &apos;--&gt;&apos;, i) Classes mapping: 结果显示单词背转换成从0开始的索引值audi —&gt; 0bmw —&gt; 1ford —&gt; 2toyota —&gt; 3 这时，如果遇到一组数据就可以轻松的转换它们了。（如药品数据的药品名）12345labels = [&apos;toyota&apos;, &apos;ford&apos;, &apos;audi&apos;]encoded_labels = label_encoder.transform(labels)print (&quot;\nLabels =&quot;, labels)print (&quot;Encoded labels =&quot;, list(encoded_labels)) Labels = [‘toyota’, ‘ford’, ‘audi’]Encoded labels = [3, 2, 0] 还可以数字反转回单词（或字符串）:1234encoded_labels = [2,1,0,3,1]decoded_labels = label_encoder.inverse_transform(encoded_labels)print(encoded_labels)print(list(decoded_labels)) [2, 1, 0, 3, 1][‘ford’, ‘bmw’, ‘audi’, ‘toyota’, ‘bmw’] 创建线性回归回归是估计输入数据与连续值输出数据之间关系的过程。数据通常是实数形式的，我们的目标是估计满足输入到输出映射关系的基本函数。 线性回归的目标是提取输入变量与输出变量的关联线性模型，这就要求实际输出与线性方程预测的输出的残差平方和（sum of squares of differences）最小化。这种方法被称为普通最小二乘法 （Ordinary Least Squares，OLS）。 你可能觉得用一条曲线对这些点进行拟合效果会更好，但是线性回归不允许这样做。线性回归的主要优点就是方程简单。如果你想用非线性回归，可能会得到更准确的模型，但是拟合速度会慢很多。线性回归模型就像前面那张图里显示的，用一条直线近似数据点的趋势 123456789101112import sysimport numpy as np# 加载数据filename = sys.argv[1]X = []y = []with open(&apos;data_singlevar.txt&apos;, &apos;r&apos;) as f: for line in f.readlines(): data = [float(i) for i in line.split(&apos;,&apos;)] xt, yt = data[:-1], data[-1] X.append(xt) y.append(yt) 12345678# 80%训练集和20%测试集num_train = int(0.8 * len(X))num_test = len(X) - num_trainX_train = np.array(X[:num_train]).reshape(num_train,1)y_train = np.array(y[:num_train])X_test = np.array(X[num_train:]).reshape(num_test, 1)y_test = np.array(y[num_train:]) 123from sklearn import linear_modellinear_regr = linear_model.LinearRegression()linear_regr.fit(X_train, y_train) 我们利用训练数据集训练了线性回归器。向fit 方法提供输入数据即可训练模型。用下面的代码看看它如何拟合12345678import matplotlib.pyplot as pltprint(&apos;训练集拟合效果&apos;)y_train_pred = linear_regr.predict(X_train)plt.figure()plt.scatter(X_train, y_train)plt.plot(X_train, y_train_pred, color=&apos;green&apos;, linewidth=2)plt.title(&apos;Training Data&apos;)plt.show() 123456y_test_pred = linear_regr.predict(X_test)print(&quot;测试集拟合效果&quot;)plt.scatter(X_test, y_test)plt.plot(X_test, y_test_pred, color=&apos;green&apos;)plt.title(&quot;Test Data&quot;)plt.show() 计算回归准确性现在已经建立了回归器，接下来最重要的就是如何评价回归器的拟合效果。在模型评价的相关内容中，用误差 （error）表示实际值与模型预测值之间的差值。 下面快速了解几个衡量回归器拟合效果的重要指标（metric）。回归器可以用许多不同的指标进行衡量，部分指标如下所示。 平均绝对误差（mean absolute error） ：这是给定数据集的所有数据点的绝对误差平均值。 均方误差（mean squared error） ：这是给定数据集的所有数据点的误差的平方的平均值。这是最流行的指标之一。 中位数绝对误差（median absolute error） ：这是给定数据集的所有数据点的误差的中位数。这个指标的主要优点是可以消除异常值（outlier）的干扰。测试数据集中的单个坏点不会影响整个误差指标，均值误差指标会受到异常点的影响。 解释方差分（explained variance score） ：这个分数用于衡量我们的模型对数据集波动的解释能力。如果得分1.0分，那么表明我们的模型是完美的。 R方得分（R2 score） ：这个指标读作“R方”，是指确定性相关系数，用于衡量模型对未知样本预测的效果。最好的得分是1.0，值也可以是负数。 “每个指标都描述得面面俱到是非常乏味的，因此只选择一两个指标来评估我们的模型。通常的做法是尽量保证均方误差最低，而且解释方差分最高”12345678910111213141516import sklearn.metrics as smprint(&quot;平均绝对误差（mean absolute error） ：&quot; , round(sm.mean_absolute_error(y_test, y_test_pred), 2))print(&quot;均方误差（mean squared error） ：&quot; , round(sm.mean_squared_error(y_test, y_test_pred), 2))print(&quot;中位数绝对误差（median absolute error） ：&quot; , round(sm.median_absolute_error(y_test, y_test_pred), 2))print(&quot;解释方差分（explained variance score） ：&quot; , round(sm.explained_variance_score(y_test, y_test_pred), 2))print(&quot;R方得分（R2 score） ：&quot; , round(sm.r2_score(y_test, y_test_pred))) 平均绝对误差（mean absolute error） ： 0.54均方误差（mean squared error） ： 0.38中位数绝对误差（median absolute error） ： 0.54解释方差分（explained variance score） ： 0.68R方得分（R2 score） ： 1.0 保存模型数据12345import pickleregr = pickle.dumps(linear_regr) # 保存regr1 = pickle.loads(regr) # 加载regr1.predict(X_test) array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973]) 在scikit的具体情况下，使用 joblib 替换 pickle（ joblib.dump &amp; joblib.load ）可能会更有趣，这对大数据更有效，但只能序列化 (pickle) 到磁盘而不是字符串变量: 之后，您可以加载已保存的模型（可能在另一个 Python 进程中）: 1234from sklearn.externals import joblibjoblib.dump(linear_regr, &apos;regr.pkl&apos;) regr2 = joblib.load(&apos;regr.pkl&apos;) regr2.predict(X_test) array([2.20369892, 4.45873314, 2.12918475, 3.1253216 , 3.21944477,3.75673118, 3.91360313, 2.66647116, 3.32925513, 2.77235973]) 创建岭回归 线性回归的主要问题是对异常值敏感。在真实世界的数据收集过程中，经常会遇到错误的度量结果。而线性回归使用的普通最小二乘法，其目标是使平方误差最小化。这时，由于异常值误差的绝对值很大，因此会引起问题，从而破坏整个模型。 普通最小二乘法在建模时会考虑每个数据点的影响，因此，最终模型就会瘦异常值影响较大。显然，我们发现这个模型不是最优的。为了避免这个问题，我们引入正则化项 的系数作为阈值来消除异常值的影响。这个方法被称为岭回归 。 1234567891011121314151617import pandas as pdX = []y = []with open(&apos;data_multivar.txt&apos;, &apos;r&apos;) as f: for line in f.readlines(): data = [float(i) for i in line.split(&apos;,&apos;)] xt, yt = data[:-1], data[-1] X.append(xt) y.append(yt)# 80%训练集和20%测试集num_train = int(0.8 * len(X))num_test = len(X) - num_trainX_train = np.array(X[:num_train]).reshape(num_train,3)y_train = np.array(y[:num_train])X_test = np.array(X[num_train:]).reshape(num_test, 3)y_test = np.array(y[num_train:]) alpha 参数控制回归器的复杂程度。当alpha 趋于0 时，岭回归器就是用普通最小二乘法的线性回归器。因此，如果你希望模型对异常值不那么敏感，就需要设置一个较大的alpha 值。这里把alpha 值设置为0.01 。123456789101112131415161718192021rid = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)rid.fit(X_train, y_train)y_test_pred = rid.predict(X_test)import sklearn.metrics as smprint(&quot;平均绝对误差（mean absolute error） ：&quot; , round(sm.mean_absolute_error(y_test, y_test_pred), 2))print(&quot;均方误差（mean squared error） ：&quot; , round(sm.mean_squared_error(y_test, y_test_pred), 2))print(&quot;中位数绝对误差（median absolute error） ：&quot; , round(sm.median_absolute_error(y_test, y_test_pred), 2))print(&quot;解释方差分（explained variance score） ：&quot; , round(sm.explained_variance_score(y_test, y_test_pred), 2))print(&quot;R方得分（R2 score） ：&quot; , round(sm.r2_score(y_test, y_test_pred))) 平均绝对误差（mean absolute error） ： 3.95均方误差（mean squared error） ： 23.15中位数绝对误差（median absolute error） ： 3.69解释方差分（explained variance score） ： 0.84R方得分（R2 score） ： 1.0 1234567891011121314from pyecharts import Lineline = Line(&quot;期望值测试对比&quot;)line.add(&apos;测试目标值&apos;, np.linspace(-20,40,len(y_test)), y_test, mark_line=[&quot;average&quot;], is_datazoom_show=True)line.add(&apos;实际测试值&apos;, np.linspace(-20,40,len(y_test)), y_test_pred, mark_line=[&quot;average&quot;], is_datazoom_show=True)line# 80%训练集和20%测试集num_train = int(0.8 * len(X))num_test = len(X) - num_trainX_train = np.array(X[:num_train]).reshape(num_train,1)y_train = np.array(y[:num_train])X_test = np.array(X[num_train:]).reshape(num_test, 1)y_test = np.array(y[num_train:]) 创建多项式回归器（重点）数据点本身的模式中带有自然的曲线，而线性模型是不能捕捉到这一点的。多项式回归模型的曲率是由多项式的次数决定的。随着模型曲率的增加，模型变得更准确。但是，增加曲率的同时也增加了模型的复杂性，因此拟合速度会变慢。当我们对模型的准确性的理想追求与计算能力限制的残酷现实发生冲突时，就需要综合考虑了。 下面使用岭回归的数据，注意和简单线性回归的区别。 12345678910111213141516from sklearn.preprocessing import PolynomialFeatures#将曲线的多项式次数初始值设置为3poly = PolynomialFeatures(degree = 20)# “其中，X_train_transformed 表示多项式形式的输入，与线性回归模型是一样的。”X_train_transformed = poly.fit_transform(X_train)#测试一下dp = X_train[0].reshape(1,-1)poly_dp = poly.fit_transform(dp)poly_liner = linear_model.LinearRegression()poly_liner.fit(X_train_transformed, y_train) #这里注意输入转换后的X_trainprint (&quot;\nLinear regression:&quot;, rid.predict(dp)[0])print (&quot;\nPolynomial regression:&quot;, poly_liner.predict(poly_dp)[0]) ##这输入转换后的X_test Linear regression: -11.058646635286552Polynomial regression: -8.070076359128953 多项式次数为1时 返回预测结果为：-11.058729498335897，欠拟合 多项式次数为10时 返回预测结果为：-8.206005341193759，这里与真实值-8.07已经非常接近了 多项式次数为20时 返回预测结果为：-8.070076359128953，针对这个值的预测最完美 多项式次数为100时 返回预测结果为：10.01397529328105，说明出现过拟合 AdaBoost算法估算房屋价格利用AdaBoost算法的决策树回归器（decision tree regreessor）来估算房屋价格 决策树是一个树状模型，每个节点都做出一个决策，从而影响最终结果。叶子节点表示输出数值，分支表示根据输入特征做出的中间决策。AdaBoost算法是指自适应增强（adaptive boosting）算法，这是一种利用其他系统增强模型准确性的技术。这种技术是将不同版本的算法结果进行组合，用加权汇总的方式获得最终结果，被称为弱学习器 （weak learners）。AdaBoost算法在每个阶段获取的信息都会反馈到模型中，这样学习器就可以在后一阶段重点训练难以分类的样本。这种学习方式可以增强系统的准确性。 首先使用AdaBoost算法对数据集进行回归拟合，再计算误差，然后根据误差评估结果，用同样的数据集重新拟合。可以把这些看作是回归器的调优过程，直到达到预期的准确性。假设你拥有一个包含影响房价的各种参数的数据集，我们的目标就是估计这些参数与房价的关系，这样就可以根据未知参数估计房价了。123456789import numpy as npfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import AdaBoostRegressorfrom sklearn import datasetsfrom sklearn.metrics import mean_squared_error, explained_variance_scorefrom sklearn.utils import shuffleimport matplotlib.pyplot as plthous_data = datasets.load_boston() 123456789101112131415161718192021222324252627# 利用shuffle函数把数据的顺序打乱（参数random_state用来控制如何打乱数据）X, y = shuffle(hous_data.data, hous_data.target, random_state=7)num = int(0.8 * len(X))X_train, y_train = X[:num], y[:num]X_test, y_test = X[num:], y[num:]# 选择最大深度为5的决策树回归模型dtre = DecisionTreeRegressor(max_depth=5)dtre.fit(X_train, y_train)# 再用带AdaBoost算法的决策树回归模型进行拟合与上面进行比较abre = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=400, random_state=7)abre.fit(X_train, y_train)# 看看AdaBoost算法对决策树回归器的训练效果有多大改善y_pred_dt = dtre.predict(X_test)mse = mean_squared_error(y_test, y_pred_dt)evs = explained_variance_score(y_test, y_pred_dt)print(&quot;决策树-均方误差: &quot;, mse)print(&quot;决策树-解释方差: &quot;, evs)y_pred_ab = abre.predict(X_test)mse = mean_squared_error(y_test, y_pred_ab)evs = explained_variance_score(y_test, y_pred_ab)print(&quot;\nAbaBoost决策树-均方误差: &quot;, mse)print(&quot;AbaBoost决策树-解释方差: &quot;, evs) 决策树-均方误差: 12.74782456548819决策树-解释方差: 0.8454595720920495AbaBoost决策树-均方误差: 7.015648111222207AbaBoost决策树-解释方差: 0.9147414844474588 计算特征的相对重要性 （如交通案例计算各出口贡献率）(_modle.feature__importances_) 在这个案例中，我们用了13个特征，它们对模型都有贡献。但是，有一个重要的问题出现了：如何判断哪个特征更加重要？显然，所有的特征对结果的贡献是不一样的。如果需要忽略一些特征，就需要知道哪些特征不太重要。scikit-learn里面有这样的功能。123456789101112131415161718192021def plot_feature_importances(feature_importances, title, feature_names): # 将重要性值标准化 feature_importances = 100.0 * (feature_importances / max(feature_importances)) # 将得分从高到低排序 index_sorted = np.flipud(np.argsort(feature_importances)) # 让X坐标轴上的标签居中显示 pos = np.arange(index_sorted.shape[0]) + 0.5 # 画条形图 plt.figure() plt.bar(pos, feature_importances[index_sorted], align=&apos;center&apos;) plt.xticks(pos, feature_names[index_sorted]) plt.ylabel(&apos;Relative Importance&apos;) plt.title(title) plt.show()# 画出特征的相对重要性plot_feature_importances(dtre.feature_importances_, &apos;Decision Tree regressor&apos;, hous_data.feature_names)plot_feature_importances(abre.feature_importances_, &apos;AdaBoost regressor&apos;, hous_data.feature_names) 上图可以看出不带AbaBoost的决策树回归器显示最重要的特征是RM，而带AbaBoost算法的决策回归器现实的最主要特征是LASTAT。现实生活中如果对这个数据集建立不同的回归器会发现最重要的特征就是LSTAT，这足以体现AbaBoost算法对决策树训练效果的改善。1234567891011from pyecharts import Pieattr = f_namev1 = rf_regr.feature_importances_pie = Pie(&quot;影响房价的因素分析&quot;)pie.add(&quot;决策树回归器&quot;, hous_data.feature_names, dtre.feature_importances_, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[35, 50],radius=[0, 50])pie.add(&quot;AbaBoost决策树&quot;, hous_data.feature_names, abre.feature_importances_,is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;,center=[75, 50],radius=[0, 50])pie 随机森林评估共享单车的需求分布采用随机森林回归器(random forest regressor)估计输出结果。 随机森林死一个决策树合集，它基本上就是用一组由数据集的若干子集构建的决策树构成，再用决策树平均值改善整体学习效果 我们将使用bike_day.csv文件中的数据集，它可以在 https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset 获取。这份数据集一共16列，前两列是序列号与日期，分析的时候可以不用；最后三列数据是不同类型的输出结果；最后一列是第十四列与第十五列的和，因此建立模型时可以不考虑第十四列与第十五列。 参数n_estimators是指评估器（estimator）的数量，表示随机森林需要使用的决策树数量；参数max_depth 是指每个决策树的最大深度；参数min_samples_split是指决策树分裂一个节点需要用到的最小数据样本量。123456789101112131415161718192021222324252627import pandas as pdfrom sklearn.ensemble import RandomForestRegressorfrom housing import plot_feature_importances #这个方法源码参考上例data = pd.read_csv(&apos;bike_day.csv&apos;,sep=&apos;,&apos;)X = data[data.columns[2:13]]y = data[data.columns[-1]]f_name = X.columnsX, y = shuffle(X, y, random_state=7)num = int(0.9 * len(X))X_train, y_train = X[:num], y[:num]X_test, y_test = X[num:], y[num:]rf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=12)rf_regr.fit(X_train, y_train)y_pred = rf_regr.predict(X_test)mse = mean_squared_error(y_test, y_pred)evs = explained_variance_score(y_test, y_pred)print(&quot;随机森林回归器效果：&quot;)print(&quot;均方误差：&quot;, round(mse, 2))print(&quot;解释方差分：&quot;, round(evs, 2)) 随机森林回归器效果：均方误差： 368026.24解释方差分： 0.89 12345678from pyecharts import Pieattr = f_namev1 = rf_regr.feature_importances_pie = Pie(&quot;共享单车因素分析&quot;)pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)pie 利用按小时的数据计算相关性 这里要用到3～14列1len(X_train) 15641 123456789101112131415161718hour_data = pd.read_csv(&apos;bike_hour.csv&apos;, sep=&apos;,&apos;)X = hour_data[hour_data.columns[2:14]]y = hour_data[hour_data.columns[-1]]X, y = shuffle(X, y, random_state=7)num = int(0.9*len(X))X_train, y_train = X[:num], y[:num]X_test, y_test = X[num:], y[num:]f_names = X.columnshrf_regr = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_split=10)hrf_regr.fit(X_train, y_train)y_pred = hrf_regr.predict(X_test)mse = mean_squared_error(y_test, y_pred)evs = explained_variance_score(y_test, y_pred)print(&quot;均方误差：&quot;, mse)print(&quot;解释方差分：&quot;, evs) 均方误差： 1884.1767363623571解释方差分： 0.9414038595964176 1234567attr = f_namesv1 = hrf_regr.feature_importances_pie = Pie(&quot;共享单车因素分析&quot;)pie.add(&quot;因素&quot;, attr, v1, is_label_show=True, label_emphasis_textcolor=&apos;red&apos;,label_emphasis_textsize=14, is_random=True, legend_orient=&apos;vertical&apos;, legend_pos=&apos;1&apos;, legend_top=&apos;40&apos;)pie 由图可见，其中最重要的特征是一天中的不同时间点（hr），其次重要的是温度，这完全符合人们的直觉。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之逻辑回归]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 逻辑回归基础Sigmoid预测函数在逻辑回归中，定义预测函数为： h_\theta (x) = g(z)其中，\(z=\theta^Tx\)是分类边界，且\(g(z)=\frac{1}{1+e^{-z}}\)g(z)称之为 Sigmoid Function，亦称 Logic Function，其函数图像如下： 可以看到，预测函数 hθ(x)被很好地限制在了 0、1 之间，并且，sigmoid 是一个非常好的阈值函数：阈值为 0.5，大于 0.5为 1 类，反之为 0 类。函数曲线过渡光滑自然，关于 0.5中心对称也极具美感。 决策边界 线性决策边界 非线性决策边界 预测代价函数下面两幅图中，左图这样犬牙差互的代价曲线（非凸函数）显然会使我们在做梯度下降的时候陷入迷茫，任何一个极小值都有可能被错认为最小值，但无法获得最优预测精度。但在右图的代价曲线中，就像滑梯一样，我们就很容易达到最小值： 逻辑回归定义的代价函数为： J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})为保证代价函数呈凸形曲线，则定义 \(Cost(h_\theta(x^{(i)}),y^{(i)})\)： Cost(h_\theta(x),y)= \begin{cases} -log(h_\theta(x)),&\mbox{if $y=1$}\\ -log(1-h_\theta(x)),&\mbox{if $y=0$} \end{cases}该函数等价于： \begin{align*} Cost(h_\theta(x),y) &=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \\ &= (\,log\,(g(X\theta))^Ty+(\,log\,(1-g(X\theta))^T(1-y) \end{align*}代价函数随预测值 hθ(x)hθ(x) 的变化如下： 手推LR 过拟合问题正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。 如下例所示，我们将 θ3 及 θ4 减小（惩罚）到趋近于 00，原本过拟合的曲线就变得更加平滑，趋近于一条二次曲线（在本例中，二次曲线显然更能反映住房面积和房价的关系），也就能够更好的根据住房面积来预测房价。 直观来看，如果我们想解决这个例子中的过拟合问题，最好能将的影响消除，也就是让。假设我们对进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大惩罚项，例如： 这样在最小化Cost函数的时候，。正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为： lambda是正则项系数： 1.如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象； 2.如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。 正则化后的梯度下降中θ的更新变为： 正则化后的线性回归的Normal Equation的公式为： scikit-learn 逻辑回归类带L1与L2正则的逻辑回归损失函数scikit-learn在LogisticRegression的sklearn.linear_model.LogisticRegression类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。 作为优化问题，带 L2 正则的二分类 logistic 回归要最小化以下代价函数（cost function）： \underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)类似地，带 L1 正则的 logistic 回归解决的是如下优化问题： \underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) L1 范数作为正则项由以下几个用途： 特征选择： 它会让模型参数向量里的元素为0的点尽量多。 因此可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以 L1 范数解决过拟合措施实际上是减少特征数量。 可解释性： 模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。 这样我们就容易解释模型的因果关系。 比如针对某个癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性成关键作用。 稀疏化后，只留下几个关键特征，就更容易看到因果关系 由此可见， L1 范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。 大部分情况下，我们解决过拟合问题，还是选择 L2 单数作为正则项， 这也是 sklearn 里的默认值。 优化方法参数solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。 saga：线性收敛的随机优化算法的的变重。 Case Solver L1正则 “liblinear” or “saga” 多项式损失（multinomial loss） “lbfgs”, “sag”, “saga” or “newton-cg” 大数据集（n_samples） “sag” or “saga” “saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear” 乳腺癌检测使用逻辑回归算法解决乳腺癌检测问题。 我们需要先采集肿瘤病灶造影图片， 然后对图片进行分析， 从图片中提取特征， 在根据特征来训练模型。 最终使用模型来检测新采集到的肿瘤病灶造影， 判断是良性还是恶性。 这个是典型的二元分类问题。12345678910# 加载数据import numpy as npimport pandas as pdfrom sklearn.datasets import load_breast_cancercancer = load_breast_cancer()X = cancer.datay = cancer.targetprint(X.shape, y.shape,&apos;\n&apos;, X[0], &apos;\n&apos;, y[0]) (569, 30) (569,) [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01 4.601e-01 1.189e-01] 0 实际上它只关注了 10 个特征，然后又构造出来每个特征的标准差及最大值，这样每个特征又衍生出了两个特征，所以共有30个特征。 疑问： 该方式是否直接会导致多重共线性的出现？ 代码实现1234567891011from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)from sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(X_train, y_train)train_score = model.score(X_train, y_train)test_score = model.score(X_test, y_test)y_pred = model.predict(X_test)print(&quot;train_score:&quot;, train_score)print(&quot;test_score:&quot;, test_score) train_score: 0.9626373626373627test_score: 0.9473684210526315 1234567```from sklearn.metrics import precision_score, recall_score, f1_scoreprint(&quot;查准率：&quot;, precision_score(y_test, y_pred))print(&quot;召回率：&quot;, recall_score(y_test, y_pred))print(&quot;F1Score：&quot;, f1_score(y_test, y_pred))print(np.equal(y_pred, y_test).shape[0], y_test.shape[0]) # 输出预测匹配成功数量和测试样本的数量 查准率： 0.9358974358974359 召回率： 0.9733333333333334F1Score： 0.954248366013072114 114 这里数量上显示全部都预测正确，而test_score却不是1，是因为sklearn不是使用这个数据来计算得分，因为这个数据不能完全反映误差情况，而是使用预测概率来计算模型得分。 查看预测自信度二元分类模型会针对每个样本输出的两个概率，即0和1的概率，哪个概率高就预测器哪个类别。我们可以找出针对测试数据集，模型预测的“自信度”低于90%的样本。我们先计算出测试数据集里每个样本的预测概率数据，针对每个样本会有两个数据：一个预测为0，一个预测为1。结合找出预测为阴性和阳性的概率大于0.1的样本。我们可以看下概率数据： 1234# 计算每个测试样本的预测概率：y_pred_proba = model.predict_proba(X_test)print(&quot;自信度示例：&quot;,y_pred_proba[0]) 自信度示例： [0.00452578 0.99547422] 12345y_pred_proba_0 = y_pred_proba[:, 0] &gt; 0.1result = y_pred_proba[y_pred_proba_0]y_pred_proba_1 = result[:, 1] &gt; 0.1print(result[y_pred_proba_1]) [[0.11338788 0.88661212] [0.18245824 0.81754176] [0.13110396 0.86889604] [0.35245276 0.64754724] [0.30664405 0.69335595] [0.24931118 0.75068882] [0.8350464 0.1649536 ] [0.44807883 0.55192117] [0.74071324 0.25928676] [0.43085792 0.56914208] [0.13388416 0.86611584] [0.33507985 0.66492015] [0.53672412 0.46327588] [0.11422612 0.88577388] [0.42946531 0.57053469] [0.69759146 0.30240854] [0.25982004 0.74017996] [0.12179042 0.87820958] [0.88546887 0.11453113]] 模型优化12345678910#这里使用Pipeline来增加多项式特征from sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.pipeline import Pipelinedef poly_model(degree=2, penalty=penalty): poly_features = PolynomialFeatures(degree=degree, include_bias=False) log_regr = LogisticRegression(penalty=penalty) # 注意这里是L1而不是11，指的是使用L1范式作为其正则项 pipeline = Pipeline([(&quot;poly_features&quot;,poly_features),(&quot;log_regr&quot;,log_regr)]) return pipeline 12345678910# 接着增加二阶多项式特征，创建并训练模型import timemodel = poly_model(degree=2, penalty=&apos;l1&apos;)start = time.clock()model.fit(X_train, y_train)print(&quot;train_score:&quot;,model.score(X_train, y_train))print(&quot;test_score:&quot;,model.score(X_test, y_test)) train_score: 0.9934065934065934test_score: 0.9649122807017544 这里要注意的是使用L1范式作为其正则项，参数为penalty=l1。L1范数作为其正则项，可以实现参数的稀疏化，即自动帮我买选择出哪些对模型有关联的特征。我买可以观察下有多少个特征没有被丢弃即对应的模型参数θj非0： 123log_regr = model.named_steps[&apos;log_regr&apos;]print(&quot;特征总量：&quot;,log_regr.coef_.shape[1])print(&quot;特征保留量：&quot;, np.count_nonzero(log_regr.coef_)) 特征总量： 495特征保留量： 114 逻辑回归模型的coef_属性里保存的就是模型参数。 从输出结果看，增加二阶多项式特征后，输入特征由原来的30个增加到了595个，在L1范数的“惩罚”下最终只保留了92个有效特征 实验：利用决策树画出原始数据对预测相关性非0对特征12345678910from sklearn.tree import DecisionTreeRegressordtmodel = DecisionTreeRegressor(max_depth=5)dtmodel.fit(X_train, y_train)print(&quot;train_score&quot;, dtmodel.score(X_train, y_train))print(&quot;test_score&quot;, dtmodel.score(X_test, y_test))from pyecharts import Barindex = np.nonzero(dtmodel.feature_importances_)bar = Bar()bar.add(&quot;&quot;, cancer.feature_names[index],dtmodel.feature_importances_[index])bar train_score 0.9910875596851206test_score 0.6296416546416548 评估模型：画出学习曲线首先画出L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：123456789101112131415161718192021222324%matplotlib inlinefrom matplotlib import pyplot as pltfrom sklearn.model_selection import learning_curvefrom sklearn.model_selection import ShuffleSplitdef plot_learn_curve(estimator, title, X, y, ylim = None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1., 5)): plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel(&quot;train exs&quot;) plt.ylabel(&quot;Score&quot;) train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) train_score_mean = np.mean(train_scores, axis=1) train_score_std = np.std(train_scores, axis=1) test_score_mean = np.mean(test_scores, axis=1) test_score_std = np.std(test_scores, axis=1) plt.grid() plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=&apos;r&apos;) plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std, alpha=0.1, color=&apos;g&apos;) plt.plot(train_sizes, train_score_mean, &apos;o-&apos;, color=&apos;r&apos;, label=&apos;train score训练得分&apos;) plt.plot(train_sizes, test_score_mean, &apos;o-&apos;, color=&apos;g&apos;, label=&apos;cross-validation score交叉验证得分&apos;) plt.legend(loc=&apos;best&apos;) return plt 1234567891011121314cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)titles = [&quot;degree:1 penalty=L1&quot;,&quot;degree:2 penalty=L1&quot;]degrees = [1,2]penalty = &apos;l1&apos;start = time.clock()plt.figure(figsize=(12,4), dpi=120)for i in range(len(degrees)): plt.subplot(1, len(degrees), i + 1) plot_learn_curve(poly_model(degree=degrees[i], penalty=penalty), titles[i], X, y, ylim = (0.8, 1.01), cv = cv)print(&apos;耗时：&apos;, time.clock() - start)plt.show() L2范数作为正则项画出对应一阶和二阶多项式学习曲线123456789101112131415import timecv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)titles = [&quot;degree:1 penalty=L2&quot;,&quot;degree:2 penalty=L2&quot;]degrees = [1,2]penalty = &apos;l2&apos;start = time.clock()plt.figure(figsize=(12,4), dpi=120)for i in range(len(degrees)): plt.subplot(1, len(degrees), i + 1) plot_learn_curve(poly_model(degree=degrees[i],penalty=penalty), titles[i], X, y, ylim = (0.8, 1.01), cv = cv)print(&apos;耗时：&apos;, time.clock() - start)plt.show() 从上面两个图可以看出，使用二阶多项式并使用L1范数作为正则项的模型最优，训练样本评分最高，交叉验证样本评分最高。训练样本评分和交叉验证样本评分之间的间隙还比较大，这说明可以通过采集更多数据来训练模型，以便进一步优化模型. 通过时间消耗对比上可以看出利用L1范式作为正则项需要花费的时间更多，是因为sklearn的learning_curve()函数在画学习曲线的过程中要对模型进行多次训练，并计算交叉验证样本评分。同时为了让曲线更平滑，针对每个点还会进行多次计算球平均值。这个就是ShufferSplit类的作用。在这个实例里只有569个样本是很小的数据集。如果数据集增加100倍，拿出来画学习曲线将是场灾难。 问题是针对大数据集，怎么画学习曲线？ 思路一：可以考虑从大数据集选取一小部分数据来画学习曲线，待选择好最优的模型之后，在使用全部的数据来训练模型。这时需要警惕的是，尽量保证选择出来的这部分数据的标签分布与大数据集的标签分布相同，如针对二元分类，阳性和阴性比例要一致！ 总结1.LR中损失函数的意义是什么？在LR中，最大似然函数与最小化对数损失函数等价 2. LR与线性回归的联系和区别逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 3.LR与最大熵模型逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然； 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 4.LR与svm不同点: 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。 逻辑回归对概率建模，svm对分类超平面建模 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响 逻辑回归是统计方法，svm是几何方法 5.LR与朴素贝叶斯 相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)形式跟逻辑回归是一样的。 不同的地方在于，逻辑回归为判别模型求的是p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。 6. 多分类-softmax如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。 7.LR模型在工业界的应用常见应用场景 预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等） 分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等） LR适用上述场景的原因 LR模型自身的特点具备了应用广泛性 模型易用：LR模型建模思路清晰，容易理解与掌握； 概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上； 强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性； 简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline； 参考文献：https://blog.csdn.net/joycewyj/article/details/51596797]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib基本操作]]></title>
    <url>%2F2018%2F06%2F03%2FMatplotlib%20%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一张图胜过一千句话 基础操作1、设置坐标轴12345678910111213141516171819202122232425262728293031import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2#使用`plt.figure`定义一个图像窗口. 使用`plt.plot`画(`x` ,`y2`)曲线. 使用`plt.plot`画(`x` ,`y1`)曲线，曲线的颜色属性(`color`)为红色;曲线的宽度(`linewidth`)为1.0；曲线的类型(`linestyle`)为虚线。plt.figure()plt.plot(x, y2)plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)#使用`plt.xlim`设置x坐标轴范围：(-1, 2)； 使用`plt.ylim`设置y坐标轴范围：(-2, 3)； 使用`plt.xlabel`设置x坐标轴名称：’I am x’； 使用`plt.ylabel`设置y坐标轴名称：’I am y’；plt.xlim((-1, 2))plt.ylim((-2, 3))plt.xlabel(&apos;I am x&apos;)plt.ylabel(&apos;I am y&apos;)#使用`np.linspace`定义范围以及个数：范围是(-1,2);个数是5\. 使用`print`打印出新定义的范围. 使用`plt.xticks`设置x轴刻度：范围是(-1,2);个数是5.new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)#使用`plt.yticks`设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用`plt.show`显示图像.plt.yticks([-2, -1.8, -1, 1.22, 3],[r&apos;$really\ bad$&apos;, r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;, r&apos;$really\ good$&apos;])plt.show() 2、调整坐标轴12345678910111213141516171819202122232425262728293031323334353637383940414243x = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()plt.plot(x, y2)plt.plot(x, y1, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;)plt.xlim((-1, 2))plt.ylim((-2, 3))new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3],[&apos;$really\ bad$&apos;, &apos;$bad$&apos;, &apos;$normal$&apos;, &apos;$good$&apos;, &apos;$really\ good$&apos;])ax = plt.gca()ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)#使用`.xaxis.set_ticks_position`设置x坐标刻度数字或名称的位置：`bottom`.（所有位置：`top`，`bottom`，`both`，`default`，`none`）ax.xaxis.set_ticks_position(&apos;bottom&apos;)#使用`.spines`设置边框：x轴；使用`.set_position`设置边框位置：y=0的位置；（位置所有属性：`outward`，`axes`，`data`）ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))#使用`.yaxis.set_ticks_position`设置y坐标刻度数字或名称的位置：`left`.（所有位置：`left`，`right`，`both`，`default`，`none`）ax.yaxis.set_ticks_position(&apos;left&apos;)#使用`.spines`设置边框：y轴；使用`.set_position`设置边框位置：x=0的位置；（位置所有属性：`outward`，`axes`，`data`） 使用`plt.show`显示图像。ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))# set line sylesl1, = plt.plot(x, y1, label=&apos;linear line&apos;)l2, = plt.plot(x, y2, color=&apos;red&apos;, linewidth=1.0, linestyle=&apos;--&apos;, label=&apos;square line&apos;)#参数 `loc=&apos;upper right&apos;` 表示图例将添加在图中的右上角.plt.legend(loc=&apos;upper right&apos;)plt.show() 3、辅助线和标识123456789101112131415161718192021222324252627282930313233import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1#挪动坐标系ax = plt.gca()ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.xaxis.set_ticks_position(&apos;bottom&apos;)ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0))ax.yaxis.set_ticks_position(&apos;left&apos;)ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0))#辅助线plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], &apos;k--&apos;, linewidth=2.5)# set dot stylesplt.scatter([x0, ], [y0, ], s=50, color=&apos;b&apos;)#标注，其中参数xycoords=&apos;data&apos; 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords=&apos;offset points&apos; 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置.plt.annotate(r&apos;$2x+1=%s$&apos; % y0, xy=(x0, y0), xycoords=&apos;data&apos;, xytext=(+30, -30), textcoords=&apos;offset points&apos;, fontsize=16, arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.2&quot;))#注释plt.text(-3.7, 3, r&apos;$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$&apos;, fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;) 4、3D图框1234567891011121314import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)# X, Y valueX = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y) # x-y 平面的网格R = np.sqrt(X ** 2 + Y ** 2)# height valueZ = np.sin(R) 绘制动态图使用matplotlib为Jupyter / IPython中的动画图创建一些选项： 在循环中使用display使用IPython.display.display(fig)在输出中显示图形。 使用一个循环，你需要在显示一个新数字之前清除输出。 请注意，这种技术通常不会那么流畅。 因此我会建议使用下面的任何一个。 %matplotlib notebook使用IPython magic %matplotlib notebook将后端设置为笔记本后端。 这样可以保持图形不会显示静态PNG文件，因此也可以显示动画。 %matplotlib tk使用IPython magic %matplotlib tk将后端设置为tk后端。 这将在一个新的绘图窗口中打开这个图形，这是一个互动的，因此也可以显示动画。 将动画转换为mp4视频 （已提供@Perfi选项）： 12from IPython.display import HTMLHTML(ani.to_html5_video()) 或者在笔记本的开头使用plt.rcParams[&quot;animation.html&quot;] = &quot;html5&quot; 。 这需要将ffmpeg视频编解码器转换为HTML5视频。 视频然后显示在内。 因此，这与%matplotlib inline后端兼容。 完整的例子： 将动画转换为JavaScript ： 12from IPython.display import HTMLHTML(ani.to_jshtml()) 或者在笔记本的开头使用plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; 。 这将使用JavaScript将动画显示为HTML。 这与大多数新浏览器以及%matplotlib inline后端都非常兼容。 它在matplotlib 2.1或更高版本中可用。 1、sin动态点曲线12345678910111213141516171819202122232425262728293031323334%matplotlib notebookimport numpy as np import matplotlib.pyplot as pltfrom matplotlib import animation&quot;&quot;&quot;animation example 2author: Kiterun&quot;&quot;&quot;fig, ax = plt.subplots()x = np.linspace(0, 2*np.pi, 200)y = np.sin(x)l = ax.plot(x, y)dot, = ax.plot([], [], &apos;ro&apos;)def init(): ax.set_xlim(0, 2*np.pi) ax.set_ylim(-1, 1) return ldef gen_dot(): for i in np.linspace(0, 2*np.pi, 200): newdot = [i, np.sin(i)] yield newdotdef update_dot(newd): dot.set_data(newd[0], newd[1]) return dot,ani = animation.FuncAnimation(fig, update_dot, frames = gen_dot, interval = 100, init_func=init)ani.save(&apos;sin_dot.gif&apos;, writer=&apos;imagemagick&apos;, fps=30)plt.show() 2、动态雨点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859%matplotlib notebookimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib import animation# New figure with white backgroundfig = plt.figure(figsize=(6,6), facecolor=&apos;white&apos;)# New axis over the whole figure, no frame and a 1:1 aspect ratioax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)# Number of ringn = 50size_min = 50size_max = 50 ** 2# Ring positionpos = np.random.uniform(0, 1, (n,2))# Ring colorscolor = np.ones((n,4)) * (0,0,0,1)# Alpha color channel geos from 0(transparent) to 1(opaque)color[:,3] = np.linspace(0, 1, n)# Ring sizessize = np.linspace(size_min, size_max, n)# Scatter plotscat = ax.scatter(pos[:,0], pos[:,1], s=size, lw=0.5, edgecolors=color, facecolors=&apos;None&apos;)# Ensure limits are [0,1] and remove ticksax.set_xlim(0, 1), ax.set_xticks([])ax.set_ylim(0, 1), ax.set_yticks([])def update(frame): global pos, color, size # Every ring is made more transparnt color[:, 3] = np.maximum(0, color[:,3]-1.0/n) # Each ring is made larger size += (size_max - size_min) / n # Reset specific ring i = frame % 50 pos[i] = np.random.uniform(0, 1, 2) size[i] = size_min color[i, 3] = 1 # Update scatter object scat.set_edgecolors(color) scat.set_sizes(size) scat.set_offsets(pos) # Return the modified object return scat,anim = animation.FuncAnimation(fig, update, interval=10, blit=True, frames=200)plt.show() 3、阻尼摆123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from math import sin, cosimport numpy as npfrom scipy.integrate import odeintimport matplotlib.pyplot as pltimport matplotlib.animation as animation%matplotlib notebookg = 9.8leng = 1.0b_const = 0.2# no decay case:def pendulum_equations1(w, t, l): th, v = w dth = v dv = - g/l * sin(th) return dth, dv# the decay exist case:def pendulum_equations2(w, t, l, b): th, v = w dth = v dv = -b/l * v - g/l * sin(th) return dth, dvt = np.arange(0, 20, 0.1)track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))xdata = [leng*sin(track[i, 0]) for i in range(len(track))]ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]fig, ax = plt.subplots()ax.grid()line, = ax.plot([], [], &apos;o-&apos;, lw=2)time_template = &apos;time = %.1fs&apos;time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)def init(): ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) time_text.set_text(&apos;&apos;) return line, time_textdef update(i): newx = [0, xdata[i]] newy = [0, ydata[i]] line.set_data(newx, newy) time_text.set_text(time_template %(0.1*i)) return line, time_textani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)plt.show() 4、内切滚动球123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from math import sin, cosimport numpy as npfrom scipy.integrate import odeintimport matplotlib.pyplot as pltimport matplotlib.animation as animation%matplotlib notebookg = 9.8leng = 1.0b_const = 0.2# no decay case:def pendulum_equations1(w, t, l): th, v = w dth = v dv = - g/l * sin(th) return dth, dv# the decay exist case:def pendulum_equations2(w, t, l, b): th, v = w dth = v dv = -b/l * v - g/l * sin(th) return dth, dvt = np.arange(0, 20, 0.1)track = odeint(pendulum_equations1, (1.0, 0), t, args=(leng,))#track = odeint(pendulum_equations2, (1.0, 0), t, args=(leng, b_const))xdata = [leng*sin(track[i, 0]) for i in range(len(track))]ydata = [-leng*cos(track[i, 0]) for i in range(len(track))]fig, ax = plt.subplots()ax.grid()line, = ax.plot([], [], &apos;o-&apos;, lw=2)time_template = &apos;time = %.1fs&apos;time_text = ax.text(0.05, 0.9, &apos;&apos;, transform=ax.transAxes)def init(): ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) time_text.set_text(&apos;&apos;) return line, time_textdef update(i): newx = [0, xdata[i]] newy = [0, ydata[i]] line.set_data(newx, newy) time_text.set_text(time_template %(0.1*i)) return line, time_textani = animation.FuncAnimation(fig, update, range(1, len(xdata)), init_func=init, interval=50)#ani.save(&apos;single_pendulum_decay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)ani.save(&apos;single_pendulum_nodecay.gif&apos;, writer=&apos;imagemagick&apos;, fps=100)plt.show() 5、分类超平面可视化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 算法可视化# -*- coding:utf-8 -*-%matplotlib notebookimport copyfrom matplotlib import pyplot as pltfrom matplotlib import animation training_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]]w = [0, 0]b = 0history = [] def update(item): &quot;&quot;&quot; update parameters using stochastic gradient descent :param item: an item which is classified into wrong class :return: nothing &quot;&quot;&quot; global w, b, history w[0] += 1 * item[1] * item[0][0] w[1] += 1 * item[1] * item[0][1] b += 1 * item[1] print(w, b) history.append([copy.copy(w), b]) # you can uncomment this line to check the process of stochastic gradient descent def cal(item): &quot;&quot;&quot; calculate the functional distance between &apos;item&apos; an the dicision surface. output yi(w*xi+b). :param item: :return: &quot;&quot;&quot; res = 0 for i in range(len(item[0])): res += item[0][i] * w[i] res += b res *= item[1] return res def check(): &quot;&quot;&quot; check if the hyperplane can classify the examples correctly :return: true if it can &quot;&quot;&quot; flag = False for item in training_set: if cal(item) &lt;= 0: flag = True update(item) # draw a graph to show the process if not flag: print (&quot;RESULT: w: &quot; + str(w) + &quot; b: &quot; + str(b)) return flag if __name__ == &quot;__main__&quot;: for i in range(1000): if not check(): break # first set up the figure, the axis, and the plot element we want to animate fig = plt.figure() ax = plt.axes(xlim=(0, 2), ylim=(-2, 2)) line, = ax.plot([], [], &apos;g&apos;, lw=2) label = ax.text([], [], &apos;&apos;) # initialization function: plot the background of each frame def init(): line.set_data([], []) x, y, x_, y_ = [], [], [], [] for p in training_set: if p[1] &gt; 0: x.append(p[0][0]) y.append(p[0][1]) else: x_.append(p[0][0]) y_.append(p[0][1]) plt.plot(x, y, &apos;bo&apos;, x_, y_, &apos;rx&apos;) plt.axis([-6, 6, -6, 6]) plt.grid(True) plt.xlabel(&apos;x&apos;) plt.ylabel(&apos;y&apos;) plt.title(&apos;Perceptron Algorithm&apos;) return line, label # animation function. this is called sequentially def animate(i): global history, ax, line, label w = history[i][0] b = history[i][1] if w[1] == 0: return line, label x1 = -7 y1 = -(b + w[0] * x1) / w[1] x2 = 7 y2 = -(b + w[0] * x2) / w[1] line.set_data([x1, x2], [y1, y2]) x1 = 0 y1 = -(b + w[0] * x1) / w[1] label.set_text(history[i]) label.set_position([x1, y1]) return line, label # call the animator. blit=true means only re-draw the parts that have changed. print (history) anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=1000, repeat=True, blit=True) plt.show() anim.save(&apos;perceptron.gif&apos;, fps=2, writer=&apos;imagemagick&apos;) python其他可视化模块 Traits-为Python添加类型定义 TraitsUI-制作用户界面 Chaco-交互式图表 TVTK-三维可视化数据 Visual-制作3D演示动画 Mayavi-更方便的可视化]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown公式编辑]]></title>
    <url>%2F2018%2F06%2F03%2Fmarkdown%E5%85%AC%E5%BC%8F%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[加载mathjax 引入脚本对网页进行渲染1&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 基础知识插入方式 这里分两种，一种是行间插入，另一种是另取一行 行内插入1\\(a+b\\) \(a+b\) 单独一行1$$a + b$$ a + b基本类型上、下标12345678910$$x_1$$$$x_1^2$$$$x^2_1$$$$x_&#123;22&#125;^&#123;(n)&#125;$$ #多于一位是要加 `&#123;&#125;` 包裹的。$$x_&#123;balabala&#125;^&#123;bala&#125;$$ x_1x_1^2x^2_1x_{22}^{(n)}x_{balabala}^{bala}分式123$$\frac&#123;x+y&#125;&#123;2&#125;$$$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$ \frac{x+y}{2}\frac{1}{1+\frac{1}{2}}根式12345$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$ \sqrt{2}0 \end{cases}常用数学符号希腊字母123456789101112131415161718192021222324252627$$\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;\hline&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\\hline&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\\hline&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\\hline&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\\hline&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\\hline&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\\hline&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\\hline&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\\hline&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\\hline\end&#123;array&#125;$$ \begin{array}{|c|c|c|c|c|c|c|c|} \hline {\alpha} & {\backslash alpha} & {\theta} & {\backslash theta} & {o} & {o} & {\upsilon} & {\backslash upsilon} \\\\ \hline {\beta} & {\backslash beta} & {\vartheta} & {\backslash vartheta} & {\pi} & {\backslash pi} & {\phi} & {\backslash phi} \\\\ \hline {\gamma} & {\backslash gamma} & {\iota} & {\backslash iota} & {\varpi} & {\backslash varpi} & {\varphi} & {\backslash varphi} \\\\ \hline {\delta} & {\backslash delta} & {\kappa} & {\backslash kappa} & {\rho} & {\backslash rho} & {\chi} & {\backslash chi} \\\\ \hline {\epsilon} & {\backslash epsilon} & {\lambda} & {\backslash lambda} & {\varrho} & {\backslash varrho} & {\psi} & {\backslash psi} \\\\ \hline {\varepsilon} & {\backslash varepsilon} & {\mu} & {\backslash mu} & {\sigma} & {\backslash sigma} & {\omega} & {\backslash omega} \\\\ \hline {\zeta} & {\backslash zeta} & {\nu} & {\backslash nu} & {\varsigma} & {\backslash varsigma} & {} & {} \\\\ \hline {\eta} & {\backslash eta} & {\xi} & {\backslash xi} & {\tau} & {\backslash tau} & {} & {} \\\\ \hline {\Gamma} & {\backslash Gamma} & {\Lambda} & {\backslash Lambda} & {\Sigma} & {\backslash Sigma} & {\Psi} & {\backslash Psi} \\\\ \hline {\Delta} & {\backslash Delta} & {\Xi} & {\backslash Xi} & {\Upsilon} & {\backslash Upsilon} & {\Omega} & {\backslash Omega} \\\\ \hline {\Omega} & {\backslash Omega} & {\Pi} & {\backslash Pi} & {\Phi} & {\backslash Phi} & {} & {} \\\\ \hline \end{array}参考资料 Markdown中插入数学公式的方法 LATEX数学公式基本语法 一份其实很短的 LaTeX 入门文档]]></content>
      <categories>
        <category>写作</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM可视化]]></title>
    <url>%2F2018%2F06%2F02%2Fsvm%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 12345678910111213141516171819202122232425262728293031323334353637%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as npclass1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2]])class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4]])plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Decision Boundary&apos;)plt.xlim(0, 8)plt.ylim(0, 6)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;r&apos;)plt.plot([3, 3], [0.5, 6], &apos;--b&apos;)plt.arrow(4, 4, -1, 0, shape=&apos;full&apos;, color=&apos;b&apos;, linestyle=&apos;--&apos;)plt.annotate(r&apos;margin 1&apos;, xy=(3.5, 4), xycoords=&apos;data&apos;, xytext=(3.1, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;margin 2&apos;, xy=(3.5, 3.5), xycoords=&apos;data&apos;, xytext=(4, 3.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;support vector&apos;, xy=(4, 4), xycoords=&apos;data&apos;, xytext=(5, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;support vector&apos;, xy=(2, 2), xycoords=&apos;data&apos;, xytext=(0.5, 1.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 1234567891011121314151617181920212223242526272829303132333435plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Support Vector Machine&apos;)plt.xlim(0, 8)plt.ylim(0, 6)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)plt.arrow(4, 4, -1, -1, shape=&apos;full&apos;, color=&apos;b&apos;)plt.annotate(r&apos;$w^T x + b = 0$&apos;, xy=(5, 1), xycoords=&apos;data&apos;, xytext=(6, 1), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$w^T x + b = 1$&apos;, xy=(6, 2), xycoords=&apos;data&apos;, xytext=(7, 2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$w^T x + b = -1$&apos;, xy=(3.5, 0.5), xycoords=&apos;data&apos;, xytext=(4.5, 0.2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;d&apos;, xy=(3.5, 3.5), xycoords=&apos;data&apos;, xytext=(2, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;A&apos;, xy=(4, 4), xycoords=&apos;data&apos;, xytext=(5, 4.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn.datasets import make_blobsplt.figure(figsize=(10, 4), dpi=140)# sub plot 1plt.subplot(1, 2, 1)X, y = make_blobs(n_samples=100, n_features=2, centers=[(1, 1), (2, 2)], random_state=4, shuffle=False, cluster_std=0.4)plt.title(&apos;Non-linear Separatable&apos;)plt.xlim(0, 3)plt.ylim(0, 3)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(X[y==0][:, 0], X[y==0][:, 1], marker=&apos;o&apos;)plt.scatter(X[y==1][:, 0], X[y==1][:, 1], marker=&apos;s&apos;)plt.plot([0.5, 2.5], [2.5, 0.5], &apos;-r&apos;)# sub plot 2plt.subplot(1, 2, 2)class1 = np.array([[1, 1], [1, 3], [2, 1], [1, 2], [2, 2], [1.5, 1.5], [1.2, 1.7]])class2 = np.array([[4, 4], [5, 5], [5, 4], [5, 3], [4, 5], [6, 4], [5.5, 3.5], [4.5, 4.5], [2, 1.5]])plt.title(&apos;Slack Variable&apos;)plt.xlim(0, 7)plt.ylim(0, 7)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [5, 1], &apos;-r&apos;)plt.plot([0, 4], [4, 0], &apos;--b&apos;, [2, 6], [6, 2], &apos;--b&apos;)plt.arrow(2, 1.5, 2.25, 2.25, shape=&apos;full&apos;, color=&apos;b&apos;)plt.annotate(r&apos;violate margin rule.&apos;, xy=(2, 1.5), xycoords=&apos;data&apos;, xytext=(0.2, 0.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;normal sample. $\epsilon = 0$&apos;, xy=(4, 5), xycoords=&apos;data&apos;, xytext=(4.5, 5.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$\epsilon &gt; 0$&apos;, xy=(3, 2.5), xycoords=&apos;data&apos;, xytext=(3, 1.5), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 12345678910111213141516171819202122plt.figure(figsize=(6, 4), dpi=120)plt.title(&apos;Cost&apos;)plt.xlim(0, 4)plt.ylim(0, 2)plt.xlabel(&apos;$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$&apos;)plt.ylabel(&apos;Cost&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot([0, 1], [1.5, 0], &apos;-r&apos;)plt.plot([1, 3], [0.015, 0.015], &apos;-r&apos;)plt.annotate(r&apos;$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$&apos;, xy=(0.7, 0.5), xycoords=&apos;data&apos;, xytext=(1, 1), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.annotate(r&apos;$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$&apos;, xy=(1.5, 0), xycoords=&apos;data&apos;, xytext=(1.8, 0.2), fontsize=10, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;)) 123456789101112131415161718192021222324252627282930313233343536373839plt.figure(figsize=(10, 4), dpi=144)class1 = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [3, 2], [4, 1], [5, 1]])class2 = np.array([[2.2, 4], [1.5, 5], [1.8, 4.6], [2.4, 5], [3.2, 5], [3.7, 4], [4.5, 4.5], [5.4, 3]])# sub plot 1plt.subplot(1, 2, 1)plt.title(&apos;Non-linear Separatable in Low Dimension&apos;)plt.xlim(0, 6)plt.ylim(0, 6)plt.yticks(())plt.xlabel(&apos;X1&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.spines[&apos;left&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], np.zeros(class1[:, 0].shape[0]) + 0.05, marker=&apos;o&apos;)plt.scatter(class2[:, 0], np.zeros(class2[:, 0].shape[0]) + 0.05, marker=&apos;s&apos;)# sub plot 2plt.subplot(1, 2, 2)plt.title(&apos;Linear Separatable in High Dimension&apos;)plt.xlim(0, 6)plt.ylim(0, 6)plt.xlabel(&apos;X1&apos;)plt.ylabel(&apos;X2&apos;)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.scatter(class1[:, 0], class1[:, 1], marker=&apos;o&apos;)plt.scatter(class2[:, 0], class2[:, 1], marker=&apos;s&apos;)plt.plot([1, 5], [3.8, 2], &apos;-r&apos;) 123456789101112131415161718192021222324252627282930313233def gaussian_kernel(x, mean, sigma): return np.exp(- (x - mean)**2 / (2 * sigma**2))x = np.linspace(0, 6, 500)mean = 1sigma1 = 0.1sigma2 = 0.3plt.figure(figsize=(10, 3), dpi=144)# sub plot 1plt.subplot(1, 2, 1)plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma1))plt.xlim(0, 2)plt.ylim(0, 1.1)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot(x, gaussian_kernel(x, mean, sigma1), &apos;r-&apos;)# sub plot 2plt.subplot(1, 2, 2)plt.title(&apos;Gaussian for $\sigma=&#123;0&#125;$&apos;.format(sigma2))plt.xlim(0, 2)plt.ylim(0, 1.1)ax = plt.gca() # gca 代表当前坐标轴，即 &apos;get current axis&apos;ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏坐标轴ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)plt.plot(x, gaussian_kernel(x, mean, sigma2), &apos;r-&apos;)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？]]></title>
    <url>%2F2018%2F06%2F01%2F%E5%A6%82%E4%BD%95%E7%94%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E6%AF%94%E5%96%BB%E6%8F%8F%E8%BF%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%EF%BC%9FHadoop%E3%80%81Hive%E3%80%81Spark%20%E4%B9%8B%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？这是知乎上某大神的解释： 学习很重要的是能将纷繁复杂的信息进行归类和抽象。对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题： 存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka； 计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等； 查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等； 大数据技术生态其实是一个江湖….在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…这三本秘籍分别为： 《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据； 《Google MapReduce》：论述了怎样快速计算海量的数据； 《Google BigTable》：论述了怎样实现海量数据的快速查询； 以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术； 在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。Hadoop包括三大部分，分别是hdfs、MapReduce和hbase： hdfs解决大数据的存储问题。 mapreduce解决大数据的计算问题。 hbase解决大数据量的查询问题。 之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进： 1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍； 2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。 3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。 4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼； 原文地址：知乎]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法总结]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 梯度梯度实际上就是多变量微分的一般化。 梯度是微积分中一个很重要的概念，之前提到过梯度的意义 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。 此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离。 多元函数的梯度下降我们假设有一个目标函数 现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！我们假设初始的起点为： 初始的学习率为： 函数的梯度为： 进行多次迭代： 我们发现，已经基本靠近函数的最小值点 学习率选择合适的学习速率 假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果 learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。 虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。 自适应学习速率举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate 通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate 比如 \(\eta^{t} = \eta / \sqrt{t+1}\)，t是次数。随着次数的增加，\(η_t\)减小 Feature Scaling（特征缩放） 图左边是\(x_{1}\)的scale比的scale比\(x_{2}\)要小很多，所以当要小很多，所以当\(w_{1}\)和和\(w_{2}\)做同样的变化时，做同样的变化时，\(w_{1}\)对y的变化影响是比较小的，对y的变化影响是比较小的，\(x_{2}\)对y的变化影响是比较大的。 坐标系中是两个参数的error surface（现在考虑左边蓝色），因为\(w_{1}\)对y的变化影响比较小，所以对y的变化影响比较小，所以\(w_{1}\)对损失函数的影响比较小，对损失函数的影响比较小，\(w_{1}\)对损失函数有比较小的微分，所以对损失函数有比较小的微分，所以vw_{1}\)方向上是比较平滑的。同理方向上是比较平滑的。同理\(x_{2}\)对y的影响比较大，所以对y的影响比较大，所以\(x_{2}\)对损失函数的影响比较大，所以在对损失函数的影响比较大，所以在\(x_{2}\)方向有比较尖的峡谷。 上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。 对于左边的情况，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。 常见的算法 批量梯度下降：批量梯度下降每次更新使用了所有的训练数据。如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，但是缺点是如果样本值很大的话，更新速度会很慢。 随机梯度下降：随机也就是说用一个样本的梯度来近似所有的样本，来调整θ，这样会大大加快训练数据，但是有可能由于训练数据的噪声点较多。每一次利用噪声点进行更新的过程中，不一定是朝着极小值方向更新，但是由于多次迭代，整体方向还是大致朝着极小值方向更新，提高了速度。 小批量梯度下降：小批量梯度下降法是为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的。 批量梯度下降代码12345678910111213141516171819202122232425262728import random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 #这里更新权重的时候所有的样本点都用上了 for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 12345678910111213output:iters_count 219iters_count 220iters_count 221iters_count 222iters_count 223iters_count 224iters_count 225theta: [3.0027765778748003, 3.997918297015663]final loss: 9.68238055213e-05iters: 225[Finished in 0.2s] 随机梯度下降代码1234567891011121314151617181920212223242526272829# 每次选取一个值,随机一个点更新 θimport random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 #每一次选取随机的一个点进行权重的更新 i = random.randint(0,3) pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 123456789101112#output:iters_count 1226iters_count 1227iters_count 1228iters_count 1229iters_count 1230iters_count 1231iters_count 1232theta: [3.002441488688225, 3.9975844154600226]final loss: 9.989420302e-05iters: 1232[Finished in 0.3s] 小批量梯度下降代码12345678910111213141516171819202122232425262728293031323334# 这里用2个样本点import random#This is a sample to simulate a function y = theta1*x1 + theta2*x2input_x = [[1,4], [2,5], [5,1], [4,2]] y = [19,26,19,20] theta = [1,1]loss = 10step_size = 0.001eps =0.0001max_iters = 10000error =0iter_count = 0while( loss &gt; eps and iter_count &lt; max_iters): loss = 0 i = random.randint(0,3) #注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点 j = (i+1)%4 pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0] theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1] pred_y = theta[0]*input_x[j][0]+theta[1]*input_x[j][1] theta[0] = theta[0] - step_size * (pred_y - y[j]) * input_x[j][0] theta[1] = theta[1] - step_size * (pred_y - y[j]) * input_x[j][1] for i in range (3): pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] error = 0.5*(pred_y - y[i])**2 loss = loss + error iter_count += 1 print &apos;iters_count&apos;, iter_countprint &apos;theta: &apos;,theta print &apos;final loss: &apos;, lossprint &apos;iters: &apos;, iter_count 1234567891011output:iters_count 543iters_count 544iters_count 545iters_count 546iters_count 547iters_count 548iters_count 549theta: [3.0023012574840764, 3.997553282857357]final loss: 9.81717138358e-05iters: 549 梯度下降的局限 容易陷入局部极值 还有可能卡在不是极值，但微分值是0的地方 还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点 参考：https://zhuanlan.zhihu.com/qinlibo-ml]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之Tensorflow(一)]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BTensorflow(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 Tensorflow简单示例1. 基本运算123456789101112131415161718192021222324#定义一个常量a = tf.constant([3,3])#定义一个变量x = tf.Variable([1,2])#定义一个加法opadd = tf.add(a,x)#定义一个减法sub = tf.subtract(a,x)#定义一个乘法opmul = tf.multiply(a,x)#定义初始化init = tf.global_variables_initializer()#定义多个操作add2 = tf.add(a,add)with tf.Session() as sess: sess.run(init) print(&quot;加法：&quot;,sess.run(add)) #执行加法 print(&quot;减法：&quot;,sess.run(sub)) #执行减法 print(&quot;乘法：&quot;,sess.run(mul)) #执行乘法 #同时执行乘法op和加法op result = sess.run([add,add2,sub,mul]) print(&quot;执行多个：&quot;,result) 2. 使用占位符1234567891011#Feed：先定义占位符，等需要的时候再传入数据#创建占位符input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)#定义乘法opoutput = tf.multiply(input1,input2)add = tf.add(input1,input2)with tf.Session() as sess: #feed的数据以字典的形式传入 print(sess.run(add, feed_dict=&#123;input1:[8.],input2:[2.]&#125;)) Tensorflow简单回归模型1. 最简单的线性回归模型1234567891011121314151617181920212223242526272829import tensorflow as tfimport numpy as np#使用numpy生成100个随机点#样本点x_data = np.random.rand(100)y_data = x_data*0.1 + 0.2#构造一个线性模型d = tf.Variable(1.1)k = tf.Variable(0.5)y = k*x_data + d#二次代价函数&lt;均方差&gt;loss = tf.losses.mean_squared_error(y_data,y)#定义一个梯度下降法来进行训练的优化器optimizer = tf.train.GradientDescentOptimizer(0.3)#最小化代价函数train = optimizer.minimize(loss)#初始化变量init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(1000): sess.run(train) if step%100 ==0: print(step,sess.run([k,d])) 2. 非线性回归的问题12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt#使用numpy生成200个随机点x_data = np.linspace(-0.5,0.5,200).reshape(-1,1)noise = np.random.normal(0,0.015,x_data.shape)y_data = np.square(x_data) + noise#定义两个placeholder，列数为1，行数未知x = tf.placeholder(tf.float32,[None,1])y = tf.placeholder(tf.float32,[None,1])#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元#定义神经网络中间层Weights_L1 = tf.Variable(tf.random_normal([1,20])) # 初始化1行20列权值biases_L1 = tf.Variable(tf.zeros([1,20])) # 初始化1行20列偏置Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 # 计算神经元信号L1 = tf.nn.tanh(Wx_plus_b_L1) # 使用激活函数计算神经元输出信号#定义神经网络输出层Weights_L2 = tf.Variable(tf.random_normal([20,1]))biases_L2 = tf.Variable(tf.zeros([1,1]))Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2prediction = tf.nn.tanh(Wx_plus_b_L2)# prediction = Wx_plus_b_L2#二次代价函数loss = tf.losses.mean_squared_error(y,prediction)#使用梯度下降法最小化代价函数训练train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)with tf.Session() as sess: #变量初始化 sess.run(tf.global_variables_initializer()) for _ in range(1000): sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;) #获得预测值 prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;) #画图 plt.figure() plt.scatter(x_data,y_data) plt.plot(x_data,prediction_value,&apos;r-&apos;,lw=5) plt.show() 123456这里思考下中间层和输出层的激活函数的选取问题。1\. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。2\. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。3\. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。4\. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。5\. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。 Tensorflow分类模型本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。 1. 简单的MNIST数据集分类12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义两个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])#创建一个简单的神经网络W = tf.Variable(tf.zeros([784,10]))b = tf.Variable(tf.zeros([10]))prediction = tf.nn.softmax(tf.matmul(x,W)+b)#二次代价函数loss = tf.losses.mean_squared_error(y,prediction)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction) #使用梯度下降法train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中。correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率。accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) #epoch：所有数据训练一次，就是一个epoch周期 for epoch in range(21): #batch：一般为32，64个数据 for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;) print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(acc)) 2. 过拟合解决及梯度下降优化器Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。在神经网络中它们之间也可以结合使用，dropout应用较多些。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义三个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob=tf.placeholder(tf.float32)# 784-1000-500-10W1 = tf.Variable(tf.truncated_normal([784,1000],stddev=0.1))b1 = tf.Variable(tf.zeros([1000])+0.1)L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)L1_drop = tf.nn.dropout(L1,keep_prob) W2 = tf.Variable(tf.truncated_normal([1000,500],stddev=0.1))b2 = tf.Variable(tf.zeros([500])+0.1)L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)L2_drop = tf.nn.dropout(L2,keep_prob) W3 = tf.Variable(tf.truncated_normal([500,10],stddev=0.1))b3 = tf.Variable(tf.zeros([10])+0.1)prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)#同样这里也可以使用正则项#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction)#正则后的交叉熵代价函数#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率#使用梯度下降法train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) for epoch in range(31): for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.5&#125;) #这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了 test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;) print(&quot;Iter &quot; + str(epoch) + &quot;,Testing Accuracy &quot; + str(test_acc) +&quot;,Training Accuracy &quot; + str(train_acc)) 3. 神经网络优化这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#载入数据集mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义三个placeholderx = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob=tf.placeholder(tf.float32)lr = tf.Variable(0.001, dtype=tf.float32)# 784-500-300-10#创建一个神经网络W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))b1 = tf.Variable(tf.zeros([500])+0.1)L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)L1_drop = tf.nn.dropout(L1,keep_prob)W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))b2 = tf.Variable(tf.zeros([300])+0.1)L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)L2_drop = tf.nn.dropout(L2,keep_prob)W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))b3 = tf.Variable(tf.zeros([10])+0.1)prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)#交叉熵代价函数loss = tf.losses.softmax_cross_entropy(y,prediction)#训练train_step = tf.train.AdamOptimizer(lr).minimize(loss)#初始化变量init = tf.global_variables_initializer()#结果存放在一个布尔型列表中correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) for epoch in range(21): sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch))) for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:1.0&#125;) learning_rate = sess.run(lr) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc) + &quot;, Learning Rate= &quot; + str(learning_rate)) CNN卷积神经网络以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。 CNN通过局部感受野和权值共享减少了神经网络需要训练的参数（权值）的个数。 卷积核/滤波器 卷积Padding SAME PADDING VALID PADDING 池化 max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数 mean-pooling 随机-pooling 池化Padding SAME PADDING VALID PADDING 下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=True)#每个批次的大小batch_size = 64#计算一共有多少个批次n_batch = mnist.train.num_examples // batch_size#定义两个placeholderx = tf.placeholder(tf.float32,[None,784])#28*28y = tf.placeholder(tf.float32,[None,10])#初始化权值def weight_variable(shape): initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布 return tf.Variable(initial)#初始化偏置def bias_variable(shape): initial = tf.constant(0.1,shape=shape) return tf.Variable(initial)#卷积层def conv2d(x,W): #x input tensor of shape `[batch, in_height, in_width, in_channels]` #W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels] #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长 #padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;` return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=&apos;SAME&apos;)#池化层def max_pool_2x2(x): #ksize [1,x,y,1] return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;)#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`x_image = tf.reshape(x,[-1,28,28,1])#初始化第一个卷积层的权值和偏置W_conv1 = weight_variable([5,5,1,32])#5*5的采样窗口，32个卷积核从1个平面抽取特征b_conv1 = bias_variable([32])#每一个卷积核一个偏置值#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling#初始化第二个卷积层的权值和偏置W_conv2 = weight_variable([5,5,32,64])#5*5的采样窗口，64个卷积核从32个平面抽取特征b_conv2 = bias_variable([64])#每一个卷积核一个偏置值#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14#第二次卷积后为14*14，第二次池化后变为了7*7#进过上面操作后得到64张7*7的平面#初始化第一个全连接层的权值W_fc1 = weight_variable([7*7*64,1024])#上一层有7*7*64个神经元，全连接层有1024个神经元b_fc1 = bias_variable([1024])#1024个节点#把池化层2的输出扁平化为1维h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])#求第一个全连接层的输出h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)#keep_prob用来表示神经元的输出概率keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)#初始化第二个全连接层W_fc2 = weight_variable([1024,10])b_fc2 = bias_variable([10])#计算输出prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)#交叉熵代价函数cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)#使用AdamOptimizer进行优化train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#结果存放在一个布尔列表中correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置#求准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(21): for batch in range(n_batch): batch_xs,batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;) acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;) print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy= &quot; + str(acc))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础（二）]]></title>
    <url>%2F2018%2F05%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 线性代数1.数据类型比较Scalar: 标量，可以看成一个数Vector: 向量，可以看成一个一维数组Matrix: 矩阵，可以看成二维数组Tensor: 张量，三维或三维以上的数组的统称，维度不定 几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格： 当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。 张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。 2.线性代数概念 逆矩阵：对于n阶矩阵A，如果有一个n阶矩阵B，使 AB = BA = E， 则说矩阵A是可逆的，并把矩阵B称为A的逆矩阵。当 |A| =0 时，A称为奇异矩阵。可逆矩阵一定是非奇异矩阵，因为矩阵可逆的充分必要条件是 |A|不为0。 矩阵的秩：矩阵A的行阶梯形中非零行的行数，就是矩阵A的秩。 对于n阶矩阵A，由于A的n阶子式只有一个|A|，故当 |A|不为0时R(A)=n，当|A|=0时R(A)&lt;n。因此，可逆矩阵的秩 = 矩阵的阶数，不可逆矩阵的秩 &lt; 矩阵的阶数。那么秩有什么实际意义吗？答案是肯定的。在做矩阵SVD分解的时候用于降噪，如果矩阵秩远小于样本维数（即矩阵列数），那么这些样本相当于只生活在外围空间中的一个低维子空间，这样就能实施降维操作。再者，如果把矩阵看成线性映射，那么秩就是象空间的的维数。 线性方程组的解：矩阵在数学上的基本的应用就是解线性方程组，这也是贯穿整个课本的样例。一个复杂的线性方程组可以表示为Ax=b，其中x，b是向量。通过求A的秩可以判定方程组的解：无解的充分必要条件是R(A) &lt; R(A,b)；有唯一解的充分必要条件是R(A) = R(A,b) = n；有无限多解的充分必要条件是R(A) = R(A,b) &lt; n。 向量空间的基：设V是一个向量空间，V上有r个向量a1,a2…ar，并且满足 a1,a2…ar线性无关，并且V中的任一向量都可以由a1,a2…ar线性表示，那么向量组a1,a2…ar就称为向量空间V的一个基，r是向量空间的维数，并称V为r维向量空间。可以这么理解，把向量空间看做是向量组，那么基就是一个极大线性无关组，可以用来表示其他向量的最小组合，维数就是向量组的秩。 特征值与特征向量：对于n阶矩阵A，如果数 λ 和 n 维非零列向量 x 使关系式 Ax=λ x 成立，那么 λ 称为矩阵A的特征值，向量 x 就是A的对应于λ 的特征向量。那么如何理解特征值与特征向量呢？ 我们知道，矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。实际上，这段话既讲了矩阵变换特征值及特征向量的几何意义（图形变换）也讲了其物理含义。物理的含义就是运动的图景：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。 相似矩阵：设A，B都是n阶矩阵，若有可逆矩阵P，使P-1 AP=B,则B是A的相似矩阵。 对角阵：是一个主对角线之外的元素皆为 0 的矩阵。对角线上的元素可以为 0 或其他值。 二次型：含有n个变量的二次齐次函数叫做二次型。只含平方项叫做二次型的标准型。用矩阵表示二次型就是f = xT Ax，A为对称矩阵。 3. Ax=b 的两种理解 基于列视图来理解 Ax=bAx=b，可以更好的理解矩阵。 分别对于二元线性方程组 \left\{ \begin{align*} 2x-y &=1 \\ x+y &= 5 \\ \end{align*} \right.和三元线性方程组 \left\{ \begin{align*} 2x+y+z &=5 \\ 4x-6y &= -2 \\ -2x+7y+2z&=9\\ \end{align*} \right.分别讨论他们的行视图与列视图。 行视图二元线性方程组理解为：在 XOY 坐标轴系中直线 2x−y=1 与 直线 x+y=5 的交点。 三元线性方程组理解为：在 XYZ坐标系中平面 2x+y+z=5 与平面 4x−6y=0 与平面 −2x+7y+2z=9 的交点（前两个平面交于一条直线，这条直线与第三个平面交于一点）。 列视图 x\left[ \begin{aligned}2 \\ 1 \\ \end{aligned}\right]+y\left[ \begin{aligned}-1 \\ 1 \\ \end{aligned}\right]=\left[ \begin{aligned}1 \\ 5 \\ \end{aligned}\right]二元线性方程组理解为：向量\(\left[ \begin{aligned}2 \\ 1 \\ \end{aligned}\right]\)与向量\(\left[ \begin{aligned}-1 \\ 1 \\ \end{aligned}\right]\)按一定比例缩放以后相加的结果等于向量\(\left[ \begin{aligned}1 \\ 5 \\ \end{aligned}\right]\) 线性相关和线性无关我们先看线性相关和线性无关的定义。在向量空间V的一组向量A，如果存在不全为零的数 k1, k2, ···,km , 使则称向量组A是线性相关的，否则数 k1, k2, ···,km全为0时，称它是线性无关。线性无关可以这么理解，如果是二维空间，这组向量的任意两个向量不共线；如果是三维空间，则任意三个向量不共面，如果共面了就可以用另外两个表示第三个向量了不是么。 四个基本的子空间列空间（column space）m×n维矩阵 A的列空间 C(A) 是由 A 的列的所有线性组合组成的集合。C(A)中的一个典型向量可以写成 Ax的形式，其中 x 为某向量，因为 Ax 表示 A 的列向量的线性组合。即C(A) 是线性变换 x↦Ax 的值域。 C(A) 是 Rm 的一个子空间 A的一个最大线性无关向量组就是 C(A) 的一组基。 例：矩阵\(A＝ \left[ \begin{matrix} 1 &amp; 0 \\ 4 &amp; 3 \\ 2 &amp; 3 \end{matrix}\right]\) 的列空间 C(A)=span{a1,a2} 是 R3 的一个子空间,其中\(\boldsymbol{a_1}=\left[ \begin{matrix} 1 \\ 4 \\ 2 \end{matrix}\right]\),\(\boldsymbol{a_2}=\left[ \begin{matrix} 0 \\ 3 \\ 3 \end{matrix}\right]\) 零空间（null space）m×n 维矩阵 A 的零空间 N(A) 是齐次方程 Ax=0 解的集合。 N(A)是 Rn的一个子空间 求N(A)的过程就是求Ax=0的通解的过程 行空间（row space）m×n维矩阵 A 的行空间 \(C(A^T)\)是由 A 的行的所有线性组合组成的集合。 左零空间（left null space）m×n 维矩阵 A 的左零空间 \(N(A^T)\)是齐次方程\(A^T\boldsymbol{x}=\boldsymbol{0}\)解的集合。 四个子空间关系 可以画出四个子空间如下，行空间和零空间在Rn里，他们的维数加起来等于n，列空间和左零空间在Rm里，他们的维数加起来等于m。 左零空间的向量与列空间的向量相垂直（内积为零） 左零空间与列空间交于向量 0 列空间的维数（最大线性无关的向量个数、秩、基的个数）为r，左零空间的维数为 m−r 零空间的向量与行空间的向量相垂直 行空间的维数（最大线性无关的向量个数、秩、基的个数）为 r（矩阵行向量的秩等于列向量的秩），零空间的维数为 n−r 利用子空间判断线性方程组的解Ax=b 的解 只有唯一解，则 b∈C(A)，且 N(A)的维数是0 有无穷多个解，b∈C(A)，且 N(A)的维数大于0 无解，则b∉C(A) 4. 矩阵导数 分子、分母布局：分子布局：分子为列向量，或者分母为行向量；分母布局：分母为列向量，或者分子为行向量； 运算规则： 需要注意的规则：以下公式默认在分子布局下的结果；； 常用公式 5. 特征分解1 特征分解一般矩阵相似矩阵： n阶矩阵 A、B，若有可逆矩阵 P，使 P^{-1}AP=B则称 B 是 A 的相似矩阵。 对角化定理： n 阶矩阵 A 与对角矩阵 Λ 相似（即 A 能对角化）的充分必要条件是 A 有 n 个线性无关的特征向量。 推论： 1. 如果 n 阶矩阵 A 的 n 个特征值互不相等，则 A 与对角矩阵相似。2. 实对称矩阵必可以相似对角化 正定二次型设二次型 f(x)=xTAx，如果对任何 x≠0，都有 f(x)&gt;0（显然 f(0)=0），则称 f 为正定二次型，并称对称矩阵 A 是正定的；如果对于任何 x≠0 都有 f(x)&lt;0，则称 f 为负定二次型，并称对称矩阵 A 是负定的。 半正定（positive semidefinite）：f≥0 正定（positive definite）：f&gt;0 负定（negative definite）：f&lt;0 不定（indefinite） 对称矩阵 AA 为正定的充分必要条件是：AA 的特征值全为正 2 PCA PCA：Principal Component Analysis 主成分分析，用于降维 先不考虑降维，先考虑如下的一个变换。对m×n矩阵 X，我们希望经过一个变换 Y=QX （\(Q\in \mathbb{R}^{m\times m}\)）使 Y 的‘矩阵为对角阵。协方差矩阵为对角阵意味着行向量之间的协方差为0，而每个行向量的方差尽可能大。X 的协方差矩阵为： C_X=\frac{1}{n}XX^T X=\left[\begin{matrix}a_1&a_2&\cdots&a_n \\ b_1&b_2&\cdots&b_n\end{matrix}\right]协方差矩阵为： C_X=\frac{1}{n}\left[\begin{matrix}a_1&a_2&\cdots&a_n\\ b_1&b_2&\cdots&b_n\end{matrix}\right] \left[\begin{matrix}a_1&b_1\\ a_2&b_2\\\vdots&\vdots\\a_n&b_n\end{matrix}\right] =\left[\begin{matrix} \frac{1}{n}\sum_{i=1}^{n} a_i^2 & \frac{1}{n}\sum_{i=1}^{n} a_ib_i \\ \frac{1}{n}\sum_{i=1}^{n} a_ib_i & \frac{1}{n}\sum_{i=1}^{n} b_i^2 \end{matrix}\right]所以，有 Y 的协方差矩阵： C_Y=\frac{1}{n}YY^T=\frac{1}{n}(QX)(QX)^T=\frac{1}{n}QXX^TQ^T=QC_XQ^T我们的目的就是求 Q 使 \(C_Y\) 为对角阵。利用特征分解，我们可以将对称矩阵\(C_X\) 对角化： P^{-1}C_XP=P^TC_XP=\Lambda其中，P的列向量为\(C_X\)的特征向量，Λ是 \(C_X\)的特征值组成的矩阵。 结合上面两个式子，显然有 Q=P^{-1}=P^T \begin{align*}C_X= P\Lambda P^T =[\boldsymbol{p_1},\boldsymbol{p_2}] \left[\begin{matrix}\lambda_1\\& \lambda_2 \end{matrix}\right]\left[\begin{matrix}\boldsymbol{p_1^T} \\\boldsymbol{p_2^T}\end{matrix}\right] =\lambda_1\boldsymbol{p_1}\boldsymbol{p_1^T}+\lambda_2\boldsymbol{p_2}\boldsymbol{p_2^T} \end{align*}总结PCA的要点： KL变换来的，本质是把协方差矩阵对角化。 对角化：使不同行向量间的协方差为0，每个行向量的方差尽可能大。 小例子矩阵X： 1.计算协方差矩阵 2.计算Cx的特征值为：λ1=2，λ2=2/5。特征值对应的特征向量为 3.降维：特征向量的转置*X。得到一行的矩阵。 6.奇异值分解（Singular Value Decomposition，SVD） PCA的实现一般有两种方式，分别是特征值分解和SVD分解。SVD奇异值分解可以将一个复杂的矩阵用几个更小的子矩阵表示，每个子矩阵代表了原矩阵的重要特性。 SVD奇异值分解：任何秩为r的矩阵，都可以特征分解为以下公式： 其中U和V是正交矩阵。公式表示了SVD与子空间的关系。公式(13)便于分析，但并不计算有效；公式(14)计算有效，但有时候不方便分析；公式(15)方便展开，用于低秩矩阵的计算。另外SVD还提供了计算四个子空间正交基的一种快速方法。 SVD应用之一图像压缩：给定一幅图像，256512像素，考虑用低秩矩阵近似的方式，存储奇异向量，若保留一个奇异向量k=1，压缩比大致是 (256512) / (256+512) = 170。但是k太小图像的质量也损失较大，实际中k不会这么小，下面四个图分别是原图、k=1、k=10、k=80 时图片的表现。 矩阵的特征值分解和奇异值分解有什么区别？ 首先，特征值只能作用在一个mm的正方矩阵上，而奇异值分解则可以作用在一个mn的长方矩阵上。其次，奇异值分解同时包含了旋转、缩放和投影三种作用，(1)式中，U和V都起到了对A旋转的作用，而ΣΣ起到了对A缩放的作用。特征值分解只有缩放的效果。 7.常见距离上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。 设有两个n维变量\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)和\(A=\left[ x_{11}, x_{12},…,x_{1n} \right]\)，则一些常用的距离公式定义如下： 1.闵可夫斯基距离 从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义： d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } 实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。当p=无穷时，就是切比雪夫距离 切比雪夫距离Python实现如下： 1234from numpy import *vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sqrt(abs(vector1-vector2).max) 2. 向量内积向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的余弦相似度（Cosine similarity）： 余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度（TF-IDF）和图片相似性（histogram）计算上都有它的身影。需要注意一点的是，余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。怎样才能实现平移不变性？这就是下面要说的皮尔逊相关系数（Pearson correlation），有时候也直接叫相关系数: 皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。 由于皮尔逊系数具有的良好性质，在各个领域都应用广泛，例如，在推荐系统根据为某一用户查找喜好相似的用户,进而提供推荐，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。 3. 分类数据点间的距离汉明距离（Hamming distance）是指，两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子： 还可以用简单的_匹配系数_来表示两点之间的相似度——匹配字符数/总字符数。 在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影（两个都是 0），并不能说明两者相似。反而言之，如果两个用户都看过某一部电影（序列中都是 1），则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的杰卡德相似系数（Jaccard similarity）。 在上面的例子中，用 M11 表示两个用户都看过的电影数目，M10 表示用户 A 看过，用户 B 没看过的电影数目，M01 表示用户 A 没看过，用户 B 看过的电影数目，M00 表示两个用户都没有看过的电影数目。Jaccard 相似性系数可以表示为： 概率论和统计1.统计量协方差 Pearson相关系数 2.独立与不相关 3.贝叶斯公式先看看什么是“先验概率”和“后验概率”，以一个例子来说明： 假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： P(患病) = 0.1%；即：在没有做检验之前，我们预计的患病率为P(患病)=0.1%，这个就叫作“先验概率”。 再假设现在有一种该病的检测方法，其检测的准确率为95%；即：如果真的得了这种病，该检测法有95%的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有95%的概率检测出阴性，但也有5%的概率检测为阳性。用概率条件概率表示即为：P(显示阳性|患病)=95% 现在我们想知道的是：在做完检测显示为阳性后，某人的患病率P(患病|显示阳性)，这个其实就称为“后验概率”。 而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。 这里先了解条件概率公式： P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}由条件概率可以得到乘法公式： P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)将条件概率公式和乘法公式结合可以得到： P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}再由全概率公式： P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}代入可以得到贝叶斯公式： P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }4.常见分布函数常见分布正态分布 来源：中心极限定理 定义：大量独立的随机变量之和趋向于正态分布（高斯分布） 前提：样本之间相互独立 公式： p(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp (- \frac{(x-\mu )^{2}}{2\sigma ^{2}}) 扩充：为什么测量误差服从正态分布误差公式：\(\bar{x}-x = (\frac{1}{N}\sum_{i=1}^{N}x_{i} ) - x^{}\)(这里的\(x^{}\)指的是真实值)证明：由于每次测量误差都和其余测量误差的大小无关，因此是独立条件，所以 \(\sum_{i=1}^{N}x_{i}\)就是独立同分布的，乘以\(\frac{1}{N}\)并不影响，减去\(x^{*}\)只改变了偏移量也不影响它的性质，因此测量误差服从独立同分布警告：在误差服从正态分布的情况下，测量量仍旧可以为其他分布 拉普拉斯分布 公式：\(p(x) = \frac{1}{2b}\exp (-\frac{\left | x-\mu \right |}{b})\) 描述：它是一种长尾分布，又名“双指数分布”，其中 μ 是偏移量，b是尺度参数，若当 μ=0 ，正半部分刚好是尺度为 \(\frac{1}{b}\)的指数分布的一半 。 图示： 不同于高斯分布的是，拉普拉斯分布是一种生长型分布函数，常用来处理样本空间奇葩的分布效果。常见的就是图像的边缘服从拉普拉斯分布 伯努利分布 定义：是二项分布的特殊情况，又名“01分布”，描述了二值随机变量的性质，它是离散型随机变量分布，是试验一次的二项分布 应用：在机器学习领域中，是经典二分类算法-logisitic回归的概率基础 分布律：\(F(i) = \left\{\begin{array}{lr} 1-p &amp; \text{n = 0} \\ p &amp; \text{n = 1} \end{array}\right.\) 性质： 均值：E(x)=p 方差：var(x)=p(1−p) 二项分布 定义：也是离散型随机变量分布，是N次伯努利实验得到的结果，其中结果只有两种，结果之间相互独立。（其实二项分布并不是一正一反的感觉，容易让人误解） 分布律：\(p(X=k) = C_{N}^{k}p^{k}(1-p)^{n-k}\)这里的描述就是，N次试验中取k次是成功的，所以得到的概率。为了严谨，所以还要乘上相反的概率。 性质： 期望：μ=np（即渴望得到p结果的均值） 方差：σ2=np(1−p) 应用：进行两次掷骰子（N=2），点数之和为2~12的概率 拓展：以掷骰子的点数和思考，当N趋近于无穷大的时候，二项分布所得到的骰子和非常的多，所以会被刻画出高斯分布的样子 多元正态分布协方差 定义：描述两个随机变量综合偏离程度，其中方差是协方差的特殊情况，即两个随机变量值相同 公式：cov(X,Y)=E[(X−E[X])(Y−E[Y])]，可见X,Y各自的偏移相乘后再求期望即协方差 应用：思考联合分布率（X,Y），对于随机变量X,Y大概可以分为三种情况： 当X,Y的偏移量同时满足增大或者减小，即cov(X,Y)&gt;0，呈正相关，这时它们的联合分布呈现一个X增大,Y增大趋势的椭圆 当X,Y的偏移量满足X增大Y减小或X减小Y增大，即cov(X,Y)&lt;0，呈正相关，这时它们的联合分布呈现一个X增大,Y减小趋势的椭圆 当X，Y的偏移量为0的时候，即cov(X,Y)=0，即不相关，这时候他们的联合分布为一个圆 协方差矩阵 定义：协方差矩阵的每个元素是各个向量元素之间的协方差，而协方差只能处理二维问题，所以计算多维问题时候，即使用协方差矩阵。而协方差矩阵对角线上衡量的是各个纬度的协方差，当X=Y的时候，就是各个维度的方差，即某一维上的偏移程度。 公式： Cov(X,Y) =\left [ \begin{matrix} Cov(x_{1}, y_{1})& ... & Cov(x_{1},y_{q})) \\ \vdots & \ddots &\vdots \\Cov(x_{p},y_{1}) & ... & Cov(x_{p},y_{q})\end{matrix} \right ]当X=Y的时候: Cov(X,X) =\left [ \begin{matrix} D(x_{1})& ... & Cov(x_{1},x_{q})) \\ \vdots & \ddots &\vdots \\Cov(x_{p},x_{1}) & ... & D(x_{p})\end{matrix} \right ]正定矩阵 定义：设M是n阶方阵，如果对于任何非零矩阵Z，都有\(Z^{T}MZ&gt;0\)，则M为正定矩阵 判定：常用的判定性质是利用特征值，令M−λE=0 ， 如果λ&gt;0，则M为正定矩阵 5.常见的分布总结 6.极大似然估计（MLE）极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 求极大似然函数估计值的一般步骤： （1） 写出似然函数，即每个随机实验出现概率相乘，为这个抽样出现的概率。（2） 对似然函数取对数，为了方便求导；（3） 对参数求导数。（4） 令导数=0，即求解极值，由实际情况知，该极值为极大值。解似然方程。 微积分1.导数与梯度 导数：一个一元函数函数在某一点的导数描述了这个函数在这一点附近的变化率。 梯度:多元函数的导数就是梯度。 一阶导数，即梯度（gradient） 二阶导数，Hessian矩阵 2.泰勒展开 一元函数的泰勒展开： 基尼指数的图像、熵、分类误差率三者之间的关系。 3.Lagrange乘子法对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。 对于一个要求极值的函数\(f\left( x,y \right)\)，图上的蓝圈就是这个函数的等高图，就是说 \(f\left( x,y \right) =c_{1} ,c_{2} ,…,c_{n}\)分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组\(\left( x,y \right)\)，使它的\(c_{i}\)值越大越好，但是这点必须满足约束条件\(g\left( x,y \right)\)（在黄线上）。 也就是说\(f(x,y)\)相切，或者说它们的梯度▽ \({f}\)和▽\({g}\)平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为\(\lambda\)倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。 在支持向量机模型（SVM）的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。 信息论1.信息熵熵的概念最早由统计热力学引入。 信息熵是由信息论之父香农提出来的，它用于随机变量的不确定性度量，先上信息熵的公式。 信息是用来减少随机不确定性的东西（即不确定性的减少）。 我们可以用log ( 1/P )来衡量不确定性。P是一件事情发生的概率，概率越大，不确定性越小。 可以看到信息熵的公式，其实就是log ( 1/P )的期望，就是不确定性的期望，它代表了一个系统的不确定性，信息熵越大，不确定性越大。 注意这个公式有个默认前提，就是X分布下的随机变量x彼此之间相互独立。还有log的底默认为2，实际上底是多少都可以，但是在信息论中我们经常讨论的是二进制和比特，所以用2。 信息熵在联合概率分布的自然推广，就得到了联合熵 当X, Y相互独立时，H(X, Y) = H(X) + H(Y) 信息熵的实例解释举个例子说明信息熵的作用。 例子是知乎上看来的，我觉得讲的挺好的。 比如赌马比赛，有4匹马{ A, B, C, D}，获胜概率分别为{ 1/2, 1/4, 1/8, 1/8 }，将哪一匹马获胜视为随机变量X属于 { A, B, C, D } 。 假定我们需要用尽可能少的二元问题来确定随机变量 X 的取值。 例如，问题1：A获胜了吗？ 问题2：B获胜了吗？ 问题3：C获胜了吗？ 最后我们可以通过最多3个二元问题，来确定取值。 如果X = A，那么需要问1次（问题1：是不是A？），概率为1/2 如果X = B，那么需要问2次（问题1：是不是A？问题2：是不是B？），概率为1/4 如果X = C，那么需要问3次（问题1，问题2，问题3），概率为1/8 如果X = D，那么需要问3次（问题1，问题2，问题3），概率为1/8 那么为确定X取值的二元问题的数量为 回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了： 在二进制计算机中，一个比特为0或1，其实就代表了一个二元问题的回答。也就是说，在计算机中，我们给哪一匹马夺冠这个事件进行编码，所需要的平均码长为1.75个比特。 很显然，为了尽可能减少码长，我们要给发生概率 p(x)较大的事件，分配较短的码长 l(x)。这个问题深入讨论，可以得出霍夫曼编码的概念。 霍夫曼编码就是利用了这种大概率事件分配短码的思想，而且可以证明这种编码方式是最优的。我们可以证明上述现象： 为了获得信息熵为 H(X) 的随机变量 X 的一个样本，平均需要抛掷均匀硬币（或二元问题） H(X)次（参考猜赛马问题的案例） 信息熵是数据压缩的一个临界值（参考码长部分的案例） 所以，信息熵H(X)可以看做，对X中的样本进行编码所需要的编码长度的期望值。 2.相对熵/交叉熵/K-L散度这里可以引申出交叉熵的理解，现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p。 按照真实分布p来编码样本所需的编码长度的期望为，这就是上面说的信息熵H( p ) 按照不真实分布q来编码样本所需的编码长度的期望为，这就是所谓的交叉熵H( p,q )交叉熵和熵，相当于，协方差和方差 这里引申出KL散度D(p||q) = H(p,q) - H(p) = ，也叫做相对熵，它表示两个分布的差异，差异越大，相对熵越大。 机器学习中，我们用非真实分布q去预测真实分布p，因为真实分布p是固定的，D(p||q) = H(p,q) - H(p) 中 H(p) 固定，也就是说交叉熵H(p,q)越大，相对熵D(p||q)越大，两个分布的差异越大。 所以交叉熵用来做损失函数就是这个道理，它衡量了真实分布和预测分布的差异性。 用图像形象化的表示二者之间的关系可以如下图：上面是q所含的信息量/平均编码长度H(p)第二行是cross-entropy，即用q来编码p所含的信息量/平均编码长度|或者称之为q对p的cross-entropy第三行是上面两者之间的差值即为q对p的KL距离 从编码的角度，可以这样简单总结，信息熵是最优编码（最短的平均码长），交叉熵是非最优编码（大于最短的平均码长），KL散度是两者的差异（距离最优编码的差距）。 3.互信息（信息增益）互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量。决策树中的信息增益就是互信息，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。 可以通过简单的计算得到： H(X|Y) = H(X) - I(X, Y), 互信息为0，则随机变量X和Y是互相独立的。 信息论与机器学习的关系 信息论视角 机器学习视角 接受信号 特征 信源 标签 平均互信息 特征有效性分析 最大熵模型 极大似然法 交叉熵 逻辑回归损失函数 最大熵模型最大熵模型的原则： 承认已知事物（知识）； 对未知事物不做任何假设，没有任何偏见。 对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。 因为这时概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”（Maximum Entropy）。 Logistic回归是统计学习中的经典分类方法，可以用于二类分类也可以用于多类分类。 最大熵模型由最大熵原理推导出来，最大熵原理是概率模型学习或估计的一个准则，最大熵原理认为在所有可能的概率模型的集合中，熵最大的模型是最好的模型，最大熵模型也可以用于二类分类和多类分类。 Logistic回归模型与最大熵模型都属于对数线性模型。 逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 熵总结 熵：不确定性的度量； 似然：与知识的吻合程度； 最大熵模型：对不确定度的无偏分配； 最大似然估计：对知识的无偏理解。 2.上溢和下溢在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。 一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。 另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\(\varpi\)时发生上溢。进一步的运算通常将这些无限值变为非数字。 必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为： softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 \(x_{i}\) 的类别标签 \(y_{i}\)属于K个类别的概率，最后判别 \(y_{i}\)所属类别时就是将其归为对应概率最大的那一个。 当式中的\(w_{k} x_{i} +b\)都是很小的负数时，\(e^{w_{k} x_{i} +b }\)就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的\(x_{w_{k} x_{i} +b}\)是很大的正数时，\(e^{w_{k} x_{i} +b }\)就会发生上溢导致结果是未定的。 凸优化1.凸集(Convex Sets)集合C是凸的，如果对于所有的\(x,y\in C\)和\(\theta\in\mathbb{R},0\leq\theta\leq 1\)有： \theta x+(1-\theta)y\in C可以这样理解：在集合C中任选两点，在这两点的连线上的所有点都属于集合C。 2.凸函数(Convex Fuctions)如果函数的定义域\({\cal D}(f)\)是一个凸集，并且对于所有的\(x,y\in {\cal D}(f)\)和\(\theta\in\mathbb{R},0\leq\theta\leq1\)，都有：\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\) 凸函数的一阶条件直观上可以这样理解，在函数上随便挑一个点，该点的切线必然在函数的下方 凸性质的二阶条件 琴生不等式(Jensen’s Inequality)假设凸函数的基本定义为:\(f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)\ \ \ \text{for} \ \ \ 0\leq\theta\leq1\)上述等式可以扩展到多个点:\(f\left(\sum_{i=1}^k\theta_ix_i\right)\leq\sum_{i=1}^k\theta_if(x_i)\ \ \ \text{for}\ \ \ \sum_{i=1}^k\theta_i=1,\theta_i\geq0 \ \ \forall i\)再将上述等式扩展到积分形式:\(f\left(\int p(x)xdx\right)\leq\int p(x)f(x)dx\ \ \ \text{for}\ \ \ \int p(x)dx=1,p(x)\leq0\ \ \forall x\)由于\(p(x)\)积分为1，我们可以把\(p(x)\)看作是一个概率密度函数，所以尚属等式可以用以下形式表达：\(f(\mathbb{E}[x])\leq\mathbb{E}[f(x)]\)最后一条等式就是著名的琴生不等式。 3.凸优化问题(Convex Optimization Problems)在凸优化问题中，一个最关键的点就是对于一个凸优化问题，所有的局部最优解(locally optimal)都是全局最优解(globally optimal)。最优化的基本数学模型如下： 它有三个基本要素，即： 设计变量：x是一个实数域范围内的n维向量，被称为决策变量或问题的解； 目标函数：f(x)为目标函数； 约束条件：\(h_{i} \left( x \right) =0\)称为等式约束，\(g_{i} \left( x \right) \leq 0\)为不等式约束，\(i=0,1,2,……\) 4.牛顿法牛顿法介绍牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。 从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。 或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法的推导将目标函数\(f\left( x \right)\) 在\(x_{k}\)处进行二阶泰勒展开，可得： f\left( x \right) =f\left( x_{k} \right) +f^{'} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{''}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}因为目标函数\(f\left( x \right)\)有极值的必要条件是在极值点处一阶导数为0，即：\(f^{‘} \left( x \right) =0\) 所以对上面的展开式两边同时求导（注意\({x}\)才是变量，\(x_{k}\)是常量\(\Rightarrow f^{‘} \left( x_{k} \right) ,f^{‘’} \left( x_{k} \right)\)都是常量），并令\(f^{‘} \left( x \right) =0\)可得： f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0即： x=x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }于是可以构造如下的迭代公式： x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }这样，我们就可以利用该迭代式依次产生的序列逐渐逼近\(f\left( x \right)\)的极小值点了。 牛顿法的迭代示意图如下： 5.拟牛顿法概述由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。 拟牛顿法的推导先将目标函数在\(x_{k+1}\)处展开，得到： f\left( x \right) =f\left( x_{k+1} \right) +f^{'} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{''}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}两边同时取梯度，得： f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)取上式中的\(x=x_{k}\)，得： f^{'}\left( x_{k} \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)即： g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)可得： H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}上面这个式子称为“拟牛顿条件”，由它来对Hessen矩阵做约束。 梯度下降 VS 牛顿法现在，分别写出梯度下降和牛顿法的更新公式： x_{n+1}=x_n-\eta f'(x_n)梯度下降算法是将函数在 xn 位置进行一次函数近似，也就是一条直线。计算梯度，从而决定下一步优化的方向是梯度的反方向。而牛顿法是将函数在 xn 位置进行二阶函数近似，也就是二次曲线。计算梯度和二阶导数，从而决定下一步的优化方向。一阶优化和二阶优化的示意图如下所示： 梯度下降，一阶优化： 牛顿法，二阶优化： 首先，我们来看一下牛顿法的优点。第一，牛顿法的迭代更新公式中没有参数学习因子，也就不需要通过交叉验证选择合适的学习因子了。第二，牛顿法被认为可以利用到曲线本身的信息, 比梯度下降法更容易收敛（迭代更少次数）。如下图是一个最小化一个目标方程的例子, 红色曲线是利用牛顿法迭代求解, 绿色曲线是利用梯度下降法求解。显然，牛顿法最优化速度更快一些。 然后，我们再来看一下牛顿法的缺点。我们注意到牛顿法迭代公式中除了需要求解一阶导数之外，还要计算二阶导数。从矩阵的角度来说，一阶导数和二阶导数分别对应雅可比矩阵（Jacobian matrix）和海森矩阵（Hessian matrix）。 Jacobian (雅可比)矩阵： Hessian 矩阵： 牛顿法不仅需要计算 Hessian 矩阵，而且需要计算 Hessian 矩阵的逆。当数据量比较少的时候，运算速度不会受到大的影响。但是，当数据量很大，特别在深度神经网络中，计算 Hessian 矩阵和它的逆矩阵是非常耗时的。从整体效果来看，牛顿法优化速度没有梯度下降算法那么快。所以，目前神经网络损失函数的优化策略大多都是基于梯度下降。 值得一提的是，针对牛顿法的缺点，目前已经有一些改进算法。这类改进算法统称拟牛顿算法。比较有代表性的是 BFGS 和 L-BFGS。 BFGS 算法使用近似的方法来计算 Hessian 矩阵的逆，有效地提高了运算速度。但是仍然需要将整个 Hessian 近似逆矩阵存储起来，空间成本较大。 L-BFGS 算法是对BFGS 算法的改进，不需要存储 Hessian 近似逆矩阵， 而是直接通过迭代算法获取本轮的搜索方向，空间成本大大降低。 总的来说，基于梯度下降的优化算法，在实际应用中更加广泛一些，例如 RMSprop、Adam等。但是，牛顿法的改进算法，例如 BFGS、L-BFGS 也有其各自的特点，也有很强的实用性。 计算复杂性与NP问题时间复杂度表明问题规模扩大后，程序需要的时间长度增长得有多快。程序的时间复杂度一般可以分为两种级别： 多项式级的复杂度，如O(1)，O(log(n))、O（n^a）等， 非多项式级的，如O(a^n)、O(n!)等。后者的复杂度计算机往往不能承受。 约化(Reducibility)简单的说，一个问题A可以约化为问题B的含义是，可以用问题B的解法解决问题A。（个人感觉也就是说，问题A是B的一种特殊情况。）标准化的定义是，如果能找到一个变化法则，对任意一个A程序的输入，都能按照这个法则变换成B程序的输入，使两程序的输出相同，那么我们说，问题A可以约化为问题B。 例如求解一元一次方程这个问题可以约化为求解一元二次方程，即可以令对应项系数不变，二次项的系数为0，将A的问题的输入参数带入到B问题的求解程序去求解。 另外，约化还具有传递性，A可以化约为B，B可以约化为C，那么A也可以约化为C。 基本概念P Problem假设有 n 个数要排序。一个初级的冒泡排序算法所需时间可能与 n2 成正比，快一点的算法所需时间与 nlog（n） 成正比。在某些条件下，桶排序算法所需时间甚至只和 n 成正比。最不实用的算法就是输入的数字随机排列，直到出现完全有序的情况为止……记前三个算法的时间复杂度分别记为 O(n2)、O(nlogn) 和 O(n)，最后的“猴子排序”(Bogosort)算法平均时间复杂度则达到了 O(n*n!)。 在上面的例子中，前三种算法的复杂度是 n 的多项式函数；最后一种算法的复杂度是 n 的阶乘，根据斯特林公式，n! 相当于指数级别的增长。当 n 特别小时，多项式级的算法已经快过指数级的算法。当 n 非常大时，人类根本看不到指数级复杂度算法结束的那天。自然的，大家会对多项式级别的算法抱有好感，希望对每一个问题都能找到多项式级别的算法。问题是——每个问题都能找到想要的多项式级别的算法吗？ 在一个由问题构成的集合中，如果每个问题都存在多项式级复杂度的算法，这个集合就是 P 类问题（Polynomial）。 NP (Nondeterministic Polynomial)问题NP 类问题指的是，能在多项式时间内检验一个解是否正确的问题。比如我的机器上存有一个密码文件，于是就能在多项式时间内验证另一个字符串文件是否等于这个密码，所以“破译密码”是一个 NP 类问题。NP 类问题也等价为能在多项式时间内猜出一个解的问题。这里的“猜”指的是如果有解，那每次都能在很多种可能的选择中运气极佳地选择正确的一步。 不妨举个例子：给出 n 个城市和两两之间的距离，求找到一个行走方案，使得到达每个城市一次的总路程最短。我们可以这样来“猜测”它的解：先求一个总路程不超过 100 的方案，假设我们可以依靠极好的运气“猜出”一个行走路线，使得总长度确实不超过 100，那么我们只需要每次猜一条路一共猜 n 次。接下来我们再找总长度不超过 50 的方案，找不到就将阈值提高到75…… 假设最后找到了总长度为 90 的方案，而找不到总长度小于 90 的方案。我们最终便在多项式时间内“猜”到了这个旅行商问题的解是一个长度为 90 的路线。它是一个 NP 类的问题。 也就是说，NP 问题能在多项式时间内“解决”，只不过需要好运气。显然，P 类问题肯定属于 NP 类问题。所谓“P=NP”，就是问——是不是所有的 NP 问题，都能找到多项式时间的确定性算法？ NPC Problem在与数不尽的问题搏斗的过程中，人们有时候会发现，解决问题 A 的算法可以同时用来解决问题 B。例如问题 A 是对学生的姓名与所属班级同时排序，问题 B 是对人们按照姓名做排序。这时候，我们只需要让班级全都相同，便能照搬问题 A 的算法来解决问题 B。这种情况下，数学家就说，问题 B 能归约为问题 A。 人们发现，不同的 NP 问题之间也会出现可归约的关系，甚至存在这么一类（不只是一个）问题，使得任何其它的 NP 问题都能归约到它们上。也就是说，能够解决它们的算法就能够解决所有其它的 NP 问题。这一类问题就是 NPC 问题。这样的问题人们已经找到了几千个，如果我们给其中任何一个找到了多项式级别的算法，就相当于证明了 P=NP。 但NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。 P=NP？证明 P=NP 的一个主要方法就是，给某一个 NPC 问题找到一个快速算法。但是，也不排除有人给出一个“存在性”而非“构造性”的证明，只是告诉大家存在符合要求的算法，但没法详细描述出来。如果 P=NP 被人以这种方式证明出来了，我们也没法依葫芦画瓢地把这个神奇的算法在电脑上写出来，所以对破解密码仍然没有帮助。 退一步说，假如有人构造出可以运用的多项式算法，以此证明了这个问题。这个算法恐怕也很复杂（毕竟这么难找），它的多项式级别的复杂度也可能会非常慢。假设这个算法的复杂度达到了 O(n10)，那我们依然面临着不小的麻烦。即使 n=100，运算时间也会增长到非常巨大的地步。 再退一步，假设人类的运气好到 P=NP 是真的，并且找到了复杂度不超过 O(n3) 的算法。如果到了这一步，我们就会有一个算法，能够很快算出某个帐号的密码。《基本演绎法》里面所想象的可能就要成真了，所有的加密系统都会失去效果——应该说，所有会把密码变成数字信息的系统都会失去效果，因为这个数字串很容易被“金钥匙”计算出来。 除此之外，我们需要担心或期许的事情还有很多： 一大批耳熟能详的游戏，如扫雷、俄罗斯方块、超级玛丽等，人们将为它们编写出高效的AI，使得电脑玩游戏的水平无人能及。 整数规划、旅行商问题等许多运筹学中的难题会被高效地解决，这个方向的研究将提升到前所未有的高度。 蛋白质的折叠问题也是一个 NPC 问题，新的算法无疑是生物与医学界的一个福音。 参考文献：http://www.junnanzhu.com/?p=141https://www.zybuluo.com/frank-shaw/note/139175http://colah.github.io/posts/2015-09-Visual-Information/https://www.guokr.com/article/437662/https://www.zhihu.com/question/22178202]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础（一）]]></title>
    <url>%2F2018%2F05%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[从IT时代走向DT时代 机器学习应用1、计算机视觉典型的应用包括：人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索等等。 2、自然语言处理典型的应用包括：搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译等等。 3、社会网络分析典型的应用包括：用户画像、网络关联分析、欺诈作弊发现、热点发现等等。 4、推荐系统典型的应用包括：虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”等等。 数据挖掘流程 数据采集数据分类正例(positive example)反例(negative example)训练集(training set)验证集(validation set)：用作超参数验证测试集(test set)类别不平衡数据集（class-imbalanced data set） 采样方式1、分层采样(stratified sampling)保留类别比例的采样方式通常称为分层采样 2、留出法（hold-out）直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。 3、k折交叉验证（k-fold cross validation）交叉验证先将数据集D划分为k个大小相似的互斥子集，每个子集从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值。 4、自助法(bootstrapping)对数据集D有放回的随机采样m次后，一个样本不在样本集D1出现的概率： 当n足够大时，大约有36.8%的样本不会被采到，用没采到的部分做测试集，也是包外估计（out-of-bag-estimate）。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。 数据清洗1、缺失值处理1.直接删除——适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况。2.使用一个全局常量填充—-譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用3.使用均值或中位数代替——优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好。4.插补法 1）随机插补法——从总体中随机抽取某个样本代替缺失样本 2）多重插补法——通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理 3）热平台插补——指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。优点：简单易行，准去率较高缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补 4)拉格朗日差值法和牛顿插值法5.建模法可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。 使用sklearn进行插补：其实sklearn里也有一个工具Imputer可以对缺失值进行插补。Imputer类可以对缺失值进行均值插补、中位数插补或者某行/列出现的频率最高的值进行插补，也可以对不同的缺失值进行编码。并且支持稀疏矩阵。 在稀疏矩阵中，缺失值被编码为0存储为矩阵中，这种格式是适合于缺失值比非缺失值多得多的情况。此外，Imputer类也可以用于Pipeline中。 Imputor类的参数：_class _sklearn.preprocessing.``Imputer(_missing_values=’NaN’_, _strategy=’mean’_, _axis=0_, _verbose=0_, _copy=True_) missing_values : int或”NaN”,默认NaN（String类型）strategy : string, 默认为mean，可选则mean、median、most_frequentaxis :int, 默认为0（axis = 0，对列进行插值；axis= 1，对行进行插值）verbose : int, 默认为0copy : boolean, 默认为True True：会创建一个X的副本 False：在任何合适的地方都会进行插值。 但是以下四种情况，计算设置的copy = Fasle，也会创建一个副本： 1.X不是浮点型数组 2.X是稀疏矩阵，而且miss_value = 0 3.axis= 0，X被编码为CSR矩阵 4.axis= 1，X被编码为CSC矩阵 p.s.：LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。 2、异常点处理1.简单的统计分析 拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。2.3∂原则 如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。 3.箱型图分析 箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。 4.基于模型检测 首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象 优缺点：1.有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；2.对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。 5.基于距离 通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象 优缺点：1.简单；2.缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；3.该方法对参数的选择也是敏感的；4.不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。 6.基于密度 当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。 优缺点：1.给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；2.与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；3.参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。 7.基于聚类： 基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。 优缺点：1.基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；2.簇的定义通常是离群点的补，因此可能同时发现簇和离群点；3.产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；4.聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。 处理方法： 1.删除异常值——明显看出是异常且数量较少可以直接删除2.不处理—-如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。3.平均值替代——损失信息小，简单高效。4.视为缺失值——可以按照处理缺失值的方法来处理5.标准化——如果你的数据有离群点，对数据进行均差和方差的标准化效果并不好。这种情况你可以使用sklearn中的robust_scale和 RobustScaler 作为替代。 3、去重处理dataframe格式1、DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行2、drop_duplicates方法用于返回一个移除了重复行的DataFrame3、data.drop_duplicates([‘v1’]) #只判断某列 list格式1、使用set()2、{}.fromkeys().keys()3、set()+sort()4、排序后比较相邻2个元素的数据，重复的删除 4、噪音处理 噪音，是被测量变量的随机误差或方差。 噪音与离群点 离群点： 你正在从口袋的零钱包里面穷举里面的钱，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。这个100元就是个离群点，因为并不应该常出现在口袋里.. 噪声： 你晚上去三里屯喝的酩酊大醉，很需要买点东西清醒清醒，这时候你开始翻口袋的零钱包，嘛，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。但是你突然眼晕，把那三个一角看成了三个1元…这样错误的判断使得数据集中出现了噪声。 噪音处理方法1.分箱法分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。 用箱均值光滑：箱中每一个值被箱中的平均值替换。 用箱中位数平滑：箱中的每一个值被箱中的中位数替换。 用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。 一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用. 2. 回归法 可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。 5、其他实用小技巧1.去掉文件中多余的空行空行主要指的是（\n,\r,\r\n,\n\r等），在python中有个strip()的方法，该方法可以去掉字符串两端多余的“空白”，此处的空白主要包括空格，制表符(\t)，换行符。不过亲测以后发现，strip()可以匹配掉\n,\r\n,\n\r等，但是过滤不掉单独的\r。为了万无一失，我还是喜欢用麻烦的办法。 2.如何判断文件的编码格式 12import chardetif chardet.detect(data)[&apos;encoding&apos;] != &apos;utf-8&apos; 1批量处理编码格式转换的代码已上传到github上 数据转换1、离散型1、one-hot 编码编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。 sklearn.preprocessing.OneHotEncoder(_n_values=’auto’_, _categorical_features=’all’_, _dtype=_, _sparse=True_, _handle_unknown=’error’_) n_values : ‘auto’, int or array of ints 每个特征的数量 auto : 从训练数据的范围中得到 int : 所有特征的最大值（number） array : 每个特征的最大值（number） categorical_features: “all” or array of indices or mask :确定哪些特征是类别特征 all (默认): 所有特征都是类别特征，意味着所有特征都要进行OneHot编码 array of indices: 类别特征的数组索引 mask: n_features 长度的数组，切dtype = bool非类别型特征通常会放到矩阵的右边 dtype : number type, default=np.float输出数据的类型sparse : boolean, default=True设置True会返回稀疏矩阵，否则返回数组handle_unknown : str, ‘error’ or ‘ignore’当一个不明类别特征出现在变换中时，报错还是忽略 ２、Hash编码成词向量： 2、文本型１. 词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．２. 把词袋中的词扩充到n-gram：n-gram代表n个词的组合。比如“我喜欢你”、“你喜欢我”这两句话如果用词袋表示的话，分词后包含相同的三个词，组成一样的向量：“我 喜欢 你”。显然两句话不是同一个意思，用n-gram可以解决这个问题。如果用2-gram，那么“我喜欢你”的向量中会加上“我喜欢”和“喜欢你”，“你喜欢我”的向量中会加上“你喜欢”和“喜欢我”。这样就区分开来了。３. 使用TF-IDF特征：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)，IDF(t) = ln(总文档数/ 含t的文档数)，TF-IDF权重 = TF(t) * IDF(t)。自然语言处理中经常会用到。 3、数值型归一化（Normalization）123456X_train = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])min_max_scaler = preprocessing.MinMaxScaler()X_train_minmax = min_max_scaler.fit_transform(X_train)#将上述得到的scale参数应用至测试数据X_test = np.array([[ -3., -1., 4.]]) X_test_minmax = min_max_scaler.transform(X_test) 区间缩放（scaling） 1234567X_train = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])max_abs_scaler = preprocessing.MaxAbsScaler() X_train_maxabs = max_abs_scaler.fit_transform(X_train)X_test_maxabs = max_abs_scaler.transform(X_test)X_test_maxabs = max_abs_scaler.transform(X_test) 标准化（Standardization）适用情况看模型是否具有伸缩不变性。 不是所有的模型都一定需要标准化，有些模型对量纲不同的数据比较敏感，譬如SVM等。当各个维度进行不均匀伸缩后，最优解与原来不等价，这样的模型，除非原始数据的分布范围本来就不叫接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据主导。但是如果模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression等，对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，最好也进行数据标准化。 1234from sklearn import preprocessing import numpy as np X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]]) X_scaled = preprocessing.scale(X) 123scaler = preprocessing.StandardScaler().fit(X) #测试将该scaler用于输入数据，变换之后得到的结果同上scaler.transform(X) 二值化1.特征二值化特征二值化是把数值特征转化成布尔值的过程。这个方法对符合多变量伯努利分布的输入数据进行预测概率参数很有效。详细可以见这个例子sklearn.neural_network.BernoulliRBM. 对于 Normalizer，Binarizer工具类通常是在Pipeline阶段（sklearn.pipeline.Pipeline）的前期过程会用到。 4、比赛实际场景可见，选手需要进行制定规则、数据清洗、各个种类的特征处理等，对特征的研究是非常细化的。 特征衍生组合特征1. 拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。 user_id&amp;&amp;category: 10001&amp;&amp;女裙 10002&amp;&amp;男士牛仔 user_id&amp;&amp;style: 10001&amp;&amp;蕾丝 10002&amp;&amp;全棉 2. 模型特征组合： 用GBDT产出特征组合路径 组合特征和原始特征一起放进LR训练 生成多项式特征1234X = np.arange(9).reshape(3, 3)#只需要特征的交叉项，可以设置interaction_only=Truepoly = PolynomialFeatures(degree=3, interaction_only=True)poly.fit_transform(X) 此方法经常用于核方法中 自定义特征1.想用对数据取对数，可以自己用 FunctionTransformer自定义一个转化器,并且可以在Pipeline中使用` 1234from sklearn.preprocessing import FunctionTransformer transformer = FunctionTransformer(np.log1p)#括号内的就是自定义函数X = np.array([[0, 1], [2, 3]]) transformer.transform(X) 2.如果你在做一个分类任务时，发现第一主成分与这个不相关，你可以用FunctionTransformer把第一列除去，剩下的列用PCA 特征选择特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。 1 过滤型 方法： 评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。 评价方式：Pearson相关系数， 互信息， 距离相关度。 缺点：只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。 python包：SelectKBest指定过滤个数、SelectPercentile指定过滤百分比。 2 包裹型 方法：把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果。 典型算法：“递归特征删除算法”。 应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。 python包：RFE 3 嵌入型 方法：根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。 举例：最早在电商用LR做CTR预估， 在3-5亿维的系数特征上用L1正则化的LR模型。上一篇介绍了L1正则化有截断作用，剩余2-3千万的feature， 意味着其他的feature重要度不够。 python包：feature_selection.SelectFromModel选出权重不为0的特征。 特征降维 在数据处理中，经常会遇到特征维度比样本数量多得多的情况，如果拿到实际工程中去跑，效果不一定好。 一是因为冗余的特征会带来一些噪音，影响计算的结果； 二是因为无关的特征会加大计算量，耗费时间和资源。所以我们通常会对数据重新变换一下，再跑模型。数据变换的目的不仅仅是降维，还可以消除特征之间的相关性，并发现一些潜在的特征变量。 PCA 过程1.去掉数据的类别特征（label），将去掉后的d维数据作为样本2.计算d维的均值向量（即所有数据的每一维向量的均值）3.计算所有数据的散布矩阵（或者协方差矩阵）4.计算特征值（e1,e2,…,ed）以及相应的特征向量（lambda1,lambda2,…,lambda d）5.按照特征值的大小对特征向量降序排序，选择前k个最大的特征向量，组成dk维的矩阵W（其中每一列代表一个特征向量）6.运用dK的特征向量矩阵W将样本数据变换成新的子空间。（用数学式子表达就是，其中x是d1维的向量，代表一个样本，y是K1维的在新的子空间里的向量） python里有已经写好的模块，可以直接拿来用，但是我觉得不管什么模块，都要懂得它的原理是什么。matplotlib有matplotlib.mlab.PCA()，sklearn也有专门一个模块Dimensionality reduction专门讲PCA，包括传统的PCA，也就是我上文写的，以及增量PCA，核PCA等等，除了PCA以外，还有ZCA白化等等，在图像处理中也经常会用到。 推荐一个博客，动态展示了PCA的过程：http://setosa.io/ev/principal-component-analysis/ 写的也很清楚，可以看一下；再推荐一个维基百科的，讲的真的是详细啊https://en.wikipedia.org/wiki/Principal_component_analysis 线性回归模型线性回归的正则化Lasso回归 线性回归的L1正则化通常称为Lasso回归，α来调节损失函数的均方差项和正则化项的权重。 Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0，故具有特征选择的功能，增强了模型的泛化能力。 岭回归 线性回归的L2正则化通常称为Ridge回归。 Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。 Ridge回归的求解比较简单，一般用最小二乘法。 L1正则化产生稀疏的权值, 具有特征选择的作用；L2正则化产生平滑的权值。 最小二乘法的局限性 最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了 当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。 如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。 决策树决策树构建中的分裂准则决策树可以通过一系列规则递归地分割特征空间 信息增益（information gain）属性划分减少的信息熵，信息熵是度量样本集合纯度的一种指标，假设第k类样本所占比例为pk，则数据集D的信息熵为：Ent(D)=-∑pklogpk，Ent(D)越小，D的纯度越高。 Gain(D,a)=Ent(D)-∑(Dv/D*Ent(Dv))，Dv是某个属性a的某个可能取值的样本集合 增益率（gain ratio）信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好的不利影响，使用增益率选择最优划分属性，增益率定义为:Gain_ratio(D,a)=Gain(D,a)/IV(a), IV(a)=-∑(Dv/D*log(Dv/D))，IV(a)称为为a的固有值。属性可能取值数目越多，IV(a)的值越大，增益率即增益/固有值。 基尼指数(Gini index)基尼指数是另外一种数据的不纯度的度量方法，其定义如下： 其中的m仍然表示数据集D中类别C的个数，Pi表示D中任意一个记录属于Ci的概率，计算时Pi=(D中属于Ci类的集合的记录个数/|D|)。如果所有的记录都属于同一个类中，则P1=1，Gini(D)=0，此时不纯度最低。在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树，对每个属性都会枚举其属性的非空真子集，以属性R分裂后的基尼系数为： D1为D的一个非空真子集，D2为D1在D的补集，即D1+D2=D，对于属性R来说，有多个真子集，即GiniR(D)有多个值，但我们选取最小的那么值作为R的基尼指数。最后： 对于二类分类，基尼系数和熵之半的曲线如下： 从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代 常用决策树模型决策树模型总结 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 CART决策树属性分裂方法 m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。 对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。 决策树优化方法后剪枝（postpruning）先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点子树替换成叶节点能提升泛化性能，则进行替换，后剪枝训练时间开销大。 预剪枝（prepruning）在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶节点，预剪枝基于贪心存在欠拟合的风险。 抑制单颗决策树的复杂度的方法 限制树的最大深度 限制叶子节点的最少样本数量 限制节点分裂时的最少样本数量 吸收 bagging 的思想对训练样本采样，在学习单颗决策树时只使用一部分训练样本 借鉴随机森林的思路在学习单颗决策树时只采样一部分特征，在目标函数中添加正则项惩罚复杂的树结。 决策树算法的优点 基本不需要预处理，不需要提前归一化，处理缺失值。 使用决策树预测的代价是O(log2m)。 m为样本数。 既可以处理离散值也可以处理连续值。很多算法只是4.专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 对于异常点的容错能力好，健壮性高。 决策树算法的缺陷 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 梯度在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。 梯度下降法的超参数 步长（step size）学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。 梯度下降的算法调优 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 标准化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据标准化，这样特征的新期望为0，新方差为1，迭代次数可以大大加快。 梯度下降方法总结批梯度下降(batch gradient descent/BGD)求梯度的时候就用了所有m个样本的梯度数据。 随机梯度下降（stochastic gradient descent/SGD）随机梯度下降法由于每次仅仅采用一个样本来迭代。优点是速度快以及可以跳出局部最优解，缺点是导致迭代方向变化很大，不能很快的收敛到局部最优解。 小批量随机梯度下降（mini-batch stochastic gradient descent）小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。 梯度下降法与最小二乘法 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。 梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。 分类模型指标混淆矩阵（confusion matrix） 准确率（Accuracy）准确率是预测和标签一致的样本在所有样本中所占的比例 精确率（Precision）精确率是你预测为正类的数据中，有多少确实是正类 查全率（Recall）查全率是所有正类的数据中，你预测为正类的数据占比 不同的问题，判别标准不同。对于推荐系统，更侧重于查准率；对于医学诊断系统，更侧重于查全率。查准率和查全率是一个矛盾体，往往差准率高的情况查重率比较低。 F1 Score有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。 F-beta Score有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β&gt;1, 召回率有更大影响，如果β&lt;1,精确率有更大影响。 ROC （receiver operating characteristic curve）绘制方法：首先根据分类器的预测对样例进行排序，排在前面的是分类器被认为最可能为正例的样本。按照真例y方向走一个单位，遇到假例x方向走一个单位。ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。 AUC（Area Under the Curve）ROC曲线下的面积，AUC的取值范围一般在0.5和1之间。AUC越大代表分类器效果更好。 理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 模型选择与评估算法选择 偏差和方差偏差-方差分解（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。令：x为测试样本yD：x在数据集中的标记y：x的真实标记f(x;D):训练集D上学得的模型f在x上的预测输出f(x)=E[f(x;D)] :期望预测使用样本数相同的不同训练集产生的方差（variance）为：方差描述的是预测值作为随机变量的离散程度。度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。期望输出与真实标记的差别称为偏差（bias）,偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。偏差公式如下：噪声为：（表示若不为0，就是标记错了）噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。算法的期望泛化误差为：即泛化误差可分解为偏差、方差与噪声之和。 有一些算法天生是高方差的算法。如KNN、决策树。非参数学习通常是高方差算法，对数据较为敏感，因为不对数据进行任何假设。 有一些算法天生就是高偏差算法。如线性回归。参数学习通常是高偏差算法，因为对数据具有极强的假设。 机器学习的主要挑战来自于方差，解决高方差的通常手段有： 1.降低模型复杂度 2.减少数据维度；降噪 3.增加样本数 4.使用验证集 5.模型正则化 泛化能力、欠拟合和过拟合]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git学习]]></title>
    <url>%2F2018%2F04%2F08%2Fgit%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Git 简介Git是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件，是目前世界上最先进的分布式版本控制系统。主要优势有：公共服务器压力和数据量都不会太大，任意两个开发者之间可以很容易的解决冲突并且可以进行离线工作。 Git 术语 commit提交持有的库的当前状态，每个提交的对象有父commit对象的指针。从给定的commit可以遍历寻找父指针，查看历史记录的提交。 branches分支用来创建另一条线的发展，默认情况下，git的主分支，是master分支，和上线的版本是一样的，平时要工作的新功能创建一个分支，功能完成之后，它被合并回master分支，每当做出一个commit，HEAD更新为最新提交 tagsgit中的tag指向一次commit的id。通常用来给开发做版本号。 clone克隆操作不仅仅是检出的工作拷贝，也反映了完整的信息 pullpull操作是用于两个存储库实例之间的同步 push将本地仓库中的文件同步到远端库中 headHEAD指针总是指向分支的最新提交，每当你做了一个提交。HEAD更新为最新提交,HEAD树枝存储在.git/refs/heads/中 工作区：就是你在电脑里能看到的目录。 暂存区：英文叫stage, 或index。一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Bash 基本指令123456789101112pwd : 显示当前所在的目录路径。ls(ll): 都是列出当前目录中的所有文件，只不过ll(两个ll)列出的内容更为详细。touch : 新建一个文件 如 touch index.js 就会在当前目录下新建一个index.js文件。rm: 删除一个文件, rm index.js 就会把index.js文件删除。mkdir: 新建一个目录,就是新建一个文件夹。rm -r : 删除一个文件夹, rm -r src 删除src目录， 好像不能用通配符。mv 移动文件, mv index.html src index.html 是我们要移动的文件, src 是目标文件夹,当然, 这样写,必须保证文件和目标文件夹在同一目录下。reset 重新初始化终端/清屏。clear 清屏。history 查看命令历史。elp 帮助。exit 退出。 常用基础命令安装&amp;配置 官网下载安装完，右键看到Git Bash代表安装完成 初始配置（—local 项目级；—global 当前用户级；—system 系统级）git config --global user.name&quot;Your Name&quot;git config --global user.email&quot;email@example.com&quot; 查看配置 - git config -l 初始化&amp;克隆 本地初始化：git init 仓库目录下会多了一个.git隐藏文件夹。 克隆版本库：git clone &quot;url&quot; p.s. 版本控制系统可以告诉你每次的改动，比如在第x行加了代码。而图片、视频这些二进制文件没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。 管理分支 查看分支：git branch 创建分支：git branch branch_name 切换分支：git checkout branch_name 创建+切换分支：git checkout -b branch_name 合并某分支到当前分支：git merge branch_name 重命名分支：git branch -m branch_name branch_new_name //不会覆盖已经存在的分支 重命名分支：git branch -M branch_name branch_new_name //会覆盖已经存在的分支 删除分支：git branch -d branch_name 强制删除分支： git branch -D branch_name 删除远程分支： git push origin : branch_name //可以使用这种语法，推送一个空分支到远程分支，其实就相当于删除远程分支 查看&amp;修改 拉取代码：git pull orgin branch_name 查看更改：git status;git status -s//以简短格式输出 查看更改细节：git diff file_name//尚未缓存的改动git diff --cached//查看已缓存的改动 查看谁修改过代码：git blame filename 回到上次修改：git reset --hard 查看历史记录：git log；git log --pretty=oneline//将每次commit的记录打印成一行 查看git远程地址：git remote -v 删除：git rm //将文件从缓存区中移除 添加文件 添加单个文件：git add filename.js //该文件添加到缓存 添加所有js文件：git add *.js 添加所有文件：git add 提交文件 提交添加的文件：git commit -m &quot;your description about this branch&quot;//记录缓存区的快照。 提交单个文件：git commit -m &quot;your description about this branch&quot; filename.js 推送分支：git push orgin your_branch_name 备份当前分支内容：git stash 标签操作 创建标签：git tag 1.0.0 //标签无法重命名 显示标签列表：git tag 切出标签：git checkout 1.0.0 删除标签：git tag -d 1.0.0 流程化管理 从主分支分支拉一下代码git pull origin master 创建开发分支developgit co(checkout) -b develop 如果其他分支有需要处理的bug，先将当前状态保存一下git stash 切换到别的分支修改代码git checkout -b branch_name 修复bug后提交代码查看修改git status 需要查看修改的细节git diff file_name 没有问题就提交 123git add .git commit &quot;your description&quot;git push orgin your_branch_name 解决完bug切换到原来的分支git checkout -b you_old_branch 恢复刚刚保存的内容 1234git stash //备份当前的工作区的内容，保存到git栈git stash pop //从git栈中读取最近一次保存的内容，恢复工作区的相关内容，由于会存在多个stash内容，所以用栈来保存，pop出最近一个stash中读取的内容并恢复git stash list //显示git栈内所有的备份，可以利用这个列表来决定从哪个地方恢复git stash clear //清空git栈，此时使用git等图形化工具会发现，原来stash的那些节点都消失了 最后，提交三部曲 123git add .git commit &quot;your description&quot;git push orgin your_branch_name Github pagesGit初始设置123git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; hexo初始化设置12345678cd d:/hexonpm install hexo-cli -ghexo init foldercd foldernpm installhexo g 或者hexo generatehexo s 或者hexo s -p 5000 （ctrl+c退出）hexo d #部署到远程]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学之美学习笔记]]></title>
    <url>%2F2018%2F03%2F06%2F%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%2F</url>
    <content type="text"><![CDATA[技术分为术和道，具体的做事方法是术，做事的原理和原则是道。——吴军 自然语言处理，语音识别，机器翻译基于规则的语言处理早期学术界认为，要让机器完成翻译和语音识别这种人类才能做的事情，就必须先让计算机理解自然语言，而做到这点就要让机器有类似人类的智能。这个方法论被称为“鸟飞派”（通过观察鸟的飞行方式，采用仿生的思路造出飞机）。 那么怎么让机器理解自然语言呢？受传统语言学的影响，他们觉得要让机器做好两件事：分析句子语法和获取语义。分析句子语法就是按照语法把句子拆分，分清它的主语、谓语、宾语是什么，每个部分的词性是什么，用什么标点符号。而语义分析，就是弄清句子要表达的具体意思。语法规则很容易用计算机算法描述，这让人们觉得基于规则的方法是对的。但是这种方法很快就陷入困境，因为基于语法的分析器处理不了复杂句子，同时，词的多义性无法用规则表述，例如下面的例子： The pen is in the box. 和 The box is in the pen.第二句话让非英语母语的人很难理解，盒子怎么在钢笔里呢？其实在这里，pen是围栏的意思。这里pen是钢笔还是围栏，通过上下文已经不能解决，而需要常识，即钢笔可以放在盒子里，但是盒子比钢笔大，所以不能放在盒子里，于是pen在这里是围栏的意思，盒子可以放在围栏里。 基于统计的语言处理贾里尼克（Jelinek）把语音识别问题当作通信问题，并用两个隐含马尔可夫模型（声学和语言模型）概括了语音识别，推动了基于统计的语言处理方法。 在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子。早期的做法是判断给出的句子是否合乎语法，由前文可知这条路走不通。贾里尼克从另外角度看这个问题：通过计算一个句子出现的概率大小来判断它的合理性，于是语音识别问题转换成计算概率问题，根据这个思路，贾里尼克建立了统计语言模型。 假定S表示某一个有意义的句子，由一连串特定顺序排列的词w1,w2,w3…组成。我们想知道S在文本中出现的可能性，计算S的概率P(S)，根据条件概率公式： 其中P(w1)为w1出现的概率，P(w2|w1)为已知第一个词出现的条件下，第二个词出现的概率，以此类推。前面几个概率容易计算，但是后面的概率随着变量增多，变得不可计算。在这里需要应用马尔可夫假设来简化计算。马尔可夫假设假定当前状态只与前一个状态有关，即Wi出现的概率只同它前面的词有关Wi-1，于是上面的公式可以简化为： 接下来的问题是估算条件概率P(Wi|Wi-1)，由条件概率公式得： 而估计联合概率P(Wi-1, Wi)和P(Wi-1)可以统计语料库得到，通过计算(Wi-1, Wi)这对词在语料库中前后相邻出现的次数C，以及Wi-1单独出现的次数，就可得到这些词或者二元组的相对频度。根据大数定理，只要统计量足够，相对频度就等于概率，于是 于是复杂的语序合理性问题，变成了简单的次数统计问题。 上式对应的统计语言模型是二元模型，实际应用中，google翻译用到四元模型。 中文分词对于西方拼音语言来说，词之间有明确的分界符（空格），但是中、日、韩、泰等语言没有。因此，首先要对句子进行分词，才能做进一步自然语言处理。对一个句子正确的分词结果如下： 分词前：中国航天官员应邀到美国与太空总署官员开会。分词后：中国/航天/官员/应邀/到/美国/与/太空/总署/官员/开会/。 最容易想到的分词方法是“查字典”，即把一个句子从左到右扫描一遍，遇到字典里有的词就标出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字。这个方法能解决七八成的问题，但是遇到有二义性的分割就无能为力了，例如“发展中国家”，正确的分割是“发展-中-国家”，但是按照查字典法就会分成“发展-中国-家”。另外，并不是最长匹配都一定正确，例如“上海大学城书店”，正确的分割是“上海-大学城-书店”，而不是“上海大学-城-书店”。 按照前文的成功思路，依靠语法规则无法解决分词的二义性问题，还是得靠统计语言模型。 假设一个句子S有n种分词方法，利用前文的统计语言模型，分别计算出每种分词方法的概率，概率最大的即为最好的分词方法。因为穷举所有的分词方法计算量太大，所以可以把它看成是一个动态规划问题，并利用维特比算法快速找到最佳分词。具体应用时还要考虑分词的颗粒度。 拼音输入法拼音输入法中的数学中文输入法经历了以自然音节编码输入，到偏旁笔画拆字输入，再回归自然音节输入的过程。输入法输入汉字的快慢取决于对汉字编码的平均长度，也就是击键次数乘以寻找这个键需要的时间。单纯地减少编码长度未必能提高输入速度，因为寻找一个键的时间会增长。 将汉字输入到计算机中，是将人能看懂的信息编码变成计算机约定的编码（Unicode或UTF-8）的过程。对汉字的编码分为两部分：对拼音的编码和消除（一音多字）歧义。键盘上可使用的是26个字母和10个数字键，最直接的方式是让26个字母对应拼音，用10个数字消除歧义性。只有当两个编码都缩短时，汉字的输入才能够变快。早期的输入法常常只注重第一部分而忽略第二部分，例如双拼输入法和五笔输入法。 每一个拼音对应多个汉字，把一个拼音串对应的汉字由左向右连起来，就是一张有向图，如下图所示，y1,y2,y3…是输入的拼音串，W11,W12,W13是第一个音的候选汉字（后面的文字描述用W1代替），以此类推。从第一个字到最后一个字可以组成很多句子，每个句子对应图中的一条路径。 拼音输入法就是要根据上下文在给定的拼音条件下找到最优的句子，即求 （Arg是argument的缩写，Arg Max为获得最大值的信息串）化简这个概率需要用到隐含马尔可夫模型（见2.2介绍），我们把拼音串看成能观察到的“显状态”，候选汉字看成“隐状态”，然后求在这个“显状态”下的“隐状态”概率。带入下文中的隐含马尔可夫模型公式（2.3），式（2.1）化简为： 化简连乘， 需要将等式两边取对数得 乘法变成了加法。我们定义两个词之间的距离 这样，寻找最大概率问题变成了寻找最短路径问题。 隐含马尔可夫模型上文介绍过马尔可夫假设（研究随机过程中的一个假设），即在随机状态序列中，假设其中的一个状态只于前一个状态有关。如天气预报，假设今天的天气只与昨天有关，这样就能得到近似解： 马尔可夫链 符合这个假设的随机过程称为马尔可夫过程，也叫马尔可夫链。隐含马尔可夫模型是马尔可夫链的一个扩展：任意时刻t的状态St是不可见的，但在每个时刻会输出Ot， Ot仅和St相关，这叫独立输出假设，数学公式如下： P(Ot|St)我们可以通过观察得到。 隐马尔可夫模型 解决问题通常是通过已知求未知，我们要通过观察到$o_t$求出$s_t$的概率，即求 由条件概率公式可得： 因为观察到的状态O一旦产生就不会变了，所以它是一个可忽略的常数，上式可以化简为 因为 式(2.2)可以化简为 信息论：信息的度量和作用信息熵香农在他的论文“通信的数学原理”[想到牛顿的“自然哲学与数学原理”]，提出了信息熵（shang），把信息和数字联系起来，解决了信息的度量，并量化出信息的作用。 一条信息的信息量和它的不确定性正相关，信息熵约等于不确定性的多少。香农给出的信息熵公式为 P(x)为x的概率分布。 信息熵的公式为什么取负数？因为概率小于1，小数求得的对数是负数，给整个公式加上负号，最终的结果为正。 下面举例说明信息熵公式为什么会用到log和概率。 猜中世界杯冠军需要多少次？足球世界杯共32个球队，给他们编号1-32号，第一次猜冠军是否在1-16号之中，如果对了就会接着猜是否在1-8号，如果错了就知道冠军在9-16号，第三次猜是否在9-12号，这样只需要5次就能猜中，log32 = 5。这里采用的是折半查找，所以取对数。 但实际情况不需要猜5次，因为球队有强弱，可以先把夺冠热门分一组，剩下的分一组，问冠军是否在热门组中，再继续这个过程，按照夺冠概率对剩下的球队分组。引入概率就会让查找数更少，也就是不确定性更小，信息熵更小。可以计算，当每支球队夺冠概率相等时（1/32），信息熵的结果为5。 条件墒：假定X和Y是两个随机变量，X是我们要了解的，已知X的随机分布P(X)，于是X的熵为： 假定我们还知道Y的一些情况，包括它和X一起出现的概率，即联合概率分布，以及在Y取不同值前提下X的概率分布，即条件概率分布，于是在Y条件下X的条件熵为： 可证明H(X|Y) &lt;H(X), 即引入相关信息后，不确定性下降了。 互信息信息之间的相关性如果度量呢？ 香农提出了用互信息度量两个随机事件的相关性。例如，“好闷热”和“要下雨了”的互信息很高。X与Y的互信息公式如下： 经过演算，可得到 只要有足够的语料库，P(x,y), P(x) 和P(y)是很容易计算的。 机器翻译中最难的两个问题之一是二义性，如Bush 既可以是总统布什，也可以是灌木丛，Kerry既可以是国务卿克里，也可以是小母牛。如何正确的翻译？一种思路是通过语法辨别，但效果不好； 另一种思路是用互信息，从大量文本中找出和总统布什一起出现的词语，如总统、美国、国会等，再用同样的方法找出和灌木丛一起出现的词，如土壤、植物等，有了这两组词，在翻译Bush时，看看上下文中哪类词更多就可以了。 相对熵/交叉熵相对熵（KL Divergence），衡量两个取值为正的函数的相似性: 结论： 两个完全相等的函数，相对熵为零； 相对熵越大，两个函数差异越大。 对于概率分布函数，或者概率密度函数，相对熵可以度量两个随机分布的差异性。 在自然语言处理中，常用相对熵计算两个常用词在不同文本中的概率分布，看他们是否同义；或者根据两篇文章中不同词的分布，衡量它们的内容是否相等。利用相对熵，可以得到信息检索中最重要的概念：词频率-逆向文档频率（TF-IDF），在后面的搜索章节会对它详细介绍。 搜索获取网页：网络爬虫把整个互联网看作一张大图，每个网页就是图中的一个节点，超链接是连接节点的弧。通过网络爬虫，用图的遍历算法，就能自动地访问到每个网页并把它们存起来。 网络爬虫是这样工作：假定从一家门户网站的首页出发，先下载这个网页，再通过这个网页分析出里面包含的所有超链接，接下来访问并下载这些超链接指向的网页。让计算机不同地做下去，就能下载整个互联网。 还需要用一个记事本（哈希表）记录下载了哪些网页避免重复下载。 工程实现问题： 遍历算法采用广度优先还是深度优先？搜索引擎要做到在有限的时间内，最多地爬下最重要的网页。显然各个网站最重要的是它的首页，那么就应该先下载所有网站的首页。如果把爬虫再扩大一点，就要继续下载首页直接链接的网页，因为这些网页是网站设计者自己认为相当重要的网页。在这个前提下，似乎应该采用广度优先。 但是还要考虑网络通信的“握手”问题。网络爬虫每次访问网站服务器时，都要通过“握手”建立连接（TCP协议），如果采用广度优先，每个网站先轮流下载所有首页，再回过头来下载第二级网页，这样就要频繁的访问网站，增加“握手”耗时。 实际的网络爬虫是由成百上千台服务器组成的分布式系统，由调度系统决定网页下载的顺序，对于某个网站，一般是由特定的一台或几台服务器专门下载，这些服务器先下载完一个网站再进入下一个网站，这样可以减少握手次数（深度优先）。具体到每个网站，采用广度优先，先下载首页，再下载首页直接链接的网页。 页面分析和超链接（URL）提取早期的网页都是直接用HTML书写，URL以文本的形式放在网页中，前后有明显标识，很容易提取出来。但现在很多网页都是用脚本语言（如JavaScript）生成，URL不是直接可见的文本，所以网络爬虫要模拟浏览器运行网页后才能得到隐含的URL，但很多网页的脚本写的不规范，很难解析，这就导致这样的网页无法被搜索引擎收录。 维护超链接哈希表在一台服务器上建立和维护一张哈希表并不是难事，但如果同时有成千上万台服务器一起下载网页，维护一张统一的哈希表就会遇到很多问题： 首先，这张哈希表会大到存不下来；其次，每台服务器下载前和下载后都要访问哈希表，于是哈希表服务器的通信就成了整个爬虫系统的瓶颈。解决办法是：明确分工，将某个区间的URL分给特定的几台服务器，避免所有服务器对同一个URL做判断；批量询问哈希表，减少通信次数，每次更新一大批哈希表的内容。 网页检索：布尔代数最简单的索引结构是用一个很长的二进制数表示一个关键字是否在每个网页中，有多少个网页就有多少位数，每一位对应一个网页，1代表相应的网页有这个关键字，0代表没有。比如关键字“原子能”对应的二进制数是0100 1000 1100 0001…表示（从左到右）第二、第五、第九、第十、第十六个网页包含这个关键字。假定关键字“应用”对应的二进制数是0010 1001 1000 0001…，那么要找到同时包含“原子能”和“应用”的网页时，只需要将这两个二进制数进行布尔AND运算，结果是0000 1000 0000 0001…表示第五和第十六个网页满足要求。 这个二进制数非常长，但是计算机做布尔运算非常快，现在最便宜的微机，在一个指令周期进行32位布尔运算，一秒钟十亿次以上。 为了保证对任何搜索都能提供相关网页，主要的搜索引擎都是对所有词进行索引，假如互联网上有100亿个有意义的网页，词汇表大小是30万，那么这个索引至少是100亿x30万=3000万亿。考虑到大多数的词只出现在一部分文本中，压缩比是100：1，也是30万亿的量级。为了网页排名方便，索引中还要存其他附加信息，如每个词出现的位置，次数等等。因此整个索引就变得非常大，需要通过分布式存储到不同服务器上（根据网页编号划分为很多小块，根据网页重要性建立重要索引和非重要索引）。 度量网页和查询的相关性：TF-IDF我们以查找包含“原子能的应用”网页举例，“原子能的应用”可以分成三个关键词：原子能、的、应用。凭直觉，我们认为包含这三个关键词较多的网页，比包含它们较少的网页相关。但这并不可取，因为这样的话，内容长的网页比内容短的网页占便宜，所以要根据网页长度对关键词的次数进行归一化，用关键词的次数，除以网页的总字数，这个商叫做“关键词的频率”或“单文本频率”（TF：Term Frequency）。比如，某个网页上有1000词，其中“原子能”“的”“应用”分别出现了2次、35次、5次，那么它们的词频就是0.002、0.035、0.005，将这三个数相加就是相应网页和查询“原子能的应用”的单文本频率。所以，度量网页和查询的相关性，一个简单的方法就是直接使用各个关键词在网页中出现的总频率。 但是这也有不准确的地方，例如上面的例子中，“的”占了总词频的80%以上，但是它对确定网页的主题几乎没什么用，我们叫这样的词为停止词（stop word），类似的还有“是”“和”等。 另外“应用”是很普通的词，而“原子能”是专业词，后者在相关性排名中比前者重要。因此需要给每个词给一个权重，权重的设定满足两个条件： 一个词预测主题的能力越强，权重就越大； 停止词权重为零。 在信息检索中，使用最多的是“逆文本频率指数”（IDF：Inverse Document Frequency），公式为 （D是全部网页数，Dw为关键词w出现的网页个数）。最终确定查询相关性，是利用TF和IDF的加权求和。 （IDF其实是在特定条件下关键词概率分布的交叉熵） 搜索结果页排序：Page Rank算法这是拉里·佩奇和谢尔盖·布林发明的计算网页自身质量的数学模型，google凭借该算法，使搜索的相关性有了质的飞跃，圆满解决了以往搜索页中排序不好的问题。该算法的核心思想为：如果一个网页被很多其他网页所链接，说明它收到普遍的承认和信赖，那么它的排名就高。当然，在具体应用中还要加上权重，给排名高的网页链接更高的权重。这里有一个怪圈，计算搜索结果网页排名过程中需要用到网页本身的排名，这不是“先有鸡还是先有蛋的问题”吗？ 谢尔盖·布林解决了这个问题，他把这个问题变成了一个二维矩阵问题，先假定所有网页排名相同（1/N），在根据这个初始值不断迭代排名，最后能收敛到真实排名。 新闻分类：余弦定理google有新闻频道，里面的内容是由计算机聚合、整理并分类各网站内容。以前门户网站的内容是由编辑在读懂之后，再根据主题分类。但是计算机根本读不懂新闻，它只会计算，所以要让计算机分类新闻，首先就要把文字变成可计算的数字，再设计一个算法来计算任意两篇新闻的相似性。 计算一篇新闻中所有实词的TF-IDF值，再把这些值按照对应的实词在词汇表的位置依次排列，就得到一个向量。例如词汇表中有64000个词，其编号和词如左下表所示，在某一篇新闻中，这64000个词的TF-IDF值如右下表所示，这64000个数就组成了一个64000维的向量，我们就用这个向量代表这篇新闻，成为这篇新闻的特征向量。每篇新闻都有一个特征向量，向量中的每个数代表对应的词对这篇新闻主题的贡献。 同一类的新闻，一定某些主题词用的较多，两篇相似的新闻，它们的特征向量一定在某几个纬度的值比较大。如果两个向量的方向一致，就说明新闻的用词比例基本一致，我们采用余弦定理计算两个向量间的夹角： 新闻分类算法分为有目标和无目标：第一种是已知一些新闻类别的特征向量，拿它分别和所有待分类的新闻计算余弦相似性，并分到对应的类别中，这些已知的新闻类别特征向量既可以手工建立，也可以自动建立； 第二种是没有分好类的特征向量做参考，它采用自底向上的聚类方法，计算所有新闻两两之间的余弦相似性，把相似性大于一个阈值的新闻分作一个小类，再比较各小类之间的余弦相似性，就这样不断待在聚合，一直到某一类因为太大而导致里面的新闻相似性很小时停止。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>统计算法</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优雅高效地使用windows]]></title>
    <url>%2F2018%2F02%2F26%2F%E4%BC%98%E9%9B%85%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8windows%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器！ 【日常工具】文件搜索 Listary：Windows文件浏览增强工具，double-ctrl调用，并且可以在(Xftp，clover 等等)窗口中使用，极其方便！ Wox：免费开源的效率启动器,不仅可以搜索文件还可以浏览网页，Alt+Space调用，以及定制的插件(wox-plugin)，堪比MAC上的 Alfred 视频播放 Potplayer：拥有强大的内置解码器，不用额外针对某类视频去下载了 下载神器 IDM(cracked version)： 全宇宙最快的下载器!唯一缺陷是不支持P2P，想下载磁力或者BT可以先用百度云网盘的离线功能，再通过油猴脚本抓取链接进行下载。 EagleGet：下载后自动安装Chrome扩展探测视频，缺陷也是不支持BT/磁力链接，方法同上 硕鼠：主要是下载网站的视频，不过现在不支持像腾讯视频之类的大网站了 PDF阅读 福昕阅读器：功能算比较齐全了(会占用端口4000) ABBYY_FineReader：PDF转WORD 写作工具 snipaste：开源、免费的国产截图神器。比QQ截图工具清晰很多。 Yu writer：windows上好用的markdown工具！ Ditto：剪切板工具，保存所有复制过的文字和图片，用ctrl+`调用 Pasteasy：全平台跨设备复制粘贴 素材库 OfficePLUS：微软Office官方在线模板网站！ Iconfont：阿里巴巴矢量图标库 Free Images - Pixabay icons8：icon素材库 NASA IMAGE：NASA素材库 pixabay:高清免费图片素材库 PPT制作 Nordri Tools：超级好用的ppt插件 Photozoom pro：利用插值算法提高图片分辨率 PPT遥控器：代替遥控笔 FILEminimizer：ppt压缩神器 Screen to Gif: Gif制作软件 Tagul：文字云生成器 思维导图 Xmind：付费，全平台，模板多，支持鱼骨图、二维图、树形图等格式，可以与Evernote同步 幕布：笔记一键生成思维导图 视频录制 OBS Studio：功能齐全的视频录制工具，直播必备 Adobe Premiere Pro CC：视频剪辑工具 文件整理 Q-dir：需要在文件夹之间移动文件的时候，这个整理神器就能派上用场了！ Clover 3：为资源管理器添加多标签页功能，可以将常用文件夹添加为书签 Goodsyne：强大的数据同步工具 bandzip：win10下好用的压缩软件 快速启动 TrueLaunchBar：对快速启动项进行分组；允许你把任何文件夹组织成菜单的形式；实现剪切板管理、性能监视等功能 Wgestures：全局鼠标手势！ 【系统开发与优化工具】桌面优化 Fences：付费,桌面文件分类整理软件 Wallpapaer：动态壁纸软件，装逼神器！ 屏保 Fliqlo：数字时钟的屏幕保护，逼格满满 Flux： 视力保护，通过根据时间调节屏幕颜色，减少蓝光对视力的影响 系统管理 PowerTool：查看系统进程等信息，安全修复！ Dism++：简洁的系统管理软件，集成了很多小工具，还可以系统备份 Ccleaner：系统清理工具 文件修改 Bulk Rename Utility：批量重命名工具 remove empty directories：删除空文件夹，强迫症的福音 系统安装 清华大学开源软件镜像站：可以下载到Linux镜像文件以及python第三方库文件等，速度很快！ Ultraiso：制作启动盘 网络 Fiddler：抓包工具 文本编辑器 Sublime Text3：文本神器 Atom：中文友好，渲染插件多 IDE Pycharm：社区版免费，Python开发必备 Anaconda：集成了python科学计算的第三方库，内置spyder和jupyter notebook IntelliJ IDEA：前端必备IDE Cmder：monokai配色主题，完美代替原生cmd 【chrome插件】开发必备 Vimium(键盘浏览插件) JSONView(json数据进行转码和格式化) Proxy SwitchyOmega (代理) Qiniu upload files (七牛图床插件) Markdown Here (转化为markdown格式） The QR Code Extension (二维码生成器) 日常管理 Extensity (扩展管理工具) LastPass (密码管理器) 浏览优化 书签侧边栏 Imtranslator（翻译） Imagus (悬停放大图片) OneTab (内存优化神器) Better History (查看历史记录) Sexy Undo Close Tab (恢复关闭网页) Infinity (方便的新标签页定制) CrxMouse Chrome Gestures (鼠标手势、超级拖拽) Tampermonkey (油猴：脚本管理平台，神器！！) IE Tab (打开用IE内核支持的网页，常用于银行支付环境) Listen 1 (集成各大平台的音乐，再也不用为音乐版权的问题头疼了) 下载&amp;收藏 印象笔记裁剪 网页截图：注释&amp;录屏 RSS Subscription Extension Eagleget Free Download]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text3 快捷键]]></title>
    <url>%2F2018%2F02%2F22%2Fsublime%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[选择类Ctrl + D 选择单文本 Alt + F3 选中文件所有相同文本 Ctrl + L 选中整行 Ctrl + shift + M 选中括号中文本（与搜狗有热键冲突） Ctrl + M 光标移动结束或开始位置 Ctrl + Enter 在下一行插入新行 Ctrl + Shift + Enter 在上一行插入新行 Ctrl + Shift + [ 选中，折叠代码 Ctrl + Shift + ] 选中，展开代码 Ctrl + K + 0 展开所有折叠代码 Ctrl + ←/→ 快速移动光标 shift + ↑/↓ 向上/向下选中多行 Shift + ←/→ 向左/向右选中文本 Ctrl + Shift + ←/→ 向左/向右快速选择文本 编辑类Ctrl + J 合并多行代码为一行 Ctrl + Shift + D 复制整行，插入到下一行 Tab 向右缩进 &amp; Shift + Tab 向左缩进 Ctrl + K + K 从光标处开始删除代码至行尾。 Ctrl + Shift + K 删除整行。 Ctrl + / 注释单行。 Ctrl + Shift + / 注释多行。 Ctrl + K + U/L 转换大/小写。 Ctrl + Z 撤销 Ctrl + Y 恢复撤销 Ctrl + F2 设置书签 Ctrl + T 左右字母互换。 F6 单词检测拼写 搜索类Ctrl + F 文件内搜索 Ctrl + shift + F 文件夹内搜索 Ctrl + P 按类别搜索。举个栗子：1、输入当前项目中的文件名；快速搜索文件，2、输入@和关键字，查找文件中函数名；3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。 Ctrl + G 数字定位搜索 Ctrl + R 函数定位搜索 Ctrl + ： 变量、属性名定位搜索 Ctrl + Shift + P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 显示类Ctrl + Tab 按浏览顺序切换窗口 Ctrl + PageDown 向左切换当前窗口的标签页 Ctrl + PageUp 向右切换当前窗口的标签页。 Alt + Shift + “1/2/3” 分屏 Ctrl + K + B 开启/关闭侧边栏。 F11 全屏模式 Shift + F11 免打扰模式]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>网页配置</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
</search>
